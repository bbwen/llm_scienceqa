{"question_id": "f6346828c2f44529dc307abf04dd246bfeb4a9b2", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: An LM's overall performance on *X can be measured simply by taking the proportion of correct predictions across the 67,000 minimal pairs from all paradigms. GPT-2 achieves the highest score and the $n$-gram the lowest. Transformer-XL and the LSTM LM perform in the middle, and at roughly the same level as each other. All models perform well below estimated human agreement (as described in Section SECREF11). The $n$-gram model's poor overall performance confirms *X is not solvable from co-occurrence information alone. Rather, success at *X is driven by the more abstract features learned by neural networks. There are no categories in which the $n$-gram approaches human performance.\n\nQuestion: Which of the model yields the best performance?\n\nExplanation:  The GPT-2 model yields the best performance according to [Document 1].\n\nAnswer: GPT-2\n\nExample 2:\n\nQuestion: Do they evaluate existing methods in terms of desired properties?\n\nExplanation: \n\nAccording to [Document 1], the authors \"compare the performance of popular named entity recognition algorithms on our data.\" This indicates that they are evaluating existing methods. Furthermore, [Document 1] states that the authors \"trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15.\" This indicates that the authors are evaluating the methods in terms of their performance.\n\nAnswer: Yes\n\nExample 3:\n\n[Document 1]: As previous systems collect relevant data from knowledge bases after observing questions during evaluation BIBREF24 , BIBREF25 , we also explore using this option. Namely, we build a customized text corpus based on questions in commonsense reasoning tasks. It is important to note that this does not include the answers and therefore does not provide supervision to our resolvers. In particular, we aggregate documents from the CommonCrawl dataset that has the most overlapping n-grams with the questions. The score for each document is a weighted sum of $F_1(n)$ scores when counting overlapping n-grams: $Similarity\\_Score_{document} = \\frac{\\sum _{n=1}^4nF_1(n)}{\\sum _{n=1}^4n}$\n\n[Document 2]: The top 0.1% of highest ranked documents is chosen as our new training corpus. Details of the ranking is shown in Figure 2 . This procedure resulted in nearly 1,000,000 documents, with the highest ranking document having a score of $8\\times 10^{-2}$ , still relatively small to a perfect score of $1.0$ . We name this dataset STORIES since most of the constituent documents take the form of a story with long chain of coherent events.\n\n[Document 3]: Figure 5 -left and middle show that STORIES always yield the highest accuracy for both types of input processing. We next rank the text corpora based on ensemble performance for more reliable results. Namely, we compare the previous ensemble of 10 models against the same set of models trained on each single text corpus. This time, the original ensemble trained on a diverse set of text corpora outperforms all other single-corpus ensembles including STORIES. This highlights the important role of diversity in training data for commonsense reasoning accuracy of the final system.\n\nQuestion: Which of their training domains improves performance the most?\n\nExplanation: \n\nAccording to [Document 1], the documents from the CommonCrawl dataset that have the most overlapping n-grams with the question are used to build a customized text corpus. This information is further supported by [Document 2] and [Document 3].\n\nAnswer: documents from the CommonCrawl dataset that has the most overlapping n-grams with the question\n\nExample 4:\n\n[Document 1]: In this paper, we demonstrate that a combination of matrix factorization and weight pruning is better than single solutions for BERT-oriented model compression. Similar phenomena has been reported in the computer vision scenarios BIBREF28, which shows that low-rank and sparsity are complementary to each other. Here we provide another explanation to support this observation.\n\nQuestion: Do they evaluate which compression method yields the most gains?\n\nExplanation:"}
{"question_id": "c5221bb28e58a4f13cf2eccce0e1b1bec7dd3c13", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: What domains are present in the data?\n\nExplanation:  The domains are listed in [Document 1].\n\nAnswer: Alarm, Banks, Buses, Calendar, Events, Flights, Homes, Hotels, Media, Messaging, Movies, Music, Payment, Rental Cars, Restaurants, Ride Sharing, Services, Train, Travel, Weather\n\nExample 2:\n\n[Document 1]: We perform our experiments using ActivityNet Captions dataset BIBREF2 that is considered as the standard benchmark for dense video captioning task. The dataset contains approximately 20k videos from YouTube and split into 50/25/25 % parts for training, validation, and testing, respectively. Each video, on average, contains 3.65 temporally localized captions, around 13.65 words each, and two minutes long. In addition, each video in the validation set is annotated twice by different annotators. We report all results using the validation set (no ground truth is provided for the test set).\n\nQuestion: What domain does the dataset fall into?\n\nExplanation:  The dataset contains videos from YouTube, so the domain is YouTube videos.\n\nAnswer: YouTube videos\n\nExample 3:\n\nQuestion: How big is dataset domain-specific embedding are trained on?\n\nExplanation:  There is not enough information in the documents to answer the question.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: The Semantic Scholar corpus BIBREF21 (SemScholar) is a large dataset of scientific publications made available by AI2. From its 39M articles, we downloaded 3,3M PDFs (the rest were behind paywalls, did not have a link or it was broken) and extracted 12.5M figures and captions through PDFFigures2 BIBREF22. We randomly selected 500K papers to train the FCC task on their figures and captions and another 500K to train Vecsigrafo on the text of their titles and abstracts.\n\n[Document 2]: Springer Nature's SciGraph contains 7M scientific publications organized in 22 scientific fields or categories. Since SciGraph does not provide a link to the PDF of the publication, we selected the intersection with SemScholar, producing a smaller corpus of 80K papers (in addition to the 1M papers from SemScholar mentioned above) and 82K figures that we used for training certain FCC configurations and supervised baselines (section SECREF14).\n\n[Document 3]: The Textbook Question Answering corpus BIBREF23 includes 1,076 lessons and 26,260 multi-modal test questions from middle school science curricula. Its complexity and scope make it a challenging textual and visual question answering dataset.\n\n[Document 4]: Wikipedia. We used the January 2018 English Wikipedia dataset as one of the corpora on which to train Vecsigrafo. As opposed to SciGraph or SemScholar, specific of the scientific domain, Wikipedia is a source of general-purpose information.\n\nQuestion: Is the data specific to a domain?\n\nExplanation:"}
{"question_id": "10edfb9428b8a4652274c13962917662fdf84f8a", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We focus primarily on the smaller data set of 645 hand-labeled articles provided to task participants, both for training and for validation. We take the first 80% of this data set for our training set and the last 20% for the validation set. Since the test set is also hand-labeled we found that the 645 articles are much more representative of the final test set than the articles labeled by publisher. The model's performance on articles labeled by publisher was not much above chance level.\n\n[Document 2]: Our first experiment was checking the importance of pre-training. We pre-trained BERT-base on the 600,000 articles without labels by using the same Cloze task BIBREF5 that BERT had originally used for pre-training. We then trained the model on sequence lengths of 100, 250 and 500. The accuracy for each sequence length after 100 epochs is shown in TABREF7 and is labeled as UP (unsupervised pre-training). The other column shows how well BERT-base trained without pre-training. We found improvements for lower sequence lengths, but not at 500 word pieces. Since the longer chunk should have been more informative, and since our hand-labeled training set only contained 516 articles, this likely indicates that BERT experiences training difficulty when dealing with long sequences on such a small dataset. As the cost to do pre-training was only a one time cost all of our remaining experiments use a pre-trained model.\n\nQuestion: How long is the dataset?\n\nExplanation:  [Document 1] states that the dataset contains 645 articles. [Document 2] states that the dataset contains 600,000 articles.\n\nAnswer: 645, 600000\n\nExample 2:\n\n[Document 1]: Data set. For the cross-domain polarity classification experiments, we use the second version of Multi-Domain Sentiment Dataset BIBREF0 . The data set contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). Reviews contain star ratings (from 1 to 5) which are converted into binary labels as follows: reviews rated with more than 3 stars are labeled as positive, and those with less than 3 stars as negative. In each domain, there are 1000 positive and 1000 negative reviews.\n\nQuestion: How long is the dataset?\n\nExplanation:  The dataset contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). In each domain, there are 1000 positive and 1000 negative reviews. This means that there are a total of 8000 reviews in the dataset.\n\nAnswer: 8000\n\nExample 3:\n\n[Document 1]: The corpus includes 2000 sentences. Theses sentence are random renderings of 200 sentences, which we have taken from Sorani Kurdish books of the grades one to three of the primary school in the Kurdistan Region of Iraq. The reason that we have taken only 200 sentences is to have a smaller dictionary and also to increase the repetition of each word in the narrated speech. We transformed the corpus sentences, which are in Persian-Arabic script, into the format which complies with the suggested phones for the related Sorani letters (see Section SECREF6).\n\nQuestion: How long is the dataset?\n\nExplanation:  The dataset consists of 2000 sentences, as stated in [Document 1].\n\nAnswer: 2000\n\nExample 4:\n\n[Document 1]: For training our classification models, we use a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, provided by BIBREF23. These repositories are split into training, validation, and test splits containing 808, 265, and 264 commits, respectively. In order to minimize the occurrence of duplicate commits in two of these splits (such as in both training and test), commits from no repository belong to more than one split. However, 808 commits may not be sufficient to train deep learning models. Hence, in order to answer RQ4, we augment the training split with commits mined using regular expression matching on the commit messages from the same set of open-source Java projects. This almost doubles the number of commits in the training split to 1493. We then repeat our experiments for the first three research questions on the augmented dataset, and evaluate our trained models on the same validation and test splits.\n\nQuestion: How long is the dataset?\n\nExplanation:"}
{"question_id": "389cc454ac97609e9d0f2b2fe70bf43218dd8ba7", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: WikiTableQuestions BIBREF1 is a question answering dataset over semi-structured tables. It is comprised of question-answer pairs on HTML tables, and was constructed by selecting data tables from Wikipedia that contained at least 8 rows and 5 columns. Amazon Mechanical Turk workers were then tasked with writing trivia questions about each table. In contrast to common NLIDB benchmarks BIBREF2 , BIBREF0 , BIBREF15 , WikiTableQuestions contains 22,033 questions and is an order of magnitude larger than previous state-of-the-art datasets. Its questions were not designed by predefined templates but were hand crafted by users, demonstrating high linguistic variance. Compared to previous datasets on knowledge bases it covers nearly 4,000 unique column headers, containing far more relations than closed domain datasets BIBREF15 , BIBREF2 and datasets for querying knowledge bases BIBREF16 . Its questions cover a wide range of domains, requiring operations such as table lookup, aggregation, superlatives (argmax, argmin), arithmetic operations, joins and unions. The complexity of its questions can be shown in Tables TABREF6 and TABREF66 .\n\nQuestion: How do they gather data for the query explanation problem?\n\nExplanation:  [Document 1] states that the questions were \"hand crafted by users\".\n\nAnswer: hand crafted by users\n\nExample 2:\n\n[Document 1]: We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small. Basic summary and examples of the data from the dataset are below:\n\nQuestion: How is the dataset collected?\n\nExplanation: \n\nAccording to [Document 1], the dataset was collected from a study conducted by Davidson et al and HEOT obtained from one of the past studies done by Mathur et al.\n\nAnswer: A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al, HEOT obtained from one of the past studies done by Mathur et al\n\nExample 3:\n\n[Document 1]: Machine-machine Interaction A related line of work explores simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers BIBREF1. Such a framework may be cost-effective and error-resistant since the underlying crowd worker task is simpler, and semantic annotations are obtained automatically.\n\n[Document 2]: It is often argued that simulation-based data collection does not yield natural dialogues or sufficient coverage, when compared to other approaches such as Wizard-of-Oz. We argue that simulation-based collection is a better alternative for collecting datasets like this owing to the factors below.\n\nQuestion: How did they gather the data?\n\nExplanation: \n\nAccording to [Document 1], the data was gathered using simulation-based dialogue generation. This information is further supported by [Document 2].\n\nAnswer: simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers \n\nExample 4:\n\n[Document 1]: In order to advance query-focused summarization with limited data, we improve the summarization model with data augmentation. Specifically, we transform Wikipedia into a large-scale query-focused summarization dataset (named as WikiRef). To automatically construct query-focused summarization examples using Wikipedia, the statements' citations in Wikipedia articles as pivots to align the queries and documents. Figure FIGREF1 shows an example that is constructed by the proposed method. We first take the highlighted statement as the summary. Its supporting citation is expected to provide an adequate context to derive the statement, thus can serve as the source document. On the other hand, the section titles give a hint about which aspect of the document is the summary's focus. Therefore, we use the article title and the section titles of the statement to form the query. Given that Wikipedia is the largest online encyclopedia, we can automatically construct massive query-focused summarization examples.\n\nQuestion: How do they use Wikipedia to automatically collect a query-focused summarization dataset?\n\nExplanation:"}
{"question_id": "dfaeb8faf04505a4178945c933ba217e472979d8", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We work with a dataset composed of 3,206 news articles, each labeled real or fake, with a perfect 50/50 split between 1,603 real and fake articles, respectively. Fake articles were sourced from online sites that were tagged as fake news sites by the non-profit independent media fact-checking organization Verafiles and the National Union of Journalists in the Philippines (NUJP). Real articles were sourced from mainstream news websites in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera.\n\nQuestion: What is the source of the dataset?\n\nExplanation: \n\nAccording to [Document 1], the dataset was sourced from online sites tagged as fake news sites by Verafiles and NUJP and news websites in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera.\n\nAnswer: Online sites tagged as fake news site by Verafiles and NUJP and news website in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera\n\nExample 2:\n\nQuestion: What is the source of the proposed dataset?\n\nExplanation:  There is not enough information in the documents to answer the question.\n\nAnswer: Unanswerable\n\nExample 3:\n\n[Document 1]: We collected all available comments in the stories from Reddit from August 2015. Reddit is popular website that allows registered users (without identity verification) to participate in forums specific a post or topic. These forums are of they hierarchical type, those that allow nested conversation, where the children of a comment are its direct response. To increase recall and make the annotation process feasible we created an inverted index with Lucene and queried for comments containing the word troll with an edit distance of 1, to include close variations of this word. We do so inspired by the method by BIBREF2 to created a bullying dataset, and because we hypothesize that such comments will be related or involved in a trolling event. As we observed in the dataset, people use the word troll in many different ways, sometimes it is to point out that some used is indeed trolling him or her or is accusing someone else of being a troll. Other times, people use the term, to express their frustration or dislike about a particular user, but there is no trolling event. Other times, people simple discuss about trolling and trolls, without actually participating or observing one directly. Nonetheless, we found that this search produced a dataset in which 44.3 % of the comments directly involved a trolling event. Moreover, as we exposed our trolling definition, it is possible for commentators in a conversation to believe that they are witnessing a trolling event and respond accordingly even where there is none. Therefore, even in the comments that do not involve trolling, we are interested in learning what triggers users interpretation of trolling where it is not present and what kind of response strategies are used. We define as a suspected trolling event in our dataset a comment in which at least one of its children contains the word troll.\n\nQuestion: what is the source of the new dataset?\n\nExplanation:  [Document 1] states that the dataset was collected from Reddit.\n\nAnswer: Reddit\n\nExample 4:\n\n[Document 1]: The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .\n\nQuestion: what is the source of their dataset?\n\nExplanation:"}
{"question_id": "0dc9050c832a6091bc9db3f7fa7be72139f51177", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: As introduced above, the `One Million Post' corpus provides annotation labels for more than 11,000 user comments. Although there is no directly comparable category capturing `offensive language' as defined in the shared task, there are two closely related categories. From the resource, we extract all those comments in which a majority of the annotators agree that they contain either `inappropriate' or `discriminating' content, or none of the aforementioned. We treat the first two cases as examples of `offense' and the latter case as examples of `other'. This results in 3,599 training examples (519 offense, 3080 other) from on the `One Million Post' corpus. We conduct pre-training of the neural model as a binary classification task (similar to the Task 1 of GermEval 2018)\n\nQuestion: What are the near-offensive language categories?\n\nExplanation:  The document states that the categories of \"inappropriate\" and \"discriminating\" are considered to be examples of \"offense\".\n\nAnswer: inappropriate, discriminating\n\nExample 2:\n\n[Document 1]: In sub-task C the goal is to classify the target of the offensive language. Only posts labeled as targeted insults (TIN) in sub-task B are considered in this task BIBREF17 . Samples are annotated with one of the following:\n\n[Document 2]: Individual (IND): Posts targeting a named or unnamed person that is part of the conversation. In English this could be a post such as @USER Is a FRAUD Female @USER group paid for and organized by @USER. In Danish this could be a post such as USER du er sku da syg i hoved. These examples further demonstrate that this category captures the characteristics of cyberbullying, as it is defined in section \"Background\" .\n\n[Document 3]: Group (GRP): Posts targeting a group of people based on ethnicity, gender or sexual orientation, political affiliation, religious belief, or other characteristics. In English this could be a post such as #Antifa are mentally unstable cowards, pretending to be relevant. In Danish this could be e.g. \u00c5h nej! Svensk lorteret!\n\n[Document 4]: Other (OTH): The target of the offensive language does not fit the criteria of either of the previous two categories. BIBREF17 . In English this could be a post such as And these entertainment agencies just gonna have to be an ass about it.. In Danish this could be a post such as Netto er jo et tempel over lort.\n\nQuestion: How many categories of offensive language were there?\n\nExplanation: \n\nAccording to [Document 1], there are three categories of offensive language: Individual (IND), Group (GRP), and Other (OTH). This is further supported by [Document 2], [Document 3], and [Document 4].\n\nAnswer: 3\n\nExample 3:\n\n[Document 1]: Generic Sarcasm. We first examine the different patterns learned on the Gen dataset. Table TABREF29 show examples of extracted patterns for each class. We observe that the not-sarcastic patterns appear to capture technical and scientific language, while the sarcastic patterns tend to capture subjective language that is not topic-specific. We observe an abundance of adjective and adverb patterns for the sarcastic class, although we do not use adjective and adverb patterns in our regex retrieval method. Instead, such cues co-occur with the cues we search for, expanding our pattern inventory as we show in Table TABREF31 .\n\n[Document 2]: Rhetorical Questions. We notice that while the not-sarcastic patterns generated for RQs are similar to the topic-specific not-sarcastic patterns we find in the general dataset, there are some interesting features of the sarcastic patterns that are more unique to the RQs.\n\n[Document 3]: Many of our sarcastic questions focus specifically on attacks on the mental abilities of the addressee. This generalization is made clear when we extract and analyze the verb, subject, and object arguments using the Stanford dependency parser BIBREF32 for the questions in the RQ dataset. Table TABREF32 shows a few examples of the relations we extract.\n\n[Document 4]: Hyperbole. One common pattern for hyperbole involves adverbs and adjectives, as noted above. We did not use this pattern to retrieve hyperbole, but because each hyperbolic sarcastic utterance contains multiple cues, we learn an expanded class of patterns for hyperbole. Table TABREF33 illustrates some of the new adverb adjective patterns that are frequent, high-precision indicators of sarcasm.\n\n[Document 5]: We learn a number of verbal patterns that we had not previously associated with hyperbole, as shown in Table TABREF34 . Interestingly, many of these instantiate the observations of CanoMora2009 on hyperbole and its related semantic fields: creating contrast by exclusion, e.g. no limit and no way, or by expanding a predicated class, e.g. everyone knows. Many of them are also contrastive. Table TABREF33 shows just a few examples, such as though it in no way and so much knowledge.\n\nQuestion: What are the linguistic differences between each class?\n\nExplanation: \n\nAccording to [Document 1], [Document 2], [Document 3], [Document 4], and [Document 5], each class has different patterns in adjectives, adverbs and verbs for sarcastic and non-sarcastic classes.\n\nAnswer: Each class has different patterns in adjectives, adverbs and verbs for sarcastic and non-sarcastic classes\n\nExample 4:\n\n[Document 1]: We present BCF, a cognitively motivated Bayesian model for learning Categories and structured Features from large sets of concept mentions and their linguistic contexts (see Figure 1 ). Our model induces categories (as groups of concepts), feature types which are shared across categories (as groups of features or context words), and category-feature type associations. Figure 2 shows example output of BCF as learnt from the English Wikipedia, and Figure 21 shows example categories and features learnt for five additional languages.\n\nQuestion: do language share categories? \n\nExplanation:"}
{"question_id": "e9a0a69eacd554141f56b60ab2d1912cc33f526a", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Early approaches to Opinion Target Extraction (OTE) were unsupervised, although later on the vast majority of works have been based on supervised and deep learning models. To the best of our knowledge, the first work on OTE was published by BIBREF8 . They created a new task which consisted of generating overviews of the main product features from a collection of customer reviews on consumer electronics. They addressed such task using an unsupervised algorithm based on association mining. Other early unsupervised approaches include BIBREF9 which used a dependency parser to obtain more opinion targets, and BIBREF10 which aimed at extracting opinion targets in newswire via Semantic Role Labelling. From a supervised perspective, BIBREF11 presented an approach which learned the opinion target candidates and a combination of dependency and part-of-speech (POS) paths connecting such pairs. Their results improved the baseline provided by BIBREF8 . Another influential work was BIBREF12 , an unsupervised algorithm called Double Propagation which roughly consists of incrementally augmenting a set of seeds via dependency parsing.\n\n[Document 2]: In spite of this, Table TABREF20 shows that our system outperforms the best previous approaches across the five languages. In some cases, such as Turkish and Russian, the best previous scores were the baselines provided by the ABSA organizers, but for Dutch, French and Spanish our system is significantly better than current state-of-the-art. In particular, and despite using the same system for every language, we improve over GTI's submission, which implemented a CRF system with linguistic features specific to Spanish BIBREF28 .\n\nQuestion: What was the baseline?\n\nExplanation: \n\nAccording to [Document 1], the baseline provided by BIBREF8 was an early unsupervised approach to Opinion Target Extraction. This information is further supported by [Document 2], which states that the baselines provided by the ABSA organizers were the best previous scores for some languages.\n\nAnswer: the baseline provided by BIBREF8, the baselines provided by the ABSA organizers\n\nExample 2:\n\nQuestion: What was the baseline?\n\nExplanation:  There is not enough information provided to answer the question.\n\nAnswer: Unanswerable\n\nExample 3:\n\nQuestion: What was the baseline?\n\nExplanation:  There is not enough information provided to answer the question.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: The baseline systems are developed by randomly assigning any of the sentiment values to each of the test instances. Then, similar evaluation techniques are applied to the baseline system and the results are presented in Table TABREF29 .\n\nQuestion: what was the baseline?\n\nExplanation:"}
{"question_id": "b584739622d0c53830e60430b13fd3ae6ff43669", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: When testing across languages, we report accuracy for two setups: average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All). The latter setting is also used for the embeddings model. We report accuracy for all experiments.\n\nQuestion: What are the evaluation metrics used?\n\nExplanation: \n\nAccording to [Document 1], the evaluation metrics used are average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All).\n\nAnswer: average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All)\n\nExample 2:\n\n[Document 1]: Deep learning has unquestionably advanced the state of the art in many natural language processing tasks, from syntactic dependency parsing BIBREF0 to named-entity recognition BIBREF1 to machine translation BIBREF2 . The same certainly applies to language modeling, where recent advances in neural language models (NLMs) have led to dramatically better approaches as measured using standard metrics such as perplexity BIBREF3 , BIBREF4 .\n\nQuestion: What is a commonly used evaluation metric for language models?\n\nExplanation: \n\nAccording to [Document 1], perplexity is a commonly used evaluation metric for language models. This is further supported by [Document 3] and [Document 4].\n\nAnswer: perplexity\n\nExample 3:\n\n[Document 1]: We compare the performance of translation approaches based on four metrics:\n\n[Document 2]: [align=left,leftmargin=0em,labelsep=0.4em,font=]\n\n[Document 3]: As in BIBREF20 , EM is 1 if a predicted plan matches exactly the ground truth; otherwise it is 0.\n\n[Document 4]: The harmonic average of the precision and recall over all the test set BIBREF26 .\n\n[Document 5]: The minimum number of insertions, deletions or swap operations required to transform a predicted sequence of behaviors into the ground truth sequence BIBREF27 .\n\n[Document 6]: GM is 1 if a predicted plan reaches the ground truth destination (even if the full sequence of behaviors does not match exactly the ground truth). Otherwise, GM is 0.\n\nQuestion: What evaluation metrics are used?\n\nExplanation: \n\nAccording to [Document 1], four metrics are used to compare the performance of translation approaches. These metrics are further described in [Document 2], [Document 3], [Document 4], [Document 5], and [Document 6].\n\nAnswer: exact match, f1 score, edit distance and goal match\n\nExample 4:\n\n[Document 1]: Experiments ::: Automatic Evaluation Metrics\n\n[Document 2]: Perplexity Since the canonical measure of a good language model is perplexity, which indicates the error rate of the expected word. We choose perplexity to evaluate the model performance.\n\n[Document 3]: Response-Intent Prediction (RIP) $\\&$ Response-Slot Prediction (RSP) Different from open-domain dialog systems, we care about the intents of the system response in non-collaborative tasks as we hope to know if the system response satisfies user intents. For example, in the anti-scam task, if the attacker elicits information from the system, we need to know if the system refuses or agrees to provide the information. Therefore we care about intent prediction for the generated system response. Since our baselines are more suited for social chat as they cannot produce system intents, we use the system intent and slot classifiers trained in our model to predict their responses' intents and slots. The intent predictor achieves a $84\\%$ accuracy and the semantic slot predictor achieves $77\\%$ on the AntiScam dataset. Then we compare the predicted values with human-annotated ground truth in the dataset to compute the response-intent prediction (RIP) and response-slot prediction (RSP).\n\n[Document 4]: Extended Response-Intent Prediction (ERIP) $\\&$ Extended Response-Slot Prediction (ERSP) With Response-Intent Prediction, we verify the predicted intents to evaluate the coherence of the dialog. However, the real mapping between human-intent and system-intent is much more complicated as there might be multiple acceptable system-intents for the same human-intent. Therefore, we also design a metric to evaluate if the predicted system-intent is in the set of acceptable intents. Specifically, we estimate the transition probability $p(I_i|I_j)$ by counting the frequency of all the bi-gram human-intent and system-intent pairs in the training data. During the test stage, if the predicted intent matches the ground truth, we set the score as 1, otherwise we set the score as $p(I_{predict}|I_i)$ where $I_i$ is the intent of the input human utterance. We then report the average value of those scores over turns as the final extended response-intent prediction result.\n\n[Document 5]: Automatic metrics only validate the system\u2019s performance on a single dimension at a time. The ultimate holistic evaluation should be conducted by having the trained system interact with human users. Therefore we also conduct human evaluations for the dialog system built on AntiScam. We test our models and baselines with 15 college-student volunteers. Each of them is asked to pretend to be an attacker and interact with all the models for at least three times to avoid randomness. We in total collect 225 number of dialogs. Each time, volunteers are required to use similar sentences and strategies to interact with all five models and score each model based on the metrics listed below at the end of the current round. Each model receives a total of 45 human ratings, and the average score is reported as the final human-evaluation score. In total, we design five different metrics to assess the models' conversational ability whilst interacting with humans. The results are shown in Table TABREF19.\n\n[Document 6]: Fluency Fluency is used to explore different models' language generation quality.\n\n[Document 7]: Coherence Different from single sentence's fluency, coherence focuses more on the logical consistency between sentences in each turn.\n\n[Document 8]: Engagement In the anti-scam scenario, one of our missions is to keep engaging with the attackers to waste their time. So we directly ask volunteers (attackers) to what extend they would like to continue chatting with the system.\n\n[Document 9]: Dialog length (Length) Engagement is a subjective metric. Anti-scam system's goal is to engage user in the conversation longer in order to limit their harm to other potential victims. So we count the dialog length as another metric to evaluate system performance.\n\n[Document 10]: Task Success Score (TaskSuc) The other goal of the anti-scam system is to elicit attacker's personal information. We count the average type of information (name, address and phone number) that the system obtained from attackers as the task success score.\n\nQuestion: What are the evaluation metrics and criteria used to evaluate the model performance?\n\nExplanation:"}
{"question_id": "f13a5b6a67a9b10fde68e8b33792879b8146102c", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: The system consists of: (i) Local, shallow features based mostly on orthographic, word shape and n-gram features plus their context; and (ii) three types of simple clustering features, based on unigram matching: (i) Brown BIBREF32 clusters, taking the 4th, 8th, 12th and 20th node in the path; (ii) Clark BIBREF33 clusters and, (iii) Word2vec BIBREF34 clusters, based on K-means applied over the extracted word vectors using the skip-gram algorithm.\n\nQuestion: What shallow local features are extracted?\n\nExplanation:  According to [Document 1], the system extracts local, shallow features based mostly on orthographic, word shape, and n-gram features plus their context.\n\nAnswer:  Local, shallow features based mostly on orthographic, word shape and n-gram features plus their context\n\nExample 2:\n\n[Document 1]: Satirical news is not based on or does not aim to state the fact. Rather, it uses parody or humor to make statement, criticisms, or just amusements. In order to achieve such effect, contradictions are greatly utilized. Therefore, inconsistencies significantly exist in different parts of a satirical news tweet. In addition, there is a lack of entity or inconsistency between entities in news satire. We extracted these features at semantic level from different sub-structures of the news tweet. Different structural parts of the sentence are derived by part-of-speech tagging and named entity recognition by Flair. The inconsistencies in different structures are measured by cosine similarity of word phrases where words are represented by Glove word vectors. We explored three different aspects of inconsistency and designed metrics for their measurements. A word level feature using tf-idf BIBREF22 is added for robustness.\n\nQuestion: What features do they extract?\n\nExplanation:  The features that they extract are listed in [Document 1]. They are: Inconsistency in Noun Phrase Structures,  Inconsistency Between Clauses, Inconsistency Between Named Entities and Noun Phrases, Word Level Feature Using TF-IDF.\n\nAnswer: Inconsistency in Noun Phrase Structures,  Inconsistency Between Clauses, Inconsistency Between Named Entities and Noun Phrases, Word Level Feature Using TF-IDF\n\nExample 3:\n\n[Document 1]: General lexical features are often used in natural language processing as they are somewhat task-independent and reasonably effective in terms of classification accuracy. In our experiments, we used TF.IDF-based features over the title and over the content of the article we wanted to classify. We had these features twice \u2013 once for the title and once for the the content of the article, as we wanted to have two different representations of the same article. Thus, we used a total of 1,100 TF.IDF-weighted features (800 content + 300 title), limiting the vocabulary to the top 800 and 300 words, respectively (which occurred in more than five articles). We should note that TF.IDF features should be used with caution as they may not remain relevant over time or in different contexts without retraining.\n\nQuestion: what lexical features did they experiment with?\n\nExplanation:  [Document 1] mentions that the experiment used TF.IDF-based features.\n\nAnswer: TF.IDF-based features\n\nExample 4:\n\n[Document 1]: Usually, INLINEFORM0 is a INLINEFORM1 weight matrix and INLINEFORM2 is the transposition. Since the answer in our task is a simple \u201cyes\u201d or \u201cno\u201d, we use a INLINEFORM3 matrix for INLINEFORM4 . As the distance between a clause and an emotion words is a very important feature according to BIBREF31 , we simply add this distance into the softmax function as an additional feature in our work.\n\nQuestion: what lexical features are extracted?\n\nExplanation:"}
{"question_id": "6657ece018b1455035421b822ea2d7961557c645", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Prior to the MT experiments, sentences were randomly split in three disjoint datasets: training, development, and test. Approximately 13,000 sentences were allocated in the development and test sets, while the remaining was used for training. For the SMT experiment, we followed the instructions of Moses baseline system. For the NMT experiment, we used the Torch implementation to train a 2-layer LSTM model with 500 hidden units in both encoder and decoder, with 12 epochs. During translation, the option to replace UNK words by the word in the input language was used, since this is also the default in Moses.\n\nQuestion: Which NMT models did they experiment with?\n\nExplanation:  [Document 1] mentions that they used a 2-layer LSTM model with 500 hidden units in both the encoder and the decoder for the NMT experiment.\n\nAnswer: 2-layer LSTM model with 500 hidden units in both encoder and decoder\n\nExample 2:\n\n[Document 1]: The neural machine translation was trained using Nematus. For the NMT system as well as for the PreMT system, we used the default configuration. In order to limit the vocabulary size, we use BPE as described in BIBREF0 with 40K operations. We run the NMT system for 420K iterations and stored a model every 30K iterations. We selected the model that performed best on the development data. For the ensemble system we took the last four models. We did not perform an additional fine-tuning.\n\nQuestion: Which NMT architecture do they use?\n\nExplanation: \n\nAccording to [Document 1], the neural machine translation was trained using Nematus with the default configuration.\n\nAnswer: trained using Nematus, default configuration\n\nExample 3:\n\n[Document 1]: FlowSeq is a flow-based sequence-to-sequence model, which is (to our knowledge) the first non-autoregressive seq2seq model utilizing generative flows. It allows for efficient parallel decoding while modeling the joint distribution of the output sequence. Experimentally, on three benchmark datasets for machine translation \u2013 WMT2014, WMT2016 and IWSLT-2014, FlowSeq achieves comparable performance with state-of-the-art non-autoregressive models, and almost constant decoding time w.r.t. the sequence length compared to a typical left-to-right Transformer model, which is super-linear.\n\nQuestion: What are three neural machine translation (NMT) benchmark datasets used for evaluation?\n\nExplanation:  The three NMT benchmark datasets used for evaluation are WMT2014, WMT2016 and IWSLT-2014, as stated in [Document 1].\n\nAnswer: WMT2014, WMT2016 and IWSLT-2014\n\nExample 4:\n\n[Document 1]: We trained translation models for two established NMT architectures for each language, namely, ConvS2S and Transformer. As the purpose of this work is to provide a baseline benchmark, we have not performed significant hyperparameter optimization, and have left that as future work.\n\nQuestion: What NMT techniques did they explore?\n\nExplanation:"}
{"question_id": "5e29f16d7302f24ab93b7707d115f4265a0d14b0", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We performed experiments on synthetically generated dataset since it gives us a better control over the distribution of the data. Specifically we compared the gains obtained using our approach versus the variance of the distribution. We created dataset from the following generative process. [H] Generative Process [1] Generate data\n\n[Document 2]: Pick k points INLINEFORM0 as domain -1 means and a corresponding set of k points INLINEFORM1 as domain-2 means, and covariance matrices INLINEFORM2\n\n[Document 3]: iter INLINEFORM0 upto num INLINEFORM1 samples Sample class INLINEFORM2 Sample INLINEFORM3 Sample INLINEFORM4 Add q and a so sampled to the list of q,a pairs We generated the dataset from the above sampling process with means selected on a 2 dimensional grid of size INLINEFORM5 with variance set as INLINEFORM6 in each dimension.10000 sample points were generated. The parameter INLINEFORM7 of the above algorithm was set to 0.5 and k was set to 9 (since the points could be generated from one of the 9 gaussians with centroids on a INLINEFORM8 grid).\n\nQuestion: How do they generate the synthetic dataset?\n\nExplanation:  [Document 1] states that the synthetic dataset was generated using a generative process. This is further supported by [Document 2] and [Document 3], which describe the specific steps involved in the generative process.\n\nAnswer: using generative process\n\nExample 2:\n\n[Document 1]: To achieve a finer-grained understanding of what errors the evaluated translations exhibit, we perform a categorisation of 150 randomly sampled sentences based on the classification used by BIBREF3. We expand the classification with a Context category, which we use to mark errors that are only apparent in larger context (e. g., regarding poor register choice, or coreference errors), and which do not clearly fit into one of the other categories. BIBREF3 perform this classification only for the machine-translated outputs, and thus the natural question of whether the mistakes that humans and computers make are qualitatively different is left unanswered. Our analysis was performed by one of the co-authors who is a bi-lingual native Chinese/English speaker. Sentences were shown in the context of the document, to make it easier to determine whether the translations were correct based on the context. The analysis was performed on one machine translation (MT$_1$) and two human translation outputs (H$_A$, H$_B$), using the same 150 sentences, but blinding their origin by randomising the order in which the documents were presented. We show the results of this analysis in Table TABREF32.\n\nQuestion: What percentage fewer errors did professional translations make?\n\nExplanation:  In [Document 1], it is stated that \"the natural question of whether the mistakes that humans and computers make are qualitatively different is left unanswered.\" However, in Table TABREF32, it is shown that professional translations made 36% fewer errors.\n\nAnswer: 36%\n\nExample 3:\n\nQuestion: How large is the dataset they generate?\n\nExplanation: \n\n[Document 1]: We generate 4.756 million sentences by randomly sampling from the Google Books Ngrams corpus BIBREF1 .\n\nThis information is further supported by [Document 2]:\n\n[Document 2]: We use the Google Books Ngrams corpus BIBREF1 as our training data. This corpus contains over 500 billion words from 5.2 million books .\n\nAnswer: 4.756 million sentences\n\nExample 4:\n\n[Document 1]: FLOAT SELECTED: Figure 1: The pipeline of proposed method. \u201cSMT\u201d and \u201cDRP\u201d denote statistical machine translation and discourse relation parser respectively.\n\nQuestion: How much additional data do they manage to generate from translations?\n\nExplanation:"}
{"question_id": "6cb3007a09ab0f1602cdad20cc0437fbdd4d7f3e", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Prior to the MT experiments, sentences were randomly split in three disjoint datasets: training, development, and test. Approximately 13,000 sentences were allocated in the development and test sets, while the remaining was used for training. For the SMT experiment, we followed the instructions of Moses baseline system. For the NMT experiment, we used the Torch implementation to train a 2-layer LSTM model with 500 hidden units in both encoder and decoder, with 12 epochs. During translation, the option to replace UNK words by the word in the input language was used, since this is also the default in Moses.\n\nQuestion: Which NMT models did they experiment with?\n\nExplanation:  [Document 1] mentions that they used a 2-layer LSTM model with 500 hidden units in both the encoder and the decoder for the NMT experiment.\n\nAnswer: 2-layer LSTM model with 500 hidden units in both encoder and decoder\n\nExample 2:\n\nQuestion: What supervised models are experimented with?\n\nExplanation:  The documents do not mention any supervised models that were experimented with.\n\nAnswer: Unanswerable\n\nExample 3:\n\nQuestion: Do they experiment with language modeling on large datasets?\n\nExplanation: \n\nThere is no mention of language modeling or large datasets in any of the documents.\n\nAnswer: No\n\nExample 4:\n\n[Document 1]: Every classical model was considered on the condition it could take matrices as input for fitting and was trained with the default settings because of the size of the dataset. Five models were trained: Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), both with a value of 1 in the penalty parameter C of the error term. The gamma value of the RBF SVM which indicates how much influence a single training example has, was set to 2. The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning. The gradient of the loss is estimated each sample at a time and the SGDC is updated along the way with a decreasing learning rate. The parameters for maximum epochs and the stopping criterion were defined using the default values in scikit-learn. The final classifier was two models based on the Bayes theorem: Multinomial Na\u00efve Bayes, which works with occurrence counts, and Bernoulli Na\u00efve Bayes, which is designed for binary features.\n\n[Document 2]: Six different deep learning models were considered. All of these models have been used in an aggression detection task. The models are Pooled GRU BIBREF25, Stacked LSTM with Attention BIBREF25, LSTM and GRU with Attention BIBREF25, 2D Convolution with Pooling BIBREF26, GRU with Capsule BIBREF27, LSTM with Capsule and Attention BIBREF26 and BERT BIBREF24. These models has been used in HASOC 2019 and achieved a third place finish in English task and a eighth place finish in German and Hindi subtasks BIBREF26. Parameters described in BIBREF26 were used as the default parameters in order to ease the training process. The code for the deep learning has been made available on Github .\n\nQuestion: What models do they experiment on?\n\nExplanation:"}
{"question_id": "fb06ed5cf9f04ff2039298af33384ca71ddbb461", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Machine-machine Interaction A related line of work explores simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers BIBREF1. Such a framework may be cost-effective and error-resistant since the underlying crowd worker task is simpler, and semantic annotations are obtained automatically.\n\n[Document 2]: It is often argued that simulation-based data collection does not yield natural dialogues or sufficient coverage, when compared to other approaches such as Wizard-of-Oz. We argue that simulation-based collection is a better alternative for collecting datasets like this owing to the factors below.\n\nQuestion: How did they gather the data?\n\nExplanation: \n\nAccording to [Document 1], the data was gathered using simulation-based dialogue generation. This information is further supported by [Document 2].\n\nAnswer: simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers \n\nExample 2:\n\n[Document 1]: Our AV in-cabin data-set includes 30 hours of multimodal data collected from 30 passengers (15 female, 15 male) in 20 rides/sessions. 10 types of passenger intents are identified and annotated as: Set/Change Destination, Set/Change Route (including turn-by-turn instructions), Go Faster, Go Slower, Stop, Park, Pull Over, Drop Off, Open Door, and Other (turn music/radio on/off, open/close window/trunk, change AC/temp, show map, etc.). Relevant slots are identified and annotated as: Location, Position/Direction, Object, Time-Guidance, Person, Gesture/Gaze (this, that, over there, etc.), and None. In addition to utterance-level intent types and their slots, word-level intent keywords are annotated as Intent as well. We obtained 1260 unique utterances having commands to AMIE from our in-cabin data-set. We expanded this data-set via Amazon Mechanical Turk and ended up with 3347 utterances having intents. The annotations for intents and slots are obtained on the transcribed utterances by majority voting of 3 annotators.\n\nQuestion: What is the size of their collected dataset?\n\nExplanation:  According to [Document 1], the dataset includes 3347 unique utterances.\n\nAnswer: 3347 unique utterances \n\nExample 3:\n\n[Document 1]: WikiTableQuestions BIBREF1 is a question answering dataset over semi-structured tables. It is comprised of question-answer pairs on HTML tables, and was constructed by selecting data tables from Wikipedia that contained at least 8 rows and 5 columns. Amazon Mechanical Turk workers were then tasked with writing trivia questions about each table. In contrast to common NLIDB benchmarks BIBREF2 , BIBREF0 , BIBREF15 , WikiTableQuestions contains 22,033 questions and is an order of magnitude larger than previous state-of-the-art datasets. Its questions were not designed by predefined templates but were hand crafted by users, demonstrating high linguistic variance. Compared to previous datasets on knowledge bases it covers nearly 4,000 unique column headers, containing far more relations than closed domain datasets BIBREF15 , BIBREF2 and datasets for querying knowledge bases BIBREF16 . Its questions cover a wide range of domains, requiring operations such as table lookup, aggregation, superlatives (argmax, argmin), arithmetic operations, joins and unions. The complexity of its questions can be shown in Tables TABREF6 and TABREF66 .\n\nQuestion: How do they gather data for the query explanation problem?\n\nExplanation:  [Document 1] states that the questions were \"hand crafted by users\".\n\nAnswer: hand crafted by users\n\nExample 4:\n\n[Document 1]: Subsequently, we extracted parallel sentences using the trained model, and parallel articles collected from Wikipedia. There were INLINEFORM0 bilingual English-Tamil and INLINEFORM1 English-Hindi titles on the Wikimedia dumps collected in December 2017.\n\nQuestion: How much data do they manage to gather online?\n\nExplanation:"}
{"question_id": "a064d01d45a33814947161ff208abb88d4353b26", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We adopt the annotation machinery of BIBREF5 implemented using Amazon's Mechanical Turk, and annotate each predicate by 2 trained workers independently, while a third consolidates their annotations into a final set of roles and arguments. In this consolidation task, the worker validates questions, merges, splits or modifies answers for the same role according to guidelines, and removes redundant roles by picking the more naturally phrased questions. For example, in Table TABREF4 ex. 1, one worker could have chosen \u201c47 people\u201d, while another chose \u201cthe councillor\u201d; in this case the consolidator would include both of those answers. In Section SECREF4, we show that this process yields better coverage. For example annotations, please refer to the appendix.\n\nQuestion: What is different in the improved annotation protocol?\n\nExplanation: \n\nAccording to [Document 1], in the improved annotation protocol, a trained worker consolidates existing annotations. This information is further supported by [Document 4].\n\nAnswer: a trained worker consolidates existing annotations \n\nExample 2:\n\n[Document 1]: All the capabilities described in this paper come together in an end-to-end cloud-based platform that we have built. The platform has two main features: First, it provides an annotation interface that allows users to define content elements, upload documents, and annotate documents; a screenshot is shown in Figure FIGREF12. We have invested substantial effort in making the interface as easy to use as possible; for example, annotating content elements is as easy as selecting text from the document. Our platform is able to ingest documents in a variety of formats, including PDFs and Microsoft Word, and converts these formats into plain text before presenting them to the annotators.\n\nQuestion: What type of documents are supported by the annotation platform?\n\nExplanation:  The annotation platform supports a variety of document formats as stated in [Document 1]. Furthermore, the user can define the content elements of the document as also stated in [Document 1].\n\nAnswer: Variety of formats supported (PDF, Word...), user can define content elements of document\n\nExample 3:\n\nQuestion: What annotations are in the dataset?\n\nExplanation:  The documents do not provide enough information to answer the question.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: Existing annotation tools BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 mainly focus on providing a visual interface for user annotation process but rarely consider the post-annotation quality analysis, which is necessary due to the inter-annotator disagreement. In addition to the annotation quality, efficiency is also critical in large-scale annotation task, while being relatively less addressed in existing annotation tools BIBREF6 , BIBREF7 . Besides, many tools BIBREF6 , BIBREF4 require a complex system configuration on either local device or server, which is not friendly to new users.\n\nQuestion: what are the existing annotation tools?\n\nExplanation:"}
{"question_id": "e56c1f0e9eabda41f929d0dfd5cfa50edd69fa89", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Datasets: We experimented on four standard datasets: WN18 and FB15k are extracted by BIBREF5 from Wordnet BIBREF32 Freebase BIBREF33 . We used the same train/valid/test sets as in BIBREF5 . WN18 contains 40,943 entities, 18 relations and 141,442 train triples. FB15k contains 14,951 entities, 1,345 relations and 483,142 train triples. In order to test the expressiveness ability rather than relational pattern learning power of models, FB15k-237 BIBREF34 and WN18RR BIBREF35 exclude the triples with inverse relations from FB15k and WN18 which reduced the size of their training data to 56% and 61% respectively. Baselines: We compare MDE with several state-of-the-art relational learning approaches. Our baselines include, TransE, TransH, TransD, TransR, STransE, DistMult, NTN, RESCAL, ER-MLP, and ComplEx and SimplE. We report the results of TransE, DistMult, and ComplEx from BIBREF13 and the results of TransR and NTN from BIBREF36 , and ER-MLP from BIBREF15 . The results on the inverse relation excluded datasets are from BIBREF11 , Table 13 for TransE and RotatE and the rest are from BIBREF35 .\n\nQuestion: What datasets are used to evaluate the model?\n\nExplanation: \n\nAccording to [Document 1], the WN18 and FB15k datasets are used to evaluate the model.\n\nAnswer: WN18 and FB15k\n\nExample 2:\n\nQuestion: What datasets are used to evaluate the model?\n\nExplanation: \n\n[Document 1]: We use the WN18 and FB15k datasets for evaluation.\n\n[Document 2]: The WN18 dataset is a subset of WordNet, and the FB15k dataset is a subset of Freebase.\n\nAnswer: WN18 and FB15k\n\nExample 3:\n\n[Document 1]: As we show below, STransE performs better than the SE and TransE models and other state-of-the-art link prediction models on two standard link prediction datasets WN18 and FB15k, so it can serve as a new baseline for KB completion. We expect that the STransE will also be able to serve as the basis for extended models that exploit a wider variety of information sources, just as TransE does.\n\nQuestion: What datasets are used to evaluate the model?\n\nExplanation: \n\nAccording to [Document 1], the WN18 and FB15k datasets are used to evaluate the model.\n\nAnswer: WN18, FB15k\n\nExample 4:\n\n[Document 1]: The evaluation is on two widely used public machine translation datasets: IWSLT14 German-to-English (De-En) BIBREF9, BIBREF1 and WMT14 English-to-German (En-De) dataset BIBREF4, BIBREF10. To compare with previous works, we also reverse WMT14 English-to-German dataset and obtain WMT14 German-to-English dataset.\n\nQuestion: Were any datasets other than WMT used to test the model?\n\nExplanation:"}
{"question_id": "e14e3e0944ec3290d1985e9a3da82a7df17575cd", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Recently, neural models pre-trained on a language modeling task, such as ELMo BIBREF0 , OpenAI GPT BIBREF1 , and BERT BIBREF2 , have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. The success of BERT can largely be associated to the notion of context-aware word embeddings, which differentiate it from common approaches such as word2vec BIBREF3 that establish a static semantic embedding. Since the introduction of BERT, the NLP community continues to be impressed by the amount of ideas produced on top of this powerful language representation model. However, despite its success, it remains unclear whether the representations produced by BERT can be utilized for tasks such as commonsense reasoning. Particularly, it is not clear whether BERT shed light on solving tasks such as the Pronoun Disambiguation Problem (PDP) and Winograd Schema Challenge (WSC). These tasks have been proposed as potential alternatives to the Turing Test, because they are formulated to be robust to statistics of word co-occurrence BIBREF4 .\n\n[Document 2]: In this paper, we show that the attention maps created by an out-of-the-box BERT can be directly exploited to resolve coreferences in long sentences. As such, they can be simply repurposed for the sake of commonsense reasoning tasks while achieving state-of-the-art results on the multiple task. On both PDP and WSC, our method outperforms previous state-of-the-art methods, without using expensive annotated knowledge bases or hand-engineered features. On a Pronoun Disambiguation dataset, PDP-60, our method achieves 68.3% accuracy, which is better than the state-of-art accuracy of 66.7%. On a WSC dataset, WSC-273, our method achieves 60.3%. As of today, state-of-the-art accuracy on the WSC-273 for single model performance is around 57%, BIBREF14 and BIBREF13 . These results suggest that BERT implicitly learns to establish complex relationships between entities such as coreference resolution. Although this helps in commonsense reasoning, solving this task requires more than employing a language model learned from large text corpora.\n\nQuestion: Which datasets do they evaluate on?\n\nExplanation: \n\nAccording to [Document 2], the PDP-60 and WSC-273 datasets were used for evaluation.\n\nAnswer: PDP-60, WSC-273\n\nExample 2:\n\n[Document 1]: Traditional text-to-speech (TTS) systems are composed of complex pipelines BIBREF0 , these often include accoustic frontends, duration model, acoustic prediction model and vocoder models. The complexity of the TTS problem coupled with the requirement for deep domain expertise means these systems are often brittle in design and results in un-natural synthesized speech.\n\n[Document 2]: The open source LJSpeech Dataset was used to train our TTS model. This dataset contains around 13k <text,audio> pairs of a single female english speaker collect from across 7 different non-fictional books. The total training data time is around 21 hours of audio.\n\n[Document 3]: The architecture of our model utilizes RNN-based Seq2Seq model for generating mel spectrogram from text. The architecture is similar to that of Tacotron 2 BIBREF4 . The generated mel spetrogram can either be inverted via iterative algorithms such as Griffin Lim, or through more complicated neural vocoder networks such as a mel spectrogram conditioned Wavenet BIBREF11 .\n\nQuestion: Which dataset(s) do they evaluate on?\n\nExplanation:  [Document 2] mentions that the LJSpeech dataset was used to train the TTS model. This dataset was used to evaluate the model.\n\nAnswer: LJSpeech\n\nExample 3:\n\n[Document 1]: Datasets\n\n[Document 2]: We conduct experiments on two Chinese question answering datasets from NLPCC-2016 evaluation task BIBREF13 .\n\n[Document 3]: DBQA is a document based question answering dataset. There are 8.8k questions with 182k question-sentence pairs for training and 6k questions with 123k question-sentence pairs in the test set. In average, each question has 20.6 candidate sentences and 1.04 golden answers. The average length for questions is 15.9 characters, and each candidate sentence has averagely 38.4 characters. Both questions and sentences are natural language sentences, possibly sharing more similar word choices and expressions compared to the KBQA case. But the candidate sentences are extracted from web pages, and are often much longer than the questions, with many irrelevant clauses.\n\n[Document 4]: KBRE is a knowledge based relation extraction dataset. We follow the same preprocess as BIBREF14 to clean the dataset and replace entity mentions in questions to a special token. There are 14.3k questions with 273k question-predicate pairs in the training set and 9.4k questions with 156k question-predicate pairs for testing. Each question contains only one golden predicate. Each question averagely has 18.1 candidate predicates and 8.1 characters in length, while a KB predicate is only 3.4 characters long on average. Note that a KB predicate is usually a concise phrase, with quite different word choices compared to the natural language questions, which poses different challenges to solve.\n\nQuestion: Which dataset(s) do they evaluate on?\n\nExplanation:  [Document 2] and [Document 3] both state that the DBQA dataset was used. [Document 4] states that the KBRE dataset was used.\n\nAnswer: DBQA, KBRE\n\nExample 4:\n\n[Document 1]: Our experiments are conducted on a large scale Chinese conversation corpus comprised of 20 million context-response pairs. We compare our model with generative models and retrieval models in terms of fluency, relevance, diversity and originality. The experiments show that our method outperforms traditional generative models on relevance, diversity and originality. We further find that the revised response achieves better relevance compared to its prototype and other retrieval results, demonstrating that the editing process does not only promote response originality but also improve the relevance of retrieval results.\n\nQuestion: Which dataset do they evaluate on?\n\nExplanation:"}
{"question_id": "18fbfb1f88c5487f739aceffd23210a7d4057145", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: The test-set accuracies obtained by different learning methods, including the current state-of-the-art results, are presented in Table TABREF11 . The results indicate that the bag-of-words MVN outperforms most methods, but obtains lower accuracy than the state-of-the-art results achieved by the tree-LSTM BIBREF21 , BIBREF22 and the high-order CNN BIBREF16 . However, when augmented with 4 convolutional features as described in Section SECREF9 , the MVN strategy surpasses both of these, establishing a new state-of-the-art on this benchmark.\n\nQuestion: what models did they compare to?\n\nExplanation:  The table in [Document 1] compares the test-set accuracies of the bag-of-words MVN model to the high-order CNN, Tree-LSTM, DRNN, DCNN, CNN-MC, NBoW and SVM models.\n\nAnswer: High-order CNN, Tree-LSTM, DRNN, DCNN, CNN-MC, NBoW and SVM \n\nExample 2:\n\n[Document 1]: Although we create new models for tasks such as sentiment analysis and gender detection as part of AraNet, our focus is more on putting together the toolkit itself and providing strong baselines that can be compared to. Hence, although we provide some baseline models for some of the tasks, we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models) . For many of the tasks we model, there have not been standard benchmarks for comparisons across models. This makes it difficult to measure progress and identify areas worthy of allocating efforts and budgets. As such, by publishing our toolkit models, we believe model-based comparisons will be one way to relieve this bottleneck. For these reasons, we also package models from our recent works on dialect BIBREF12 and irony BIBREF14 as part of AraNet .\n\nQuestion: What models did they compare to?\n\nExplanation:  [Document 1] states that the authors did not compare to previous research because most existing works would not provide a fair comparison.\n\nAnswer:  we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models)\n\nExample 3:\n\n[Document 1]: To evaluate the summarization performance of AttSum, we implement rich extractive summarization methods. Above all, we introduce two common baselines. The first one just selects the leading sentences to form a summary. It is often used as an official baseline of DUC, and we name it \u201cLEAD\u201d. The other system is called \u201cQUERY_SIM\u201d, which directly ranks sentences according to its TF-IDF cosine similarity to the query. In addition, we implement two popular extractive query-focused summarization methods, called MultiMR BIBREF2 and SVR BIBREF20 . MultiMR is a graph-based manifold ranking method which makes uniform use of the sentence-to-sentence relationships and the sentence-to-query relationships. SVR extracts both query-dependent and query-independent features and applies Support Vector Regression to learn feature weights. Note that MultiMR is unsupervised while SVR is supervised. Since our model is totally data-driven, we introduce a recent summarization system DocEmb BIBREF9 that also just use deep neural network features to rank sentences. It initially works for generic summarization and we supplement the query information to compute the document representation.\n\n[Document 2]: To verify the effectiveness of the joint model, we design a baseline called ISOLATION, which performs saliency ranking and relevance ranking in isolation. Specifically, it directly uses the sum pooling over sentence embeddings to represent the document cluster. Therefore, the embedding similarity between a sentence and the document cluster could only measure the sentence saliency. To include the query information, we supplement the common hand-crafted feature TF-IDF cosine similarity to the query. This query-dependent feature, together with the embedding similarity, are used in sentence ranking. ISOLATION removes the attention mechanism, and mixtures hand-crafted and automatically learned features. All these methods adopt the same sentence selection process illustrated in Section \"Sentence Selection\" for a fair comparison.\n\nQuestion: What models do they compare to?\n\nExplanation:  [Document 1] and [Document 2] list the models that are being compared. LEAD, QUERY_SIM, MultiMR, SVR, and DocEmb are compared in [Document 1]. ISOLATION is compared in [Document 2].\n\nAnswer: LEAD, QUERY_SIM, MultiMR, SVR, DocEmb, ISOLATION\n\nExample 4:\n\n[Document 1]: In the sequence labeling task,we explore sub-task named entity recognition using CoNLL03 dataset BIBREF5 , which is a public available used in many studies to test the accuracy of their proposed methods BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF1 . For strategy finetune-only and strategy stack-and-finetune, we implemented two models: one with BERT and the other with BERT adding a Bi-LSTM on top. Eval measure is accuracy and F1 score.\n\n[Document 2]: In the task of text categorization, we used Yahoo Answer Classification Dataset. The Dataset is consists of 10 classes, but due to the huge amount of the dataset, we just select two class of them. As for the upper model,we choose DenseNet BIBREF33 and HighwayLSTM BIBREF34 .\n\n[Document 3]: Apart from the BERT fine-tuning only model and BERT+ BIMPM model, we also devise two new network structures by modifying the BIMPM model. In the first model is to remove the first bi-LSTM of BIMPM, which is the input layer for the matching layer in BIMPM. In the second model, we combine the matching layer of BIMPM and with a transformer BIBREF16 , a model we call Sim-Transformer by replacing the output layer of the matching layer, originally a bi-LSTM model, with a transformer model. From the experimental results shown in Table 4, we can see that due to the strong expressive ability of the BERT, there is almost no difference in the experimental results of removing the first bi-LSTM and BIMPM. In addition, we also find that Sim-Transformer's performance without fine-tuning is nearly four percentage points lower than BIMPM, but it out-performs BIMPM after fine-tuning. In general, the results show that BERT + Sim-Transformer out-performs BERT-only model by 4.7%, thus confirming our hypotheses again.\n\nQuestion: what models did they compare with?\n\nExplanation:"}
{"question_id": "c4c06f36454fbfdc5d218fb84ce74eaf7f78fc98", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We tested three model versions against all datasets. The model we call Base is the BiLSTM-CNN-CRF model described in Section SECREF2 with the associated hyperparameters. Another model, Small, uses the same architecture as Base but reduces the number of convolutional layers to 1, the convolutional filters to 40, the LSTM dimension $l$ to 50, and the phone embedding size $d$ to 100. We also tested a Base-Softmax model, which replaces the CRF output of the Base model with a softmax. A comparison of the results of these three models can be seen in Table TABREF25. This comparison empirically motivates the CRF output because Base almost always outperforms Base-Softmax. Of these three models, the Base model performed the best with the exception of the French and Manipuri datasets. The differences in the French results can be considered negligible because the accuracies are all near $100\\%$. The Small model performed best on Manipuri, which may suggest that reducing the number of parameters of the Base model leads to better accuracy on smaller datasets.\n\nQuestion: What is the accuracy of the model for the six languages tested?\n\nExplanation:  The answer can be found in the first paragraph of [Document 1].\n\nAnswer: Authors report their best models have following accuracy: English CELEX (98.5%), Dutch CELEX (99.47%), Festival (99.990%), OpenLexique (100%), IIT-Guwahat (95.4%), E-Hitz (99.83%)\n\nExample 2:\n\nQuestion: Is the model tested for language identification?\n\nExplanation: \n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task\n\nAnswer: No\n\nExample 3:\n\n[Document 1]: In this paper, we explore the benefit of explicitly modeling variations in the surface forms of words using methods from deep latent variable modeling in order to improve the translation accuracy in low-resource and morphologically-rich languages. Latent variable models allow us to inject inductive biases relevant to the task, which, in our case, is word formation, and we believe that follows a certain hierarchical procedure. Our model translates words one character at a time based on word representations learned compositionally from sub-lexical components, which are parameterized by a hierarchical latent variable model mimicking the process of morphological inflection, consisting of a continuous-space dense vector capturing the lexical semantics, and a set of (approximately) discrete features, representing the morphosyntactic role of the word in a given sentence. Each word representation during decoding is reformulated based on the shared latent morphological features, aiding in learning more reliable representations of words under sparse settings by generalizing across their different surface forms. We evaluate our method in translating English into three morphologically-rich languages each with a distinct morphological typology: Arabic, Czech and Turkish, and show that our model is able to obtain better translation accuracy and generalization capacity than conventional approaches to open-vocabulary NMT.\n\nQuestion: What are the three languages studied in the paper?\n\nExplanation:  The three languages studied in the paper are Arabic, Czech and Turkish, as stated in [Document 1].\n\nAnswer: Arabic, Czech and Turkish\n\nExample 4:\n\n[Document 1]: Table TABREF12 shows the performance of our cross-lingual models in German, Italian and Dutch. We summarize the results as follows:\n\n[Document 2]: FLOAT SELECTED: Table 1: Results of zero-shot cross-lingual semantic parsing for models trained in English and tested in German, Italian and Dutch.2\n\nQuestion: What is the performance for the three languages tested?\n\nExplanation:"}
{"question_id": "6d8a51e2790043497ed2637a1abc36bdffb39b71", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We extract data from the WMT'14 English-French (En-Fr) and English-German (En-De) datasets. To create a larger discrepancy between the tasks, so that there is a clear dataset size imbalance, the En-De data is artificially restricted to only 1 million parallel sentences, while the full En-Fr dataset, comprising almost 40 million parallel sentences, is used entirely. Words are split into subwords units with a joint vocabulary of 32K tokens. BLEU scores are computed on the tokenized output with multi-bleu.perl from Moses BIBREF10.\n\nQuestion: What datasets are used for experiments?\n\nExplanation:  [Document 1] states that the WMT'14 English-French (En-Fr) and English-German (En-De) datasets are used for experiments.\n\nAnswer: the WMT'14 English-French (En-Fr) and English-German (En-De) datasets.\n\nExample 2:\n\n[Document 1]: Due to the reduction of computational time complexity we expect our method to yield performance gains especially for longer sequences and tasks where the source can be compactly represented in a fixed-size memory matrix. To investigate the trade-off between speed and performance, we compare our technique to standard models with and without attention on a Sequence Copy Task of varying length like in BIBREF14 . We generated 4 training datasets of 100,000 examples and a validation dataset of 1,000 examples. The vocabulary size was 20. For each dataset, the sequences had lengths randomly chosen between 0 to $L$ , for $L\\in \\lbrace 10, 50, 100, 200\\rbrace $ unique to each dataset.\n\n[Document 2]: Next, we explore if the memory-based attention mechanism is able to fit complex real-world datasets. For this purpose we use 4 large machine translation datasets of WMT'17 on the following language pairs: English-Czech (en-cs, 52M examples), English-German (en-de, 5.9M examples), English-Finish (en-fi, 2.6M examples), and English-Turkish (en-tr, 207,373 examples). We used the newly available pre-processed datasets for the WMT'17 task. Note that our scores may not be directly comparable to other work that performs their own data pre-processing. We learn shared vocabularies of 16,000 subword units using the BPE algorithm BIBREF19 . We use newstest2015 as a validation set, and report BLEU on newstest2016.\n\nQuestion: Which datasets are used in experiments?\n\nExplanation: \n\n[Document 1] mentions the Sequence Copy Task dataset, while [Document 2] mentions the WMT'17 dataset.\n\nAnswer: Sequence Copy Task and WMT'17\n\nExample 3:\n\n[Document 1]: Our experiments are run using the English$\\rightarrow $Italian/German portions of the MuST-C corpus BIBREF25, which is extracted from TED talks, using the same train/validation/test split as provided with the corpus (see Table TABREF18). As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De). While our main goal is to verify our hypotheses on a large data condition, thus the need to include proprietary data, for the sake of reproducibility in both languages we also provide results with systems only trained on TED Talks (small data condition). When training on large scale data we use Transformer with layer size of 1024, hidden size of 4096 on feed forward layers, 16 heads in the multi-head attention, and 6 layers in both encoder and decoder. When training only on TED talks, we set layer size of 512, hidden size of 2048 for the feed forward layers, multi-head attention with 8 heads and again 6 layers in both encoder and decoder.\n\nQuestion: What dataset do they use for experiments?\n\nExplanation:  The dataset used for experiments is described in [Document 1].\n\nAnswer: English$\\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)\n\nExample 4:\n\n[Document 1]: We conduct experiments on the AliMe and Quora dataset for the query-bag matching based information-seeking conversation. Compared with baselines, we verify the effectiveness of our model. Our model obtains 0.05 and 0.03 $\\text{R}_{10}@1$ gains comparing to the strongest baseline in the two datasets. The ablation study shows the usefulness of the components. The contributions in this paper are summarized as follows: 1) To the best of our knowledge, we are the first to adopt query-bag matching in the information-seeking conversation. 2) We propose the mutual coverage model to measure the information coverage in the query-bag matching. 3) We release the composite Quora dataset to facilitate the research in this area.\n\nQuestion: What datasets are used for experiments?\n\nExplanation:"}
{"question_id": "4ae0b50c88a174cfc283b90cd3c9407de13fd370", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: In this study, we tackle a task of describing (defining) a phrase when given its local context as BIBREF2 , while allowing access to other usage examples via word embeddings trained from massive text (global contexts) BIBREF0 , BIBREF1 . We present LOG-Cad, a neural network-based description generator (Figure FIGREF1 ) to directly solve this task. Given a word with its context, our generator takes advantage of the target word's embedding, pre-trained from massive text (global contexts), while also encoding the given local context, combining both to generate a natural language description. The local and global contexts complement one another and are both essential; global contexts are crucial when local contexts are short and vague, while the local context is crucial when the target phrase is polysemous, rare, or unseen.\n\nQuestion: Do they use pretrained word embeddings?\n\nExplanation:  [Document 1] mentions that the system \"takes advantage of the target word's embedding, pre-trained from massive text (global contexts)\". This means that the word embeddings are pretrained.\n\nAnswer: Yes\n\nExample 2:\n\n[Document 1]: Trafficking detection: There have been several software products designed to aid anti-trafficking efforts. Examples include Memex which focuses on search functionalities in the dark web; Spotlight which flags suspicious ads and links images appearing in multiple ads; Traffic Jam which seeks to identify patterns that connect multiple ads to the same trafficking organization; and TraffickCam which aims to construct a crowd-sourced database of hotel room images to geo-locate victims. These research efforts have largely been isolated, and few research articles on machine learning for trafficking detection have been published. Closest to our work is the Human Trafficking Deep Network (HTDN) BIBREF9 . HTDN has three main components: a language network that uses pretrained word embeddings and a long short-term memory network (LSTM) to process text input; a vision network that uses a convolutional network to process image input; and another convolutional network to combine the output of the previous two networks and produce a binary classification. Compared to the language network in HTDN, our model replaces LSTM with a gated-feedback recurrent neural network, adopts certain regularizations, and uses an ordinal regression layer on top. It significantly improves HTDN's benchmark despite only using text input. As in the work of E. Tong et al. ( BIBREF9 ), we pre-train word embeddings using a skip-gram model BIBREF4 applied to unlabeled data from escort ads, however, we go further by analyzing the emojis' embeddings and thereby expand the trafficking lexicon.\n\nQuestion: Do they use pretrained word embeddings?\n\nExplanation:  [Document 1] mentions that the model uses \"pretrained word embeddings\".\n\nAnswer: Yes\n\nExample 3:\n\nQuestion: Do they use pretrained word vectors for dialogue context embedding?\n\nExplanation: \n\n[Document 1]: We use pre-trained word vectors for dialogue context embedding, which are fine-tuned during training.\n\n[Document 2]: We use pre-trained word vectors for dialogue context embedding, which are fine-tuned during training.\n\n[Document 3]: We use pre-trained word vectors for dialogue context embedding, which are fine-tuned during training.\n\nAccording to [Document 1], [Document 2], and [Document 3], the word vectors used for dialogue context embedding are pretrained and fine-tuned during training.\n\nAnswer: Yes\n\nExample 4:\n\n[Document 1]: In this effort, we develop our supervised WSD model that leverages a Bidirectional Long Short-Term Memory (BLSTM) network. This network works with neural sense vectors (i.e. sense embeddings), which are learned during model training, and employs neural word vectors (i.e. word embeddings), which are learned through an unsupervised deep learning approach called GloVe (Global Vectors for word representation) BIBREF2 for the context words. By evaluating our one-model-fits-all WSD network over the public gold standard dataset of SensEval-3 BIBREF3 , we demonstrate that the accuracy of our model in terms of F-measure is comparable with the state-of-the-art WSD algorithms'.\n\nQuestion: Do they use pretrained word embeddings?\n\nExplanation:"}
{"question_id": "61a2599acfbd3d75de58e97ecdba2d9cf0978324", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: What are the baseline systems that are compared against?\n\nExplanation: \n\n[Document 1]: In this work, we propose a novel approach for learning the latent tree structure in Tree-LSTM models. We compare our method against three baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM.\n\n[Document 2]: The LSTM model is a special kind of RNN, which is designed to model long-term dependencies.\n\n[Document 3]: The RL-SPINN model is a recurrent neural network that uses reinforcement learning to learn how to build a tree structure.\n\n[Document 4]: The Gumbel Tree-LSTM is a tree-structured LSTM model that uses the Gumbel-Softmax distribution to learn the tree structure.\n\nAnswer: The system is compared to baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM\n\nExample 2:\n\nQuestion: What are the competing models?\n\nExplanation: \n\n[Document 1]: We compare the performance of our proposed method with several state-of-the-art methods, including Teacher Forcing (TF) BIBREF1 , Scheduled Sampling (SS) BIBREF2 , SeqGAN BIBREF3 , RankGAN BIBREF4 , and LeakGAN BIBREF5 .\n\n[Document 2]: Teacher Forcing is a method for training recurrent neural networks (RNNs) that has been shown to be effective in practice BIBREF6 . The idea is to use the true output sequence y as the input to the RNN at each time step, rather than using the predicted output sequence y\u2032.\n\n[Document 3]: Scheduled Sampling is a method for training recurrent neural networks (RNNs) that has been shown to be effective in practice BIBREF7 . The idea is to use the predicted output sequence y\u2032 as the input to the RNN at each time step, rather than using the true output sequence y.\n\n[Document 4]: SeqGAN is a method for training recurrent neural networks (RNNs) that has been shown to be effective in practice BIBREF8 . The idea is to use a generative adversarial\n\nAnswer: TEACHER FORCING (TF), SCHEDULED SAMPLING (SS),  SEQGAN, RANKGAN, LEAKGAN.\n\nExample 3:\n\nQuestion: What is a strong feature-based method?\n\nExplanation:  The documents do not provide a definition for \"strong feature-based method.\"\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: The previous work that is most relevant to our work here is that of BIBREF9, who developed the ClaimBuster system, which assigns each sentence in a document a score, i.e., a number between 0 and 1 showing how worthy it is of fact-checking. The system is trained on their own dataset of about eight thousand debate sentences (1,673 of them check-worthy), annotated by students, university professors, and journalists. Unfortunately, this dataset is not publicly available and it contains sentences without context as about 60% of the original sentences had to be thrown away due to lack of agreement.\n\nQuestion: What is the strong rivaling system?\n\nExplanation:"}
{"question_id": "8330242b56b63708a23c6a92db4d4bcf927a4576", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Our annotation guidelines are similar to the guidelines used by BIBREF9 . We define hateful speech to be the language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation. The labeling of hateful speech in our corpus is binary. A comment will be labeled as hateful or non-hateful.\n\nQuestion: What is their definition of hate speech?\n\nExplanation:  The definition of hate speech according to [Document 1] is \"language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation.\"\n\nAnswer: language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation\n\nExample 2:\n\nQuestion: What definition of hate speech do they use?\n\nExplanation:  The documents do not provide a definition of hate speech.\n\nAnswer: Unanswerable\n\nExample 3:\n\n[Document 1]: Participants who were shown the definition were more likely to suggest to ban the tweet. In fact, participants in group one very rarely gave different answers to questions one and two (18 of 500 instances or 3.6%). This suggests that participants in that group aligned their own opinion with the definition.\n\nQuestion: How did the authors demonstrate that showing a hate speech definition caused annotators to partially align their own opinion with the definition?\n\nExplanation:  The authors demonstrate that showing a hate speech definition caused annotators to partially align their own opinion with the definition by stating that participants in group one very rarely gave different answers to questions one and two (18 of 500 instances or 3.6%).\n\nAnswer: participants in group one very rarely gave different answers to questions one and two (18 of 500 instances or 3.6%)\n\nExample 4:\n\nQuestion: What types of hate speech are considered?\n\nExplanation:"}
{"question_id": "e96b0d64c8d9fdd90235c499bf1ec562d2cbb8b2", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Majority: the text picks the label of the largest size.\n\n[Document 2]: ESA: A dataless classifier proposed in BIBREF0. It maps the words (in text and label names) into the title space of Wikipedia articles, then compares the text with label names. This method does not rely on train.\n\n[Document 3]: We implemented ESA based on 08/01/2019 Wikipedia dump. There are about 6.1M words and 5.9M articles.\n\n[Document 4]: Word2Vec BIBREF23: Both the representations of the text and the labels are the addition of word embeddings element-wisely. Then cosine similarity determines the labels. This method does not rely on train either.\n\n[Document 5]: Binary-BERT: We fine-tune BERT on train, which will yield a binary classifier for entailment or not; then we test it on test \u2013 picking the label with the maximal probability in single-label scenarios while choosing all the labels with \u201centailment\u201d decision in multi-label cases.\n\nQuestion: What are their baseline models?\n\nExplanation:  [Document 1], [Document 2], [Document 4], and [Document 5] all mention different baseline models. [Document 3] provides additional evidence that ESA is a baseline model by describing how it was implemented.\n\nAnswer: Majority, ESA, Word2Vec , Binary-BERT\n\nExample 2:\n\n[Document 1]: We compared our proposed approaches to a standard CNN that exploits a single set of word embeddings BIBREF3 . We also compared to a baseline of simply concatenating embeddings for each word to form long vector inputs. We refer to this as Concatenation-CNN C-CNN. For all multiple embedding approaches (C-CNN, MG-CNN and MGNC-CNN), we explored two combined sets of embedding: word2vec+Glove, and word2vec+syntactic, and one three sets of embedding: word2vec+Glove+syntactic. For all models, we tuned the l2 norm constraint INLINEFORM0 over the range INLINEFORM1 on a validation set. For instantiations of MGNC-CNN in which we exploited two embeddings, we tuned both INLINEFORM2 , and INLINEFORM3 ; where we used three embedding sets, we tuned INLINEFORM4 and INLINEFORM5 .\n\nQuestion: What are the baseline models?\n\nExplanation: \n\nAccording to [Document 1], the baseline models are MC-CNN, MVCNN, and CNN. This information is further supported by [Document 2], [Document 3], and [Document 5].\n\nAnswer: MC-CNN\nMVCNN\nCNN\n\nExample 3:\n\n[Document 1]: Methods ::: Models Tested ::: Recurrent Neural Network (RNN) Language Models\n\n[Document 2]: are trained to output the probability distribution of the upcoming word given a context, without explicitly representing the structure of the context BIBREF9, BIBREF10. We trained two two-layer recurrent neural language models with long short-term memory architecture BIBREF11 on a relatively small corpus. The first model, referred as `LSTM (PTB)' in the following sections, was trained on the sentences from Penn Treebank BIBREF12. The second model, referred as `LSTM (FTB)', was trained on the sentences from French Treebank BIBREF13. We set the size of input word embedding and LSTM hidden layer of both models as 256.\n\n[Document 3]: Methods ::: Models Tested ::: ActionLSTM\n\n[Document 4]: models the linearized bracketed tree structure of a sentence by learning to predict the next action required to construct a phrase-structure parse BIBREF16. The action space consists of three possibilities: open a new non-terminal node and opening bracket; generate a terminal node; and close a bracket. To compute surprisal values for a given token, we approximate $P(w_i|w_{1\\cdots i-1)}$ by marginalizing over the most-likely partial parses found by word-synchronous beam search BIBREF17.\n\n[Document 5]: Methods ::: Models Tested ::: Generative Recurrent Neural Network Grammars (RNNG)\n\n[Document 6]: jointly model the word sequence as well as the underlying syntactic structure BIBREF18. Following BIBREF19, we estimate surprisal using word-synchronous beam search BIBREF17. We use the same hyper-parameter settings as BIBREF18.\n\nQuestion: What are the baseline models?\n\nExplanation:  [Document 1], [Document 3], and [Document 5] list the baseline models as RNN, ActionLSTM, and RNNG respectively. This information is further supported by [Document 2], [Document 4], and [Document 6].\n\nAnswer: Recurrent Neural Network (RNN), ActionLSTM, Generative Recurrent Neural Network Grammars (RNNG)\n\nExample 4:\n\n[Document 1]: Our first experiment measures tagging and parsing accuracy on the TLE and approximates the global impact of grammatical errors on automatic annotation via performance comparison between the original and error corrected sentence versions. In this, and subsequent experiments, we utilize version 2.2 of the Turbo tagger and Turbo parser BIBREF18 , state of the art tools for statistical POS tagging and dependency parsing.\n\nQuestion: What are their baseline models?\n\nExplanation:"}
{"question_id": "ec5e84a1d1b12f7185183d165cbb5eae66d9833e", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Three different datasets have been used to train our models: the Toronto book corpus, Wikipedia sentences and tweets. The Wikipedia and Toronto books sentences have been tokenized using the Stanford NLP library BIBREF10 , while for tweets we used the NLTK tweets tokenizer BIBREF11 . For training, we select a sentence randomly from the dataset and then proceed to select all the possible target unigrams using subsampling. We update the weights using SGD with a linearly decaying learning rate.\n\n[Document 2]: Unsupervised Similarity Evaluation Results. In Table TABREF19 , we see that our Sent2Vec models are state-of-the-art on the majority of tasks when comparing to all the unsupervised models trained on the Toronto corpus, and clearly achieve the best averaged performance. Our Sent2Vec models also on average outperform or are at par with the C-PHRASE model, despite significantly lagging behind on the STS 2014 WordNet and News subtasks. This observation can be attributed to the fact that a big chunk of the data that the C-PHRASE model is trained on comes from English Wikipedia, helping it to perform well on datasets involving definition and news items. Also, C-PHRASE uses data three times the size of the Toronto book corpus. Interestingly, our model outperforms C-PHRASE when trained on Wikipedia, as shown in Table TABREF21 , despite the fact that we use no parse tree information. Official STS 2017 benchmark. In the official results of the most recent edition of the STS 2017 benchmark BIBREF35 , our model also significantly outperforms C-PHRASE, and in fact delivers the best unsupervised baseline method.\n\nQuestion: Do they report results only on English data?\n\nExplanation:  [Document 1] and [Document 2] both report results on English data from the Toronto book corpus, Wikipedia sentences, and tweets.\n\nAnswer: Yes\n\nExample 2:\n\n[Document 1]: A serious challenge in the field of AV is the lack of publicly available (and suitable) corpora, which are required to train and evaluate AV methods. Among the few publicly available corpora are those that were released by the organizers of the well-known PAN-AV competitions BIBREF11 , BIBREF5 , BIBREF12 . In regard to our experiments, however, we cannot use these corpora, due to the absence of relevant meta-data such as the precise time spans where the documents have been written as well as the topic category of the texts. Therefore, we decided to compile our own corpora based on English documents, which we crawled from different publicly accessible sources. In what follows, we describe our three constructed corpora, which are listed together with their statistics in Table TABREF23 . Note that all corpora are balanced such that verification cases with matching (Y) and non-matching (N) authorships are evenly distributed.\n\nQuestion: Do they report results only on English data?\n\nExplanation:  [Document 1] states that the corpora are based on English documents.\n\nAnswer: Yes\n\nExample 3:\n\nQuestion: Do they report results only on English data?\n\nExplanation: \n\n[Document 1] reports results only on English data. This is supported by [Document 2], which states that the spaCy 2.0 algorithm performs within 1% of the current state-of-the-art for English.\n\nAnswer: Yes\n\nExample 4:\n\n[Document 1]: To account for spelling mistakes, rather than attempt to correct to a vocabulary of correctly spelled words, we constructed an embedding with a vocabulary that contains both correct and incorrectly spelled words. We do this by using standard algorithms BIBREF25 on a large corpus of student responses (approximately 160 million responses). The embedding we created reflects the imperfect manner in which students use words BIBREF26 . For example, while the words 'happems' and 'ocures' are both incorrectly spelled versions of 'happens' and 'occurs' respectively, our embedding exhibits a high cosine similarity between the word vectors of the correct and incorrect versions. The embedding we created was an embedding into 200 dimensional space with a vocabulary consisting of 1.12 million words. Using spelling dictionaries we approximate that the percentage of correctly spelled words in the vocabulary of this embedding is approximately 7%, or roughly 80,000 words, while the remaining 93% are either misspellings, made up words or words from other languages. Lastly, due to the prevalence of words that are concatenated (due to a missing space), we split up any word with a Levenstein distance that is greater than two from our vocabulary into smaller words that are in the vocabulary. This ensures that any sentence is tokenized into a list of elements, almost all of which have valid embeddings.\n\nQuestion: Do they report results only on English data?\n\nExplanation:"}
{"question_id": "8795bb1f874e5f3337710d8c3d5be49e672ab43a", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: The data presented in this paper was collected and validated via Mozilla's Common Voice initiative. Using either the Common Voice website or iPhone app, contributors record their voice by reading sentences displayed on the screen (see Figure (FIGREF5)). The recordings are later verified by other contributors using a simple voting system. Shown in Figure (FIGREF6), this validation interface has contributors mark $<$audio,transcript$>$ pairs as being either correct (up-vote) or incorrect (down-vote).\n\nQuestion: What crowdsourcing platform is used for data collection and data validation?\n\nExplanation: \n\nAccording to [Document 1], the data was collected and validated via Mozilla's Common Voice initiative. This information is further supported by [Document 2], which states that the Common Voice website and iPhone app are used for data collection and data validation.\n\nAnswer: the Common Voice website,  iPhone app\n\nExample 2:\n\n[Document 1]: We use crowdsourcing to annotate different aspects of the dataset. We used Amazon Mechanical Turk (AMT) for our annotations, restricting the task to workers in five English-speaking countries (USA, UK, Canada, New Zealand, and Australia), more than 1000 finished HITs and at least a 95% acceptance rate. To ensure the diversity of responses, we do not require additional qualifications or demographic information from our annotators.\n\nQuestion: What crowdsourcing platform did they use?\n\nExplanation:  [Document 1] mentions that Amazon Mechanical Turk (AMT) was used for the annotations.\n\nAnswer: Amazon Mechanical Turk (AMT)\n\nExample 3:\n\n[Document 1]: We recruited 176 AMT workers to participate in our conceptualization task. Of these workers, 90 were randomly assigned to the Control group and 86 to the AUI group. These workers completed 1001 tasks: 496 tasks in the control and 505 in the AUI. All responses were gathered within a single 24-hour period during April, 2017.\n\nQuestion: What crowdsourcing platform was used?\n\nExplanation: \n\n[Document 1] mentions that the workers were recruited from AMT, so we can infer that AMT was the crowdsourcing platform used.\n\nAnswer: AMT\n\nExample 4:\n\n[Document 1]: We crowdsourced questions about these paragraphs on Mechanical Turk. We asked workers to find two or more co-referring spans in the paragraph, and to write questions such that answering them would require the knowledge that those spans are coreferential. We did not ask them to explicitly mark the co-referring spans. Workers were asked to write questions for a random sample of paragraphs from our pool, and we showed them examples of good and bad questions in the instructions (see Appendix ). For each question, the workers were also required to select one or more spans in the corresponding paragraph as the answer, and these spans are not required to be same as the coreferential spans that triggered the questions. We used an uncased base BERT QA model BIBREF9 trained on SQuAD 1.1 BIBREF0 as an adversary running in the background that attempted to answer the questions written by workers in real time, and the workers were able to submit their questions only if their answer did not match the adversary's prediction. Appendix further details the logistics of the crowdsourcing tasks. Some basic statistics of the resulting dataset can be seen in Table .\n\nQuestion: What crowdsourcing platform did they obtain the data from?\n\nExplanation:"}
{"question_id": "818c85ee26f10622c42ae7bcd0dfbdf84df3a5e0", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: What are the 12 categories devised?\n\nExplanation: \n\nThe 12 categories are listed in [Document 1].\n\nAnswer: Economics, Genocide, Geography, History, Human Rights, Kurdish, Kurdology, Philosophy, Physics, Theology, Sociology, Social Study\n\nExample 2:\n\n[Document 1]: In this section, we describe our approach to classify user reactions into one of eight types of discourse: agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, or question, or as none of the given labels, which we call \u201cother\u201d, using linguistically-infused neural network models.\n\nQuestion: What are the nine types?\n\nExplanation: \n\nAccording to [Document 1], there are nine types of discourse: agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, question, and other.\n\nAnswer: agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, question, other\n\nExample 3:\n\nQuestion: What are the predefined morpho-syntactic patterns used to filter the training data?\n\nExplanation:  The question cannot be answered with the given documents.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: FLOAT SELECTED: Table 3: Categories of Movie Reviews\n\nQuestion: What are the eight predefined categories?\n\nExplanation:"}
{"question_id": "141f23e87c10c2d54d559881e641c983e3ec8ef3", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: To compare our results, we use the provided baseline, which is a non-parameter optimized linear-kernel SVM that uses TF-IDF bag-of-word vectors as inputs. For pre-processing, in this case we do not preserve casing and delete English stopwords.\n\nQuestion: What was the baseline model?\n\nExplanation:  [Document 1] states that the baseline model was a non-parameter optimized linear-kernel SVM that uses TF-IDF bag-of-word vectors as inputs.\n\nAnswer: a non-parameter optimized linear-kernel SVM that uses TF-IDF bag-of-word vectors as inputs\n\nExample 2:\n\n[Document 1]: Next, we compared our VTN model with an RNN-based seq2seq VC model called ATTS2S BIBREF8. This model is based on the Tacotron model BIBREF32 with the help of context preservation loss and guided attention loss to stabilize training and maintain linguistic consistency after conversion. We followed the configurations in BIBREF8 but used mel spectrograms instead of WORLD features.\n\nQuestion: What is the baseline model?\n\nExplanation: \n\nAccording to [Document 1], the baseline model is a RNN-based seq2seq VC model called ATTS2S based on the Tacotron model.\n\nAnswer: a RNN-based seq2seq VC model called ATTS2S based on the Tacotron model\n\nExample 3:\n\nQuestion: What is the baseline model?\n\nExplanation:  There is not enough information provided to answer the question.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: Models. As the baseline model, we used BERT-large BIBREF3. We fine-tuned it on the original training set of each dataset and evaluated it on a modified development set. For $\\sigma _4$ vocabulary anonymization, we train the model after the anonymization. For ARC, MCTest, and MultiRC, we fine-tuned a model that had already been trained on RACE to see the performance gained by transfer learning BIBREF27. We report the hyperparameters of our models in Appendix C. Although we trained the baseline model on the original training set, it is assumed that the upper-bound performance can be achieved by a model trained on the modified training set. Therefore, in Section SECREF16, we also see the extent to which the performance improves when the model is trained on the modified training set.\n\nQuestion: what was the baseline model?\n\nExplanation:"}
{"question_id": "71fd0efea1b441d86d9a75255815ba3efe09779b", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: How do the authors measure performance?\n\nExplanation: \n\n[Document 1]: In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am. This dataset is comparable in size with the test sets of other languages (Table TABREF10 ). Included sentences are from political, sports, local and world news (Figures FIGREF8 , FIGREF9 ), covering the period between August 2012 and July 2018. The dataset provides annotations for 3 popular named entity classes: people (PER), organizations (ORG), and locations (LOC), and is released in CoNLL03 format with IOB tagging scheme. Tokens and sentences were segmented according to the UD standards for the Armenian language BIBREF11 .\n\n[Document 2]: We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 3]: The main model that we focused on was the recurrent model with a CRF\n\nAnswer: Accuracy across six datasets\n\nExample 2:\n\n[Document 1]: Several promising approaches to interpreting RNNs have been developed recently. BIBREF3 have approached this by using gradient boosting trees to predict LSTM output probabilities and explain which features played a part in the prediction. They do not model the internal structure of the LSTM, but instead approximate the entire architecture as a black box. BIBREF4 showed that in LSTM language models, around 10% of the memory state dimensions can be interpreted with the naked eye by color-coding the text data with the state values; some of them track quotes, brackets and other clearly identifiable aspects of the text. Building on these results, we take a somewhat more systematic approach to looking for interpretable hidden state dimensions, by using decision trees to predict individual hidden state dimensions (Figure 2 ). We visualize the overall dynamics of the hidden states by coloring the training data with the k-means clusters on the state vectors (Figures 3 , 3 ).\n\n[Document 2]: We interpret the HMM and LSTM states in the hybrid algorithm with 10 LSTM state dimensions and 10 HMM states in Figures 3 and 3 , showing which features are identified by the HMM and LSTM components. In Figures 3 and 3 , we color-code the training data with the 10 HMM states. In Figures 3 and 3 , we apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters. The HMM and LSTM states pick up on spaces, indentation, and special characters in the data (such as comment symbols in Linux data). We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data. In Figure 2 , we see that some individual LSTM hidden state dimensions identify similar features, such as comment symbols in the Linux data.\n\nQuestion: Which methods do the authors use to reach the conclusion that LSTMs and HMMs learn complementary information?\n\nExplanation:  The authors use decision trees to predict individual hidden state dimensions and apply k-means clustering to the LSTM state vectors. By color-coding the training data with the clusters, the authors are able to reach the conclusion that LSTMs and HMMs learn complementary information.\n\nAnswer: decision trees to predict individual hidden state dimensions, apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters\n\nExample 3:\n\n[Document 1]: The results for the major features and minor features are shown in Figures FIGREF26 and FIGREF35 , respectively. For each feature, we measure the MCC of the sentences including that feature. We plot the mean of these results across the different restarts for each model, and error bars mark the mean INLINEFORM0 standard deviation. For the Violations features, MCC is technically undefined because these features only contain unacceptable sentences. We report MCC in these cases by including for each feature a single acceptable example that is correctly classified by all models.\n\n[Document 2]: The most challenging features are all related to Violations. Low performance on Infl/Agr Violations, which marks morphological violations (He washed yourself, This is happy), is especially striking because a relatively high proportion (29%) of these sentences are Simple. These models are likely to be deficient in encoding morphological features is that they are word level models, and do not have direct access sub-word information like inflectional endings, which indicates that these features are difficult to learn effectively purely from lexical distributions.\n\nQuestion: Do the authors have a hypothesis as to why morphological agreement is hardly learned by any model?\n\nExplanation:  The answer to the question can be found in [Document 2].\n\nAnswer: These models are likely to be deficient in encoding morphological features is that they are word level models, and do not have direct access sub-word information like inflectional endings, which indicates that these features are difficult to learn effectively purely from lexical distributions.\n\nExample 4:\n\n[Document 1]: The first syntaxes that LGI has learned are the \u2018move left\u2019 and \u2018move right\u2019 random pixels, with the corresponding results shown in Figure 3. After 50000 steps training, LGI could not only reconstruct the input image with high precision but also predict the 'mentally' moved object with specified morphology, correct manipulated direction and position just after the command sentence completed. The predicted text can complete the word \u2018move\u2019 given the first letter \u2018m\u2019 (till now, LGI has only learned syntaxes of \u2018move left or right\u2019). LGI tried to predict the second word \u2018right\u2019 with initial letter \u2018r\u2019, however, after knowing the command text is \u2018l\u2019, it turned to complete the following symbols with \u2018eft\u2019. It doesn\u2019t care if the sentence length is 12 or 11, the predicted image and text just came at proper time and position. Even if the command asked to move out of screen, LGI still could reconstruct the partially occluded image with high fidelity.\n\n[Document 2]: Based on the same network, LGI continued to learn syntax \u2018this is \u2026\u2019. Just like a parent teaching child numbers by pointing to number instances, Figure 4 demonstrates that, after training of 50000 steps, LGI could classify figures in various morphology with correct identity (accuracy = 72.7%). Note that, the classification process is not performed by softmax operation, but by directly textizing operation (i.e. rounding followed by a symbol mapping operation), which is more biologically plausible than the softmax operation.\n\nQuestion: How do the authors measure the extent to which LGI has learned the task?\n\nExplanation:"}
{"question_id": "347dc2fd6427b39cf2358d43864750044437dff8", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: How big is the Universal Dependencies corpus?\n\nExplanation:  The documents do not mention the size of the Universal Dependencies corpus.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: In task 1, there are two top categories, namely, chit-chat and task-oriented dialogue. The task-oriented dialogue also includes 30 sub categories. In this evaluation, we only consider to classify the user intent in single utterance.\n\nQuestion: How many intents were classified?\n\nExplanation: \n\nAccording to [Document 1], there are two top categories, namely, chit-chat and task-oriented dialogue.\n\nAnswer: two\n\nExample 3:\n\n[Document 1]: We created eight different classifiers, each of which used one of the following eight features available from a tweet as retrieved from a stream of the Twitter API:\n\n[Document 2]: User location (uloc): This is the location the user specifies in their profile. While this feature might seem a priori useful, it is somewhat limited as this is a free text field that users can leave empty, input a location name that is ambiguous or has typos, or a string that does not match with any specific locations (e.g., \u201cat home\u201d). Looking at users' self-reported locations, Hecht et al. BIBREF49 found that 66% report information that can be translated, accurately or inaccurately, to a geographic location, with the other 34% being either empty or not geolocalisable.\n\n[Document 3]: User language (ulang): This is the user's self-declared user interface language. The interface language might be indicative of the user's country of origin; however, they might also have set up the interface in a different language, such as English, because it was the default language when they signed up or because the language of their choice is not available.\n\n[Document 4]: Timezone (tz): This indicates the time zone that the user has specified in their settings, e.g., \u201cPacific Time (US & Canada)\u201d. When the user has specified an accurate time zone in their settings, it can be indicative of their country of origin; however, some users may have the default time zone in their settings, or they may use an equivalent time zone belonging to a different location (e.g., \u201cEurope/London\u201d for a user in Portugal). Also, Twitter's list of time zones does not include all countries.\n\n[Document 5]: Tweet language (tlang): The language in which a tweet is believed to be written is automatically detected by Twitter. It has been found to be accurate for major languages, but it leaves much to be desired for less widely used languages. Twitter's language identifier has also been found to struggle with multilingual tweets, where parts of a tweet are written in different languages BIBREF50 .\n\n[Document 6]: Offset (offset): This is the offset, with respect to UTC/GMT, that the user has specified in their settings. It is similar to the time zone, albeit more limited as it is shared with a number of countries.\n\n[Document 7]: User name (name): This is the name that the user specifies in their settings, which can be their real name, or an alternative name they choose to use. The name of a user can reveal, in some cases, their country of origin.\n\n[Document 8]: User description (description): This is a free text where a user can describe themselves, their interests, etc.\n\n[Document 9]: Tweet content (content): The text that forms the actual content of the tweet. The use of content has a number of caveats. One is that content might change over time, and therefore new tweets might discuss new topics that the classifiers have not seen before. Another caveat is that the content of the tweet might not be location-specific; in a previous study, Rakesh et al. BIBREF51 found that the content of only 289 out of 10,000 tweets was location-specific.\n\nQuestion: What are the eight features mentioned?\n\nExplanation:  The eight features are user location (uloc), user language (ulang), timezone (tz), tweet language (tlang), offset (offset), user name (name), user description (description), and tweet content (content). This information is supported by [Document 1], [Document 2], [Document 3], [Document 4], [Document 5], [Document 6], [Document 7], and [Document 8].\n\nAnswer: User location (uloc), User language (ulang), Timezone (tz), Tweet language (tlang), Offset (offset), User name (name), User description (description), Tweet content (content)\n\nExample 4:\n\nQuestion: How many Universal Dependency features are considered?\n\nExplanation:"}
{"question_id": "539f5c27e1a2d240e52b711d0a50a3a6ddfa5cb2", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: For in-cabin intent understanding, we described 4 groups of usages to support various natural commands for interacting with the vehicle: (1) Set/Change Destination/Route (including turn-by-turn instructions), (2) Set/Change Driving Behavior/Speed, (3) Finishing the Trip Use-cases, and (4) Others (open/close door/window/trunk, turn music/radio on/off, change AC/temperature, show map, etc.). According to those scenarios, 10 types of passenger intents are identified and annotated as follows: SetDestination, SetRoute, GoFaster, GoSlower, Stop, Park, PullOver, DropOff, OpenDoor, and Other. For slot filling task, relevant slots are identified and annotated as: Location, Position/Direction, Object, Time Guidance, Person, Gesture/Gaze (e.g., `this', `that', `over there', etc.), and None/O. In addition to utterance-level intents and slots, word-level intent related keywords are annotated as Intent. We obtained 1331 utterances having commands to AMIE agent from our in-cabin dataset. We expanded this dataset via the creation of similar tasks on Amazon Mechanical Turk BIBREF21 and reached 3418 utterances with intents in total. Intent and slot annotations are obtained on the transcribed utterances by majority voting of 3 annotators. Those annotation results for utterance-level intent types, slots and intent keywords can be found in Table TABREF7 and Table TABREF8 as a summary of dataset statistics.\n\nQuestion: Are the intent labels imbalanced in the dataset?\n\nExplanation:  The intent labels are imbalanced in the dataset according to [Document 1].\n\nAnswer: Yes\n\nExample 2:\n\n[Document 1]: Two-talker overlapped speech is artificially generated by mixing these waveform segments. To maximize the speech overlap, we developed a procedure to mix similarly sized segments at around 0dB. First, we sort the speech segments by length. Then, we take segments in pairs, zero-padding the shorter segment so both have the same length. These pairs are then mixed together to create the overlapped speech data. The overlapping procedure is similar to BIBREF13 except that we make no modification to the signal levels before mixing . After overlapping, there's 150 hours data in the training, called 150 hours dataset, and 915 utterances in the test set. After decoding, there are 1830 utterances for evaluation, and the shortest utterance in the hub5e-swb dataset is discarded. Additionally, we define a small training set, the 50 hours dataset, as a random 50 hour subset of the 150 hours dataset. Results are reported using both datasets.\n\nQuestion: How are the two datasets artificially overlapped?\n\nExplanation:  [Document 1] explains that the two datasets are artificially overlapped by sorting the speech segments by length, taking segments in pairs, zero-padding the shorter segment so both have the same length, and then mixing the pairs together.\n\nAnswer: we sort the speech segments by length, we take segments in pairs, zero-padding the shorter segment so both have the same length, These pairs are then mixed together\n\nExample 3:\n\n[Document 1]: We submit 4,000 segments, sampled equally from each domain and uniformly within domains, to Mechanical Turk to be annotated by at least five Workers each and include in each batch one of 50 \u201ccheck questions\u201d which have been previously verified by two in-house annotators. After removing annotations which failed the check questions, and data points for which at least half of the annotators selected \u201cCan't Tell\u201d, we are left with 3,553 labeled data points from 2,929 different posts. We take the annotators' majority vote as the label for each segment and record the percentage of annotators who agreed. The resulting dataset is nearly balanced, with 52.3% of the data (1,857 instances) labeled stressful.\n\nQuestion: Is the dataset balanced across categories?\n\nExplanation:  The dataset is nearly balanced, with 52.3% of the data (1,857 instances) labeled stressful.\n\nAnswer: Yes\n\nExample 4:\n\n[Document 1]: Imbalanced dataset is a challenge for the task because the top patterns are taking too much attention so that most weights might be determined by the easier ones. We have tried different methods to deal with this problem. The first method is data expansion using oversampling. Attempts include duplicating the text with low pattern proportion, replacing first few characters with paddings in the window, randomly changing digits, and shifting the context window. The other method is to add loss control in the model as mentioned in SECREF2. The loss function helps the model to focus on harder cases in different classes and therefore reduce the impact of the imbalanced data. The experimental results are in SECREF11.\n\nQuestion: How do they deal with imbalanced datasets?\n\nExplanation:"}
{"question_id": "b79c85fa84712d3028cb5be2af873c634e51140e", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: The UNGA speeches dataset, compiled by Baturo et al. UNGAspeeches, contains the text from 7,507 speeches given between 1970-2015 inclusive. Over the course of this period a variety of topics are discussed, with many debated throughout (such as nuclear disarmament). Although the linguistic style has changed over this period, the context of these speeches constrains the variability to the formal domain. Before training the model, the dataset is split into 283,593 paragraphs, cleaned by removing paragraph deliminators and other excess noise, and tokenized using the spaCy tokenizer BIBREF4 .\n\nQuestion: how many speeches are in the dataset?\n\nExplanation:  [Document 1] states that the dataset contains the text from 7,507 speeches.\n\nAnswer: 7,507\n\nExample 2:\n\nQuestion: How many speeches are in the dataset?\n\nExplanation: \n\n[Document 1]: The dataset contains a total of 5575 speeches, with an average of 2439 words per speech.\n\nThis document provides the answer to the question directly.\n\nAnswer: 5575 speeches\n\nExample 3:\n\n[Document 1]: Table TABREF13 shows the inconsistency rates of back translation between Adv-C and our method on MUSE. Compared with Adv-C, our model significantly reduces the inconsistency rates on all language pairs, which explains the overall improvement in Table TABREF12. Table TABREF14 gives several word translation examples. In the first three cases, our regularizer successfully fixes back translation errors. In the fourth case, ensuring cycle consistency does not lead to the correct translation, which explains some errors by our system. In the fifth case, our model finds a related word but not the same word in the back translation, due to the use of cosine similarity for regularization.\n\nQuestion: What 6 language pairs is experimented on?\n\nExplanation: \n\nAccording to [Document 1], the inconsistency rates of back translation were compared between Adv-C and our method on MUSE for six language pairs: EN<->ES, EN<->DE, EN<->IT, EN<->EO, EN<->MS, and EN<->FI.\n\nAnswer: EN<->ES\nEN<->DE\nEN<->IT\nEN<->EO\nEN<->MS\nEN<->FI\n\nExample 4:\n\n[Document 1]: Using Searle's speech act taxonomy BIBREF3 , we established a list of six speech act categories that are commonly seen on Twitter: Assertion, Recommendation Expression, Question, Request, and Miscellaneous. Table TABREF1 shows an example tweet for each of these categories.\n\nQuestion: Which six speech acts are included in the taxonomy?\n\nExplanation:"}
{"question_id": "452e2d7d7d9e1bb4914903479cd7caff9f6fae42", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: In this study, we tackle a task of describing (defining) a phrase when given its local context as BIBREF2 , while allowing access to other usage examples via word embeddings trained from massive text (global contexts) BIBREF0 , BIBREF1 . We present LOG-Cad, a neural network-based description generator (Figure FIGREF1 ) to directly solve this task. Given a word with its context, our generator takes advantage of the target word's embedding, pre-trained from massive text (global contexts), while also encoding the given local context, combining both to generate a natural language description. The local and global contexts complement one another and are both essential; global contexts are crucial when local contexts are short and vague, while the local context is crucial when the target phrase is polysemous, rare, or unseen.\n\nQuestion: Do they use pretrained word embeddings?\n\nExplanation:  [Document 1] mentions that the system \"takes advantage of the target word's embedding, pre-trained from massive text (global contexts)\". This means that the word embeddings are pretrained.\n\nAnswer: Yes\n\nExample 2:\n\n[Document 1]: Trafficking detection: There have been several software products designed to aid anti-trafficking efforts. Examples include Memex which focuses on search functionalities in the dark web; Spotlight which flags suspicious ads and links images appearing in multiple ads; Traffic Jam which seeks to identify patterns that connect multiple ads to the same trafficking organization; and TraffickCam which aims to construct a crowd-sourced database of hotel room images to geo-locate victims. These research efforts have largely been isolated, and few research articles on machine learning for trafficking detection have been published. Closest to our work is the Human Trafficking Deep Network (HTDN) BIBREF9 . HTDN has three main components: a language network that uses pretrained word embeddings and a long short-term memory network (LSTM) to process text input; a vision network that uses a convolutional network to process image input; and another convolutional network to combine the output of the previous two networks and produce a binary classification. Compared to the language network in HTDN, our model replaces LSTM with a gated-feedback recurrent neural network, adopts certain regularizations, and uses an ordinal regression layer on top. It significantly improves HTDN's benchmark despite only using text input. As in the work of E. Tong et al. ( BIBREF9 ), we pre-train word embeddings using a skip-gram model BIBREF4 applied to unlabeled data from escort ads, however, we go further by analyzing the emojis' embeddings and thereby expand the trafficking lexicon.\n\nQuestion: Do they use pretrained word embeddings?\n\nExplanation:  [Document 1] mentions that the model uses \"pretrained word embeddings\". This is further supported by the fact that the model is based on the work of E. Tong et al. (BIBREF9), which also uses pretrained word embeddings.\n\nAnswer: Yes\n\nExample 3:\n\nQuestion: So we do not use pre-trained embedding in this case?\n\nExplanation: \n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task\n\nAnswer: Yes\n\nExample 4:\n\n[Document 1]: We experimented with a dataset of 16K annotated tweets made available by the authors of BIBREF0 . Of the 16K tweets, 3383 are labeled as sexist, 1972 as racist, and the remaining are marked as neither sexist nor racist. For the embedding based methods, we used the GloVe BIBREF5 pre-trained word embeddings. GloVe embeddings have been trained on a large tweet corpus (2B tweets, 27B tokens, 1.2M vocab, uncased). We experimented with multiple word embedding sizes for our task. We observed similar results with different sizes, and hence due to lack of space we report results using embedding size=200. We performed 10-Fold Cross Validation and calculated weighted macro precision, recall and F1-scores.\n\nQuestion: Are pretrained embeddings used?\n\nExplanation:"}
{"question_id": "f741d32b92630328df30f674af16fbbefcad3f93", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Optimizations ::: a) Parallel Scan Inference\n\n[Document 2]: The commutative properties of semiring algorithms allow flexibility in the order in which we compute $A(\\ell )$. Typical implementations of dynamic programming algorithms are serial in the length of the sequence. On parallel hardware, an appealing approach is a parallel scan ordering BIBREF35, typically used for computing prefix sums. To compute, $A(\\ell )$ in this manner we first pad the sequence length $T$ out to the nearest power of two, and then compute a balanced parallel tree over the parts, shown in Figure FIGREF21. Concretely each node layer would compute a semiring matrix multiplication, e.g. $ \\bigoplus _c \\ell _{t, \\cdot , c} \\otimes \\ell _{t^{\\prime }, c, \\cdot }$. Under this approach, we only need $O(\\log N)$ steps in Python and can use parallel GPU operations for the rest. Similar parallel approach can also be used for computing sequence alignment and semi-Markov models.\n\n[Document 3]: Optimizations ::: b) Vectorized Parsing\n\n[Document 4]: Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized. The log-partition for parsing is computed with the Inside algorithm. This algorithm must compute each width from 1 through T in serial; however it is important to parallelize each inner step. Assuming we have computed all inside spans of width less than $d$, computing the inside span of width $d$ requires computing for all $i$,\n\n[Document 5]: Optimizations ::: c) Semiring Matrix Operations\n\n[Document 6]: The two previous optimizations reduce most of the cost to semiring matrix multiplication. In the specific case of the $(\\sum , \\times )$ semiring these can be computed very efficiently using matrix multiplication, which is highly-tuned on GPU hardware. Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient. For instance, for matrices $T$ and $U$ of sized $N \\times M$ and $M \\times O$, we can broadcast with $\\otimes $ to a tensor of size $N \\times M \\times O$ and then reduce dim $M$ by $\\bigoplus $ at a huge memory cost. To avoid this issue, we implement custom CUDA kernels targeting fast and memory efficient tensor operations. For log, this corresponds to computing,\n\nQuestion: What baselines are used in experiments?\n\nExplanation:  [Document 1] and [Document 2] describe a parallel scan ordering which is used as a baseline in the experiments. [Document 3] and [Document 4] describe vectorized parsing which is used as a baseline in the experiments. [Document 5] and [Document 6] describe semiring matrix operations which are used as a baseline in the experiments.\n\nAnswer: Typical implementations of dynamic programming algorithms are serial in the length of the sequence, Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized, Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient\n\nExample 2:\n\n[Document 1]: We decided to evaluate our model using weighted F1-score, i.e., the per-class F1 score is calculated and averaged by weighting each label by its support. Notice, since our setup deviates from the shared task setup (single-label versus multi-label classification), the final evaluation metric is different. We will report on weighted F1-score for the development and test set (with simple macro averaging), but use Exact-Accuracy and Micro F1 over all labels when presenting official results on the test sets. The latter two metrics were part of the official evaluation metrics. For details we refer the reader to the shared task overview paper BIBREF5 .\n\nQuestion: what evaluation metrics were used?\n\nExplanation: \n\nAccording to [Document 1], the evaluation metric used was weighted F1-score.\n\nAnswer: weighted F1-score\n\nExample 3:\n\n[Document 1]: A two-step validation was conducted for English-speaking customers. An initial A/B testing for the LR model in a production setting was performed to compare the labelling strategies. A second offline comparison of the models was conducted on historical data and a selected labelling strategy. One month of data and a subset of the customers was used (approx. eighty thousand). The sampled dataset presents a fraction of positive labels of approximately 0.5 for reuse and 0.2 for one-day return. Importantly, since this evaluation is done on a subset of users, the dataset characteristic's do not necessarily represent real production traffic. The joke corpus in this dataset contains thousands of unique jokes of different categories (sci-fi, sports, etc) and types (puns, limerick, etc). The dataset was split timewise into training/validation/test sets, and hyperparameters were optimized to maximize the AUC-ROC on the validation set. As a benchmark, we also consider two additional methods: a non-personalized popularity model and one that follows BIBREF16, replacing the transformer joke encoder with a CNN network (the specialized loss and other characteristics of the DL model are kept).\n\nQuestion: What evaluation metrics were used?\n\nExplanation:  According to [Document 1], AUC-ROC was used to evaluate the models.\n\nAnswer: AUC-ROC\n\nExample 4:\n\n[Document 1]: In this paper, we propose initial strong baselines which represent two complementary directions of research: one based on the aggregation of multilingual word embeddings, and another one, which directly learns multilingual sentence representations. Details on each approach are given in section \"Multilingual word representations\" and \"Multilingual sentence representations\" respectively. In contrast to previous works on cross-lingual document classification with RVC2, we explore training the classifier on all languages and transfer it to all others, ie. we do not limit our study to the transfer between English and a foreign language.\n\nQuestion: Which baselines were they used for evaluation?\n\nExplanation:"}
{"question_id": "c87966e7f497975b76a60f6be50c33d296a4a4e7", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Our annotation guidelines are similar to the guidelines used by BIBREF9 . We define hateful speech to be the language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation. The labeling of hateful speech in our corpus is binary. A comment will be labeled as hateful or non-hateful.\n\nQuestion: What is their definition of hate speech?\n\nExplanation:  The definition of hate speech according to [Document 1] is \"language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation.\"\n\nAnswer: language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation\n\nExample 2:\n\nQuestion: What definition of hate speech do they use?\n\nExplanation:  The documents do not provide a definition of hate speech.\n\nAnswer: Unanswerable\n\nExample 3:\n\nQuestion: How do the authors define fake news?\n\nExplanation:  The documents do not provide a definition for fake news.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: We asked our annotators to consider the full message thread for each tweet as displayed on Twitter's web interface. We also gave them a list of up to 15 recent mentions by the author of the tweet, directed towards any of the other accounts mentioned in the original thread. Then we asked annotators to interpret each tweet in light of this social context, and had them provide us with labels for five key cyberbullying criteria. We defined these criteria in terms of the author account (\u201cwho posted the given tweet?\u201d) and the target (\u201cwho was the tweet about?\u201d \u2013 not necessarily the first mention). We also stated that \u201cif the target is not on Twitter or their handle cannot be identified\u201d the annotator should \u201cplease write OTHER.\u201d With this framework established, we gave the definitions for our five cyberbullying criteria as follows.\n\n[Document 2]: Aggressive language: (aggr) Regardless of the author's intent, the language of the tweet could be seen as aggressive. The user either addresses a group or individual, and the message contains at least one phrase that could be described as confrontational, derogatory, insulting, threatening, hostile, violent, hateful, or sexually abusive.\n\n[Document 3]: Repetition: (rep) The target user has received at least two aggressive messages in total (either from the author or from another user in the visible thread).\n\n[Document 4]: Harmful intent: (harm) The tweet was designed to tear down or disadvantage the target user by causing them distress or by harming their public image. The target does not respond agreeably as to a joke or an otherwise lighthearted comment.\n\n[Document 5]: Visibility among peers: (peer) At least one other user besides the target has liked, retweeted, or responded to at least one of the author's messages.\n\n[Document 6]: Power imbalance: (power) Power is derived from authority and perceived social advantage. Celebrities and public figures are more powerful than common users. Minorities and disadvantaged groups have less power. Bullies can also derive power from peer support.\n\nQuestion: How is cyberbullying defined?\n\nExplanation:"}
{"question_id": "16b816925567deb734049416c149747118e13963", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We focus primarily on the smaller data set of 645 hand-labeled articles provided to task participants, both for training and for validation. We take the first 80% of this data set for our training set and the last 20% for the validation set. Since the test set is also hand-labeled we found that the 645 articles are much more representative of the final test set than the articles labeled by publisher. The model's performance on articles labeled by publisher was not much above chance level.\n\n[Document 2]: Our first experiment was checking the importance of pre-training. We pre-trained BERT-base on the 600,000 articles without labels by using the same Cloze task BIBREF5 that BERT had originally used for pre-training. We then trained the model on sequence lengths of 100, 250 and 500. The accuracy for each sequence length after 100 epochs is shown in TABREF7 and is labeled as UP (unsupervised pre-training). The other column shows how well BERT-base trained without pre-training. We found improvements for lower sequence lengths, but not at 500 word pieces. Since the longer chunk should have been more informative, and since our hand-labeled training set only contained 516 articles, this likely indicates that BERT experiences training difficulty when dealing with long sequences on such a small dataset. As the cost to do pre-training was only a one time cost all of our remaining experiments use a pre-trained model.\n\nQuestion: How long is the dataset?\n\nExplanation:  [Document 1] states that the dataset contains 645 articles. [Document 2] states that the dataset contains 600,000 articles.\n\nAnswer: 645, 600000\n\nExample 2:\n\n[Document 1]: Data set. For the cross-domain polarity classification experiments, we use the second version of Multi-Domain Sentiment Dataset BIBREF0 . The data set contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). Reviews contain star ratings (from 1 to 5) which are converted into binary labels as follows: reviews rated with more than 3 stars are labeled as positive, and those with less than 3 stars as negative. In each domain, there are 1000 positive and 1000 negative reviews.\n\nQuestion: How long is the dataset?\n\nExplanation:  The dataset contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). In each domain, there are 1000 positive and 1000 negative reviews. This means that there are a total of 8000 reviews in the dataset.\n\nAnswer: 8000\n\nExample 3:\n\n[Document 1]: The corpus includes 2000 sentences. Theses sentence are random renderings of 200 sentences, which we have taken from Sorani Kurdish books of the grades one to three of the primary school in the Kurdistan Region of Iraq. The reason that we have taken only 200 sentences is to have a smaller dictionary and also to increase the repetition of each word in the narrated speech. We transformed the corpus sentences, which are in Persian-Arabic script, into the format which complies with the suggested phones for the related Sorani letters (see Section SECREF6).\n\nQuestion: How long is the dataset?\n\nExplanation:  The dataset consists of 2000 sentences, as stated in [Document 1].\n\nAnswer: 2000\n\nExample 4:\n\n[Document 1]: Datasets. In order for the results to be consistent with previous works, we experimented with the benchmark datasets from SemEval 2014 task 4 BIBREF30 and SemEval 2016 task 5 BIBREF34 competitions. The laptop dataset is taken from SemEval 2014 and is used for both AE and ASC tasks. However, the restaurant dataset for AE is a SemEval 2014 dataset while for ASC is a SemEval 2016 dataset. The reason for the difference is to be consistent with the previous works. A summary of these datasets can be seen in Tables TABREF8 and TABREF8.\n\n[Document 2]: FLOAT SELECTED: Table 1. Laptop and restaurant datasets for AE. S: Sentences; A: Aspects; Rest16: Restaurant dataset from SemEval 2016.\n\n[Document 3]: FLOAT SELECTED: Table 2. Laptop and restaurant datasets for ASC. Pos, Neg, Neu: Number of positive, negative, and neutral sentiments, respectively; Rest14: Restaurant dataset from SemEval 2014\n\nQuestion: How long is the dataset?\n\nExplanation:"}
{"question_id": "cfc73e0c82cf1630b923681c450a541a964688b9", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We use the above findings for predicting depressive behavior. Our model exploits early fusion BIBREF32 technique in feature space and requires modeling each user INLINEFORM0 in INLINEFORM1 as vector concatenation of individual modality features. As opposed to computationally expensive late fusion scheme where each modality requires a separate supervised modeling, this model reduces the learning effort and shows promising results BIBREF75 . To develop a generalizable model that avoids overfitting, we perform feature selection using statistical tests and all relevant ensemble learning models. It adds randomness to the data by creating shuffled copies of all features (shadow feature), and then trains Random Forest classifier on the extended data. Iteratively, it checks whether the actual feature has a higher Z-score than its shadow feature (See Algorithm SECREF6 and Figure FIGREF45 ) BIBREF76 .\n\nQuestion: What model is used to achieve 5% improvement on F1 for identifying depressed individuals on Twitter?\n\nExplanation: \n\nAccording to [Document 1], the Random Forest classifier is used to achieve 5% improvement on F1 for identifying depressed individuals on Twitter. This information is further supported by [Document 2], [Document 3], and [Document 5].\n\nAnswer: Random Forest classifier\n\nExample 2:\n\n[Document 1]: We used the Twitter API to gather real-time tweets from September 2018 until February 2019, selecting the ones containing any of the 51 Hatebase terms that are more common in hate speech tweets, as studied in BIBREF9. We filtered out retweets, tweets containing less than three words and tweets containing porn related terms. From that selection, we kept the ones that included images and downloaded them. Twitter applies hate speech filters and other kinds of content control based on its policy, although the supervision is based on users' reports. Therefore, as we are gathering tweets from real-time posting, the content we get has not yet passed any filter.\n\nQuestion: How is data collected, manual collection or Twitter api?\n\nExplanation: \n\nAccording to [Document 1], the data was collected using the Twitter API.\n\nAnswer: Twitter API\n\nExample 3:\n\nQuestion: Do they specify which countries they collected twitter data from?\n\nExplanation:  There is no mention of any specific countries in the documents.\n\nAnswer: No\n\nExample 4:\n\n[Document 1]: To obtain a unique representative location of each user, we extracted the sequence of all declared locations from their geolocated tweets. Using this set of locations we selected the most frequent to be the representative one, and we took it as a proxy for the user's home location. Further we limited our users to ones located throughout the French territory thus not considering others tweeting from places outside the country. This selection method provided us with $110,369$ geolocated users who are either detected as French speakers or assigned to be such by Twitter and all associated to specific 'home' GPS coordinates in France. To verify the spatial distribution of the selected population, we further assessed the correlations between the true population distributions (obtained from census data BIBREF28 ) at different administrative level and the geolocated user distribution aggregated correspondingly. More precisely, we computed the $R^2$ coefficient of variation between the inferred and official population distributions (a) at the level of 22 regions. Correlations at this level induced a high coefficient of $R^2\\simeq 0.89$ ( $p<10^{-2}$ ); (b) At the arrondissement level with 322 administrative units and coefficient $R^2\\simeq 0.87$ ( $p<10^{-2}$ ); and (c) at the canton level with 4055 units with a coefficient $R\\simeq 0.16$ ( $p<10^{-2}$ ). Note that the relatively small coefficient at this level is due to the interplay of the sparsity of the inferred data and the fine grained spatial resolution of cantons. All in all, we can conclude that our sample is highly representative in terms of spatial population distribution, which at the same time validate our selection method despite the potential inherent biases induced by the method taking the most frequented GPS coordinates as the user's home location.\n\n[Document 2]: The second dataset we used was released in December 2016 by the National Institute of Statistics and Economic Studies (INSEE) of France. This data corpus BIBREF29 contains a set of sociodemographic aggregated indicators, estimated from the 2010 tax return in France, for each 4 hectare ( $200m \\times 200m$ ) square patch across the whole French territory. Using these indicators, one can estimate the distribution of the average socioeconomic status (SES) of people with high spatial resolution. In this study, we concentrated on three indicators for each patch $i$ , which we took to be good proxies of the socioeconomic status of the people living within them. These were the $S^i_\\mathrm {inc}$ average yearly income per capita (in euros), the $S^i_{\\mathrm {own}}$ fraction of owners (not renters) of real estate, and the $S^i_\\mathrm {den}$ density of population defined respectively as\n\n[Document 3]: To overcome this limitation we combined our Twitter data with the socioeconomic maps of INSEE by assigning each geolocated Twitter user to a patch closest to their estimated home location (within 1 km). This way we obtained for all $110,369$ geolocated users their dynamical linguistic data, their egocentric social network as well as a set of SES indicators.\n\nQuestion: How do they operationalize socioeconomic status from twitter user data?\n\nExplanation:"}
{"question_id": "67104a5111bf8ea626532581f20b33b851b5abc1", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: How many parameters does the model have?\n\nExplanation:  The documents do not mention the number of parameters in the model.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: Instead of reading in the tokenized input text, our model reads raw utf-8 bytes. For English text in the ASCII range, this is equivalent to processing characters as individual tokens. Non-ASCII characters (e.g. accented characters, or non-Latin scripts) are typically two or three utf-8 bytes. We use a standard \u201ctransformer decoder\u201d (a stack of transformer layers with a causal attention mask) to process the sequence $x_{0:i-1}$ and predict the following byte $x_i$. The model's prediction is an estimate of the probability distribution over all possible 256 byte values. Our input byte embedding matrix has dimensionality 256. Our byte-level transformer model has 40 standard transformer layers with hidden size 1024, filter size 8192, and 16 heads. The model has around 836M parameters, of which only 66K are byte embeddings.\n\nQuestion: How many parameters does the model have?\n\nExplanation:  The model has around 836M parameters according to [Document 1].\n\nAnswer: model has around 836M parameters\n\nExample 3:\n\nQuestion: How many parameters does the model have?\n\nExplanation:  The documents do not contain any information regarding the number of parameters in the model.\n\nAnswer: Unanswerable\n\nExample 4:\n\nQuestion: How many parameters does their model have?\n\nExplanation:"}
{"question_id": "753a187c1dd8d96353187fbb193b5f86293a796c", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: To collect article-level labels, we utilized a platform in the company that has been used by the market research team to collect surveys from the subscribers of different news publishers. The survey works as follows: The user is first presented with a set of selected pages (usually 4 pages and around 20 articles) from the print paper the day before. The user can select an article each time that he or she has read, and answer some questions about it. We added 3 questions to the existing survey that asked the level of partisanship, the polarity of partisanship, and which pro- or anti- entities the article presents. We also asked the political standpoint of the user. The complete survey can be found in Appendices.\n\nQuestion: Did they crowdsource the annotations?\n\nExplanation:  [Document 1] mentions that the company utilized a platform to collect article-level labels. This platform is a crowdsourcing platform that has been used by the market research team to collect surveys from the subscribers of different news publishers.\n\nAnswer: Yes\n\nExample 2:\n\n[Document 1]: In this study, we constructed the first Japanese word similarity dataset. It contains various parts of speech and includes rare words in addition to common words. Crowdsourced annotators assigned similarity to word pairs during the word similarity task. We gave examples of similarity in the task request sent to annotators, so that we reduced the variance of each word pair. However, we did not restrict the attributes of words, such as the level of feeling, during annotation. Error analysis revealed that the notion of similarity should be carefully defined when constructing a similarity dataset.\n\nQuestion: did they use a crowdsourcing platform for annotations?\n\nExplanation:  [Document 1] states that \"Crowdsourced annotators assigned similarity to word pairs during the word similarity task.\" This indicates that a crowdsourcing platform was used for annotations.\n\nAnswer: Yes\n\nExample 3:\n\n[Document 1]: Reddit is popular website that allows registered users (without identity verification) to participate in fora grouped by topic or interest. Participation consists of posting stories that can be seen by other users, voting stories and comments, and comments in the story's comment section, in the form of a forum. The forums are arranged in the form of a tree, allowing nested conversations, where the replies to a comment are its direct responses. We collected all comments in the stories' conversation in Reddit that were posted in August 2015. Since it is infeasible to manually annotate all of the comments, we process this dataset with the goal of extracting threads that involve suspected trolling attempts and the direct responses to them. To do so, we used Lucene to create an inverted index from the comments and queried it for comments containing the word \u201ctroll\u201d with an edit distance of 1 in order to include close variations of this word, hypothesizing that such comments would be reasonable candidates of real trolling attempts. We did observe, however, that sometimes people use the word troll to point out that another user is trolling. Other times, people use the term to express their frustration about a particular user, but there is no trolling attempt. Yet other times people simply discuss trolling and trolls without actually observing one. Nonetheless, we found that this search produced a dataset in which 44.3% of the comments are real trolling attempts. Moreover, it is possible for commenters to believe that they are witnessing a trolling attempt and respond accordingly even where there is none due to misunderstanding. Therefore, the inclusion of comments that do not involve trolling would allow us to learn what triggers a user's interpretation of trolling when it is not present and what kind of response strategies are used.\n\n[Document 2]: We had two human annotators who were trained on snippets (i.e., (suspected trolling attempt, responses) pairs) taken from 200 conversations and were allowed to discuss their findings. After this training stage, we asked them to independently label the four aspects for each snippet. We recognize that this limited amount of information is not always sufficient to recover the four aspects we are interested in, so we give the annotators the option to discard instances for which they couldn't determine the labels confidently. The final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens. The distribution over the classes per trolling aspect is shown in the table TABREF19 in the column \u201cSize\u201d.\n\nQuestion: Do they use a crowdsourcing platform for annotation?\n\nExplanation:  [Document 1] mentions that they used two human annotators who were trained on snippets taken from 200 conversations. [Document 2] mentions that the final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens. There is no mention of a crowdsourcing platform.\n\nAnswer: No\n\nExample 4:\n\n[Document 1]: In order to address the drawbacks of existing datasets, we introduce a new corpus based on the Snopes fact-checking website. Our corpus consists of 6,422 validated claims with comprehensive annotations based on the data collected by Snopes fact-checkers and our crowd-workers. The corpus covers multiple domains, including discussion blogs, news, and social media, which are often found responsible for the creation and distribution of unreliable information. In addition to validated claims, the corpus comprises over 14k documents annotated with evidence on two granularity levels and with the stance of the evidence with respect to the claims. Our data allows training machine learning models for the four steps of the automated fact-checking process described above: document retrieval, evidence extraction, stance detection, and claim validation.\n\nQuestion: did they crowdsource annotations?\n\nExplanation:"}
{"question_id": "a222dc5d804a7b453a0f7fbc1d6c1b165a3ccdd6", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: In Twitter social networking, people express their opinions containing sub-sentences. These sub-sentences using specific PoS particles (Conjunction and Conjunctive adverbs), like \"but, while, however, despite, however\" have different polarities. However, the overall sentiment of tweets often focus on certain sub-sentences. For example:\n\n[Document 2]: @lonedog bwahahah...you are amazing! However, it was quite the letdown.\n\n[Document 3]: @kirstiealley my dentist is great but she's expensive...=(\n\n[Document 4]: In two tweets above, the overall sentiment is negative. However, the main sentiment is only in the sub-sentences following but and however. This inspires a processing step to remove unessential parts in a tweet. Rule-based approach can assists these problems in handling negation and dealing with specific PoS particles led to effectively affect the final output of classification BIBREF11 BIBREF16 . BIBREF11 summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after using the rules. Table TABREF16 illustrates the number of processed sentences on each dataset.\n\nQuestion: What semantic rules are proposed?\n\nExplanation: \n\nAccording to [Document 4], the proposed semantic rules are rules that compute polarity of words after POS tagging or parsing steps. This information is further supported by [Document 1], which states that the proposed rules are designed to effectively affect the final output of classification.\n\nAnswer: rules that compute polarity of words after POS tagging or parsing steps\n\nExample 2:\n\n[Document 1]: An extrinsic evaluation was carried out on the task of Open IE BIBREF7. It revealed that when applying DisSim as a preprocessing step, the performance of state-of-the-art Open IE systems can be improved by up to 346% in precision and 52% in recall, i.e. leading to a lower information loss and a higher accuracy of the extracted relations. For details, the interested reader may refer to niklaus-etal-2019-transforming.\n\nQuestion: Is the semantic hierarchy representation used for any task?\n\nExplanation: \n\nAccording to [Document 1], the semantic hierarchy representation is used for the task of Open IE.\n\nAnswer: Yes, Open IE\n\nExample 3:\n\n[Document 1]: Given the theoretical results, here we design a general framework for scholarly data exploration by using semantic queries on knowledge graph embedding space. Figure FIGREF19 shows the architecture of the proposed framework. There are three main components, namely data processing, task processing, and query processing.\n\nQuestion: What is the general framework for data exploration by semantic queries?\n\nExplanation: \n\nAccording to [Document 1], the general framework for data exploration by semantic queries has three main components, namely data processing, task processing, and query processing.\n\nAnswer: three main components, namely data processing, task processing, and query processing\n\nExample 4:\n\nQuestion: What is the Semantic Web?\n\nExplanation:"}
{"question_id": "e462efb58c71f186cd6b315a2d861cbb7171f65b", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We recruited 176 AMT workers to participate in our conceptualization task. Of these workers, 90 were randomly assigned to the Control group and 86 to the AUI group. These workers completed 1001 tasks: 496 tasks in the control and 505 in the AUI. All responses were gathered within a single 24-hour period during April, 2017.\n\nQuestion: What crowdsourcing platform was used?\n\nExplanation: \n\n[Document 1] mentions that the workers were recruited from AMT, so we can infer that AMT was the crowdsourcing platform used.\n\nAnswer: AMT\n\nExample 2:\n\n[Document 1]: We use crowdsourcing to annotate different aspects of the dataset. We used Amazon Mechanical Turk (AMT) for our annotations, restricting the task to workers in five English-speaking countries (USA, UK, Canada, New Zealand, and Australia), more than 1000 finished HITs and at least a 95% acceptance rate. To ensure the diversity of responses, we do not require additional qualifications or demographic information from our annotators.\n\nQuestion: What crowdsourcing platform did they use?\n\nExplanation:  [Document 1] mentions that Amazon Mechanical Turk (AMT) was used for the annotations.\n\nAnswer: Amazon Mechanical Turk (AMT)\n\nExample 3:\n\n[Document 1]: Given the full text of a scientific publication, we want to rank its citations according to the author's judgments. We collected recent publications from the open-access PLoS journals and asked the authors to rank by closeness five citations we selected from their paper. PLoS articles were selected because its journals cover a wide array of topics and the full text articles are available in XML format. We selected the most recent publications as previous work in crowd-sourcing annotation shows that authors' willingness to participate in an unpaid annotation task declines with the age of publication ( BIBREF23 ). We then extracted the abstract, citations, full text, authors, and corresponding author email address from each document. The titles and abstracts of the citations were retrieved from PubMed, and the cosine similarity between the PLoS abstract and the citation's abstract was calculated. We selected the top five most similar abstracts using TF*IDF weighted cosine similarity, shuffled their order, and emailed them to the corresponding author for annotation. We believe that ranking five articles (rather than the entire collection of the references) is a more manageable task for an author compared to asking them to rank all references. Because the documents to be annotated were selected based on text similarity, they also represent a challenging baseline for models based on text-similarity features. In total 416 authors were contacted, and 92 responded (22% response rate). Two responses were removed from the dataset for incomplete annotation.\n\nQuestion: what crowdsourcing platform is used?\n\nExplanation:  [Document 1] states that the authors were asked to rank by closeness five citations selected from their paper. This crowdsourcing platform is not named, but it can be inferred that it is some sort of online platform where the authors can login and rank the citations.\n\nAnswer: asked the authors to rank by closeness five citations we selected from their paper\n\nExample 4:\n\n[Document 1]: Our goal is to identify which of the actions extracted from the transcripts are visually depicted in the videos. We create an annotation task on Amazon Mechanical Turk (AMT) to identify actions that are visible.\n\nQuestion: What platform was used for crowdsourcing?\n\nExplanation:"}
{"question_id": "13fb28e8b7f34fe600b29fb842deef75608c1478", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: To answer RQ3, which is concerned with domain adaptation, we can see in the grayed out cells in tab:results, which correspond to the cross-domain adaption case where the BERT language model is trained on the target domain, that domain adaptation works well with $2.2\\%$ absolute accuracy improvement on the laptops test set and even $3.6\\%$ accuracy improvement on the restaurants test set compared to BERT-base.\n\nQuestion: By how much does their model outperform the baseline in the cross-domain evaluation?\n\nExplanation:  The answer to the question can be found in the grayed out cells in tab:results in [Document 1].\n\nAnswer: $2.2\\%$ absolute accuracy improvement on the laptops test set, $3.6\\%$ accuracy improvement on the restaurants test set\n\nExample 2:\n\n[Document 1]: First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively. These results suggest that providing the behavioral navigation graph to the model and allowing it to process this information as a knowledge base in an end-to-end fashion is beneficial.\n\n[Document 2]: Lastly, it is worth noting that our proposed model (last row of Table TABREF28 ) outperforms all other models in previously seen environments. In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models.\n\nQuestion: By how much did their model outperform the baseline?\n\nExplanation:  [Document 1] and [Document 2] both support that the proposed model outperforms the baseline model by a significant margin.\n\nAnswer: increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively, over INLINEFORM0 increase in EM and GM between our model and the next best two models\n\nExample 3:\n\nQuestion: How much in-domain data is enough for joint models to outperform baselines?\n\nExplanation:  The documents do not provide enough information to answer the question.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: Table TABREF28 shows results on the event expression tasks. Our initial submits RUN 4 and 5 outperformed the memorization baseline on every metric on every task. The precision of event span identification is close to the max report. However, our system got lower recall. One of the main reason is that our training objective function is accuracy-oriented. Table TABREF29 shows results on the phase 2 subtask.\n\nQuestion: By how much did their model outperform baselines?\n\nExplanation:"}
{"question_id": "a4a9971799c8860b50f219c93f050ebf6a627b3d", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter. It contains tweets collected based on keyword search that were posted between July 17, 2014 (the day of the plane crash) and December 9, 2016.\n\n[Document 2]: BIBREF4 provide annotations for a subset of the English tweets contained in the dataset. A tweet is annotated with one of three classes that indicate the framing of the tweet with respect to responsibility for the plane crash. A tweet can either be pro-Russian (Ukrainian authorities, NATO or EU countries are explicitly or implicitly held responsible, or the tweet states that Russia is not responsible), pro-Ukrainian (the Russian Federation or Russian separatists in Ukraine are explicitly or implicitly held responsible, or the tweet states that Ukraine is not responsible) or neutral (neither Ukraine nor Russia or any others are blamed). Example tweets for each category can be found in Table TABREF9. These examples illustrate that the framing annotations do not reflect general polarity, but polarity with respect to responsibility to the crash. For example, even though the last example in the table is in general pro-Ukrainian, as it displays the separatists in a bad light, the tweet does not focus on responsibility for the crash. Hence the it is labeled as neutral. Table TABREF8 shows the label distribution of the annotated portion of the data as well as the total amount of original tweets, and original tweets plus their retweets/duplicates in the network. A retweet is a repost of another user's original tweet, indicated by a specific syntax (RT @username: ). We consider as duplicate a tweet with text that is identical to an original tweet after preprocessing (see Section SECREF18). For our classification experiments, we exclusively consider original tweets, but model predictions can then be propagated to retweets and duplicates.\n\nQuestion: What languages are included in the dataset?\n\nExplanation:  The dataset only contains English tweets, as stated in [Document 2].\n\nAnswer: English\n\nExample 2:\n\n[Document 1]: We begin by filtering the corpus to keep only those tweets where the user's self-declared language and the tweet's detected language correspond; that language becomes the tweet's correct language label. This operation cuts out roughly half the tweets, and leaves us with a corpus of about 900 million tweets in 54 different languages. Table TABREF6 shows the distribution of languages in that corpus. Unsurprisingly, it is a very imbalanced distribution of languages, with English and Japanese together accounting for 60% of all tweets. This is consistent with other studies and statistics of language use on Twitter, going as far back as 2013. It does however make it very difficult to use this corpus to train a LID system for other languages, especially for one of the dozens of seldom-used languages. This was our motivation for creating a balanced Twitter dataset.\n\nQuestion: What languages are represented in the dataset?\n\nExplanation:  [Document 1] provides a list of languages represented in the dataset.\n\nAnswer: EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO\n\nExample 3:\n\n[Document 1]: The data stems from a joint ADAPT-Microsoft project. An overview of the provided dataset is given in Table TABREF16 . Notice that the available amount of data differs per language.\n\nQuestion: is the dataset balanced across the four languages?\n\nExplanation:  [Document 1] provides a table that shows the amount of data available for each language. It is clear from the table that the dataset is not balanced across the four languages.\n\nAnswer: No\n\nExample 4:\n\n[Document 1]: Building on the findings of previous research efforts, this paper aims to study the effects of using new textual and psycholinguistic signals to detect extremist content online. These signals are developed based on insights gathered from analyzing propaganda material published by known extremist groups. In this study, we focus mainly on the ISIS group as they are one of the leading terrorist groups that utilise social media to share their propaganda and recruit individuals. We analyze the propaganda material they publish in their online English magazine called Dabiq, and use data-mining techniques to computationally uncover contextual text and psychological properties associated with these groups. From our analysis of these texts, we are able to extract a set of signals that provide some insight into the mindset of the radical group. This allows us to create a general radical profile that we apply as a signal to detect pro-ISIS supporters on Twitter. Our results show that these identified signals are indeed critical to help improve existing efforts to detect online radicalization.\n\n[Document 2]: Another angle for analyzing written text is by looking at the psychological properties that can be inferred regarding their authors. This is typically called psycholinguistics, where one examines how the use of the language can be indicative of different psychological states. Examples of such psychological properties include introversion, extroversion, sensitivity, and emotions. One of the tools that automates the process of extracting psychological meaning from text is the Linguistic Inquiry and Word Count (LIWC) BIBREF8 tool. This approach has been used in the literature to study the behaviour of different groups and to predict their psychological states, such as predicting depression BIBREF9 . More recently, it has also been applied to uncover different psychological properties of extremist groups and understand their intentions behind the recruitment campaigns BIBREF10 .\n\n[Document 3]: We acquired a publicly available dataset of tweets posted by known pro-ISIS Twitter accounts that was published during the 2015 Paris attacks by Kaggle data science community. The dataset consists of around INLINEFORM0 tweets posted by more than 100 users. These tweets were labelled as being pro-ISIS by looking at specific indicators, such as a set of keywords used (in the user's name, description, tweet text), their network of follower/following of other known radical accounts, and sharing of images of the ISIS flag or some radical leaders. To validate that these accounts are indeed malicious, we checked the current status of the users' accounts in the dataset and found that most of them had been suspended by Twitter. This suggests that they did, in fact, possess a malicious behaviour that opposes the Twitter platform terms of use which caused them to be suspended. We filter out any tweets posted by existing active users and label this dataset as known-bad.\n\nQuestion: What languages feature in the dataset?\n\nExplanation:"}
{"question_id": "0ba8f04c3fd64ee543b9b4c022310310bc5d3c23", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We first compared the performance of Transformer Transducer (T-T) models with full attention on audio to an RNN-T model using a bidirectional LSTM audio encoder. As shown in Table TABREF12, the T-T model significantly outperforms the LSTM-based RNN-T baseline. We also observed that T-T models can achieve competitive recognition accuracy with existing wordpiece-based end-to-end models with similar model size. To compare with systems using shallow fusion BIBREF18, BIBREF25 with separately trained LMs, we also trained a Transformer-based LM with the same architecture as the label encoder used in T-T, using the full 810M word token dataset. This Transformer LM (6 layers; 57M parameters) had a perplexity of $2.49$ on the dev-clean set; the use of dropout, and of larger models, did not improve either perplexity or WER. Shallow fusion was then performed using that LM and both the trained T-T system and the trained bidirectional LSTM-based RNN-T baseline, with scaling factors on the LM output and on the non-blank symbol sequence length tuned on the LibriSpeech dev sets. The results are shown in Table TABREF12 in the \u201cWith LM\u201d column. The shallow fusion result for the T-T system is competitive with corresponding results for top-performing existing systems.\n\nQuestion: What was previous state of the art model?\n\nExplanation:  The table in [Document 1] shows that the T-T model outperforms the LSTM-based RNN-T baseline, which was the previous state of the art model.\n\nAnswer: LSTM-based RNN-T\n\nExample 2:\n\n[Document 1]: In our paper, we opt for a state-of-the-art character bigram CNN classifier BIBREF4 , and investigate various ways in which the discourse information can be featurized and integrated into the CNN. Specifically,\n\nQuestion: What was the previous state-of-the-art?\n\nExplanation:  [Document 1] mentions that the character bigram CNN classifier was the state-of-the-art method, and this is further supported by [Document 2].\n\nAnswer: character bigram CNN classifier\n\nExample 3:\n\n[Document 1]: The comparison of our method with various baselines and state-of-the-art methods is provided in table TABREF26 for VQA 1.0 and table TABREF27 for VQG-COCO dataset. The comparable baselines for our method are the image based and caption based models in which we use either only the image or the caption embedding and generate the question. In both the tables, the first block consists of the current state-of-the-art methods on that dataset and the second contains the baselines. We observe that for the VQA dataset we achieve an improvement of 8% in BLEU and 7% in METEOR metric scores over the baselines, whereas for VQG-COCO dataset this is 15% for both the metrics. We improve over the previous state-of-the-art BIBREF35 for VQA dataset by around 6% in BLEU score and 10% in METEOR score. In the VQG-COCO dataset, we improve over BIBREF5 by 3.7% and BIBREF36 by 3.5% in terms of METEOR scores.\n\nQuestion: What were the previous state of the art benchmarks?\n\nExplanation: \n\nAccording to [Document 1], the previous state of the art benchmarks were BIBREF35 for VQA dataset, BIBREF5, and BIBREF36.\n\nAnswer: BIBREF35 for VQA dataset, BIBREF5, BIBREF36\n\nExample 4:\n\n[Document 1]: As with the last task, we first extract the vector representation of all the tweets in the dataset using Tweet2Vec and use that to train a logistic regression classifier using the vector representations. Even though there are three classes, the SemEval task is a binary task. The performance is measured as the average F1-score of the positive and the negative class. Table 3 shows the performance of our model compared to the top four models in the SemEval 2015 competition (note that only the F1-score is reported by SemEval for this task) and ParagraphVec. Our model outperforms all these models, again without resorting to any feature engineering.\n\nQuestion: what are the previous state of the art for sentiment categorization?\n\nExplanation:"}
{"question_id": "a8f189fad8b72f8b2b4d2da4ed8475d31642d9e7", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Unsupervised Evaluation\n\n[Document 2]: The unsupervised tasks include five tasks from SemEval Semantic Textual Similarity (STS) in 2012-2016 BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 and the SemEval2014 Semantic Relatedness task (SICK-R) BIBREF35 .\n\n[Document 3]: The cosine similarity between vector representations of two sentences determines the textual similarity of two sentences, and the performance is reported in Pearson's correlation score between human-annotated labels and the model predictions on each dataset.\n\n[Document 4]: Supervised Evaluation\n\n[Document 5]: It includes Semantic relatedness (SICK) BIBREF35 , SemEval (STS-B) BIBREF36 , paraphrase detection (MRPC) BIBREF37 , question-type classification (TREC) BIBREF38 , movie review sentiment (MR) BIBREF39 , Stanford Sentiment Treebank (SST) BIBREF40 , customer product reviews (CR) BIBREF41 , subjectivity/objectivity classification (SUBJ) BIBREF42 , opinion polarity (MPQA) BIBREF43 .\n\nQuestion: How do they evaluate the sentence representations?\n\nExplanation: \n\nAccording to [Document 1] and [Document 2], the unsupervised tasks include five tasks from SemEval Semantic Textual Similarity (STS) in 2012-2016 BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 and the SemEval2014 Semantic Relatedness task (SICK-R) BIBREF35 . The cosine similarity between vector representations of two sentences determines the textual similarity of two sentences, and the performance is reported in Pearson's correlation score between human-annotated labels and the model predictions on each dataset.\n\nAccording to [Document 4] and [Document 5], the supervised tasks include Semantic relatedness (SICK) BIBREF35 , SemEval (STS-B) BIBREF36 , paraphrase detection (MRPC) BIBREF37 , question-type classification (TREC) BIBREF38 , movie review sentiment (MR) BIBREF39 , Stanford Sentiment Treebank (SST) BIBREF40 , customer product reviews (CR) BIBREF41 , subjectivity/objectivity classification (SUBJ) BIBREF42 , opinion polarity (MPQA)\n\nAnswer: The unsupervised tasks include five tasks from SemEval Semantic Textual Similarity (STS) in 2012-2016 BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 and the SemEval2014 Semantic Relatedness task (SICK-R) BIBREF35 .\n\nThe cosine similarity between vector representations of two sentences determines the textual similarity of two sentences, and the performance is reported in Pearson's correlation score between human-annotated labels and the model predictions on each dataset., Supervised Evaluation\nIt includes Semantic relatedness (SICK) BIBREF35 , SemEval (STS-B) BIBREF36 , paraphrase detection (MRPC) BIBREF37 , question-type classification (TREC) BIBREF38 , movie review sentiment (MR) BIBREF39 , Stanford Sentiment Treebank (SST) BIBREF40 , customer product reviews (CR) BIBREF41 , subjectivity/objectivity classification (SUBJ) BIBREF42 , opinion polarity (MPQA) BIBREF43 .\n\nExample 2:\n\n[Document 1]: We follow a similar evaluation protocol to those presented in BIBREF6 , BIBREF8 , BIBREF9 which is to use our learned representations as features for a low complexity classifier (typically linear) on a novel supervised task/domain unseen during training without updating the parameters of our sentence representation model. We also consider such a transfer learning evaluation in an artificially constructed low-resource setting. In addition, we also evaluate the quality of our learned individual word representations using standard benchmarks BIBREF36 , BIBREF37 .\n\nQuestion: How do they evaluate their sentence representations?\n\nExplanation:  [Document 1] states that they use standard benchmarks BIBREF36 , BIBREF37, to use our learned representations as features for a low complexity classifier (typically linear) on a novel supervised task/domain unseen during training without updating the parameters, and a transfer learning evaluation in an artificially constructed low-resource setting.\n\nAnswer: standard benchmarks BIBREF36 , BIBREF37, to use our learned representations as features for a low complexity classifier (typically linear) on a novel supervised task/domain unseen during training without updating the parameters, transfer learning evaluation in an artificially constructed low-resource setting\n\nExample 3:\n\n[Document 1]: In this section we describe how to evaluate and compare the outcomes of algorithms which assign relevance scores to words (such as LRP or SA) through intrinsic validation. Furthermore, we propose a measure of model explanatory power based on an extrinsic validation procedure. The latter will be used to analyze and compare the relevance decompositions or explanations obtained with the neural network and the BoW/SVM classifier. Both types of evaluations will be carried out in section \"Results\" .\n\nQuestion: Are the document vectors that the authors introduce evaluated in any way other than the new way the authors propose?\n\nExplanation: \n\nAccording to [Document 1], the document vectors that the authors introduce are evaluated in two ways: through intrinsic validation and extrinsic validation. This means that the document vectors are evaluated in a way other than the new way that the authors propose.\n\nAnswer: Yes\n\nExample 4:\n\n[Document 1]: To assess the performance of binary paragraph vectors, we carried out experiments on three datasets: 20 Newsgroups, a cleansed version (also called v2) of Reuters Corpus Volume 1 (RCV1) and English Wikipedia. As paragraph vectors can be trained with relatively large vocabularies, we did not perform any stemming of the source text. However, we removed stop words as well as words shorter than two characters and longer than 15 characters. Results reported by BIBREF15 indicate that performance of PV-DBOW can be improved by including n-grams in the model. We therefore evaluated two variants of Binary PV-DBOW: one predicting words in documents and one predicting words and bigrams. Since 20 Newsgroups is a relatively small dataset, we used all words and bigrams from its documents. This amounts to a vocabulary with slightly over one million elements. For the RCV1 dataset we used words and bigrams with at least 10 occurrences in the text, which gives a vocabulary with approximately 800 thousands elements. In case of English Wikipedia we used words and bigrams with at least 100 occurrences, which gives a vocabulary with approximately 1.5 million elements.\n\n[Document 2]: In the experiments presented thus far we had at our disposal training sets with documents similar to the documents for which we inferred binary codes. One could ask a question, if it is possible to use binary paragraph vectors without collecting a domain-specific training set? For example, what if we needed to hash documents that are not associated with any available domain-specific corpus? One solution could be to train the model with a big generic text corpus, that covers a wide variety of domains. BIBREF21 evaluated this approach for real-valued paragraph vectors, with promising results. It is not obvious, however, whether short binary codes would also perform well in similar settings. To shed light on this question we trained Binary PV-DBOW with bigrams on the English Wikipedia, and then inferred binary codes for the test parts of the 20 Newsgroups and RCV1 datasets. The results are presented in Table TABREF14 and in Figure FIGREF11 . The model trained on an unrelated text corpus gives lower retrieval precision than models with domain-specific training sets, which is not surprising. However, it still performs remarkably well, indicating that the semantics it captured can be useful for different text collections. Importantly, these results were obtained without domain-specific finetuning.\n\nQuestion: How do they show that binary paragraph vectors capture semantics?\n\nExplanation:"}
{"question_id": "5181aefb8a7272b4c83a1f7cb61f864ead6a1f1f", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: how well this method is compared to other method?\n\nExplanation: \n\nThis question is unanswerable based on the given documents.\n\nAnswer: Unanswerable\n\nExample 2:\n\nQuestion: Do they compare DeepER against other approaches?\n\nExplanation: \n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task\n\nAnswer: Yes\n\nExample 3:\n\n[Document 1]: The MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments. The reason for this is that, instead of focusing on only one intuitive but rather arbitrary aspect of compositional generalization, the MCD splits aim to optimize divergence across all compounds directly.\n\nQuestion: What are results of comparison between novel method to other approaches for creating compositional generalization benchmarks?\n\nExplanation: \n\nAccording to [Document 1], the MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments.\n\nAnswer: The MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments\n\nExample 4:\n\nQuestion: Do they compare to other methods?\n\nExplanation:"}
{"question_id": "811b67460e65232b8f363dc3f329ffecdfcc4ab2", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each.\n\nQuestion: Where did they collect their dataset from?\n\nExplanation:  The answer can be found in [Document 1].\n\nAnswer: from Arabic WikiNews site https://ar.wikinews.org/wiki\n\nExample 2:\n\n[Document 1]: We collect utterances from the $\\mathbf {C}$hinese $\\mathbf {A}$rtificial $\\mathbf {I}$ntelligence $\\mathbf {S}$peakers (CAIS), and annotate them with slot tags and intent labels. The training, validation and test sets are split by the distribution of intents, where detailed statistics are provided in the supplementary material. Since the utterances are collected from speaker systems in the real world, intent labels are partial to the PlayMusic option. We adopt the BIOES tagging scheme for slots instead of the BIO2 used in the ATIS, since previous studies have highlighted meaningful improvements with this scheme BIBREF30 in the sequence labeling field.\n\nQuestion: What is the domain of their collected corpus?\n\nExplanation: \n\nAccording to [Document 1], the utterances in the corpus were collected from speaker systems in the real world.\n\nAnswer: speaker systems in the real world\n\nExample 3:\n\n[Document 1]: By choosing English (En) as the pivot language, we perform pivot alignments for identical English segments on Europarl Fr-En and En-De parallel corpora BIBREF18 , constructing a multi-parallel corpus of Fr-En-De. Then each of the Fr*-De and Fr-De* pseudo parallel corpora is established from the multi-parallel data by applying the pivot language-based translation described in the previous section. For automatic translation, we utilize a pre-trained and publicly released NMT model for En $\\rightarrow $ De and train another NMT model for En $\\rightarrow $ Fr using the WMT'15 En-Fr parallel corpus BIBREF19 . A beam of size 5 is used to generate synthetic sentences. Lastly, to match the size of the training data, PSEUDOmix is established by randomly sampling half of each Fr*-De and Fr-De* corpus and mixing them together.\n\nQuestion: Where do they collect the synthetic data?\n\nExplanation:  The synthetic data is collected from the Europarl Fr-En and En-De parallel corpora.\n\nAnswer: Yes\n\nExample 4:\n\n[Document 1]: Although GitHub provides a set of APIs (application programming interfaces) that allow end-users to access its data in a programmatic manner, it doesn't allow flexible querying on the repository meta data necessary for our data collection purposes. Therefore, we turn to GH Archive, which collects all the GitHub event data and make them accessible through flexible APIs. Specifically, we collected every repository from GH Archive that:\n\n[Document 2]: Has at least one pull request or pull request review comment event between November 2017 and September 2019,\n\n[Document 3]: Has 50 or more starts,\n\n[Document 4]: Has a size between 1MB and 1GB, and\n\n[Document 5]: Has a permissive license.\n\n[Document 6]: Note the \u201cand\u201d in the list above\u2014a repository needs to meet all the conditions mentioned above to be eligible. The first two criteria (pull request events and the number of starts) are a sign of a quality repository. As for the license, we allowed apache-2.0 (Apache License 2.0), mit (MIT License), bsd-3-clause (BSD 3-Clause License), bsd-2-clause (BSD 2-Clause License), cc0-1.0 (Creative Commons Zero v1.0 Universal), unlicense (Unlicense), cc-by-4.0 (Creative Commons Attribution 4.0), and bsl-1.0 (Boost Software License 1.0 (BSL-1.0). A repository's number of stars, size, and license are determined as of the event in the first condition.\n\nQuestion: Which repositories did they collect from?\n\nExplanation:"}
{"question_id": "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We focus primarily on the smaller data set of 645 hand-labeled articles provided to task participants, both for training and for validation. We take the first 80% of this data set for our training set and the last 20% for the validation set. Since the test set is also hand-labeled we found that the 645 articles are much more representative of the final test set than the articles labeled by publisher. The model's performance on articles labeled by publisher was not much above chance level.\n\n[Document 2]: Our first experiment was checking the importance of pre-training. We pre-trained BERT-base on the 600,000 articles without labels by using the same Cloze task BIBREF5 that BERT had originally used for pre-training. We then trained the model on sequence lengths of 100, 250 and 500. The accuracy for each sequence length after 100 epochs is shown in TABREF7 and is labeled as UP (unsupervised pre-training). The other column shows how well BERT-base trained without pre-training. We found improvements for lower sequence lengths, but not at 500 word pieces. Since the longer chunk should have been more informative, and since our hand-labeled training set only contained 516 articles, this likely indicates that BERT experiences training difficulty when dealing with long sequences on such a small dataset. As the cost to do pre-training was only a one time cost all of our remaining experiments use a pre-trained model.\n\nQuestion: How long is the dataset?\n\nExplanation:  [Document 1] states that the dataset contains 645 articles. [Document 2] states that the dataset contains 600,000 articles.\n\nAnswer: 645, 600000\n\nExample 2:\n\n[Document 1]: Data set. For the cross-domain polarity classification experiments, we use the second version of Multi-Domain Sentiment Dataset BIBREF0 . The data set contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). Reviews contain star ratings (from 1 to 5) which are converted into binary labels as follows: reviews rated with more than 3 stars are labeled as positive, and those with less than 3 stars as negative. In each domain, there are 1000 positive and 1000 negative reviews.\n\nQuestion: How long is the dataset?\n\nExplanation:  The dataset contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). In each domain, there are 1000 positive and 1000 negative reviews. This means that there are a total of 8000 reviews in the dataset.\n\nAnswer: 8000\n\nExample 3:\n\n[Document 1]: The corpus includes 2000 sentences. Theses sentence are random renderings of 200 sentences, which we have taken from Sorani Kurdish books of the grades one to three of the primary school in the Kurdistan Region of Iraq. The reason that we have taken only 200 sentences is to have a smaller dictionary and also to increase the repetition of each word in the narrated speech. We transformed the corpus sentences, which are in Persian-Arabic script, into the format which complies with the suggested phones for the related Sorani letters (see Section SECREF6).\n\nQuestion: How long is the dataset?\n\nExplanation:  The dataset consists of 2000 sentences, as stated in [Document 1].\n\nAnswer: 2000\n\nExample 4:\n\nQuestion: How long is their dataset?\n\nExplanation:"}
{"question_id": "3ccc4ccebc3b0de5546b1208e8094a839fd4a4ab", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We use the dropout technique of Gal & Ghahramani gal as a baseline because it is the most similar dropout technique to our approach and denote it VBD (variational binary dropout).\n\nQuestion: What is binary variational dropout?\n\nExplanation:  [Document 1] refers to the dropout technique of Gal & Ghahramani gal as binary variational dropout.\n\nAnswer: the dropout technique of Gal & Ghahramani gal\n\nExample 2:\n\n[Document 1]: In this work we focus on end-to-end stateless temporal modeling which can take advantage of a large context while limiting computation and avoiding saturation issues. By end-to-end model, we mean a straight-forward model with a binary target that does not require a precise phoneme alignment beforehand. We explore an architecture based on a stack of dilated convolution layers, effectively operating on a broader scale than with standard convolutions while limiting model size. We further improve our solution with gated activations and residual skip-connections, inspired by the WaveNet style architecture explored previously for text-to-speech applications BIBREF10 and voice activity detection BIBREF9 , but never applied to KWS to our knowledge. In BIBREF11 , the authors explore Deep Residual Networks (ResNets) for KWS. ResNets differ from WaveNet models in that they do not leverage skip-connections and gating, and apply convolution kernels in the frequency domain, drastically increasing the computational cost.\n\n[Document 2]: Standard convolutional networks cannot capture long temporal patterns with reasonably small models due to the increase in computational cost yielded by larger receptive fields. Dilated convolutions skip some input values so that the convolution kernel is applied over a larger area than its own. The network therefore operates on a larger scale, without the downside of increasing the number of parameters. The receptive field $r$ of a network made of stacked convolutions indeed reads: $r = \\sum _i d_i (s_i - 1),$\n\nQuestion: What are dilated convolutions?\n\nExplanation: \n\nAccording to [Document 1], dilated convolutions are similar to standard convolutional networks but instead they skip some input values effectively operating on a broader scale. This information is further supported by [Document 2].\n\nAnswer: Similar to standard convolutional networks but instead they skip some input values effectively operating on a broader scale.\n\nExample 3:\n\n[Document 1]: A number of important downstream and upstream applications rely on accurate confidence scores in graph-like structures, such as confusion networks (CN) in Fig. 2 and lattices in Fig. 2 , where arcs connected by nodes represent hypothesised words. This section describes an extension of BiRNNs to CNs and lattices.\n\nQuestion: What is a confusion network or lattice?\n\nExplanation: \n\n[Document 1] contains a description of an extension of BiRNNs to confusion networks and lattices. A confusion network is a graph-like structure where arcs connect nodes representing multiple hypothesized words, thus allowing multiple incoming arcs unlike 1-best sequences. A lattice is a similar structure, but with the addition of a start and end node, representing the beginning and end of the hypothesized word sequence.\n\nAnswer: graph-like structures where arcs connect nodes representing multiple hypothesized words, thus allowing multiple incoming arcs unlike 1-best sequences\n\nExample 4:\n\n[Document 1]: Our summary finding is that character-level models lag the oracle in nearly all languages (\u00a7 \"Experiments\" ). The difference is small, but suggests that there is value in modeling morphology. When we tease apart the results by part of speech and dependency type, we trace the difference back to the character-level model's inability to disambiguate words even when encoded with arbitrary context (\u00a7 \"Analysis\" ). Specifically, it struggles with case syncretism, in which noun case\u2014and thus syntactic function\u2014is ambiguous. We show that the oracle relies on morphological case, and that a character-level model provided only with morphological case rivals the oracle, even when case is provided by another predictive model (\u00a7 \"Characters and case syncretism\" ). Finally, we show that the crucial morphological features vary by language (\u00a7 \"Understanding head selection\" ).\n\nQuestion: What is case syncretism?\n\nExplanation:"}
{"question_id": "0ba1514fb193c52a15c31ffdcd5c3ddbb2bb2c40", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: What is their best performance on the largest language direction dataset?\n\nExplanation:  The question asks for the best performance on the largest language direction dataset, but the documents do not provide any information on performance.\n\nAnswer: Unanswerable\n\nExample 2:\n\nQuestion: Which of the two speech recognition models works better overall on CN-Celeb?\n\nExplanation: \n\n[Document 1]: In this section, we describe the two speech recognition models that we evaluated on the CN-Celeb dataset. The first model is a deep neural network (DNN) that uses Mel-frequency cepstral coefficients (MFCCs) as input features. The second model is an x-vector that uses a deep neural network to extract speaker embeddings from raw waveform data.\n\n[Document 2]: The x-vector model outperformed the MFCC model in terms of both accuracy and speed.\n\nAnswer: x-vector\n\nExample 3:\n\n[Document 1]: Table TABREF21 shows results for Korean INLINEFORM0 English, using the same configurations (1, 2 and 8) as for German\u2013English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by gu-EtAl:2018:EMNLP1.\n\nQuestion: what are the methods they compare with in the korean-english dataset?\n\nExplanation:  The method gu-EtAl:2018:EMNLP1 is compared with in the korean-english dataset according to [Document 1].\n\nAnswer: gu-EtAl:2018:EMNLP1\n\nExample 4:\n\nQuestion: How much better is performance of Nigerian Pitdgin English sentiment classification of models that use additional Nigerian English data compared to orginal English-only models?\n\nExplanation:"}
{"question_id": "b0e894536857cb249bd75188c3ca5a04e49ff0b6", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: Is there a way of converting existing convolution layers into self-attention to perform very same convolution?\n\nExplanation:  The documents do not mention anything about converting convolution layers into self-attention.\n\nAnswer: Unanswerable\n\nExample 2:\n\nQuestion: How does the model differ from Generative Adversarial Networks?\n\nExplanation:  The documents provided do not contain enough information to answer the question.\n\nAnswer: Unanswerable\n\nExample 3:\n\nQuestion: Are the contexts in a language different from the questions?\n\nExplanation:  The documents do not provide any information that can answer the question.\n\nAnswer: Unanswerable\n\nExample 4:\n\nQuestion: How do attention, recurrent and convolutional networks differ on the language classes they accept?\n\nExplanation:"}
{"question_id": "e459760879f662b2205cbdc0f5396dbfe41323ae", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: What is the perWhat are the tasks evaluated?\n\nExplanation: \n\nThe question is unanswerable with the given information.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: We demonstrate our approach on the task of answering open-domain fill-in-the-blank natural language questions. By giving open vocabulary semantic parsers direct access to KB information, we improve mean average precision on this task by over 120%.\n\nQuestion: What task do they evaluate on?\n\nExplanation:  The task is directly stated in [Document 1].\n\nAnswer: Fill-in-the-blank natural language questions\n\nExample 3:\n\n[Document 1]: Neural network models, and specifically sequence-to-sequence models, have pushed the state of the art for morphological inflection \u2013 the task of learning a mapping from lemmata to their inflected forms \u2013 in the last years BIBREF6. Thus, in this work, we experiment on such models, asking not what they learn, but, motivated by the respective research on human subjects, the related question of how what they learn depends on their prior knowledge. We manually investigate the errors made by artificial neural networks for morphological inflection in a target language after pretraining on different source languages. We aim at finding answers to two main questions: (i) Do errors systematically differ between source languages? (ii) Do these differences seem explainable, given the properties of the source and target languages? In other words, we are interested in exploring if and how L2 acquisition of morphological inflection depends on the L1, i.e., the \"native language\", in neural network models.\n\n[Document 2]: For our qualitative analysis, we make use of the validation set. Therefore, we show validation set accuracies in Table TABREF19 for comparison. As we can see, the results are similar to the test set results for all language combinations. We manually annotate the outputs for the first 75 development examples for each source\u2013target language combination. All found errors are categorized as belonging to one of the following categories.\n\nQuestion: How is the performance on the task evaluated?\n\nExplanation: \n\nAccording to [Document 1], the performance of neural network models on an inflection task is evaluated by comparison of test accuracies. This information is further supported by [Document 2], which states that validation set accuracies are shown for comparison. Additionally, [Document 2] mentions that the errors are manually annotated, which is another form of qualitative evaluation.\n\nAnswer: Comparison of test accuracies of neural network models on an inflection task and qualitative analysis of the errors\n\nExample 4:\n\n[Document 1]: The results of our experiments can be summarized in three statements. First, our preliminary experiments show that doing bias-based attribute representation and attention-based injection is not an effective method to incorporate user and product information in sentiment classification models. Second, despite using only a simple BiLSTM with attention classifier, we significantly outperform previous state-of-the-art models that use more complicated architectures (e.g., models that use hierarchical models, external memory networks, etc.). Finally, we show that these attribute representations transfer well to other tasks such as product category classification and review headline generation.\n\nQuestion: Which other tasks are evaluated?\n\nExplanation:"}
{"question_id": "f8264609a44f059b74168995ffee150182a0c14f", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: In our experiment, we use a conversational Mandarin-English code-switching speech corpus called SEAME Phase II (South East Asia Mandarin-English). The data are collected from spontaneously spoken interviews and conversations in Singapore and Malaysia by bilinguals BIBREF21 . As the data preprocessing, words are tokenized using Stanford NLP toolkit BIBREF22 and all hesitations and punctuations were removed except apostrophe. The split of the dataset is identical to BIBREF9 and it is showed in Table TABREF6 .\n\nQuestion: What languages are explored in this paper?\n\nExplanation: \n\nAccording to [Document 1], the SEAME Phase II corpus contains data from bilingual speakers of Mandarin and English.\n\nAnswer: Mandarin, English\n\nExample 2:\n\nQuestion: What languages are explored in this paper?\n\nExplanation:  This question cannot be answered based on the given documents.\n\nAnswer: Unanswerable\n\nExample 3:\n\n[Document 1]: Since our focus in this paper is an additional loss exploiting negative examples (Section method), we fix the baseline LM throughout the experiments. Our baseline is a three-layer LSTM-LM with 1,150 hidden units at internal layers trained with the standard cross-entropy loss. Word embeddings are 400-dimensional, and input and output embeddings are tied BIBREF11. Deviating from some prior work BIBREF0, BIBREF1, we train LMs at sentence level as in sequence-to-sequence models BIBREF12. This setting has been employed in some previous work BIBREF3, BIBREF6.\n\nQuestion: What neural language models are explored?\n\nExplanation:  [Document 1] states that the focus of the paper is on an additional loss exploiting negative examples, and the baseline LM is a three-layer LSTM-LM.\n\nAnswer: LSTM-LM \n\nExample 4:\n\n[Document 1]: These previous approaches lack a clear contextual analysis used in NLP. We considered the semantic meaning of each word and we feel that the presence of particular words influence the meaning. We reckoned this important since we felt the contextual meaning of the text needs to be preserved and analyzed for better classification. Other studies emphasize the user and features related to them. In [4], \u201c45 features\u2026[were used] for predicting accuracy...across four types: structural, user, content, and temporal,\u201d so features included characteristics beyond the text. Article [6] \"learn[s] the representations of news articles, creators and subjects simultaneously.\" In our project, we emphasize the content by working with articles whose labels only relate to the text, nothing outside that scope, and have used SVM, Logistic Regression, ANN, LSTM, and Random Forest.\n\nQuestion: What models are explored in this paper?\n\nExplanation:"}
{"question_id": "2d924e888a92dc0b14cdb5584e73e87254c3d1ee", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Traditional text-to-speech (TTS) systems are composed of complex pipelines BIBREF0 , these often include accoustic frontends, duration model, acoustic prediction model and vocoder models. The complexity of the TTS problem coupled with the requirement for deep domain expertise means these systems are often brittle in design and results in un-natural synthesized speech.\n\n[Document 2]: The architecture of our model utilizes RNN-based Seq2Seq model for generating mel spectrogram from text. The architecture is similar to that of Tacotron 2 BIBREF4 . The generated mel spetrogram can either be inverted via iterative algorithms such as Griffin Lim, or through more complicated neural vocoder networks such as a mel spectrogram conditioned Wavenet BIBREF11 .\n\n[Document 3]: Direct comparison of model parameters between ours and the open-source tacotron 2, our model contains 4.5 million parameters, whereas the Tacotron 2 contains around 13 million parameters with default setting. By helping our model learn attention alignment faster, we can afford to use a smaller overall model to achieve similar quality speech quality.\n\nQuestion: Do they reduce the number of parameters in their architecture compared to other direct text-to-speech models?\n\nExplanation:  [Document 3] states that their model contains 4.5 million parameters, whereas the Tacotron 2 contains around 13 million parameters. This means that their model has reduced the number of parameters.\n\nAnswer: Yes\n\nExample 2:\n\nQuestion: Which language has the lowest error rate reduction?\n\nExplanation: \n\n[Document 1]: We evaluate our system on two languages, English and Thai, and compare it with the state-of-the-art system for each language.\n\n[Document 2]: We use the CoNLL 2002 and CoNLL 2003 shared task data sets for English and Thai, respectively.\n\n[Document 3]: The English data set contains 14041 sentences and 204584 tokens, while the Thai data set contains 2772 sentences and 37861 tokens.\n\n[Document 4]: The English data set has 4 different entity types (LOC, PER, ORG, and MISC), while the Thai data set has 3 different entity types (LOC, PER, and ORG).\n\n[Document 5]: For English, the error rate reduction is 2.16%, while for Thai, the error rate reduction is 1.39%.\n\nAnswer: thai\n\nExample 3:\n\n[Document 1]: Our attempt at language pre-training fell short of our expectations in all but one tested dataset. We had hoped that more stable language understanding would improve results in general. As previously mentioned, using more general and comprehensive corpora of language could help grow semantic ability.\n\n[Document 2]: Our pre-training was unsuccessful in improving accuracy, even when applied to networks larger than those reported. We may need to use more inclusive language, or pre-train on very math specific texts to be successful. Our results support our thesis of infix limitation.\n\nQuestion: Does pre-training on general text corpus improve performance?\n\nExplanation:  [Document 1] and [Document 2] both state that pre-training did not improve performance.\n\nAnswer: No\n\nExample 4:\n\nQuestion: Do they reduce language variation of text by enhancing frequencies?\n\nExplanation:"}
{"question_id": "f3b851c9063192c86a3cc33b2328c02efa41b668", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: Why authors think that researches do not pay attention to the research of the Chinese-oriented ABSA task?\n\nExplanation:  The documents do not mention why the authors think that researches do not pay attention to the research of the Chinese-oriented ABSA task.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: Last but not least, ethics and fairness are important considerations, that deserve to be studied. In that sense, detection of individual and global bias should be prioritized in order to give useful feedbacks to practitioners. Furthermore we are considering using adversarial learning as in BIBREF33 in order to ensure fairness during the training process.\n\nQuestion: Is there any ethical consideration in the research?\n\nExplanation:  There is no mention of any ethical consideration in the research in [Document 1].\n\nAnswer: No\n\nExample 3:\n\n[Document 1]: The central problem we consider is category induction: given some instances of a category, predict which other individuals are likely to be instances of that category. When enough instances are given, standard approaches such as the Gaussian classifier from Section UNKREF9, or even a simple SVM classifier, can perform well on this task. For many categories, however, we only have access to a few instances, either because the considered ontology is highly incomplete or because the considered category only has few actual instances. The main research question which we want to analyze is whether (predicted) conceptual neighborhood can help to obtain better category induction models in such cases. In Section SECREF16, we first provide more details about the experimental setting that we followed. Section SECREF23 then discusses our main quantitative results. Finally, in Section SECREF26 we present a qualitative analysis.\n\n[Document 2]: As explained in Section SECREF3, we used BabelNet BIBREF29 as our reference taxonomy. BabelNet is a large-scale full-fledged taxonomy consisting of heterogeneous sources such as WordNet BIBREF36, Wikidata BIBREF37 and WiBi BIBREF38, making it suitable to test our hypothesis in a general setting.\n\n[Document 3]: BabelNet category selection. To test our proposed category induction model, we consider all BabelNet categories with fewer than 50 known instances. This is motivated by the view that conceptual neighborhood is mostly useful in cases where the number of known instances is small. For each of these categories, we split the set of known instances into 90% for training and 10% for testing. To tune the prior probability $\\lambda _A$ for these categories, we hold out 10% from the training set as a validation set.\n\nQuestion: What experiments they perform to demonstrate that their approach leads more accurate region based representations?\n\nExplanation:  The information in [Document 1] introduces the concept of category induction and states that the main research question is whether predicted conceptual neighborhood can help to obtain better category induction models in such cases. [Document 2] explains that they used BabelNet as their reference taxonomy. [Document 3] provides details about the experimental setting, specifically that they considered all BabelNet categories with fewer than 50 known instances and split the set of known instances into 90% for training and 10% for testing.\n\nAnswer:  To test our proposed category induction model, we consider all BabelNet categories with fewer than 50 known instances. This is motivated by the view that conceptual neighborhood is mostly useful in cases where the number of known instances is small. For each of these categories, we split the set of known instances into 90% for training and 10% for testing.\n\nExample 4:\n\n[Document 1]: A more sophisticated approach to classify based on the ingredients was adopted by using the K Nearest Neighbors Model. The Yummly dataset from Kaggle is used to train the model. The ingredients extracted from the images are used as a test set. The model was run successfully for k-values ranging from 1-25. The radar charts for some of the k values are shown in Fig 7, 8 and 9.\n\n[Document 2]: Thus from these charts, we see that the user likes to eat Italian and Mexican food on most occasions. This is also in sync with the rudimentary method that we had used earlier.\n\nQuestion: Does this study perform experiments to prove their claim that indeed personalized profiles will have inclination towards particular cuisines?\n\nExplanation:"}
{"question_id": "627ce8a1db08a732d5a8f7e1f8a72e3de89847e6", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: The 17 domains (`Alarm' domain not included in training) present in our dataset are listed in Table TABREF5. We create synthetic implementations of a total of 34 services or APIs over these domains. Our simulator framework interacts with these services to generate dialogue outlines, which are a structured representation of dialogue semantics. We then used a crowd-sourcing procedure to paraphrase these outlines to natural language utterances. Our novel crowd-sourcing procedure preserves all annotations obtained from the simulator and does not require any extra annotations after dialogue collection. In this section, we describe these steps in detail and then present analyses of the collected dataset.\n\nQuestion: What are the domains covered in the dataset?\n\nExplanation:  The domains are listed in Table TABREF5 in [Document 1].\n\nAnswer: Alarm\nBank\nBus\nCalendar\nEvent\nFlight\nHome\nHotel\nMedia\nMovie\nMusic\nRentalCar\nRestaurant\nRideShare\nService\nTravel\nWeather\n\nExample 2:\n\nQuestion: What domains are present in the data?\n\nExplanation:  The domains are listed in [Document 1].\n\nAnswer: Alarm, Banks, Buses, Calendar, Events, Flights, Homes, Hotels, Media, Messaging, Movies, Music, Payment, Rental Cars, Restaurants, Ride Sharing, Services, Train, Travel, Weather\n\nExample 3:\n\n[Document 1]: According to the above analysis, we proposed a weighted version of DIRL to address the problem caused by the shift of $\\rm {P}(\\rm {Y})$ to DIRL. The key idea of this framework is to first align $\\rm {P}(\\rm {Y})$ across domains before performing domain-invariant learning, and then take account the shift of $\\rm {P}(\\rm {Y})$ in the label prediction procedure. Specifically, it introduces a class weight $\\mathbf {w}$ to weigh source domain examples by class. Based on the weighted source domain, the domain shift problem is resolved in two steps. In the first step, it applies DIRL on the target domain and the weighted source domain, aiming to alleviate the influence of the shift of $\\rm {P}(\\rm {Y})$ during the alignment of $\\rm {P}(\\rm {X}|\\rm {Y})$. In the second step, it uses $\\mathbf {w}$ to reweigh the supervised classifier $\\rm {P}_S(\\rm {Y}|\\rm {X})$ obtained in the first step for target domain label prediction. We detail these two steps in \u00a7SECREF10 and \u00a7SECREF14, respectively.\n\n[Document 2]: The motivation behind this practice is to adjust data distribution of the source domain or the target domain to alleviate the shift of $\\rm {P}(\\rm {Y})$ across domains before applying DIRL. Consider that we only have labels of source domain data, we choose to adjust data distribution of the source domain. To achieve this purpose, we introduce a trainable class weight $\\mathbf {w}$ to reweigh source domain examples by class when performing DIRL, with $\\mathbf {w}_i > 0$. Specifically, we hope that:\n\n[Document 3]: and we denote $\\mathbf {w}^*$ the value of $\\mathbf {w}$ that makes this equation hold. We shall see that when $\\mathbf {w}=\\mathbf {w}^*$, DIRL is to align $\\rm {P}_S(G(\\rm {X})|\\rm {Y})$ with $\\rm {P}_T(G(\\rm {X})|\\rm {Y})$ without the shift of $\\rm {P}(\\rm {Y})$. According to our analysis, we know that due to the shift of $\\rm {P}(\\rm {Y})$, there is a conflict between the training objects of the supervised learning $\\mathcal {L}_{sup}$ and the domain-invariant learning $\\mathcal {L}_{inv}$. And the conflict degree will decrease as $\\rm {P}_S(\\rm {Y})$ getting close to $\\rm {P}_T(\\rm {Y})$. Therefore, during model training, $\\mathbf {w}$ is expected to be optimized toward $\\mathbf {w}^*$ since it will make $\\rm {P}(\\rm {Y})$ of the weighted source domain close to $\\rm {P}_T(\\rm {Y})$, so as to solve the conflict.\n\nQuestion: How are different domains weighted in WDIRL?\n\nExplanation: \n\nAccording to [Document 1], a class weight $\\mathbf {w}$ is introduced to weigh source domain examples by class. This information is further supported by [Document 2] and [Document 3].\n\nAnswer: To achieve this purpose, we introduce a trainable class weight $\\mathbf {w}$ to reweigh source domain examples by class when performing DIRL, with $\\mathbf {w}_i > 0$\n\nExample 4:\n\n[Document 1]: We use 303 sub areas from Stack Exchange data dumps. The full list of area names is in the appendix. We do not include Stack Overflow because it is too specific to programming related questions. We also exclude all questions under the following language sub areas: Chinese, German, Spanish, Russian, Japanese, Korean, Latin, Ukrainian. This ensures that the questions in MQR are mostly English sentences. Having questions from 303 Stack Exchange sites makes the MQR dataset cover a broad range of domains.\n\nQuestion: What characterizes the 303 domains? e.g. is this different subject tags?\n\nExplanation:"}
{"question_id": "04796aaa59eeb2176339c0651838670fd916074d", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Team 9 BIBREF24: This team submitted the winning entry, beating the second-placed team by around 9% in terms of joint goal accuracy. They use two separate models for categorical and non-categorical slots, and treat numerical categorical slots as non-categorical. They also use the entire dialogue history as input. They perform data augmentation by back translation between English and Chinese, which seems to be one of the distinguishing factors resulting in a much higher accuracy.\n\nQuestion: What data augmentation techniques are used?\n\nExplanation: \n\nAccording to [Document 1], back translation between English and Chinese is used as a data augmentation technique.\n\nAnswer: back translation between English and Chinese\n\nExample 2:\n\n[Document 1]: To explore how helpful these transformations are, we incorporated 20 simple transformations and 15 additional sequences of transformations in our experiment to see their effect on different type of metrics on four different ML models (See Figure FIGREF3 ).\n\nQuestion: What preprocessing techniques are used in the experiments?\n\nExplanation:  The information in [Document 1] is presented in a figure, which is Figure FIGREF3. This figure shows the preprocessing techniques used in the experiments.\n\nAnswer: See Figure FIGREF3\n\nExample 3:\n\n[Document 1]: We explored different normalization techniques. FBanks with cepstral mean normalization (CMN) perform better than raw FBanks. We found using variance with mean normalization (CMVN) unnecessary for the task. Using deltas and delta-deltas improves model, so we used them in other experiments. Models trained with spectrogram features converge slower and to worse minimum, but the difference when using CMN is not very big compared to FBanks.\n\nQuestion: What normalization techniques are mentioned?\n\nExplanation:  [Document 1] mentions FBanks with cepstral mean normalization (CMN) and variance with mean normalization (CMVN) as normalization techniques.\n\nAnswer: FBanks with cepstral mean normalization (CMN), variance with mean normalization (CMVN)\n\nExample 4:\n\n[Document 1]: We investigate an important family of ensemble methods known as boosting, and more specifically a Light Gradient Boosting Machine (LGBM) algorithm, which consists of an implementation of fast gradient boosting on decision trees. In this study, we use a library implemented by Microsoft BIBREF18 . In our model, we learn a linear combination of the prediction given by the base classifiers and the input text features to predict the labels. As features, we consider the average term frequency-inverse document frequency (TF-IDF) score for each instance and the frequency of occurrence of quantitative information elements (QIEF) (e.g. percentage, population, dose of medicine). Finally, the output of the binary cross entropy with logits layer (predicted probabilities for the three classes) and the feature information are fed to the LGBM.\n\nQuestion: what boosting techniques were used?\n\nExplanation:"}
{"question_id": "7d503b3d4d415cf3e91ab08bd5a1a2474dd1047b", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: What is the size of the open vocabulary?\n\nExplanation:  The question cannot be answered with the given documents. [Document 1] discusses the training data used for the model, [Document 2] discusses the model architecture, [Document 3] discusses the results of the model, and [Document 4] discusses the evaluation data used for the model. None of the documents discuss the size of the open vocabulary.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: We pair 11'248 standard German written words with their phonetical representations in six different Swiss dialects: Z\u00fcrich, St. Gallen, Basel, Bern, Visp, and Stans (Figure FIGREF1). The phonetic words were written in a modified version of the Speech Assessment Methods Phonetic Alphabet (SAMPA). The Swiss German phonetic words are also paired with Swiss German writings in the latin alphabet. (From here onwards, a phonetic representation of a Swiss German word will be called a SAMPA and a written Swiss German word will be called a GSW.)\n\nQuestion: How many words are coded in the dictionary?\n\nExplanation:  The number of words is stated explicitly in the first sentence of the document.\n\nAnswer: 11'248\n\nExample 3:\n\n[Document 1]: Determining the sentiment polarity of tweets has become a landmark homework exercise in natural language processing (NLP) and data science classes. This is perhaps because the task is easy to understand and it is also easy to get good results with very simple methods (e.g. positive - negative words counting). The practical applications of this task are wide, from monitoring popular events (e.g. Presidential debates, Oscars, etc.) to extracting trading signals by monitoring tweets about public companies. These applications often benefit greatly from the best possible accuracy, which is why the SemEval-2017 Twitter competition promotes research in this area. The competition is divided into five subtasks which involve standard classification, ordinal classification and distributional estimation. For a more detailed description see BIBREF0 .\n\nQuestion: What were the five English subtasks?\n\nExplanation: \n\nAccording to [Document 1], the five English subtasks were standard classification, ordinal classification and distributional estimation. This information is further supported by [Document 2].\n\nAnswer:  five subtasks which involve standard classification, ordinal classification and distributional estimation. For a more detailed description see BIBREF0\n\nExample 4:\n\n[Document 1]: In this section, we examine the pre-trained language models of ESuLMo in terms of PPL. All the models' training and evaluation are done on One Billion Word dataset BIBREF19 . During training, we strictly follow the same hyper-parameter published by ELMo, including the hidden size, embedding size, and the number of LSTM layers. Meanwhile, we train each model on 4 Nvidia P40 GPUs, which takes about three days for each epoch. Table TABREF5 shows that our pre-trained language models can improve the performance of RNN-based language models by a large margin and our subword-aware language models outperform all previous RNN-based language models, including ELMo, in terms of PPL. During the experiment, we find that 500 is the best vocabulary size for both segmentation algorithms, and BPE is better than ULM in our setting.\n\nQuestion: How long is the vocabulary of subwords?\n\nExplanation:"}
{"question_id": "73a5783cad4ed468a8dbb31b5de2c618ce351ad1", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: By how much did the results improve?\n\nExplanation:  The question is unanswerable with the given documents. [Document 1] describes the experiments conducted, [Document 2] describes the Stanford NER algorithm, [Document 3] describes the spaCy 2.0 algorithm, [Document 4] describes the dataset used, and [Document 5] describes the recurrent model with a CRF top layer. None of the documents provide information on the results of the experiments.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: Adversarial training can often improve model robustness BIBREF25, BIBREF27. We also fine-tune the target models using adversarial training. At each training step, we train the model with a batch of original examples along with adversarial examples with balanced labels. The adversarial examples account for around 10% in a batch. During training, we generate adversarial examples with the current model as the target and update the model parameters with the hybrid batch iteratively. The beam size for generation is set to 1 to reduce the computation cost, since the generation success rate is minor in adversarial training. We evaluate the adversarially trained models, as shown in Table TABREF18.\n\n[Document 2]: After adversarial training, the performance of all the target models raises significantly, while that on the original examples remain comparable. Note that since the focus of this paper is on model robustness which can hardly be reflected in original data, we do not expect performance improvement on original data. The results demonstrate that adversarial training with our adversarial examples can significantly improve the robustness we focus on without remarkably hurting the performance on original data. Moreover, although the adversarial example generation is constrained by a BERT language model, BiMPM and DIIN which do not use the BERT language model can also significantly benefit from the adversarial examples, further demonstrating the effectiveness of our method.\n\nQuestion: How much in experiments is performance improved for models trained with generated adversarial examples?\n\nExplanation: \n\nAccording to [Document 1], the performance of all the target models raises significantly, while that on the original examples remain comparable. This information is further supported by [Document 2].\n\nAnswer: Answer with content missing: (Table 1) The performance of all the target models raises significantly, while that on the original\nexamples remain comparable (e.g. the overall accuracy of BERT on modified examples raises from 24.1% to 66.0% on Quora)\n\nExample 3:\n\n[Document 1]: As shown in Table TABREF24, MUSE outperforms all previously models on En-De and En-Fr translation, including both state-of-the-art models of stand alone self-attention BIBREF0, BIBREF13, and convolutional models BIBREF11, BIBREF15, BIBREF10. This result shows that either self-attention or convolution alone is not enough for sequence to sequence learning. The proposed parallel multi-scale attention improves over them both on En-De and En-Fr.\n\n[Document 2]: Compared to Evolved Transformer BIBREF19 which is constructed by NAS and also mixes convolutions of different kernel size, MUSE achieves 2.2 BLEU gains in En-Fr translation.\n\nQuestion: How big is improvement in performance over Transformers?\n\nExplanation:  [Document 1] and [Document 2] both mention that MUSE outperforms Transformers by 2.2 BLEU gains.\n\nAnswer: 2.2 BLEU gains\n\nExample 4:\n\n[Document 1]: FLOAT SELECTED: Table 6: The accuracy of various models and baselines on the original PERSONACHAT test set.\n\n[Document 2]: Our main result, reported in Table TABREF16 , is that utilizing the deployment examples improves accuracy on the Dialogue task regardless of the number of available supervised (HH) Dialogue examples. The boost in quality is naturally most pronounced when the HH Dialogue training set is small (i.e., where the learning curve is steepest), yielding an increase of up to 9.4 accuracy points, a 31% improvement. However, even when the entire PersonaChat dataset of 131k examples is used\u2014a much larger dataset than what is available for most dialogue tasks\u2014adding deployment examples is still able to provide an additional 1.6 points of accuracy on what is otherwise a very flat region of the learning curve. It is interesting to note that the two types of deployment examples appear to provide complementary signal, with models performing best when they use both example types, despite them coming from the same conversations. We also calculated hit rates with 10,000 candidates (instead of 20), a setup more similar to the interactive setting where there may be many candidates that could be valid responses. In that setting, models trained with the deployment examples continue to outperform their HH-only counterparts by significant margins (see Appendix SECREF8 ).\n\nQuestion: by how much did performance improve?\n\nExplanation:"}
{"question_id": "9a9774eacb8f75bcfa07a4e60ed5eb02646467e3", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: For source and cited documents, we use English-language computer science articles and annotation from the S2-GORC dataset BIBREF7. S2-GORC is a large citation graph dataset which includes full texts of 8.1 million scientific documents. We select a subset of 154K computer science articles as our corpus. From these, we extract 622K citing sentences that link back to other documents in our corpus. We hold 2500 examples for each of the validation and test sets. Detailed statistics can be found in Table TABREF4.\n\nQuestion: What is the size of the corpus?\n\nExplanation:  The size of the corpus is 8.1 million scientific documents, 154K computer science articles, 622K citing sentences according to [Document 1].\n\nAnswer: 8.1 million scientific documents, 154K computer science articles, 622K citing sentences\n\nExample 2:\n\n[Document 1]: We asked authors to rank documents by how \u201cclose to your work\u201d they were. The definition of closeness was left to the discretion of the author. The dataset is composed of 90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations.\n\nQuestion: what is the size of this built corpus?\n\nExplanation:  The size of the corpus is described in [Document 1].\n\nAnswer: 90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations\n\nExample 3:\n\nQuestion: How large is the corpus?\n\nExplanation: \n\n[Document 1]: The corpus contains 106,350 documents, including news articles, blog posts, and product reviews.\n\nThis is the only mention of the size of the corpus in the documents, so we can assume that it contains 106,350 documents.\n\nAnswer: It contains 106,350 documents\n\nExample 4:\n\n[Document 1]: In order to address the drawbacks of existing datasets, we introduce a new corpus based on the Snopes fact-checking website. Our corpus consists of 6,422 validated claims with comprehensive annotations based on the data collected by Snopes fact-checkers and our crowd-workers. The corpus covers multiple domains, including discussion blogs, news, and social media, which are often found responsible for the creation and distribution of unreliable information. In addition to validated claims, the corpus comprises over 14k documents annotated with evidence on two granularity levels and with the stance of the evidence with respect to the claims. Our data allows training machine learning models for the four steps of the automated fact-checking process described above: document retrieval, evidence extraction, stance detection, and claim validation.\n\nQuestion: what is the size of their corpus?\n\nExplanation:"}
{"question_id": "3d547a7dda18a2dd5dc89f12d25d7fe782d66450", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: To formalize what it means for attention matrices to encode linguistic features, we use an attention probe, an analog of edge probing BIBREF11 . An attention probe is a task for a pair of tokens, $(token_i, token_j)$ where the input is a model-wide attention vector formed by concatenating the entries $a_{ij}$ in every attention matrix from every attention head in every layer. The goal is to classify a given relation between the two tokens. If a linear model achieves reliable accuracy, it seems reasonable to say that the model-wide attention vector encodes that relation. We apply attention probes to the task of identifying the existence and type of dependency relation between two words.\n\n[Document 2]: Our first experiment is an exploratory visualization of how word sense affects context embeddings. For data on different word senses, we collected all sentences used in the introductions to English-language Wikipedia articles. (Text outside of introductions was frequently fragmentary.) We created an interactive application, which we plan to make public. A user enters a word, and the system retrieves 1,000 sentences containing that word. It sends these sentences to BERT-base as input, and for each one it retrieves the context embedding for the word from a layer of the user's choosing.\n\nQuestion: What linguistic features were probed for?\n\nExplanation: \n\nAccording to [Document 1], an attention probe was used to probe for the dependency relation between two words. This information is further supported by [Document 2], which states that the word sense was probed for.\n\nAnswer: dependency relation between two words, word sense\n\nExample 2:\n\n[Document 1]: The functional dissimilarity score was computed using sentences from the test set in CoNLL 2017 Universal Dependencies task BIBREF20 for the relevant languages with the provided UPOS sequences. Furthermore, none of the evaluated models, including the proposed method, were trained with CoNLL2017 data.\n\n[Document 2]: We computed the nearest neighbours experiment for all languages in the training data for the above models. The results are shown in Table TABREF27. The results show that general purpose language models do capture syntax information, which varies greatly across languages and models.\n\nQuestion: Which evaluation metrics do they use for language modelling?\n\nExplanation: \n\nAccording to [Document 1], the functional dissimilarity score was computed using sentences from the test set in CoNLL 2017 Universal Dependencies task. Furthermore, the nearest neighbours experiment was computed for all languages in the training data.\n\nAnswer:  functional dissimilarity score, nearest neighbours experiment\n\nExample 3:\n\n[Document 1]: In this section, we consider 3 categories of data augmentation techniques that are effectively applicable to video datasets.\n\n[Document 2]: To further increase training data size and diversity, we can create new audios via superimposing each original audio with additional noisy audios in time domain. To obtain diverse noisy audios, we use AudioSet, which consists of 632 audio event classes and a collection of over 2 million manually-annotated 10-second sound clips from YouTube videos BIBREF28.\n\n[Document 3]: We consider applying the frequency and time masking techniques \u2013 which are shown to greatly improve the performance of end-to-end ASR models BIBREF23 \u2013 to our hybrid systems. Similarly, they can be applied online during each epoch of LF-MMI training, without the need for realignment.\n\n[Document 4]: Consider each utterance (i.e. after the audio segmentation in Section SECREF5), and we compute its log mel spectrogram with $\\nu $ dimension and $\\tau $ time steps:\n\n[Document 5]: Frequency masking is applied $m_F$ times, and each time the frequency bands $[f_0$, $f_0+ f)$ are masked, where $f$ is sampled from $[0, F]$ and $f_0$ is sampled from $[0, \\nu - f)$.\n\n[Document 6]: Time masking is optionally applied $m_T$ times, and each time the time steps $[t_0$, $t_0+ t)$ are masked, where $t$ is sampled from $[0, T]$ and $t_0$ is sampled from $[0, \\tau - t)$.\n\n[Document 7]: Data augmentation ::: Speed and volume perturbation\n\n[Document 8]: Both speed and volume perturbation emulate mean shifts in spectrum BIBREF18, BIBREF19. To perform speed perturbation of the training data, we produce three versions of each audio with speed factors $0.9$, $1.0$, and $1.1$. The training data size is thus tripled. For volume perturbation, each audio is scaled with a random variable drawn from a uniform distribution $[0.125, 2]$.\n\nQuestion: What are the best within-language data augmentation methods?\n\nExplanation: \n\nAccording to [Document 3], frequency masking and time masking are effective data augmentation methods. This is further supported by [Document 5] and [Document 6]. Additionally, [Document 2] suggests that additive noise is also an effective data augmentation method, while [Document 8] suggests that speed and volume perturbation are effective methods.\n\nAnswer: Frequency masking, Time masking, Additive noise, Speed and volume perturbation\n\nExample 4:\n\nQuestion: What computational linguistic methods were used for the analysis?\n\nExplanation:"}
{"question_id": "b3432f52af0b95929e6723dd1f01ce029d90a268", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: Do they analyze ELMo?\n\nExplanation:  There is no mention of ELMo in any of the documents.\n\nAnswer: No\n\nExample 2:\n\n[Document 1]: The usage of a purely character-based input would allow us to directly recover and model these features. Consequently, our architecture is based on Embeddings from Language Model or ELMo BIBREF10 . The ELMo layer allows to recover a rich 1,024-dimensional dense vector for each word. Using CNNs, each vector is built upon the characters that compose the underlying words. As ELMo also contains a deep bi-directional LSTM on top of this character-derived vectors, each word-level embedding contains contextual information from their surroundings. Concretely, we use a pre-trained ELMo model, obtained using the 1 Billion Word Benchmark which contains about 800M tokens of news crawl data from WMT 2011 BIBREF24 .\n\n[Document 2]: Subsequently, the contextualized embeddings are passed on to a BiLSTM with 2,048 hidden units. We aggregate the LSTM hidden states using max-pooling, which in our preliminary experiments offered us better results, and feed the resulting vector to a 2-layer feed-forward network, where each layer has 512 units. The output of this is then fed to the final layer of the model, which performs the binary classification.\n\nQuestion: What type of model are the ELMo representations used in?\n\nExplanation:  [Document 1] mentions that ELMo representations are used in a model with a bi-LSTM on top of it. This is further supported by [Document 2], which mentions that the ELMo representations are passed on to a bi-LSTM with max-pooling.\n\nAnswer: A bi-LSTM with max-pooling on top of it\n\nExample 3:\n\nQuestion: Is the new model evaluated on the tasks that BERT and ELMo are evaluated on?\n\nExplanation: \n\n[Document 1]: We evaluate our model on four tasks: part-of-speech (POS) tagging, named entity recognition (NER), semantic role labeling (SRL), and question answering (QA).\n\n[Document 2]: We compare our model to BERT and ELMo on all four tasks.\n\nAccording to [Document 1] and [Document 2], the new model is evaluated on the tasks that BERT and ELMo are evaluated on.\n\nAnswer: Yes\n\nExample 4:\n\n[Document 1]: Switching to ELMo word embeddings improves performance by 2.9 percentage points on an average, corresponding to about 53 test sentences. Of these, about 32 sentences (60% of the improvement) correspond to A-but-B and negation style sentences, which is substantial when considering that only 24.5% of test sentences include these discourse relations ([tab:sst2]Table tab:sst2). As further evidence that ELMo helps on these specific constructions, the non-ELMo baseline model (no-project, no-distill) gets 255 sentences wrong in the test corpus on average, only 89 (34.8%) of which are A-but-B style or negations.\n\nQuestion: Does Elmo learn all possible logic rules?\n\nExplanation:"}
{"question_id": "181027f398a6b79b1ba44d8d41cc1aba0d6f5212", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: In this subsubsection, each of the three raw data sets (associated with their labels) shown in Table 1 is used. The clause data are not used. In other words, the training data used in this subsubsection are the same as those used in previous studies. For each data corpus, 1000 raw data samples are used as the test data, and the rest are used as the training data. The involved algorithms are detailed as follows.\n\n[Document 2]: CNN-C denotes the CNN with (Chinese) character embedding.\n\n[Document 3]: CNN-W denotes the CNN with (Chinese) word embedding.\n\n[Document 4]: CNN-Lex-C denotes the algorithm which also integrates polar words in CNN which is proposed by Shin et al. BIBREF24 . The (Chinese) character embedding is used.\n\n[Document 5]: CNN-Lex-W denotes the algorithm which also integrates polar words in CNN which is proposed by Shin et al. BIBREF24 . The (Chinese) word embedding is used.\n\n[Document 6]: Bi-LSTM-C denotes the BI-LSTM with (Chinese) character embedding.\n\n[Document 7]: Bi-LSTM-W denotes the Bi-LSTM with (Chinese) word embedding.\n\n[Document 8]: Lex-rule denotes the rule-based approach shows in Fig. 1. This approach is unsupervised.\n\n[Document 9]: BOW denotes the conventional algorithm which is based of bag-of-words features.\n\nQuestion: What are the other models they compare to?\n\nExplanation:  [Document 2], [Document 3], [Document 4], [Document 5], [Document 6], [Document 7], [Document 8], and [Document 9] all mention other models that are being compared.\n\nAnswer: CNN-C, CNN-W, CNN-Lex-C, CNN-Lex-W, Bi-LSTM-C , Bi-LSTM-W, Lex-rule, BOW\n\nExample 2:\n\nQuestion: Do they compare to other models?\n\nExplanation: \n\nThe documents do not mention any other models that were compared to the Stanford NER algorithm, the spaCy 2.0 algorithm, and the recurrent model with a CRF top layer.\n\nAnswer: No\n\nExample 3:\n\n[Document 1]: To evaluate the summarization performance of AttSum, we implement rich extractive summarization methods. Above all, we introduce two common baselines. The first one just selects the leading sentences to form a summary. It is often used as an official baseline of DUC, and we name it \u201cLEAD\u201d. The other system is called \u201cQUERY_SIM\u201d, which directly ranks sentences according to its TF-IDF cosine similarity to the query. In addition, we implement two popular extractive query-focused summarization methods, called MultiMR BIBREF2 and SVR BIBREF20 . MultiMR is a graph-based manifold ranking method which makes uniform use of the sentence-to-sentence relationships and the sentence-to-query relationships. SVR extracts both query-dependent and query-independent features and applies Support Vector Regression to learn feature weights. Note that MultiMR is unsupervised while SVR is supervised. Since our model is totally data-driven, we introduce a recent summarization system DocEmb BIBREF9 that also just use deep neural network features to rank sentences. It initially works for generic summarization and we supplement the query information to compute the document representation.\n\n[Document 2]: To verify the effectiveness of the joint model, we design a baseline called ISOLATION, which performs saliency ranking and relevance ranking in isolation. Specifically, it directly uses the sum pooling over sentence embeddings to represent the document cluster. Therefore, the embedding similarity between a sentence and the document cluster could only measure the sentence saliency. To include the query information, we supplement the common hand-crafted feature TF-IDF cosine similarity to the query. This query-dependent feature, together with the embedding similarity, are used in sentence ranking. ISOLATION removes the attention mechanism, and mixtures hand-crafted and automatically learned features. All these methods adopt the same sentence selection process illustrated in Section \"Sentence Selection\" for a fair comparison.\n\nQuestion: What models do they compare to?\n\nExplanation:  [Document 1] and [Document 2] list the models that are being compared. LEAD, QUERY_SIM, MultiMR, SVR, and DocEmb are compared in [Document 1]. ISOLATION is compared in [Document 2].\n\nAnswer: LEAD, QUERY_SIM, MultiMR, SVR, DocEmb, ISOLATION\n\nExample 4:\n\n[Document 1]: Table TABREF15 presents P@10, MAP and MRR results of our KeyVec model and competing embedding methods in academic paper retrieval. word2vec averaging generates an embedding for a document by averaging the word2vec vectors of its constituent words. In the experiment, we used two different versions of word2vec: one from public release, and the other one trained specifically on our own academic corpus (113 GB). From Table TABREF15 , we observe that as a document-embedding model, Paragraph Vector gave better retrieval results than word2vec averagings did. In contrast, our KeyVec outperforms all the competitors given its unique capability of capturing and embedding the key information of documents.\n\nQuestion: what other representations do they compare with?\n\nExplanation:"}
{"question_id": "6bce04570d4745dcfaca5cba64075242308b65cf", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Early approaches to Opinion Target Extraction (OTE) were unsupervised, although later on the vast majority of works have been based on supervised and deep learning models. To the best of our knowledge, the first work on OTE was published by BIBREF8 . They created a new task which consisted of generating overviews of the main product features from a collection of customer reviews on consumer electronics. They addressed such task using an unsupervised algorithm based on association mining. Other early unsupervised approaches include BIBREF9 which used a dependency parser to obtain more opinion targets, and BIBREF10 which aimed at extracting opinion targets in newswire via Semantic Role Labelling. From a supervised perspective, BIBREF11 presented an approach which learned the opinion target candidates and a combination of dependency and part-of-speech (POS) paths connecting such pairs. Their results improved the baseline provided by BIBREF8 . Another influential work was BIBREF12 , an unsupervised algorithm called Double Propagation which roughly consists of incrementally augmenting a set of seeds via dependency parsing.\n\n[Document 2]: In spite of this, Table TABREF20 shows that our system outperforms the best previous approaches across the five languages. In some cases, such as Turkish and Russian, the best previous scores were the baselines provided by the ABSA organizers, but for Dutch, French and Spanish our system is significantly better than current state-of-the-art. In particular, and despite using the same system for every language, we improve over GTI's submission, which implemented a CRF system with linguistic features specific to Spanish BIBREF28 .\n\nQuestion: What was the baseline?\n\nExplanation: \n\nAccording to [Document 1], the baseline provided by BIBREF8 was an early unsupervised approach to Opinion Target Extraction. This information is further supported by [Document 2], which states that the baselines provided by the ABSA organizers were the best previous scores for some languages.\n\nAnswer: the baseline provided by BIBREF8, the baselines provided by the ABSA organizers\n\nExample 2:\n\nQuestion: What was the baseline?\n\nExplanation:  There is not enough information provided to answer the question.\n\nAnswer: Unanswerable\n\nExample 3:\n\nQuestion: What was the baseline?\n\nExplanation:  There is not enough information provided to answer the question.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: The BiGRU+CRF model was used as the baseline model. Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.\n\nQuestion: what was the baseline?\n\nExplanation:"}
{"question_id": "cb58605a7c230043bd0d6e8d5b068f8b533f45fe", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Our application is the escalation of Internet chats. To maintain quality of service, users are transferred to human representatives when their conversations with an intelligent virtual assistant (IVA) fail to progress. These transfers are known as escalations. We apply Han to such conversations in a sequential manner by feeding each user turn to Han as they occur, to determine if the conversation should escalate. If so, the user will be transferred to a live chat representative to continue the conversation. To help the human representative quickly determine the cause of the escalation, we generate a visualization of the user's turns using the attention weights to highlight the turns influential in the escalation decision. This helps the representative quickly scan the conversation history and determine the best course of action based on problematic turns.\n\n[Document 2]: Our application requires that the visualizations be generated in real-time at the point of escalation. The user must wait for the human representative to review the IVA chat history and resume the failed task. Therefore, we seek visualization methods that do not add significant latency to the escalation transfer. Using the attention weights for turn influence is fast as they were already computed at the time of classification. However, these weights will not generate useful visualizations for the representatives when their values are similar across all turns (see Han Weight in Table TABREF1 ). To overcome this problem, we develop a visualization method to be applied in the instances where the attention weights are uniform. Our method produces informative visuals for determining influential samples in a sequence by observing the changes in sample importance over the cumulative sequence (see Our Weight in Table TABREF1 ). Note that we present a technique that only serves to resolve situations when the existing attention weights are ambiguous; we are not developing a new attention mechanism, and, as our method is external, it does not require any changes to the existing model to apply.\n\nQuestion: How do they gather human reviews?\n\nExplanation:  [Document 1] states that \"These transfers are known as escalations. We apply Han to such conversations in a sequential manner by feeding each user turn to Han as they occur, to determine if the conversation should escalate. If so, the user will be transferred to a live chat representative to continue the conversation.\" This information is further supported by [Document 2], which states that \"The user must wait for the human representative to review the IVA chat history and resume the failed task.\"\n\nAnswer: human representative to review the IVA chat history and resume the failed task\n\nExample 2:\n\n[Document 1]: To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each.\n\nQuestion: Where did they collect their dataset from?\n\nExplanation:  The answer can be found in [Document 1].\n\nAnswer: from Arabic WikiNews site https://ar.wikinews.org/wiki\n\nExample 3:\n\n[Document 1]: Secondly, texts go through a cascade of annotation tools, enriching it with the following information:\n\n[Document 2]: Morphosyntactic interpretations (sets of tags), using Morfeusz 0.82 BIBREF25 ,\n\n[Document 3]: Tagging (selection of the most probable interpretation), using a transformation-based learning tagger, PANTERA 0.9.1 BIBREF26 ,\n\n[Document 4]: Syntactic groups (possibly nested) with syntactic and semantic heads, using a rule-based shallow parser Spejd 1.3.7 BIBREF27 with a Polish grammar, including improved version of modifications by BIBREF28 , enabling lemmatisation of nominal syntactic groups,\n\n[Document 5]: Named entities, using two available tools: NERF 0.1 BIBREF29 and Liner2 2.3 BIBREF30 .\n\nQuestion: How is the data in RAFAEL labelled?\n\nExplanation:  [Document 1] states that texts go through a cascade of annotation tools, which [Document 2-5] list as Morfeusz, PANTERA, Spejd, NERF and Liner.\n\nAnswer: Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner\n\nExample 4:\n\n[Document 1]: The data set obtained from Pypestream is permitted to be used for the research purpose only, and for a security reason, we are not allowed to share the data set. It was once originaly used for creating a conversational intelligence system(chatbot) to support customer inqueries about a particular service. The data set is a two-column comma separated value format data with one column of \"sentence\" and the other column of \"class\". It contains 2,212 unique sentences of user expressions asking questions and aswering to the questions the chatbot asked to the users(see Table.TABREF9). The sentences are all in English without having any missspelled words, and labeled with 19 distinct classes that are identified and designed by humans. Additional data set that only contains the sentences was made for the purpose of this study by taking out the \"class\" column from the original data set.\n\nQuestion: did they collect the human labeled data?\n\nExplanation:"}
{"question_id": "d5bce5da746a075421c80abe10c97ad11a96c6cd", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We chose the best system based on the dev set, and used that for submitting private test predictions on both FEVER and HotpotQA .\n\n[Document 2]: As can be seen in Table TABREF8, with the proposed hierarchical system design, the whole pipeline system achieves new start-of-the-art on HotpotQA with large-margin improvements on all the metrics. More specifically, the biggest improvement comes from the EM for the supporting fact which in turn leads to doubling of the joint EM on previous best results. The scores for answer predictions are also higher than all previous best results with $\\sim $8 absolute points increase on EM and $\\sim $9 absolute points on F1. All the improvements are consistent between test and dev set evaluation.\n\n[Document 3]: Similarly for FEVER, we showed F1 for evidence, the Label Accuracy, and the FEVER Score (same as benchmark evaluation) for models in Table TABREF9. Our system obtained substantially higher scores than all previously published results with a $\\sim $4 and $\\sim $3 points absolute improvement on Label Accuracy and FEVER Score. In particular, the system gains 74.62 on the evidence F1, 22 points greater that of the second system, demonstrating its ability on semantic retrieval.\n\nQuestion: What baseline approaches do they compare against?\n\nExplanation: \n\nAccording to [Document 2], the proposed hierarchical system design outperforms Yang, Ding, and Muppet on HotpotQA. Similarly, according to [Document 3], the proposed system outperforms Hanselowski, Yoneda, and Nie on Fever.\n\nAnswer: HotspotQA: Yang, Ding, Muppet\nFever: Hanselowski, Yoneda, Nie\n\nExample 2:\n\nQuestion: What baseline models do they compare against?\n\nExplanation: \n\nThe table in [Document 1] shows the baselines that the different models were compared against. The baselines include the SLQA model, the Rusalka model, the HMA Model (single), the TriAN (single), the jiangnan (ensemble), the MITRE (ensemble), the TriAN (ensemble), and the HMA Model (ensemble).\n\nAnswer: SLQA, Rusalka, HMA Model (single), TriAN (single), jiangnan (ensemble), MITRE (ensemble), TriAN (ensemble), HMA Model (ensemble)\n\nExample 3:\n\nQuestion: What are the baseline systems that are compared against?\n\nExplanation: \n\n[Document 1]: In this work, we propose a novel approach for learning the latent tree structure in Tree-LSTM models. We compare our method against three baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM.\n\n[Document 2]: The LSTM model is a special kind of RNN, which is designed to model long-term dependencies.\n\n[Document 3]: The RL-SPINN model is a recurrent neural network that uses reinforcement learning to learn how to build a tree structure.\n\n[Document 4]: The Gumbel Tree-LSTM is a tree-structured LSTM model that uses the Gumbel-Softmax distribution to learn the tree structure.\n\nAnswer: The system is compared to baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM\n\nExample 4:\n\n[Document 1]: Table TABREF28 shows results on the event expression tasks. Our initial submits RUN 4 and 5 outperformed the memorization baseline on every metric on every task. The precision of event span identification is close to the max report. However, our system got lower recall. One of the main reason is that our training objective function is accuracy-oriented. Table TABREF29 shows results on the phase 2 subtask.\n\n[Document 2]: FLOAT SELECTED: Table 3: System performance comparison. Note that Run4 means the window size is 4, Run5 means the window size is 5\n\n[Document 3]: FLOAT SELECTED: Table 4: Phase 2: DocTimeRel\n\nQuestion: Which baselines did they compare against?\n\nExplanation:"}
{"question_id": "134a66580c363287ec079f353ead8f770ac6d17b", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Our lexical features include 1-, 2-, and 3-grams in both word and character levels. For each type of INLINEFORM0 -grams, we utilize only the top 1,000 INLINEFORM1 -grams based on the term frequency-inverse document frequency (tf-idf) values. That is, each INLINEFORM2 -gram appearing in a tweet becomes an entry in the feature vector with the corresponding feature value tf-idf. We also use the number of characters and the number of words as features.\n\n[Document 2]: We use the NLTK toolkit to tokenize and annotate part-of-speech tags (POS tags) for all tweets in the dataset. We then use all the POS tags with their corresponding tf-idf values as our syntactic features and feature values, respectively.\n\n[Document 3]: Firstly, we employ 300-dimensional pre-trained word embeddings from GloVe BIBREF29 to compute a tweet embedding as the average of the embeddings of words in the tweet.\n\n[Document 4]: Secondly, we apply the latent semantic indexing BIBREF30 to capture the underlying semantics of the dataset. Here, each tweet is represented as a vector of 100 dimensions.\n\n[Document 5]: Thirdly, we also extract tweet representation by applying the Brown clustering algorithm BIBREF31 , BIBREF32 \u2014a hierarchical clustering algorithm which groups the words with similar meaning and syntactical function together. Applying the Brown clustering algorithm, we obtain a set of clusters, where each word belongs to only one cluster. For example in Table TABREF13 , words that indicate the members of a family (e.g., \u201cmum\u201d, \u201cdad\u201d) or positive sentiment (e.g., \u201cinteresting\u201d, \u201cawesome\u201d) are grouped into the same cluster. We run the algorithm with different number of clustering settings (i.e., 80, 100, 120) to capture multiple semantic and syntactic aspects. For each clustering setting, we use the number of tweet words in each cluster as a feature. After that, for each tweet, we concatenate the features from all the clustering settings to form a cluster-based tweet embedding.\n\n[Document 6]: Motivated by the verbal irony by means of polarity contrast, such as \u201cI really love this year's summer; weeks and weeks of awful weather\u201d, we use the number of polarity signals appearing in a tweet as the polarity features. The signals include positive words (e.g., love), negative words (e.g., awful), positive emoji icon and negative emoji icon. We use the sentiment dictionaries provided by BIBREF33 to identify positive and negative words in a tweet. We further use boolean features that check whether or not a negation word is in a tweet (e.g., not, n't).\n\nQuestion: What type of lexical, syntactic, semantic and polarity features are used?\n\nExplanation: \n\nAccording to [Document 1], our lexical features include 1-, 2-, and 3-grams in both word and character levels, number of characters and the number of words. This information is further supported by [Document 2], which states that we use POS tags as our syntactic features.\n\nAccording to [Document 3], we use 300-dimensional pre-trained word embeddings from GloVe to compute a tweet embedding. This information is further supported by [Document 4], which states that we use latent semantic indexing to capture the underlying semantics of the dataset.\n\nAccording to [Document 5], we use the Brown clustering algorithm to extract tweet representation. This information is further supported by [Document 6], which states that we use the number of polarity signals appearing in a tweet as the polarity features.\n\nAnswer: Our lexical features include 1-, 2-, and 3-grams in both word and character levels., number of characters and the number of words, POS tags, 300-dimensional pre-trained word embeddings from GloVe, latent semantic indexing, tweet representation by applying the Brown clustering algorithm, positive words (e.g., love), negative words (e.g., awful), positive emoji icon and negative emoji icon, boolean features that check whether or not a negation word is in a tweet\n\nExample 2:\n\n[Document 1]: For goal-oriented generation, many of the models evaluated using the Inform/Request metrics have made use of structured data to semantically condition the generation model in order to generate better responses BIBREF35, BIBREF1, BIBREF0. BIBREF35 proposed to encode each individual dialog act as a unique vector and use it as an extra input feature, in order to influence the generated response. This method has been shown to work well when tested on single domains where the label space is limited to a few dialog acts. However, as the label space grows, using a one-hot encoding representation of the dialog act is not scalable. To deal with this problem, BIBREF1 introduced a semantically conditioned generation model using Hierarchical Disentangled Self-Attention (HDSA) . This model deals with the large label space by representing dialog acts using a multi-layer hierarchical graph that merges cross-branch nodes. For example, the distinct trees for hotel-recommend-area and attraction-recommend-area can be merged at the second and third levels sharing semantic information about actions and slots but maintaining specific information about the domains separate. This information can then be used as part of an attention mechanism when generating a response. This model achieves the state-of-the-art result for generation in both BLEU and Inform/Request metrics.\n\nQuestion: what semantically conditioned models did they compare with?\n\nExplanation:  [Document 1] states that the semantically conditioned generation model using Hierarchical Disentangled Self-Attention is the state-of-the-art result for generation in both BLEU and Inform/Request metrics.\n\nAnswer: Hierarchical Disentangled Self-Attention\n\nExample 3:\n\n[Document 1]: In the vector space model approach, we follow the sparse context-based disambiguated method BIBREF12 , BIBREF13 . For estimating the sense of the word INLINEFORM0 in a sentence, we search for such a synset INLINEFORM1 that maximizes the cosine similarity to the sentence vector: DISPLAYFORM0\n\nQuestion: What measure of semantic similarity is used?\n\nExplanation:  [Document 1] mentions that the cosine similarity is used to measure the semantic similarity between a word and a sentence.\n\nAnswer: cosine similarity\n\nExample 4:\n\nQuestion: What fine-grained semantic types are considered?\n\nExplanation:"}
{"question_id": "909ecf675f874421eecc926a9f7486475aa1423c", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: The first phase is to extract a knowledge graph from the story that depicts locations, characters, objects, and the relations between these entities. We present two techniques. The first uses neural question-answering technique to extract relations from a story text. The second, provided as a baseline, uses OpenIE5, a commonly used rule-based information extraction technique. For the sake of simplicity, we considered primarily the location-location and location-character/object relations, represented by the \u201cnext to\u201d and \u201chas\u201d edges respectively in Figure FIGREF4.\n\n[Document 2]: While many neural models already exist that perform similar tasks such as named entity extraction and part of speech tagging, they often come at the cost of large amounts of specialized labeled data suited for that task. We instead propose a new method that leverages models trained for context-grounded question-answering tasks to do entity extraction with no task dependent data or fine-tuning necessary. Our method, dubbed AskBERT, leverages the Question-Answering (QA) model ALBERT BIBREF15. AskBERT consists of two main steps as shown in Figure FIGREF7: vertex extraction and graph construction.\n\n[Document 3]: The first step is to extract the set of entities\u2014graph vertices\u2014from the story. We are looking to extract information specifically regarding characters, locations, and objects. This is done by using asking the QA model questions such as \u201cWho is a character in the story?\u201d. BIBREF16 have shown that the phrasing of questions given to a QA model is important and this forms the basis of how we formulate our questions\u2014questions are asked so that they are more likely to return a single answer, e.g. asking \u201cWhere is a location in the story?\u201d as opposed to \u201cWhere are the locations in the story?\u201d. In particular, we notice that pronoun choice can be crucial; \u201cWhere is a location in the story?\u201d yielded more consistent extraction than \u201cWhat is a location in the story?\u201d. ALBERT QA is trained to also output a special <$no$-$answer$> token when it cannot find an answer to the question within the story. Our method makes use of this by iteratively asking QA model a question and masking out the most likely answer outputted on the previous step. This process continues until the <$no$-$answer$> token becomes the most likely answer.\n\n[Document 4]: The next step is graph construction. Typical interactive fiction worlds are usually structured as trees, i.e. no cycles except between locations. Using this fact, we use an approach that builds a graph from the vertex set by one relation\u2014or edge\u2014at a time. Once again using the entire story plot as context, we query the ALBERT-QA model picking a random starting location $x$ from the set of vertices previously extracted.and asking the questions \u201cWhat location can I visit from $x$?\u201d and \u201cWho/What is in $x$?\u201d. The methodology for phrasing these questions follows that described for the vertex extraction. The answer given by the QA model is matched to the vertex set by picking the vertex $u$ that contains the best word-token overlap with the answer. Relations between vertices are added by computing a relation probability on the basis of the output probabilities of the answer given by the QA model. The probability that vertices $x,u$ are related:\n\n[Document 5]: We compared our proposed AskBERT method with a non-neural, rule-based approach. This approach is based on the information extracted by OpenIE5, followed by some post-processing such as named-entity recognition and part-of-speech tagging. OpenIE5 combines several cutting-edge ideas from several existing papers BIBREF17, BIBREF18, BIBREF19 to create a powerful information extraction tools. For a given sentence, OpenIE5 generates multiple triples in the format of $\\langle entity, relation, entity\\rangle $ as concise representations of the sentence, each with a confidence score. These triples are also occasionally annotated with location information indicating that a triple happened in a location.\n\n[Document 6]: As in the neural AskBERT model, we attempt to extract information regarding locations, characters, and objects. The entire story plot is passed into the OpenIE5 and we receive a set of triples. The location annotations on the triples are used to create a set of locations. We mark which sentences in the story contain these locations. POS tagging based on marking noun-phrases is then used in conjunction with NER to further filter the set of triples\u2014identifying the set of characters and objects in the story.\n\n[Document 7]: The graph is constructed by linking the set of triples on the basis of the location they belong to. While some sentences contain very explicit location information for OpenIE5 to mark it out in the triples, most of them do not. We therefore make the assumption that the location remains the same for all triples extracted in between sentences where locations are explicitly mentioned. For example, if there exists $location A$ in the 1st sentence and $location B$ in the 5th sentence of the story, all the events described in sentences 1-4 are considered to take place in $location A$. The entities mentioned in these events are connected to $location A$ in the graph.\n\nQuestion: How is the information extracted?\n\nExplanation: \n\nAccording to [Document 1], information is extracted using a neural question-answering technique and OpenIE5. This is further supported by [Document 2], [Document 3], [Document 4], [Document 5], [Document 6], and [Document 7].\n\nAnswer: neural question-answering technique to extract relations from a story text, OpenIE5, a commonly used rule-based information extraction technique\n\nExample 2:\n\n[Document 1]: These sentiment scores were automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15 and pretrained on in-house psychiatric EHR text. In our paper we also showed that this automatic pipeline achieves reasonably strong F-scores, with an overall performance of 0.828 F1 for the topic extraction component and 0.5 F1 on the clinical sentiment component.\n\nQuestion: How do they extract topics?\n\nExplanation:  [Document 1] states that the topics were automatically extracted through the topic extraction and sentiment analysis pipeline introduced in their prior work.\n\nAnswer:  automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15\n\nExample 3:\n\nQuestion: What are the categories being extracted?\n\nExplanation:  The documents do not provide enough information to answer the question.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: A commonly known limitation of the current ASR systems is their inability to recognize long sequences of words precisely. In this paper, we propose a new method of incorporating domain knowledge into automatic speech recognition which alleviates this weakness. Our approach allows performing fast ASR domain adaptation by providing a library of intent examples used for lattice rescoring. The method guides the best lattice path selection process by increasing the probability of intent recognition. At the same time, the method does not rescore paths of unessential turns which do not contain intent examples. As a result, our approach improves the understanding of spontaneous conversations by recognizing semantically important transcription segments while adding minimal computational overhead. Our method is domain agnostic and can be easily adapted to a new one by providing the library of intent examples expected to appear in the new domain. The increased intent annotation coverage allows us to train more sophisticated models for downstream tasks, opening the prospects of true spoken language understanding.\n\nQuestion: How do they use extracted intent to rescore?\n\nExplanation:"}
{"question_id": "22488c8628b6db5fd708b6471c31a8eac31f66df", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: How big is multilingual dataset?\n\nExplanation:  The documents do not contain any information regarding the size of the multilingual dataset.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: In this work, we introduce IndoSum, a new benchmark dataset for Indonesian text summarization, and evaluated several well-known extractive single-document summarization methods on the dataset. The dataset consists of online news articles and has almost 200 times more documents than the next largest one of the same domain BIBREF2 . To encourage further research in this area, we make our dataset publicly available. In short, the contribution of this work is two-fold:\n\n[Document 2]: We used a dataset provided by Shortir, an Indonesian news aggregator and summarizer company. The dataset contains roughly 20K news articles. Each article has the title, category, source (e.g., CNN Indonesia, Kumparan), URL to the original article, and an abstractive summary which was created manually by a total of 2 native speakers of Indonesian. There are 6 categories in total: Entertainment, Inspiration, Sport, Showbiz, Headline, and Tech. A sample article-summary pair is shown in Fig. FIGREF4 .\n\nQuestion: What is the size of the dataset?\n\nExplanation:  The dataset consists of 20,000 news articles according to [Document 2].\n\nAnswer: 20K\n\nExample 3:\n\n[Document 1]: The English language is well studied under the umbrella of NLP, hence many resources and datasets for the different problems are available. However, research on English-Roman Urdu bilingual text lags behind because of non-availability of gold standard datasets. Our second contribution is that we present a large scale annotated dataset in Roman Urdu and English language with code-switching, for multi-class classification. The dataset consists of more than $0.3$ million records and has been made available for future research.\n\nQuestion: What is the size of the dataset?\n\nExplanation:  The size of the dataset is explicitly stated in [Document 1].\n\nAnswer: $0.3$ million records\n\nExample 4:\n\n[Document 1]: We split our dataset into training (80%, 44,972), validation (10%, 5,622), and test (10%, 5,622) sets. Table TABREF5 compares Multi-News to other news datasets used in experiments below. We choose to compare Multi-News with DUC data from 2003 and 2004 and TAC 2011 data, which are typically used in multi-document settings. Additionally, we compare to the single-document CNNDM dataset, as this has been recently used in work which adapts SDS to MDS BIBREF11 . The number of examples in our Multi-News dataset is two orders of magnitude larger than previous MDS news data. The total number of words in the concatenated inputs is shorter than other MDS datasets, as those consist of 10 input documents, but larger than SDS datasets, as expected. Our summaries are notably longer than in other works, about 260 words on average. While compressing information into a shorter text is the goal of summarization, our dataset tests the ability of abstractive models to generate fluent text concise in meaning while also coherent in the entirety of its generally longer output, which we consider an interesting challenge.\n\nQuestion: What is the size of Multi-news dataset?\n\nExplanation:"}
{"question_id": "77bbe1698e001c5889217be3164982ea36e85752", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We compared our proposed approaches to a standard CNN that exploits a single set of word embeddings BIBREF3 . We also compared to a baseline of simply concatenating embeddings for each word to form long vector inputs. We refer to this as Concatenation-CNN C-CNN. For all multiple embedding approaches (C-CNN, MG-CNN and MGNC-CNN), we explored two combined sets of embedding: word2vec+Glove, and word2vec+syntactic, and one three sets of embedding: word2vec+Glove+syntactic. For all models, we tuned the l2 norm constraint INLINEFORM0 over the range INLINEFORM1 on a validation set. For instantiations of MGNC-CNN in which we exploited two embeddings, we tuned both INLINEFORM2 , and INLINEFORM3 ; where we used three embedding sets, we tuned INLINEFORM4 and INLINEFORM5 .\n\nQuestion: What are the baseline models?\n\nExplanation: \n\nAccording to [Document 1], the baseline models are MC-CNN, MVCNN, and CNN. This information is further supported by [Document 2], [Document 3], and [Document 5].\n\nAnswer: MC-CNN\nMVCNN\nCNN\n\nExample 2:\n\n[Document 1]: Methods ::: Models Tested ::: Recurrent Neural Network (RNN) Language Models\n\n[Document 2]: are trained to output the probability distribution of the upcoming word given a context, without explicitly representing the structure of the context BIBREF9, BIBREF10. We trained two two-layer recurrent neural language models with long short-term memory architecture BIBREF11 on a relatively small corpus. The first model, referred as `LSTM (PTB)' in the following sections, was trained on the sentences from Penn Treebank BIBREF12. The second model, referred as `LSTM (FTB)', was trained on the sentences from French Treebank BIBREF13. We set the size of input word embedding and LSTM hidden layer of both models as 256.\n\n[Document 3]: Methods ::: Models Tested ::: ActionLSTM\n\n[Document 4]: models the linearized bracketed tree structure of a sentence by learning to predict the next action required to construct a phrase-structure parse BIBREF16. The action space consists of three possibilities: open a new non-terminal node and opening bracket; generate a terminal node; and close a bracket. To compute surprisal values for a given token, we approximate $P(w_i|w_{1\\cdots i-1)}$ by marginalizing over the most-likely partial parses found by word-synchronous beam search BIBREF17.\n\n[Document 5]: Methods ::: Models Tested ::: Generative Recurrent Neural Network Grammars (RNNG)\n\n[Document 6]: jointly model the word sequence as well as the underlying syntactic structure BIBREF18. Following BIBREF19, we estimate surprisal using word-synchronous beam search BIBREF17. We use the same hyper-parameter settings as BIBREF18.\n\nQuestion: What are the baseline models?\n\nExplanation:  [Document 1], [Document 3], and [Document 5] list the baseline models as RNN, ActionLSTM, and RNNG respectively. This information is further supported by [Document 2], [Document 4], and [Document 6].\n\nAnswer: Recurrent Neural Network (RNN), ActionLSTM, Generative Recurrent Neural Network Grammars (RNNG)\n\nExample 3:\n\n[Document 1]: Majority: the text picks the label of the largest size.\n\n[Document 2]: ESA: A dataless classifier proposed in BIBREF0. It maps the words (in text and label names) into the title space of Wikipedia articles, then compares the text with label names. This method does not rely on train.\n\n[Document 3]: We implemented ESA based on 08/01/2019 Wikipedia dump. There are about 6.1M words and 5.9M articles.\n\n[Document 4]: Word2Vec BIBREF23: Both the representations of the text and the labels are the addition of word embeddings element-wisely. Then cosine similarity determines the labels. This method does not rely on train either.\n\n[Document 5]: Binary-BERT: We fine-tune BERT on train, which will yield a binary classifier for entailment or not; then we test it on test \u2013 picking the label with the maximal probability in single-label scenarios while choosing all the labels with \u201centailment\u201d decision in multi-label cases.\n\nQuestion: What are their baseline models?\n\nExplanation:  [Document 1], [Document 2], [Document 4], and [Document 5] all mention different baseline models. [Document 3] provides additional information about the ESA baseline model.\n\nAnswer: Majority, ESA, Word2Vec , Binary-BERT\n\nExample 4:\n\n[Document 1]: This paper explores two types of cross-structures to help cope with the problem: Cross-BiLSTM-CNN and Att-BiLSTM-CNN. Previous studies have tried to stack multiple LSTMs for sequence-labeling NER BIBREF1. As they follow the trend of stacking forward and backward LSTMs independently, the Baseline-BiLSTM-CNN is only able to learn higher-level representations of past or future per se. Instead, Cross-BiLSTM-CNN, which interleaves every layer of the two directions, models cross-context in an additive manner by learning higher-level representations of the whole context of each token. On the other hand, Att-BiLSTM-CNN models cross-context in a multiplicative manner by capturing the interaction between past and future with a dot-product self-attentive mechanism BIBREF5, BIBREF6.\n\nQuestion: What are the baseline models?\n\nExplanation:"}
{"question_id": "d0dc6729b689561370b6700b892c9de8871bb44d", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: In this work, we propose to augment the integer linear programming (ILP)-based summarization framework with a low-rank approximation of the co-occurrence matrix, and further evaluate the approach on a broad range of datasets exhibiting high lexical diversity. The ILP framework, being extractive in nature, has demonstrated considerable success on a number of summarization tasks BIBREF20 , BIBREF21 . It generates a summary by selecting a set of sentences from the source documents. The sentences shall maximize the coverage of important source content, while minimizing the redundancy among themselves. At the heart of the algorithm is a sentence-concept co-occurrence matrix, used to determine if a sentence contains important concepts and whether two sentences share the same concepts. We introduce a low-rank approximation to the co-occurrence matrix and optimize it using the proximal gradient method. The resulting system thus allows different sentences to share co-occurrence statistics. For example, \u201cThe activity with the bicycle parts\" will be allowed to partially contain \u201cbike elements\" although the latter phrase does not appear in the sentence. The low-rank matrix approximation provides an effective way to implicitly group lexically-diverse but semantically-similar expressions. It can handle out-of-vocabulary expressions and domain-specific terminologies well, hence being a more principled approach than heuristically calculating similarities of word embeddings.\n\nQuestion: What do they constrain using integer linear programming?\n\nExplanation: \n\nAccording to [Document 1], the integer linear programming (ILP)-based summarization framework is augmented with a low-rank approximation of the co-occurrence matrix. This means that the ILP framework constrains the low-rank approximation of the co-occurrence matrix.\n\nAnswer: low-rank approximation of the co-occurrence matrix\n\nExample 2:\n\n[Document 1]: Table TABREF2 specifies the formulation of each model. Note that we adopt a parsimonious design principle in our modelling: both Centroid and Na\u00efve Bayes are parameter-free models, $k$NN only depends on the choice of $k$, and KDE uses a single bandwidth parameter $h$.\n\n[Document 2]: A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule;\n\n[Document 3]: A Na\u00efve Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class;\n\nQuestion: How does the parameter-free model work?\n\nExplanation: \n\nAccording to [Document 1], the Centroid model and the Na\u00efve Bayes model are parameter-free models. This information is further supported by [Document 2] and [Document 3].\n\nAnswer: A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule;, A Na\u00efve Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class;\n\nExample 3:\n\n[Document 1]: Each drug label is a collection of sections (e.g., DOSAGE & ADMINISTRATION, CONTRAINDICATIONS, and WARNINGS) where each section contains one or more sentences. Each sentence is annotated with a list of zero or more mentions and interactions. The training data released for this task contains 22 drug labels, referred to as Training-22, with gold standard annotations. Two test sets of 57 and 66 drug labels, referred to as Test Set 1 and 2 respectively, with gold standard annotations are used to evaluate participating systems. As Training-22 is a relatively small dataset, we additionally utilize an external dataset with 180 annotated drug labels dubbed NLM-180 BIBREF5 (more later). We provide summary statistics about these datasets in Table TABREF3 . Test Set 1 closely resembles Training-22 with respect to the sections that are annotated. However, Test Set 1 is more sparse in the sense that there are more sentences per drug label (144 vs. 27), with a smaller proportion of those sentences having gold annotations (23% vs. 51%). Test Set 2 is unique in that it contains annotations from only two sections, namely DRUG INTERACTIONS and CLINICAL PHARMACOLOGY, the latter of which is not represented in Training-22 (nor Test Set 1). Lastly, Training-22, Test Set 1, and Test Set 2 all vary with respect to the distribution of interaction types, with Training-22, Test Set 1, and Test Set 2 containing a higher proportion of PD, UN, and PK interactions respectively.\n\nQuestion: What training data did they use?\n\nExplanation:  The training data used was Training-22 and NLM-180, as stated in [Document 1].\n\nAnswer: Training-22, NLM-180\n\nExample 4:\n\n[Document 1]: In this paper, we give a method for substantially improving NMT results on these languages. Neural models learn representations of their input that are often useful across tasks. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model, letting the rest be fine-tuned by the child model. We report NMT improvements from transfer learning of 5.6 Bleu on average, and we provide an analysis of why the method works. The final NMT system approaches strong SBMT baselines in in all four language pairs, and exceeds SBMT performance in one of them. Furthermore, we show that NMT is an exceptional re-scorer of `traditional' MT output; even NMT that on its own is worse than SBMT is consistently able to improve upon SBMT system output when incorporated as a re-scoring model.\n\n[Document 2]: A justification for this approach is that in scenarios where we have limited training data, we need a strong prior distribution over models. The parent model trained on a large amount of bilingual data can be considered an anchor point, the peak of our prior distribution in model space. When we train the child model initialized with the parent model, we fix parameters likely to be useful across tasks so that they will not be changed during child-model training. In the French-English to Uzbek-English example, as a result of the initialization, the English word embeddings from the parent model are copied, but the Uzbek words are initially mapped to random French embeddings. The English embeddings should be kept but the Uzbek embeddings should be modified during training of the child model. Freezing certain portions of the parent model and fine tuning others can be considered a hard approximation to a tight prior or strong regularization applied to some of the parameters. We also experiment with ordinary L2 regularization, but find it does not significantly improve over the parameter freezing described above.\n\nQuestion: How did they constrain training using the parameters?\n\nExplanation:"}
{"question_id": "06d5de706348dbe8c29bfacb68ce65a2c55d0391", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: The DBpedia lookup service, which is based on the Spotlight index BIBREF18 , is used for entity lookup (retrieval). The DBpedia SPARQL endpoint is used for query answering and reasoning. The reported results are based on the following settings: the Adam optimizer together with cross-entropy loss are used for network training; $d_r$ and $d_a$ are set to 200 and 50 respectively; $N_0$ is set to 1200; word2vec trained with the latest Wikipedia article dump is adopted for word embedding; and ( $T_s$ , $T_p$ , $T_l$ ) are set to (12, 4, 12) for S-Lite and (12, 4, 15) for R-Lite. The experiments are run on a workstation with Intel(R) Xeon(R) CPU E5-2670 @2.60GHz, with programs implemented by Tensorflow.\n\nQuestion: What is the reasoning method that is used?\n\nExplanation:  [Document 1] mentions that the DBpedia SPARQL endpoint is used for query answering and reasoning.\n\nAnswer: SPARQL\n\nExample 2:\n\n[Document 1]: Experimental results of entity classification on the test sets of all the datasets is shown in Table TABREF25. The results is clearly demonstrate that our proposed method significantly outperforms state-of-art results on accuracy for three datasets. For more in-depth performance analysis, we note: (1) Among all baselines, Path-based methods and Attribute-incorporated methods outperform three typical methods. This indicates that incorporating extra information can improve the knowledge graph embedding performance; (2) Four variants of KANE always outperform baseline methods. The main reasons why KANE works well are two fold: 1) KANE can capture high-order structural information of KGs in an efficient, explicit manner and passe these information to their neighboring; 2) KANE leverages rich information encoded in attribute triples. These rich semantic information can further improve the performance of knowledge graph; (3) The variant of KANE that use LSTM Encoder and Concatenation aggregator outperform other variants. The main reasons is that LSTM encoder can distinguish the word order and concatenation aggregator combine all embedding of multi-head attention in a higher leaver feature space, which can obtain sufficient expressive power.\n\nQuestion: How much better is performance of proposed method than state-of-the-art methods in experiments?\n\nExplanation:  The answer can be directly read off from Table TABREF25 in [Document 1].\n\nAnswer: Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively.\n\nExample 3:\n\n[Document 1]: In DDI extraction task, NLP methods or machine learning approaches are proposed by most of the work. Chowdhury BIBREF14 and Thomas et al. BIBREF11 proposed methods that use linguistic phenomenons and two-stage SVM to classify DDIs. FBK-irst BIBREF10 is a follow-on work which applies kernel method to the existing model and outperforms it.\n\n[Document 2]: Neural network based approaches have been proposed by several works. Liu et al. BIBREF9 employ CNN for DDI extraction for the first time which outperforms the traditional machine learning based methods. Limited by the convolutional kernel size, the CNN can only extracted features of continuous 3 to 5 words rather than distant words. Liu et al. BIBREF8 proposed dependency-based CNN to handle distant but relevant words. Sahu et al. BIBREF12 proposed LSTM based DDI extraction approach and outperforms CNN based approach, since LSTM handles sentence as a sequence instead of slide windows. To conclude, Neural network based approaches have advantages of 1) less reliance on extra NLP toolkits, 2) simpler preprocessing procedure, 3) better performance than text analysis and machine learning methods.\n\nQuestion: What are the existing methods mentioned in the paper?\n\nExplanation:  [Document 1] and [Document 2] both mention different methods that have been proposed by other works in the field.\n\nAnswer: Chowdhury BIBREF14 and Thomas et al. BIBREF11, FBK-irst BIBREF10, Liu et al. BIBREF9, Sahu et al. BIBREF12\n\nExample 4:\n\n[Document 1]: Text: \u201cI like kitties and doggies\u201d\n\n[Document 2]: Window: 2\n\n[Document 3]: Bigrams: {(I like), (like kitties), (kitties and), (and doggies)} and this one:\n\n[Document 4]: Window: 4\n\n[Document 5]: Bigrams: {(I like), (I kitties), (I and), (like kitties), (like and), (like doggies), (kitties and), (kitties doggies), (and doggies)}.\n\nQuestion: What is the computational complexity of old method\n\nExplanation:"}
{"question_id": "f637bba86cfb94ca8ac4b058faf839c257d5eaa0", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We select the following training objectives to learn general-purpose sentence embeddings. Our desiderata for the task collection were: sufficient diversity, existence of fairly large datasets for training, and success as standalone training objectives for sentence representations.\n\n[Document 2]: Multi-task training setup\n\nQuestion: Which model architecture do they for sentence encoding?\n\nExplanation: \n\nAccording to [Document 1], the authors select the following training objectives to learn general-purpose sentence embeddings:\n\n\"Our desiderata for the task collection were: sufficient diversity, existence of fairly large datasets for training, and success as standalone training objectives for sentence representations.\"\n\nThis information is further supported by [Document 2], which states that the authors used a \"multi-task training setup\" to learn sentence embeddings.\n\nAnswer: Answer with content missing: (Skip-thought vectors-Natural Language Inference paragraphs) The encoder for the current sentence and the decoders for the previous (STP) and next sentence (STN) are typically parameterized as separate RNNs\n- RNN\n\nExample 2:\n\n[Document 1]: Our baseline decoder is a standard beam search decoder BIBREF5 with several straightforward performance optimizations:\n\nQuestion: What baseline decoder do they use?\n\nExplanation: \n\n[Document 1] mentions that the baseline decoder is a standard beam search decoder with several straightforward performance optimizations.\n\nAnswer: a standard beam search decoder BIBREF5 with several straightforward performance optimizations\n\nExample 3:\n\n[Document 1]: To enable sentence-level classification we need to obtain a sentence representation from the word vectors INLINEFORM0 . We achieved this by using a BiLSTM with max pooling, which was shown to be a good universal sentence encoding mechanism BIBREF13 .\n\nQuestion: Which model architecture do they use to obtain representations?\n\nExplanation:  The document states that they use a BiLSTM with max pooling to obtain representations.\n\nAnswer: BiLSTM with max pooling\n\nExample 4:\n\n[Document 1]: Prior work BIBREF11 has figured out how to edit prototype in an unconditional setting, but it cannot be applied to the response generation directly. In this paper, we propose a prototype editing method in a conditional setting. Our idea is that differences between responses strongly correlates with differences in their contexts (i.e. if a word in prototype context is changed, its related words in the response are probably modified in the editing.). We realize this idea by designing a context-aware editing model that is built upon a encoder-decoder model augmented with an editing vector. The edit vector is computed by the weighted average of insertion word embeddings and deletion word embeddings. Larger weights mean that the editing model should pay more attention on corresponding words in revision. For instance, in Table TABREF1 , we wish words like \u201cdessert\", \u201cTofu\" and \u201cvegetables\" get larger weights than words like \u201cand\" and \u201c at\". The encoder learns the prototype representation with a gated recurrent unit (GRU), and feeds the representation to a decoder together with the edit vector. The decoder is a GRU language model, that regards the concatenation of last step word embedding and the edit vector as inputs, and predicts the next word with an attention mechanism.\n\nQuestion: What model architecture do they use for the decoder?\n\nExplanation:"}
{"question_id": "8de0e1fdcca81b49615a6839076f8d42226bf1fe", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Our experiments are run using the English$\\rightarrow $Italian/German portions of the MuST-C corpus BIBREF25, which is extracted from TED talks, using the same train/validation/test split as provided with the corpus (see Table TABREF18). As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De). While our main goal is to verify our hypotheses on a large data condition, thus the need to include proprietary data, for the sake of reproducibility in both languages we also provide results with systems only trained on TED Talks (small data condition). When training on large scale data we use Transformer with layer size of 1024, hidden size of 4096 on feed forward layers, 16 heads in the multi-head attention, and 6 layers in both encoder and decoder. When training only on TED talks, we set layer size of 512, hidden size of 2048 for the feed forward layers, multi-head attention with 8 heads and again 6 layers in both encoder and decoder.\n\nQuestion: What dataset do they use?\n\nExplanation:  [Document 1] states that the English$\\rightarrow $Italian/German portions of the MuST-C corpus are used. It also states that, as additional data, a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De) is used.\n\nAnswer: English$\\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)\n\nExample 2:\n\n[Document 1]: The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type. To make this diversity, after cleaning raw text input, we use multiple types of word tokenizers. Each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words BIBREF5, pre-trained embedding as fasttext (trained on Wiki Vietnamese language) BIBREF6 and sonvx (trained on Vietnamese newspaper) BIBREF7. Each sentence has a set of words corresponding to a set of word vectors, and that set of word vectors is a representation of a sentence. We also make a sentence embedding by using RoBERTa architecture BIBREF8. CBOW and RoBERTa models trained on text from some resources including VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD and text crawled from Facebook. After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13. With the multiply output results, we will use an ensemble method to combine them and output the final result. Ensemble method we use here is Stacking method will be introduced in the section SECREF16.\n\n[Document 2]: The dataset in this HSD task is really imbalance. Clean class dominates with 91.5%, offensive class takes 5% and the rest belongs to hate class with 3.5%. To make model being able to learn with this imbalance data, we inject class weight to the loss function with the corresponding ratio (clean, offensive, hate) is $(0.09, 0.95, 0.96)$. Formular DISPLAY_FORM17 is the loss function apply for all models in our system. $w_i$ is the class weight, $y_i$ is the ground truth and $\\hat{y}_i$ is the output of the model. If the class weight is not set, we find that model cannot adjust parameters. The model tends to output all clean classes.\n\nQuestion: What dataset do they use?\n\nExplanation: \n\nAccording to [Document 1], they used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings. This information is further supported by [Document 2], which states that they used the dataset provided in HSD task to train their model.\n\nAnswer: They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).\n\nExample 3:\n\n[Document 1]: SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language.\n\nQuestion: What dataset do they use?\n\nExplanation:  [Document 1] mentions that they use a parallel corpus for training where the source is English and the target is Python. This is further supported by the fact that the parallel corpus has 18805 aligned data points.\n\nAnswer: A parallel corpus where the source is an English expression of code and the target is Python code.\n\nExample 4:\n\n[Document 1]: To evaluate the effectiveness of the proposed algorithm, we have sampled a dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain. The correctness of the rescoring was judged by two annotators, who labeled 250 examples each. The annotators read the whole conversation transcript and listened to the recording to establish whether the rescoring is meaningful. In cases when a rescored word was technically incorrect (e.g., mistaken tense of a verb), but the rescoring led to the recognition of the correct intent, we labeled the intent annotation as correct. The results are shown in Table TABREF22. Please note that every result above 50% indicates an improvement over the ASR best path recognition, since we correct more ASR errors than we introduce new mistakes.\n\nQuestion: Which dataset do they use?\n\nExplanation:"}
{"question_id": "f5bc07df5c61dcb589a848bd36f4ce9c22abd46a", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: Are there privacy concerns with clinical data?\n\nExplanation:  The documents do not mention privacy concerns with clinical data specifically.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: Last but not least, ethics and fairness are important considerations, that deserve to be studied. In that sense, detection of individual and global bias should be prioritized in order to give useful feedbacks to practitioners. Furthermore we are considering using adversarial learning as in BIBREF33 in order to ensure fairness during the training process.\n\nQuestion: Is there any ethical consideration in the research?\n\nExplanation:  There is no mention of any ethical consideration in the research in [Document 1].\n\nAnswer: No\n\nExample 3:\n\n[Document 1]: blackIn the paper, we first proposed a taxonomy to organize GAN based text-to-image synthesis frameworks into four major groups: semantic enhancement GANs, resolution enhancement GANs, diversity enhancement GANs, and motion enhancement GANs. The taxonomy provides a clear roadmap to show the motivations, architectures, and difference of different methods, and also outlines their evolution timeline and relationships. Following the proposed taxonomy, we reviewed important features of each method and their architectures. We indicated the model definition and key contributions from some advanced GAN framworks, including StackGAN, StackGAN++, AttnGAN, DC-GAN, AC-GAN, TAC-GAN, HDGAN, Text-SeGAn, StoryGAN etc. Many of the solutions surveyed in this paper tackled the highly complex challenge of generating photo-realistic images beyond swatch size samples. In other words, beyond the work of BIBREF8 in which images were generated from text in 64$\\times $64 tiny swatches. Lastly, all methods were evaluated on datasets that included birds, flowers, humans, and other miscellaneous elements. We were also able to allocate some important papers that were as impressive as the papers we finally surveyed. Though, these notable papers have yet to contribute directly or indirectly to the expansion of the vast computer vision AI field. Looking into the future, an excellent extension from the works surveyed in this paper would be to give more independence to the several learning methods (e.g. less human intervention) involved in the studies as well as increasing the size of the output images.\n\nQuestion: What challenges remain unresolved?\n\nExplanation:  The author states in the conclusion that two challenges remain unresolved: giving more independence to the several learning methods involved in the studies, and increasing the size of the output images.\n\nAnswer: give more independence to the several learning methods (e.g. less human intervention) involved in the studies, increasing the size of the output images\n\nExample 4:\n\n[Document 1]: Unlocking knowledge from free text in the health domain has a tremendous societal value. However, discrimination can occur when individuals or groups receive unfair treatment as a result of automated processing, which might be a result of biases in the data that were used to train models. The question is therefore what the most important biases are and how to overcome them, not only out of ethical but also legal responsibility. Related to the question of bias is so-called algorithm transparency BIBREF44 , BIBREF45 , as this right to explanation requires that influences of bias in training data are charted. In addition to sampling bias, which we introduced in section 2, we discuss in this section further sources of bias. Unlike sampling bias, which is a corpus-level bias, these biases here are already present in documents, and therefore hard to account for by introducing larger corpora.\n\n[Document 2]: paragraph4 0.9ex plus1ex minus.2ex-1em Data quality Texts produced in the clinical settings do not always tell a complete or accurate patient story (e.g. due to time constraints or due to patient treatment in different hospitals), yet important decisions can be based on them. As language is situated, a lot of information may be implicit, such as the circumstances in which treatment decisions are made BIBREF47 . If we fail to detect a medical concept during automated processing, this can not necessarily be a sign of negative evidence. Work on identifying and imputing missing values holds promise for reducing incompleteness, see Lipton et al. LiptonEtAl2016 for an example in sequential modeling applied to diagnosis classification.\n\n[Document 3]: paragraph4 0.9ex plus1ex minus.2ex-1em Reporting bias Clinical texts may include bias coming from both patient's and clinician's reporting. Clinicians apply their subjective judgments to what is important during the encounter with patients. In other words, there is separation between, on the one side, what is observed by the clinician and communicated by the patient, and on the other, what is noted down. Cases of more serious illness may be more accurately documented as a result of clinician's bias (increased attention) and patient's recall bias. On the other hand, the cases of stigmatized diseases may include suppressed information. In the case of traffic injuries, documentation may even be distorted to avoid legal consequences BIBREF48 .\n\n[Document 4]: We need to be aware that clinical notes may reflect health disparities. These can originate from prejudices held by healthcare practitioners which may impact patients' perceptions; they can also originate from communication difficulties in the case of ethnic differences BIBREF49 . Finally, societal norms can play a role. Brady et al. BradyEtAl2016 find that obesity is often not documented equally well for both sexes in weight-addressing clinics. Young males are less likely to be recognized as obese, possibly due to societal norms seeing them as \u201cstocky\" as opposed to obese. Unless we are aware of such bias, we may draw premature conclusions about the impact of our results.\n\n[Document 5]: paragraph4 0.9ex plus1ex minus.2ex-1em Observational bias Although variance in health outcome is affected by social, environmental and behavioral factors, these are rarely noted in clinical reports BIBREF13 . The bias of missing explanatory factors because they can not be identified within the given experimental setting is also known as the streetlight effect. In certain cases, we could obtain important prior knowledge (e.g. demographic characteristics) from data other than clinical notes.\n\n[Document 6]: paragraph4 0.9ex plus1ex minus.2ex-1em Dual use We have already mentioned linking personal health information from online texts to clinical records as a motivation for exploring surrogate data sources. However, this and many other applications also have potential to be applied in both beneficial and harmful ways. It is easy to imagine how sensitive information from clinical notes can be revealed about an individual who is present in social media with a known identity. More general examples of dual use are when the NLP tools are used to analyze clinical notes with a goal of determining individuals' insurability and employability.\n\nQuestion: Other than privacy, what are the other major ethical challenges in clinical data?\n\nExplanation:"}
{"question_id": "6c50871294562e4886ede804574e6acfa8d1a5f9", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: What were their results?\n\nExplanation: \n\n[Document 1]: We report the results of our experiments in Table TABREF1 . The best model achieved an F-score of 74.7 on the NALCS and an F-score of 70.0 on the LMS on the test set.\n\n[Document 2]: The results in Table TABREF2 show that the proposed system outperforms the state-of-the-art system on both the NALCS and the LMS.\n\n[Document 3]: The results of our experiments are shown in Table TABREF3 . The proposed system outperforms the state-of-the-art system on both the NALCS and the LMS.\n\n[Document 4]: The results of our experiments are shown in Table TABREF4 . The proposed system outperforms the state-of-the-art system on both the NALCS and the LMS.\n\n[Document 5]: The results of our experiments are shown in Table TABREF5 . The proposed system outperforms the state-of-the-art system on both the NALCS and the LMS.\n\nAccording to [Document 1], [Document 2], [Document 3], [Document 4], and [Document 5],\n\nAnswer: Best model achieved F-score 74.7 on NALCS and F-score of 70.0 on LMS on test set\n\nExample 2:\n\n[Document 1]: As for ${\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent neural network sequence encoder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the final forward and backward hidden states.\n\n[Document 2]: We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$, $\\mathcal {L}_{\\rm ACP}$, and $\\mathcal {L}_{\\rm ACP} + \\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$.\n\nQuestion: What are the results?\n\nExplanation: \n\n[Document 1] describes the two types of neural networks that were compared (BiGRU and BERT). [Document 2] provides the results of the four combinations of datasets that were used to train the model.\n\nAnswer: Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.\n\nExample 3:\n\nQuestion: What were their performance results?\n\nExplanation: \n\n[Document 1]: We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15.\n\n[Document 2]: The results are summarized in Table TABREF1.\n\n[Document 3]: The results are summarized in Table TABREF2.\n\n[Document 4]: The results are summarized in Table TABREF3.\n\n[Document 5]: The results are summarized in Table TABREF4.\n\nAccording to [Document 1], Stanford NER, spaCy 2.0, and a recurrent model were evaluated. The performance results can be found in [Document 2], [Document 3], [Document 4], and [Document 5], which are summarized in Table TABREF1, TABREF2, TABREF3, and TABREF4, respectively.\n\nAnswer: On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%\n\nExample 4:\n\n[Document 1]: To evaluate our model, we wrapped the provided scripts for the shared task and calculated the Pearson correlation coefficient and the Spearman rank coefficient with the gold standard in the validation set, as well as the same values over a subset of the same data formed by taking every instance with a gold emotion intensity score greater than or equal to 0.5.\n\n[Document 2]: FLOAT SELECTED: Table 2: Summary of the best results.\n\n[Document 3]: In this section we report the results of the experiments we performed to test our proposed model. In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any additional resources, except pre-trained word embeddings. These results are, however, reversed for the test dataset, where our model performs worse than the baseline. This shows that the model is not able to generalize well, which we think is related to the missing semantic information due to the vocabulary gap we observed between the datasets and the GloVe embeddings.\n\nQuestion: what was their result?\n\nExplanation:"}
{"question_id": "63279ecb2ba4e51c1225e63b81cb021abc10d0d1", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: How does the conversation layer work?\n\nExplanation:  The documents do not contain any information about how the conversation layer works.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: We will elaborate two popular word segmentation methods and our newly proposed segmentation strategies in this section. The two popular segmentation methods are morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5. After word segmentation, we additionally add an specific symbol behind each separated subword unit, which aims to assist the NMT model to identify the morpheme boundaries and capture the semantic information effectively. The sentence examples with different segmentation strategies for Turkish-English machine translation task are shown in Table 1.\n\n[Document 2]: We utilize the Zemberek with a morphological disambiguation tool to segment the Turkish words into morpheme units, and utilize the morphology analysis tool BIBREF12 to segment the Uyghur words into morpheme units. We employ the python toolkits of jieba for Chinese word segmentation. We apply BPE on the target-side words and we set the number of merge operations to 35K for Chinese and 30K for English and we set the maximum sentence length to 150 tokens. The training corpus statistics of Turkish-English and Uyghur-Chinese machine translation tasks are shown in Table 2 and Table 3 respectively.\n\nQuestion: How does the word segmentation method work?\n\nExplanation: \n\nAccording to [Document 1], the word segmentation method used is morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5. This information is further supported by [Document 2], which states that the Zemberek tool is used for morpheme segmentation and the BIBREF12 tool is used for morphology analysis.\n\nAnswer: morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5, Zemberek, BIBREF12\n\nExample 3:\n\n[Document 1]: However, news reports are not all objective. We may increase bias because of some non-objective reports, if we rely on the information that is extracted from the news for prediction fully. Therefore, in order to enhance the prediction model's robustness, we will adopt differential privacy (DP) method. DP is a system for sharing information about a dataset publicly by describing groups' patterns within the dataset while withholding information about individuals in the dataset. DP can be achieved if the we are willing to add random noise to the result. For example, rather than simply reporting the sum, we can inject noise from a Laplace or gaussian distribution, producing a result that\u2019s not quite exact, that masks the contents of any given row.\n\n[Document 2]: Differential privacy is one of privacy's most popular definitions today, which is a system for publicly sharing information about a dataset by describing the patterns of groups within the dataset while withholding information about individuals in the dataset. It intuitively requires that the mechanism that outputs information about an underlying dataset is robust to one sample's any change, thus protecting privacy. A mechanism ${f}$ is a random function that takes a dataset $\\mathcal {N}$ as input, and outputs a random variable ${f}(\\mathcal {N})$. For example, suppose $\\mathcal {N}$ is a news articles dataset, then the function that outputs compound score of articles in $\\mathcal {N}$ plus noise from the standard normal distribution is a mechanism [7].\n\nQuestion: How does the differential privacy mechanism work?\n\nExplanation: \n\nThis information is provided in [Document 2].\n\nAnswer: A mechanism ${f}$ is a random function that takes a dataset $\\mathcal {N}$ as input, and outputs a random variable ${f}(\\mathcal {N})$.\n\nExample 4:\n\nQuestion: How does byte-pair encoding work?\n\nExplanation:"}
{"question_id": "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Our experiments are run using the English$\\rightarrow $Italian/German portions of the MuST-C corpus BIBREF25, which is extracted from TED talks, using the same train/validation/test split as provided with the corpus (see Table TABREF18). As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De). While our main goal is to verify our hypotheses on a large data condition, thus the need to include proprietary data, for the sake of reproducibility in both languages we also provide results with systems only trained on TED Talks (small data condition). When training on large scale data we use Transformer with layer size of 1024, hidden size of 4096 on feed forward layers, 16 heads in the multi-head attention, and 6 layers in both encoder and decoder. When training only on TED talks, we set layer size of 512, hidden size of 2048 for the feed forward layers, multi-head attention with 8 heads and again 6 layers in both encoder and decoder.\n\nQuestion: What dataset do they use?\n\nExplanation:  [Document 1] states that the English$\\rightarrow $Italian/German portions of the MuST-C corpus are used. It also states that, as additional data, a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De) is used.\n\nAnswer: English$\\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)\n\nExample 2:\n\n[Document 1]: The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type. To make this diversity, after cleaning raw text input, we use multiple types of word tokenizers. Each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words BIBREF5, pre-trained embedding as fasttext (trained on Wiki Vietnamese language) BIBREF6 and sonvx (trained on Vietnamese newspaper) BIBREF7. Each sentence has a set of words corresponding to a set of word vectors, and that set of word vectors is a representation of a sentence. We also make a sentence embedding by using RoBERTa architecture BIBREF8. CBOW and RoBERTa models trained on text from some resources including VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD and text crawled from Facebook. After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13. With the multiply output results, we will use an ensemble method to combine them and output the final result. Ensemble method we use here is Stacking method will be introduced in the section SECREF16.\n\n[Document 2]: The dataset in this HSD task is really imbalance. Clean class dominates with 91.5%, offensive class takes 5% and the rest belongs to hate class with 3.5%. To make model being able to learn with this imbalance data, we inject class weight to the loss function with the corresponding ratio (clean, offensive, hate) is $(0.09, 0.95, 0.96)$. Formular DISPLAY_FORM17 is the loss function apply for all models in our system. $w_i$ is the class weight, $y_i$ is the ground truth and $\\hat{y}_i$ is the output of the model. If the class weight is not set, we find that model cannot adjust parameters. The model tends to output all clean classes.\n\nQuestion: What dataset do they use?\n\nExplanation: \n\nAccording to [Document 1], they used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings. This information is further supported by [Document 2], which states that they used the dataset provided in HSD task to train their model.\n\nAnswer: They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).\n\nExample 3:\n\n[Document 1]: SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language.\n\nQuestion: What dataset do they use?\n\nExplanation:  [Document 1] mentions that they use a parallel corpus for training where the source is English and the target is Python. This is further supported by the description of the dataset in Fig. FIGREF12.\n\nAnswer: A parallel corpus where the source is an English expression of code and the target is Python code.\n\nExample 4:\n\n[Document 1]: We present the results of our experiments with both the Evo-Devo and the Machine Learning mechanisms described earlier using the U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12 .\n\nQuestion: Which dataset do they use?\n\nExplanation:"}
{"question_id": "180c7bea8caf05ca97d9962b90eb454be4176425", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: Are other pretrained language models also evaluated for contextual augmentation? \n\nExplanation: \n\n[Document 1]: We evaluate the proposed method on two standard natural language understanding tasks: question answering and named entity recognition. We use the SQuAD dataset BIBREF1 for question answering and the CoNLL03 dataset BIBREF2 for named entity recognition. For both tasks, we use the BERT-base model BIBREF3 pretrained on the English Wikipedia and the BookCorpus BIBREF4 .\n\n[Document 2]: We use the SQuAD dataset BIBREF1 for question answering and the CoNLL03 dataset BIBREF2 for named entity recognition.\n\n[Document 3]: We use the BERT-base model BIBREF3 pretrained on the English Wikipedia and the BookCorpus BIBREF4 .\n\nFrom [Document 1] and [Document 3], we can see that only the BERT-base model is used for contextual augmentation. This is further supported by [Document 2], which only mentions the SQuAD and CoNLL03 datasets. Therefore, we can conclude that no other pretrained language models are evaluated for contextual augmentation.\n\nAnswer: No\n\nExample 2:\n\n[Document 1]: Prior work have shown that contextual information could be useful for SF. They utilize contextual information either in word level representation (i.e., via contextualize embedding e.g., BERT BIBREF0) or in the model computation graph (e.g., concatenating the context feature to the word feature BIBREF1). However, such methods fail to capture the explicit dependence between the context of the word and its label. Moreover, such limited use of contextual information (i.e., concatenation of the feature vector and context vector) in the model cannot model the interaction between the word representation and its context. In order to alleviate these issues, in this work, we propose a novel model to explicitly increase the predictability of the word label using its context and increasing the interactivity between word representations and its context. More specifically, in our model we use the context of the word to predict its label and by doing so our model learns label-aware context for each word in the sentence. In order to improve the interactivity between the word representation and its context, we increase the mutual information between the word representations and its context. In addition to these contributions, we also propose an auxiliary task to predict which labels are expressed in a given sentence. Our model is trained in a mutli-tasking framework. Our experiments on a SF dataset for identifying semantic concepts from natural language request to edit an image show the superiority of our model compared to previous baselines. Our model achieves the state-of-the-art results on the benchmark dataset by improving the F1 score by almost 2%, which corresponds to a 12.3% error rate reduction.\n\nQuestion: How does their model utilize contextual information for each work in the given sentence in a multi-task setting? setting?\n\nExplanation:  [Document 1] states that \"In order to alleviate these issues, in this work, we propose a novel model to explicitly increase the predictability of the word label using its context and increasing the interactivity between word representations and its context.\" This means that the model uses the context of the word to predict its label, which in turn allows the model to learn label-aware context for each word in the sentence. This is further supported by the fact that the model is trained in a multi-task setting.\n\nAnswer: we use the context of the word to predict its label and by doing so our model learns label-aware context for each word in the sentence\n\nExample 3:\n\n[Document 1]: IOCs in cybersecurity articles are often described in a predictable way: being connected to a set of contextual keywords BIBREF16 , BIBREF1 . For example, a human user can infer that the word \u201cntdll.exe\u201d is the name of a malicious file on the basis of the words \u201cdownload\u201d and \u201ccompromised\u201d from the text shown in Fig. FIGREF1 . By analyzing the whole corpus, it is interesting that malicious file names tends to co-occur with words such as \"download\", \"malware\", \"malicious\", etc. In this work, we consider words that can indicate the characteristics of the neighbor words as contextual keywords and develop an approach to generate features from the automatically extracted contextual keywords.\n\nQuestion: What contextual features are used?\n\nExplanation:  [Document 1] states that contextual keywords are used to generate features. This is further supported by the example in Figure 1.\n\nAnswer: The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.\n\nExample 4:\n\n[Document 1]: To study the semantic nuances between fake news and satire, we use BERT BIBREF8, which stands for Bidirectional Encoder Representations from Transformers, and represents a state-of-the-art contextual language model. BERT is a method for pre-training language representations, meaning that it is pre-trained on a large text corpus and then used for downstream NLP tasks. Word2Vec BIBREF9 showed that we can use vectors to properly represent words in a way that captures semantic or meaning-related relationships. While Word2Vec is a context-free model that generates a single word-embedding for each word in the vocabulary, BERT generates a representation of each word that is based on the other words in the sentence. It was built upon recent work in pre-training contextual representations, such as ELMo BIBREF10 and ULMFit BIBREF11, and is deeply bidirectional, representing each word using both its left and right context. We use the pre-trained models of BERT and fine-tune it on the dataset of fake news and satire articles using Adam optimizer with 3 types of decay and 0.01 decay rate. Our BERT-based binary classifier is created by adding a single new layer in BERT's neural network architecture that will be trained to fine-tune BERT to our task of classifying fake news and satire articles.\n\nQuestion: What contextual language model is used?\n\nExplanation:"}
{"question_id": "39d20b396f12f0432770c15b80dc0d740202f98d", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Curriculum Plausibility ::: Conversational Attributes ::: Specificity\n\n[Document 2]: A notorious problem for neural dialogue generation model is that the model is prone to generate generic responses. The most unspecific responses are easy to learn, but are short and meaningless, while the most specific responses, consisting of too many rare words, are too difficult to learn, especially at the initial learning stage. Following BIBREF11, we measure the specificity of the response in terms of each word $w$ using Normalized Inverse Document Frequency (NIDF, ranging from 0 to 1):\n\n[Document 3]: Curriculum Plausibility ::: Conversational Attributes ::: Repetitiveness\n\n[Document 4]: Repetitive responses are easy to generate in current auto-regressive response decoding, where response generation loops frequently, whereas diverse and informative responses are much more complicated for neural dialogue generation. We measure the repetitiveness of a response $r$ as:\n\n[Document 5]: Curriculum Plausibility ::: Conversational Attributes ::: Continuity\n\n[Document 6]: A coherent response not only responds to the given query, but also triggers the next utterance. An interactive conversation is carried out for multiple rounds and a response in the current turn also acts as the query in the next turn. As such, we introduce the continuity metric, which is similar to the query-relatedness metric, to assess the continuity of a response $r$ with respect to the subsequent utterance $u$, by measuring the cosine similarities between them.\n\n[Document 7]: Curriculum Plausibility ::: Conversational Attributes ::: Model Confidence\n\n[Document 8]: Despite the heuristic dialogue attributes, we further introduce the model confidence as an attribute, which distinguishes the easy-learnt samples from the under-learnt samples in terms of the model learning ability. A pretrained neural dialogue generation model assigns a relatively higher confidence probability for the easy-learnt samples than the under-learnt samples. Inspired by BIBREF16, BIBREF17, we employ the negative loss value of a dialogue sample under the pretrained model as the model confidence measure, indicating whether a sampled response is easy to be generated. Here we choose the attention-based sequence-to-sequence architecture with a cross-entropy objective as the underlying dialogue model.\n\n[Document 9]: Curriculum Plausibility ::: Conversational Attributes ::: Query-relatedness\n\n[Document 10]: A conversation is considered to be coherent if the response correlates well with the given query. For example, given a query \u201cI like to paint\u201d, the response \u201cWhat kind of things do you paint?\u201d is more relevant and easier to learn than another loosely-coupled response \u201cDo you have any pets?\u201d. Following previous work BIBREF14, we measure the query-relatedness using the cosine similarities between the query and its corresponding response in the embedding space: $\\textit {cos\\_sim}(\\textit {sent\\_emb}(c), \\textit {sent\\_emb}(r))$, where $c$ is the query and $r$ is the response. The sentence embedding is computed by taking the average word embedding weighted by the smooth inverse frequency $\\textit {sent\\_emb}(e)=\\frac{1}{|e|}\\sum _{w\\in {}e}\\frac{0.001}{0.001 + p(w)}emb(w)$ of words BIBREF15, where $emb(w)$ and $p(w)$ are the embedding and the probability of word $w$ respectively.\n\nQuestion: What five dialogue attributes were analyzed?\n\nExplanation:  The five dialogue attributes analyzed were model confidence, continuity, query-relatedness, repetitiveness, and specificity. This information is supported by [Document 7], [Document 8], [Document 9], [Document 10], [Document 3], [Document 4], and [Document 2], respectively.\n\nAnswer: Model Confidence, Continuity, Query-relatedness, Repetitiveness, Specificity\n\nExample 2:\n\n[Document 1]: For each speaker, we divide all available utterances into disjoint train, development, and test sets. Using the force-aligned phone boundaries, we extract the mid-phone frame for each example across the four classes, which leads to a data imbalance. Therefore, for all utterances in the training set, we randomly sample additional examples within a window of 5 frames around the center phone, to at least 50 training examples per class per speaker. It is not always possible to reach the target of 50 examples, however, if no more data is available to sample from. This process gives a total of INLINEFORM0 10700 training examples with roughly 2000 to 3000 examples per class, with each speaker having an average of 185 examples. Because the amount of data varies per speaker, we compute a sampling score, which denotes the proportion of sampled examples to the speaker's total training examples. We expect speakers with high sampling scores (less unique data overall) to underperform when compared with speakers with more varied training examples.\n\nQuestion: How many instances does their dataset have?\n\nExplanation:  The dataset has 10700 training examples. This information is found in the first sentence of the first paragraph.\n\nAnswer: 10700\n\nExample 3:\n\n[Document 1]: Many proposed approaches formulate the entity attribute ranking problem as a post processing step of automated attribute-value extraction. In BIBREF0 , BIBREF1 , BIBREF2 , Pasca et al. firstly extract potential class-attribute pairs using linguistically motivated patterns from unstructured text including query logs and query sessions, and then score the attributes using the Bayes model. In BIBREF3 , Rahul Rai proposed to identify product attributes from customer online reviews using part-of-speech(POS) tagging patterns, and to evaluate their importance with several different frequency metrics. In BIBREF4 , Lee et al. developed a system to extract concept-attribute pairs from multiple data sources, such as Probase, general web documents, query logs and external knowledge base, and aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model. Those approaches typically suffer from the poor quality of the pattern rules, and the ranking process is used to identify relatively more precise attributes from all attribute candidates.\n\n[Document 2]: As for an already existing knowledge graph, there is plenty of work in literature dealing with ranking entities by relevance without or with a query. In BIBREF5 , Li et al. introduced the OntoRank algorithm for ranking the importance of semantic web objects at three levels of granularity: document, terms and RDF graphs. The algorithm is based on the rational surfer model, successfully used in the Swoogle semantic web search engine. In BIBREF6 , Hogan et al. presented an approach that adapted the well-known PageRank/HITS algorithms to semantic web data, which took advantage of property values to rank entities. In BIBREF7 , BIBREF8 , authors also focused on ranking entities, sorting the semantic web resources based on importance, relevance and query length, and aggregating the features together with an overall ranking model.\n\nQuestion: What are the traditional methods to identifying important attributes?\n\nExplanation:  According to [Document 1], many proposed approaches formulate the entity attribute ranking problem as a post processing step of automated attribute-value extraction. In [Document 1], Pasca et al. firstly extract potential class-attribute pairs using linguistically motivated patterns from unstructured text including query logs and query sessions, and then score the attributes using the Bayes model. In [Document 1], Rahul Rai proposed to identify product attributes from customer online reviews using part-of-speech(POS) tagging patterns, and to evaluate their importance with several different frequency metrics. In [Document 1], Lee et al. developed a system to extract concept-attribute pairs from multiple data sources, such as Probase, general web documents, query logs and external knowledge base, and aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model.\n\nAnswer: automated attribute-value extraction, score the attributes using the Bayes model, evaluate their importance with several different frequency metrics, aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model, OntoRank algorithm\n\nExample 4:\n\nQuestion: How many of the attribute-value pairs are found in images?\n\nExplanation:"}
{"question_id": "576a3ed6e4faa4c3893db632e97a52ac6e864aac", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We focus primarily on the smaller data set of 645 hand-labeled articles provided to task participants, both for training and for validation. We take the first 80% of this data set for our training set and the last 20% for the validation set. Since the test set is also hand-labeled we found that the 645 articles are much more representative of the final test set than the articles labeled by publisher. The model's performance on articles labeled by publisher was not much above chance level.\n\n[Document 2]: Our first experiment was checking the importance of pre-training. We pre-trained BERT-base on the 600,000 articles without labels by using the same Cloze task BIBREF5 that BERT had originally used for pre-training. We then trained the model on sequence lengths of 100, 250 and 500. The accuracy for each sequence length after 100 epochs is shown in TABREF7 and is labeled as UP (unsupervised pre-training). The other column shows how well BERT-base trained without pre-training. We found improvements for lower sequence lengths, but not at 500 word pieces. Since the longer chunk should have been more informative, and since our hand-labeled training set only contained 516 articles, this likely indicates that BERT experiences training difficulty when dealing with long sequences on such a small dataset. As the cost to do pre-training was only a one time cost all of our remaining experiments use a pre-trained model.\n\nQuestion: How long is the dataset?\n\nExplanation:  [Document 1] states that the dataset contains 645 articles. [Document 2] states that the dataset contains 600,000 articles.\n\nAnswer: 645, 600000\n\nExample 2:\n\n[Document 1]: Data set. For the cross-domain polarity classification experiments, we use the second version of Multi-Domain Sentiment Dataset BIBREF0 . The data set contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). Reviews contain star ratings (from 1 to 5) which are converted into binary labels as follows: reviews rated with more than 3 stars are labeled as positive, and those with less than 3 stars as negative. In each domain, there are 1000 positive and 1000 negative reviews.\n\nQuestion: How long is the dataset?\n\nExplanation:  The dataset contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). In each domain, there are 1000 positive and 1000 negative reviews. This means that there are a total of 8000 reviews in the dataset.\n\nAnswer: 8000\n\nExample 3:\n\n[Document 1]: The corpus includes 2000 sentences. Theses sentence are random renderings of 200 sentences, which we have taken from Sorani Kurdish books of the grades one to three of the primary school in the Kurdistan Region of Iraq. The reason that we have taken only 200 sentences is to have a smaller dictionary and also to increase the repetition of each word in the narrated speech. We transformed the corpus sentences, which are in Persian-Arabic script, into the format which complies with the suggested phones for the related Sorani letters (see Section SECREF6).\n\nQuestion: How long is the dataset?\n\nExplanation:  The dataset consists of 2000 sentences, as stated in [Document 1].\n\nAnswer: 2000\n\nExample 4:\n\n[Document 1]: The TLE currently contains 5,124 sentences (97,681 tokens) with POS tag and dependency annotations in the English Universal Dependencies (UD) formalism BIBREF2 , BIBREF3 . The sentences were obtained from the FCE corpus BIBREF1 , a collection of upper intermediate English learner essays, containing error annotations with 75 error categories BIBREF7 . Sentence level segmentation was performed using an adaptation of the NLTK sentence tokenizer. Under-segmented sentences were split further manually. Word level tokenization was generated using the Stanford PTB word tokenizer.\n\nQuestion: How long is the dataset?\n\nExplanation:"}
{"question_id": "63cdac43a643fc1e06da44910458e89b2c7cd921", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: To develop the dataset, we extracted 200 sentences from Sorani Kurdish books of grades one to three of the primary school in the Kurdistan Region of Iraq. We randomly created 2000 sentences from the extracted sentences.\n\nQuestion: How was the dataset collected?\n\nExplanation:  The dataset was collected by extracting text from Sorani Kurdish books of primary school and randomly creating sentences. This information is supported by [Document 1].\n\nAnswer: extracted text from Sorani Kurdish books of primary school and randomly created sentences\n\nExample 2:\n\n[Document 1]: We crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk). We use the MTurk interface of ParlAI BIBREF6 to render 360 images via WebGL and dynamically display neighborhood maps with an HTML5 canvas. Detailed task instructions, which were also given to our workers before they started their task, are shown in Appendix SECREF15 . We paired Turkers at random and let them alternate between the tourist and guide role across different HITs.\n\nQuestion: How was the dataset collected?\n\nExplanation:  According to [Document 1], the dataset was crowd-sourced on Amazon Mechanical Turk.\n\nAnswer: crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk)\n\nExample 3:\n\n[Document 1]: Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. For the taxi domain, there is no need to store the information. Instead, we can call the API directly if necessary.\n\n[Document 2]: Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. To make workers understand the task more easily, we crafted templates to generate natural language descriptions for each structured goal.\n\n[Document 3]: Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states.\n\n[Document 4]: Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality. Finally, each dialogue contains a structured goal, a task description, user states, system states, dialogue acts, and utterances.\n\nQuestion: How was the dataset collected?\n\nExplanation:  [Document 1], [Document 2], [Document 3], and [Document 4] all describe different aspects of how the dataset was collected. [Document 1] describes how the database was constructed, [Document 2] describes how the goals were generated, [Document 3] describes how the dialogues were collected, and [Document 4] describes how the dialogue annotation was done.\n\nAnswer: Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. , Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context., Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states., Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. \n\nExample 4:\n\n[Document 1]: The data was collected using crowdsourcing. Each speaker was recorded saying each wording for each intent twice. The phrases to record were presented in a random order. Participants consented to data being released and provided demographic information about themselves. The demographic information about these anonymized speakers (age range, gender, speaking ability, etc.) is included along with the dataset.\n\nQuestion: How was the dataset collected?\n\nExplanation:"}
{"question_id": "0f1f81b6d4aa0da38b4cc8b060926e7df61bb646", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: To evaluate if the sentiment feature improves the prediction accuracy, a baseline model is defined. The baseline applies linear logistic regression to a set of stock technical signals to predict the following day\u2019s stock return sign (+/\u2010). No sentiment features are provided to the baseline model.\n\nQuestion: What is the baseline machine learning prediction approach?\n\nExplanation:  The baseline machine learning prediction approach is linear logistic regression to a set of stock technical signals, according to [Document 1].\n\nAnswer: linear logistic regression to a set of stock technical signals\n\nExample 2:\n\n[Document 1]: Baselines. We compare our approach with several methods BIBREF1 , BIBREF31 , BIBREF11 , BIBREF8 , BIBREF10 , BIBREF39 in two cross-domain settings. Using string kernels, Gim\u00e9nez-P\u00e9rez et al. BIBREF10 reported better performance than SST BIBREF31 and KE-Meta BIBREF11 in the multi-source domain setting. In addition, we compare our approach with SFA BIBREF1 , CORAL BIBREF8 and TR-TrAdaBoost BIBREF39 in the single-source setting.\n\n[Document 2]: Transductive string kernels. We present a simple and straightforward approach to produce a transductive similarity measure suitable for strings. We take the following steps to derive transductive string kernels. For a given kernel (similarity) function INLINEFORM0 , we first build the full kernel matrix INLINEFORM1 , by including the pairwise similarities of samples from both the train and the test sets. For a training set INLINEFORM2 of INLINEFORM3 samples and a test set INLINEFORM4 of INLINEFORM5 samples, such that INLINEFORM6 , each component in the full kernel matrix is defined as follows: DISPLAYFORM0\n\n[Document 3]: We next present a simple yet effective approach for adapting a one-versus-all kernel classifier trained on a source domain to a different target domain. Our transductive kernel classifier (TKC) approach is composed of two learning iterations. Our entire framework is formally described in Algorithm SECREF3 .\n\nQuestion: What machine learning algorithms are used?\n\nExplanation: \n\nAccording to [Document 1], the machine learning algorithms used are string kernels, SST, KE-Meta, SFA, CORAL, TR-TrAdaBoost. This information is further supported by [Document 2] and [Document 3].\n\nAnswer: string kernels, SST, KE-Meta, SFA, CORAL, TR-TrAdaBoost, Transductive string kernels, transductive kernel classifier\n\nExample 3:\n\nQuestion: Is there a machine learning approach that tries to solve same problem?\n\nExplanation: \n\nThe documents do not mention any machine learning approaches that try to solve the same problem.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: Following BIBREF21 (BIBREF21), we evaluate on the standard multiple choice narrative cloze (MCNC) dataset BIBREF2. As SGNN proposed by BIBREF21 (BIBREF21) achieved state-of-the-art performances for this task, we use the framework of SGNN, and only replace their input event embeddings with our intent and sentiment-enhanced event embeddings.\n\nQuestion: What is the machine learning method used to make the predictions?\n\nExplanation:"}
{"question_id": "2f142cd11731d29d0c3fa426e26ef80d997862e0", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Three different datasets have been used to train our models: the Toronto book corpus, Wikipedia sentences and tweets. The Wikipedia and Toronto books sentences have been tokenized using the Stanford NLP library BIBREF10 , while for tweets we used the NLTK tweets tokenizer BIBREF11 . For training, we select a sentence randomly from the dataset and then proceed to select all the possible target unigrams using subsampling. We update the weights using SGD with a linearly decaying learning rate.\n\n[Document 2]: Unsupervised Similarity Evaluation Results. In Table TABREF19 , we see that our Sent2Vec models are state-of-the-art on the majority of tasks when comparing to all the unsupervised models trained on the Toronto corpus, and clearly achieve the best averaged performance. Our Sent2Vec models also on average outperform or are at par with the C-PHRASE model, despite significantly lagging behind on the STS 2014 WordNet and News subtasks. This observation can be attributed to the fact that a big chunk of the data that the C-PHRASE model is trained on comes from English Wikipedia, helping it to perform well on datasets involving definition and news items. Also, C-PHRASE uses data three times the size of the Toronto book corpus. Interestingly, our model outperforms C-PHRASE when trained on Wikipedia, as shown in Table TABREF21 , despite the fact that we use no parse tree information. Official STS 2017 benchmark. In the official results of the most recent edition of the STS 2017 benchmark BIBREF35 , our model also significantly outperforms C-PHRASE, and in fact delivers the best unsupervised baseline method.\n\nQuestion: Do they report results only on English data?\n\nExplanation:  [Document 1] and [Document 2] both report results on English data from the Toronto book corpus, Wikipedia sentences, and tweets.\n\nAnswer: Yes\n\nExample 2:\n\n[Document 1]: A serious challenge in the field of AV is the lack of publicly available (and suitable) corpora, which are required to train and evaluate AV methods. Among the few publicly available corpora are those that were released by the organizers of the well-known PAN-AV competitions BIBREF11 , BIBREF5 , BIBREF12 . In regard to our experiments, however, we cannot use these corpora, due to the absence of relevant meta-data such as the precise time spans where the documents have been written as well as the topic category of the texts. Therefore, we decided to compile our own corpora based on English documents, which we crawled from different publicly accessible sources. In what follows, we describe our three constructed corpora, which are listed together with their statistics in Table TABREF23 . Note that all corpora are balanced such that verification cases with matching (Y) and non-matching (N) authorships are evenly distributed.\n\nQuestion: Do they report results only on English data?\n\nExplanation:  [Document 1] states that the corpora are based on English documents.\n\nAnswer: Yes\n\nExample 3:\n\nQuestion: Do they report results only on English data?\n\nExplanation: \n\n[Document 1] reports results only on English data. This is supported by [Document 2], which states that the spaCy 2.0 algorithm performs within 1% of the current state-of-the-art for English.\n\nAnswer: Yes\n\nExample 4:\n\n[Document 1]: FLOAT SELECTED: Figure 4: Typical tokens obtained by different layers of the sifted multi-task learning method. In our proposed method, typical tokens are captured by shared layer (SL), selected sharing layer for fake news detection (SSLFND), selected sharing layer for stance detection (SSL-SD), private layer for fake news detection (PL-FND), and private layer for stance detection (PL-SD) respectively. A column of the same color represents the distribution of one token in different layers, while the last two columns denote unique tokens captured by different layers.\n\nQuestion: Do they report results only on English data?\n\nExplanation:"}
{"question_id": "9d963d385bd495a7e193f8a498d64c1612e6c20c", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Our experiments are run using the English$\\rightarrow $Italian/German portions of the MuST-C corpus BIBREF25, which is extracted from TED talks, using the same train/validation/test split as provided with the corpus (see Table TABREF18). As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De). While our main goal is to verify our hypotheses on a large data condition, thus the need to include proprietary data, for the sake of reproducibility in both languages we also provide results with systems only trained on TED Talks (small data condition). When training on large scale data we use Transformer with layer size of 1024, hidden size of 4096 on feed forward layers, 16 heads in the multi-head attention, and 6 layers in both encoder and decoder. When training only on TED talks, we set layer size of 512, hidden size of 2048 for the feed forward layers, multi-head attention with 8 heads and again 6 layers in both encoder and decoder.\n\nQuestion: What dataset do they use?\n\nExplanation:  [Document 1] states that the English$\\rightarrow $Italian/German portions of the MuST-C corpus are used. It also states that, as additional data, a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De) is used.\n\nAnswer: English$\\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)\n\nExample 2:\n\n[Document 1]: The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type. To make this diversity, after cleaning raw text input, we use multiple types of word tokenizers. Each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words BIBREF5, pre-trained embedding as fasttext (trained on Wiki Vietnamese language) BIBREF6 and sonvx (trained on Vietnamese newspaper) BIBREF7. Each sentence has a set of words corresponding to a set of word vectors, and that set of word vectors is a representation of a sentence. We also make a sentence embedding by using RoBERTa architecture BIBREF8. CBOW and RoBERTa models trained on text from some resources including VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD and text crawled from Facebook. After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13. With the multiply output results, we will use an ensemble method to combine them and output the final result. Ensemble method we use here is Stacking method will be introduced in the section SECREF16.\n\n[Document 2]: The dataset in this HSD task is really imbalance. Clean class dominates with 91.5%, offensive class takes 5% and the rest belongs to hate class with 3.5%. To make model being able to learn with this imbalance data, we inject class weight to the loss function with the corresponding ratio (clean, offensive, hate) is $(0.09, 0.95, 0.96)$. Formular DISPLAY_FORM17 is the loss function apply for all models in our system. $w_i$ is the class weight, $y_i$ is the ground truth and $\\hat{y}_i$ is the output of the model. If the class weight is not set, we find that model cannot adjust parameters. The model tends to output all clean classes.\n\nQuestion: What dataset do they use?\n\nExplanation: \n\nAccording to [Document 1], they used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings. This information is further supported by [Document 2], which states that they used the dataset provided in HSD task to train their model.\n\nAnswer: They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).\n\nExample 3:\n\n[Document 1]: SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language.\n\nQuestion: What dataset do they use?\n\nExplanation:  [Document 1] mentions that they use a parallel corpus for training where the source is English and the target is Python. This is further supported by the fact that the parallel corpus has 18805 aligned data points.\n\nAnswer: A parallel corpus where the source is an English expression of code and the target is Python code.\n\nExample 4:\n\n[Document 1]: For the evaluation on English language we use the English Penn Treebank BIBREF40 , transformed into Stanford dependencies BIBREF41 with the predicted PoS tags as in BIBREF32 .\n\n[Document 2]: We also use the spmrl datasets, a collection of parallel dependency and constituency treebanks for morphologically rich languages BIBREF42 . In this case, we use the predicted PoS tags provided by the organizers. We observed some differences between the constituency and dependency predicted input features provided with the corpora. For experiments where dependency parsing is the main task, we use the input from the dependency file, and the converse for constituency, for comparability with other work. d-mtl models were trained twice (one for each input), and dependency and constituent scores are reported on the model trained on the corresponding input.\n\nQuestion: Which dataset do they use?\n\nExplanation:"}
{"question_id": "a1a0365bf6968cbdfd1072cf3923c26250bc955c", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Feature-based models. We used state-of-the-art features that have shown to be useful in ID: some of them are language-independent (e.g., punctuation marks, positive and negative emoticons, quotations, personal pronouns, tweet's length, named entities) while others are language-dependent relying on dedicated lexicons (e.g., negation, opinion lexicons, opposition words). Several classical machine learning classifiers were tested with several feature combinations, among them Random Forest (RF) achieved the best result with all features. Neural model with monolingual embeddings. We used Convolutional Neural Network (CNN) network whose structure is similar to the one proposed by BIBREF29. For the embeddings, we relied on $AraVec$ BIBREF30 for Arabic, FastText BIBREF31 for French, and Word2vec Google News BIBREF32 for English . For the three languages, the size of the embeddings is 300 and the embeddings were fine-tuned during the training process. The CNN network was tuned with 20% of the training corpus using the $Hyperopt$ library.\n\nQuestion: What neural architectures are used?\n\nExplanation:  [Document 1] states that a Convolutional Neural Network (CNN) was used for the neural model.\n\nAnswer: Convolutional Neural Network (CNN)\n\nExample 2:\n\n[Document 1]: Since our focus in this paper is an additional loss exploiting negative examples (Section method), we fix the baseline LM throughout the experiments. Our baseline is a three-layer LSTM-LM with 1,150 hidden units at internal layers trained with the standard cross-entropy loss. Word embeddings are 400-dimensional, and input and output embeddings are tied BIBREF11. Deviating from some prior work BIBREF0, BIBREF1, we train LMs at sentence level as in sequence-to-sequence models BIBREF12. This setting has been employed in some previous work BIBREF3, BIBREF6.\n\nQuestion: What neural language models are explored?\n\nExplanation:  [Document 1] mentions that the focus of the paper is on an additional loss exploiting negative examples, and the baseline LM is a three-layer LSTM-LM.\n\nAnswer: LSTM-LM \n\nExample 3:\n\n[Document 1]: We adopt the approach in BIBREF4 , modifying it to synchronise audio with UTI data. Our model, UltraSync, consists of two streams: the first takes as input a short segment of ultrasound and the second takes as input the corresponding audio. Both inputs are high-dimensional and are of different sizes. The objective is to learn a mapping from the inputs to a pair of low-dimensional vectors of the same length, such that the Euclidean distance between the two vectors is small when they correlate and large otherwise BIBREF21 , BIBREF22 . This model can be viewed as an extension of a siamese neural network BIBREF23 but with two asymmetrical streams and no shared parameters. Figure FIGREF1 illustrates the main architecture. The visual data INLINEFORM0 (ultrasound) and audio data INLINEFORM1 (MFCC), which have different shapes, are mapped to low dimensional embeddings INLINEFORM2 (visual) and INLINEFORM3 (audio) of the same size: DISPLAYFORM0\n\nQuestion: What kind of neural network architecture do they use?\n\nExplanation:  The answer can be found in [Document 1], which states that the model consists of two streams: the first takes as input a short segment of ultrasound and the second takes as input the corresponding audio. Both inputs are high-dimensional and are of different sizes. The objective is to learn a mapping from the inputs to a pair of low-dimensional vectors of the same length, such that the Euclidean distance between the two vectors is small when they correlate and large otherwise. This model can be viewed as an extension of a siamese neural network.\n\nAnswer: CNN\n\nExample 4:\n\n[Document 1]: Sequence to Sequence approaches for dialogue modelling\n\n[Document 2]: The problem with rule-based models was that they were often domain dependent and could not be easily ported to a new domain. They also depended on hand crafted rules which was both expensive and required domain expertise. Two factors which when combined spell doom for scalbility. All of this changed in 2015 when Vinyals et al proposed an approach BIBREF2 inspired from the recent progress in machine translation BIBREF1 . Vinyals et al used the sequence to sequence learning architecture for conversation agents. Their model was the first model which could be trained end-to-end, and could generate a new output utterance based on just the input sentence and no other hand crafted features.\n\n[Document 3]: Language Model based approaches for dialogue modelling\n\n[Document 4]: Though sequence-to-sequence based models have achieved a lot of success, another push in the field has been to instead train a language model over the entire dialogue as one single sequence BIBREF18 . These works argue that a language model is better suited to dialogue modeling, as it learns how the conversation evolves as information progresses.\n\nQuestion: What type of neural models are used?\n\nExplanation:"}
{"question_id": "cb4086ad022197da79f28dc609d0de90108c4543", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: All models are trained and evaluated using the same (w.r.t. data shuffle and split) 10-fold cross-validation (CV) on Trafficking-10k, except for HTDN, whose result is read from the original paper BIBREF9 . During each train-test split, INLINEFORM0 of the training set is further reserved as the validation set for tuning hyperparameters such as L2-penalty in IT, AT and LAD, and learning rate in ORNN. So the overall train-validation-test ratio is 70%-20%-10%. We report the mean metrics from the CV in Table TABREF14 . As previous research has pointed out that there is no unbiased estimator of the variance of CV BIBREF29 , we report the naive standard error treating metrics across CV as independent.\n\n[Document 2]: We can see that ORNN has the best MAE, INLINEFORM0 and Acc. as well as a close 2nd best Wt. Acc. among all models. Its Wt. Acc. is a substantial improvement over HTDN despite the fact that the latter use both text and image data. It is important to note that HTDN is trained using binary labels, whereas the other models are trained using ordinal labels and then have their ordinal predictions converted to binary predictions. This is most likely the reason that even the baseline models except for LAD can yield better Wt. Acc. than HTDN, confirming our earlier claim that polarizing the ordinal labels during training may lead to information loss.\n\nQuestion: By how much do they outperform previous state-of-the-art models?\n\nExplanation:  [Document 1] provides the 10-fold cross-validation results for all models on Trafficking-10k. [Document 2] compares the performance of the proposed ORNN model with the best state-of-the-art model (HTDN) and reports that the ORNN model outperforms the HTDN model on all metrics.\n\nAnswer: Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)\n\nExample 2:\n\n[Document 1]: The goal of this paper is to propose a novel OOD detection method that does not require OOD data by utilizing counterfeit OOD turns in the context of a dialog. Most prior approaches do not consider dialog context and make predictions for each utterance independently. We will show that this independent decision leads to suboptimal performance even when actual OOD utterances are given to optimize the model and that the use of dialog context helps reduce OOD detection errors. To consider dialog context, we need to connect the OOD detection task with the overall dialog task. Thus, for this work, we build upon Hybrid Code Networks (HCN) BIBREF4 since HCNs achieve state-of-the-art performance in a data-efficient way for task-oriented dialogs, and propose AE-HCNs which extend HCNs with an autoencoder (Figure FIGREF8 ). Furthermore, we release new dialog datasets which are three publicly available dialog corpora augmented with OOD turns in a controlled way (exemplified in Table TABREF2 ) to foster further research.\n\n[Document 2]: The result is shown in Table TABREF23 . Since there are multiple actions that are appropriate for a given dialog context, we use per-utterance Precision@K as performance metric. We also report f1-score for OOD detection to measure the balance between precision and recall. The performances of HCN on Test-OOD are about 15 points down on average from those on Test, showing the detrimental impact of OOD utterances to such models only trained on in-domain training data. AE-HCN(-CNN) outperforms HCN on Test-OOD by a large margin about 17(20) points on average while keeping the minimum performance trade-off compared to Test. Interestingly, AE-HCN-CNN has even better performance than HCN on Test, indicating that, with the CNN encoder, counterfeit OOD augmentation acts as an effective regularization. In contrast, AE-HCN-Indep failed to robustly detect OOD utterances, resulting in much lower numbers for both metrics on Test-OOD as well as hurting the performance on Test. This result indicates two crucial points: 1) the inherent difficulty of finding an appropriate threshold value without actually seeing OOD data; 2) the limitation of the models which do not consider context. For the first point, Figure FIGREF24 plots histograms of reconstruction scores for IND and OOD utterances of bAbI6 Test-OOD. If OOD utterances had been known a priori, the threshold should have been set to a much higher value than the maximum reconstruction score of IND training data (6.16 in this case).\n\nQuestion: By how much does their method outperform state-of-the-art OOD detection?\n\nExplanation: \n\nAccording to [Document 1], the goal of the paper is to propose a novel OOD detection method. This information is further supported by [Document 2], which states that the performances of HCN on Test-OOD are about 15 points down on average from those on Test, showing the detrimental impact of OOD utterances to such models only trained on in-domain training data. AE-HCN(-CNN) outperforms HCN on Test-OOD by a large margin about 17(20) points on average while keeping the minimum performance trade-off compared to Test.\n\nAnswer: AE-HCN outperforms by 17%, AE-HCN-CNN outperforms by 20% on average\n\nExample 3:\n\n[Document 1]: Evaluation on DUC2004: DUC 2004 ( BIBREF15 ) is a commonly used benchmark on summarization task consisting of 500 news articles. Each article is paired with 4 different human-generated reference summaries, capped at 75 characters. This dataset is evaluation-only. Similar to BIBREF2 , we train our neural model on the Gigaword training set, and show the models' performances on DUC2004. Following the convention, we also use ROUGE limited-length recall as our evaluation metric, and set the capping length to 75 characters. We generate summaries with 15 words using beam-size of 10. As shown in Table TABREF35 , our method outperforms all previous methods on Rouge-1 and Rouge-L, and is comparable on Rouge-2. Furthermore, our model only uses 15k decoder vocabulary, while previous methods use 69k or 200k.\n\nQuestion: By how much does their model outperform both the state-of-the-art systems?\n\nExplanation: \n\nThe answer can be found in Table TABREF35 in [Document 1].\n\nAnswer: w.r.t Rouge-1 their model outperforms by 0.98% and w.r.t Rouge-L their model outperforms by 0.45%\n\nExample 4:\n\n[Document 1]: Experiment ::: Data\n\n[Document 2]: We evaluate our model in two English NER datasets and four Chinese NER datasets.\n\n[Document 3]: (1) CoNLL2003 is one of the most evaluated English NER datasets, which contains four different named entities: PERSON, LOCATION, ORGANIZATION, and MISC BIBREF34.\n\n[Document 4]: (2) OntoNotes 5.0 is an English NER dataset whose corpus comes from different domains, such as telephone conversation, newswire. We exclude the New Testaments portion since there is no named entity in it BIBREF8, BIBREF7. This dataset has eleven entity names and seven value types, like CARDINAL, MONEY, LOC.\n\n[Document 5]: (3) BIBREF35 released OntoNotes 4.0. In this paper, we use the Chinese part. We adopted the same pre-process as BIBREF36.\n\n[Document 6]: (4) The corpus of the Chinese NER dataset MSRA came from news domain BIBREF37.\n\n[Document 7]: (5) Weibo NER was built based on text in Chinese social media Sina Weibo BIBREF38, and it contained 4 kinds of entities.\n\n[Document 8]: (6) Resume NER was annotated by BIBREF33.\n\n[Document 9]: Experiment ::: Results on Chinese NER Datasets\n\n[Document 10]: We first present our results in the four Chinese NER datasets. Since Chinese NER is directly based on the characters, it is more straightforward to show the abilities of different models without considering the influence of word representation.\n\n[Document 11]: As shown in Table TABREF29, the vanilla Transformer does not perform well and is worse than the BiLSTM and CNN based models. However, when relative positional encoding combined, the performance was enhanced greatly, resulting in better results than the BiLSTM and CNN in all datasets. The number of training examples of the Weibo dataset is tiny, therefore the performance of the Transformer is abysmal, which is as expected since the Transformer is data-hungry. Nevertheless, when enhanced with the relative positional encoding and unscaled attention, it can achieve even better performance than the BiLSTM-based model. The superior performance of the adapted Transformer in four datasets ranging from small datasets to big datasets depicts that the adapted Transformer is more robust to the number of training examples than the vanilla Transformer. As the last line of Table TABREF29 depicts, the scaled attention will deteriorate the performance.\n\n[Document 12]: FLOAT SELECTED: Table 2: The F1 scores on Chinese NER datasets. \u2663,\u2660 are results reported in (Zhang and Yang, 2018) and (Gui et al., 2019a), respectively. \u201cw/ scale\u201d means TENER using the scaled attention in Eq.(19). \u2217 their results are not directly comparable with ours, since they used 100d pre-trained character and bigram embeddings. Other models use the same embeddings.\n\n[Document 13]: FLOAT SELECTED: Table 4: The F1 scores on English NER datasets. We only list results based on non-contextualized embeddings, and methods utilized pre-trained language models, pre-trained features, or higher dimension word vectors are excluded. TENER (Ours) uses the Transformer encoder both in the character-level and wordlevel. \u201cw/ scale\u201d means TENER using the scaled attention in Eq.(19). \u201cw/ CNN-char\u201d means TENER using CNN as character encoder instead of AdaTrans.\n\n[Document 14]: In summary, to improve the performance of the Transformer-based model in the NER task, we explicitly utilize the directional relative positional encoding, reduce the number of parameters and sharp the attention distribution. After the adaptation, the performance raises a lot, making our model even performs better than BiLSTM based models. Furthermore, in the six NER datasets, we achieve state-of-the-art performance among models without considering the pre-trained language models or designed features.\n\nQuestion: Do they outperform current NER state-of-the-art models?\n\nExplanation:"}
{"question_id": "3e3d123960e40bcb1618e11999bd2031ccc1d155", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We begin by filtering the corpus to keep only those tweets where the user's self-declared language and the tweet's detected language correspond; that language becomes the tweet's correct language label. This operation cuts out roughly half the tweets, and leaves us with a corpus of about 900 million tweets in 54 different languages. Table TABREF6 shows the distribution of languages in that corpus. Unsurprisingly, it is a very imbalanced distribution of languages, with English and Japanese together accounting for 60% of all tweets. This is consistent with other studies and statistics of language use on Twitter, going as far back as 2013. It does however make it very difficult to use this corpus to train a LID system for other languages, especially for one of the dozens of seldom-used languages. This was our motivation for creating a balanced Twitter dataset.\n\nQuestion: What languages are represented in the dataset?\n\nExplanation:  [Document 1] provides a list of languages represented in the dataset.\n\nAnswer: EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO\n\nExample 2:\n\n[Document 1]: For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter. It contains tweets collected based on keyword search that were posted between July 17, 2014 (the day of the plane crash) and December 9, 2016.\n\n[Document 2]: BIBREF4 provide annotations for a subset of the English tweets contained in the dataset. A tweet is annotated with one of three classes that indicate the framing of the tweet with respect to responsibility for the plane crash. A tweet can either be pro-Russian (Ukrainian authorities, NATO or EU countries are explicitly or implicitly held responsible, or the tweet states that Russia is not responsible), pro-Ukrainian (the Russian Federation or Russian separatists in Ukraine are explicitly or implicitly held responsible, or the tweet states that Ukraine is not responsible) or neutral (neither Ukraine nor Russia or any others are blamed). Example tweets for each category can be found in Table TABREF9. These examples illustrate that the framing annotations do not reflect general polarity, but polarity with respect to responsibility to the crash. For example, even though the last example in the table is in general pro-Ukrainian, as it displays the separatists in a bad light, the tweet does not focus on responsibility for the crash. Hence the it is labeled as neutral. Table TABREF8 shows the label distribution of the annotated portion of the data as well as the total amount of original tweets, and original tweets plus their retweets/duplicates in the network. A retweet is a repost of another user's original tweet, indicated by a specific syntax (RT @username: ). We consider as duplicate a tweet with text that is identical to an original tweet after preprocessing (see Section SECREF18). For our classification experiments, we exclusively consider original tweets, but model predictions can then be propagated to retweets and duplicates.\n\nQuestion: What languages are included in the dataset?\n\nExplanation:  The dataset only contains English tweets, as stated in [Document 2].\n\nAnswer: English\n\nExample 3:\n\n[Document 1]: The data stems from a joint ADAPT-Microsoft project. An overview of the provided dataset is given in Table TABREF16 . Notice that the available amount of data differs per language.\n\nQuestion: is the dataset balanced across the four languages?\n\nExplanation:  [Document 1] provides a table that shows the number of data points per language. The dataset is not balanced across the four languages.\n\nAnswer: No\n\nExample 4:\n\nQuestion: What are the languages of the datasets?\n\nExplanation:"}
{"question_id": "bb7c80ab28c2aebfdd0bd90b22a55dbdf3a8ed5b", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We evaluate three models of the image attention mechanism INLINEFORM0 of equation EQREF11 . They have in common the fact that at each time step INLINEFORM1 of the decoding phase, all approaches first take as input the annotation sequence INLINEFORM2 to derive a time-dependent context vector that contain relevant information in the image to help predict the current target word INLINEFORM3 . Even though these models differ in how the time-dependent context vector is derived, they share the same subsequent steps. For each mechanism, we propose two hand-picked illustrations showing where the attention is placed in an image.\n\n[Document 2]: Soft attention\n\n[Document 3]: Hard Stochastic attention\n\n[Document 4]: Local Attention\n\nQuestion: Which attention mechanisms do they compare?\n\nExplanation: \n\nAccording to [Document 1], the three models of the image attention mechanism that are being compared are soft attention, hard stochastic attention, and local attention. This is further supported by [Document 2], [Document 3], and [Document 4].\n\nAnswer: Soft attention, Hard Stochastic attention, Local Attention\n\nExample 2:\n\n[Document 1]: Tables and summarize the results from the link-prediction experiments on all three datasets, where a different ratio of edges are used for training. Results from models other than GANE are collected from BIBREF9 , BIBREF10 and BIBREF34 . We have also repeated these experiments on our own, and the results are consistent with the ones reported. Note that BIBREF34 did not report results on DMTE. Both GANE variants consistently outperform competing solutions. In the low-training-sample regime our solutions lead by a large margin, and the performance gap closes as the number of training samples increases. This indicates that our OT-based mutual attention framework can yield more informative textual representations than other methods. Note that GANE-AP delivers better results compared with GANE-OT, suggesting the attention parsing mechanism can further improve the low-level mutual attention matrix. More results on Cora and Hepth are provided in the SM.\n\nQuestion: Which of their proposed attention methods works better overall?\n\nExplanation: \n\nAccording to [Document 1], the attention parsing method works better overall. This is further supported by the fact that GANE-AP delivers better results compared with GANE-OT.\n\nAnswer: attention parsing\n\nExample 3:\n\n[Document 1]: In this section, we analyze the visual and text based attention mechanisms. We find that the visual attention is very sparse, in that just one source encoding is attended to (the maximum visual attention over source encodings, across the test set, has mean 0.99 and standard deviation 0.015), thereby limiting the use of modulation. Thus, in practice, we find that a small weight ($\\gamma =0.1$) is necessary to prevent degradation due to this sparse visual attention component. Figure FIGREF18 & FIGREF19 shows the comparison of visual and text based attention for two sentences, one long source sentence of length 21 and one short source sentence of length 7. In both cases, we find that the visual component of the attention hasn't learnt any variation over the source encodings, again suggesting that the visual embeddings do not lend themselves to enhancing token-level discriminativess during prediction. We find this to be consistent across sentences of different lengths.\n\nQuestion: What is result of their attention distribution analysis?\n\nExplanation:  [Document 1] contains the results of the attention distribution analysis. The visual attention is found to be very sparse, and the visual component of the attention is found to have not learnt any variation over the source encodings.\n\nAnswer: visual attention is very sparse,  visual component of the attention hasn't learnt any variation over the source encodings\n\nExample 4:\n\n[Document 1]: To emit a character the speller uses the attention mechanism to find a set of relevant activations of the listener INLINEFORM0 and summarize them into a context INLINEFORM1 . The history of previously emitted characters is encapsulated in a recurrent state INLINEFORM2 : DISPLAYFORM0\n\nQuestion: What type of attention is used in the recognition system?\n\nExplanation:"}
{"question_id": "8276671a4d4d1fbc097cd4a4b7f5e7fadd7b9833", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: To evaluate the performance of the proposed approach, precision (1), recall (2), f-Measure (3), and prediction accuracy (4) have been used as a performance matrices. The experimental results are shown in Table 1, where it can be seen that autoencoders outperformed MLP and CNN outperformed autoencoders with the highest achieved accuracy of 82.6%. DISPLAYFORM0 DISPLAYFORM1\n\nQuestion: Which deep learning model performed better?\n\nExplanation: \n\nAccording to [Document 1], autoencoders outperformed MLP and CNN.\n\nAnswer: autoencoders\n\nExample 2:\n\n[Document 1]: We evaluate the proposed models on three conversationet dataset such as MojiTalk BIBREF16, PersonaChat BIBREF11, Empathetic-Dialogues BIBREF26.\n\nQuestion: What three conversational datasets are used for evaluation?\n\nExplanation: \n\nAccording to [Document 1], the three conversational datasets used for evaluation are MojiTalk , PersonaChat , and Empathetic-Dialogues .\n\nAnswer: MojiTalk , PersonaChat , Empathetic-Dialogues\n\nExample 3:\n\n[Document 1]: Datasets: We experimented on four standard datasets: WN18 and FB15k are extracted by BIBREF5 from Wordnet BIBREF32 Freebase BIBREF33 . We used the same train/valid/test sets as in BIBREF5 . WN18 contains 40,943 entities, 18 relations and 141,442 train triples. FB15k contains 14,951 entities, 1,345 relations and 483,142 train triples. In order to test the expressiveness ability rather than relational pattern learning power of models, FB15k-237 BIBREF34 and WN18RR BIBREF35 exclude the triples with inverse relations from FB15k and WN18 which reduced the size of their training data to 56% and 61% respectively. Baselines: We compare MDE with several state-of-the-art relational learning approaches. Our baselines include, TransE, TransH, TransD, TransR, STransE, DistMult, NTN, RESCAL, ER-MLP, and ComplEx and SimplE. We report the results of TransE, DistMult, and ComplEx from BIBREF13 and the results of TransR and NTN from BIBREF36 , and ER-MLP from BIBREF15 . The results on the inverse relation excluded datasets are from BIBREF11 , Table 13 for TransE and RotatE and the rest are from BIBREF35 .\n\nQuestion: What datasets are used to evaluate the model?\n\nExplanation: \n\nAccording to [Document 1], the WN18 and FB15k datasets are used to evaluate the model.\n\nAnswer: WN18 and FB15k\n\nExample 4:\n\n[Document 1]: Our main goal in this section is to see if Image Captioning models could learn well with Vietnamese language. To accomplish this task, we train and evaluate our dataset with two published Image Captioning models applying encoder-decoder architecture. The models we propose are Neural Image Captioning (NIC) model BIBREF14, Image Captioning model from the Pytorch-tutorial BIBREF15 by Yunjey.\n\n[Document 2]: Overall, CNN is first used for extracting image features for encoder part. The image features which are presented in vectors will be used as layers for decoding. For decoder part, RNN - LSTM are used to embed the vectors to target sentences using words/tokens provided in vocabulary.\n\nQuestion: What deep neural network models are used in evaluation?\n\nExplanation:"}
{"question_id": "d211a37830c59aeab4970fdb2e03d9b7368b421c", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We have presented the first attempt to solve the fake news problem for Bulgarian. Our method is purely text-based, and ignores the publication date and the source of the article. It combines task-specific embeddings, produced by a two-level attention-based deep neural network model, with manually crafted features (stylometric, lexical, grammatical, and semantic), into a kernel-based SVM classifier. We further produced and shared a number of relevant language resources for Bulgarian, which we created for solving the task.\n\nQuestion: what types of features were used?\n\nExplanation:  [Document 1] mentions that the features used were stylometric, lexical, grammatical, and semantic.\n\nAnswer: stylometric, lexical, grammatical, and semantic\n\nExample 2:\n\n[Document 1]: 45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier. These features can be grouped into three categories (See Table TABREF5 for complete list of features):\n\n[Document 2]: Sociodemographics: gender, age, marital status, etc.\n\n[Document 3]: Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc.\n\n[Document 4]: Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc.\n\n[Document 5]: The Current Admission feature group has the most number of features, with 29 features included in this group alone. These features can be further stratified into two groups: `structured' clinical features and `unstructured' clinical features.\n\n[Document 6]: Feature Extraction ::: Structured Features\n\n[Document 7]: Structure features are features that were identified on the EHR using regular expression matching and include rating scores that have been reported in the psychiatric literature as correlated with increased readmission risk, such as Global Assessment of Functioning, Insight and Compliance:\n\n[Document 8]: Global Assessment of Functioning (GAF): The psychosocial functioning of the patient ranging from 100 (extremely high functioning) to 1 (severely impaired) BIBREF13.\n\n[Document 9]: Insight: The degree to which the patient recognizes and accepts his/her illness (either Good, Fair or Poor).\n\n[Document 10]: Compliance: The ability of the patient to comply with medication and to follow medical advice (either Yes, Partial, or None).\n\n[Document 11]: These features are widely-used in clinical practice and evaluate the general state and prognosis of the patient during the patient's evaluation.\n\n[Document 12]: Feature Extraction ::: Unstructured Features\n\n[Document 13]: Unstructured features aim to capture the state of the patient in relation to seven risk factor domains (Appearance, Thought Process, Thought Content, Interpersonal, Substance Use, Occupation, and Mood) from the free-text narratives on the EHR. These seven domains have been identified as associated with readmission risk in prior work BIBREF14.\n\n[Document 14]: These unstructured features include: 1) the relative number of sentences in the admission notes that involve each risk factor domain (out of total number of sentences within the admission) and 2) clinical sentiment scores for each of these risk factor domains, i.e. sentiment scores that evaluate the patient\u2019s psychosocial functioning level (positive, negative, or neutral) with respect to each of these risk factor domain.\n\nQuestion: What features are used?\n\nExplanation: \n\nAccording to [Document 1], 45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier. These features can be grouped into three categories: Sociodemographics, Past medical history, and Information from the current admission. This information is further supported by [Document 2], [Document 3], and [Document 4].\n\nAnswer: Sociodemographics: gender, age, marital status, etc., Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc., Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc.\n\nExample 3:\n\n[Document 1]: Supervised Learning. We restrict our supervised experiments to a default linear SVM learner with Stochastic Gradient Descent (SGD) training and L2 regularization, available in the SciKit-Learn toolkit BIBREF25 . We use 10-fold cross-validation, and only two types of features: n-grams and Word2Vec word embeddings. We expect Word2Vec to be able to capture semantic generalizations that n-grams do not BIBREF26 , BIBREF27 . The n-gram features include unigrams, bigrams, and trigrams, including sequences of punctuation (for example, ellipses or \"!!!\"), and emoticons. We use GoogleNews Word2Vec features BIBREF28 .\n\nQuestion: What simple features are used?\n\nExplanation: \n\nAccording to [Document 1], the simple features used are unigrams, bigrams, and trigrams, including sequences of punctuation, and Word2Vec word embeddings.\n\nAnswer: unigrams, bigrams, and trigrams, including sequences of punctuation, Word2Vec word embeddings\n\nExample 4:\n\n[Document 1]: The following surface-form features were used:\n\n[Document 2]: INLINEFORM0 -grams: word INLINEFORM1 -grams (contiguous sequences of INLINEFORM2 tokens), non-contiguous word INLINEFORM3 -grams ( INLINEFORM4 -grams with one token replaced by *), character INLINEFORM5 -grams (contiguous sequences of INLINEFORM6 characters), unigram stems obtained with the Porter stemming algorithm;\n\n[Document 3]: General-domain word embeddings:\n\n[Document 4]: dense word representations generated with word2vec on ten million English-language tweets, summed over all tokens in the tweet,\n\n[Document 5]: word embeddings distributed as part of ConceptNet 5.5 BIBREF15 , summed over all tokens in the tweet;\n\n[Document 6]: General-domain word clusters: presence of tokens from the word clusters generated with the Brown clustering algorithm on 56 million English-language tweets; BIBREF11\n\n[Document 7]: Negation: presence of simple negators (e.g., not, never); negation also affects the INLINEFORM0 -gram features\u2014a term INLINEFORM1 becomes INLINEFORM2 if it occurs after a negator and before a punctuation mark;\n\n[Document 8]: Twitter-specific features: the number of tokens with all characters in upper case, the number of hashtags, presence of positive and negative emoticons, whether the last token is a positive or negative emoticon, the number of elongated words (e.g., soooo);\n\n[Document 9]: Punctuation: presence of exclamation and question marks, whether the last token contains an exclamation or question mark.\n\nQuestion: what surface-form features were used?\n\nExplanation:"}
{"question_id": "37ac705166fa87dc74fe86575bf04bea56cc4930", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We decided to evaluate our model using weighted F1-score, i.e., the per-class F1 score is calculated and averaged by weighting each label by its support. Notice, since our setup deviates from the shared task setup (single-label versus multi-label classification), the final evaluation metric is different. We will report on weighted F1-score for the development and test set (with simple macro averaging), but use Exact-Accuracy and Micro F1 over all labels when presenting official results on the test sets. The latter two metrics were part of the official evaluation metrics. For details we refer the reader to the shared task overview paper BIBREF5 .\n\nQuestion: what evaluation metrics were used?\n\nExplanation: \n\nAccording to [Document 1], the evaluation metric used was weighted F1-score.\n\nAnswer: weighted F1-score\n\nExample 2:\n\n[Document 1]: A two-step validation was conducted for English-speaking customers. An initial A/B testing for the LR model in a production setting was performed to compare the labelling strategies. A second offline comparison of the models was conducted on historical data and a selected labelling strategy. One month of data and a subset of the customers was used (approx. eighty thousand). The sampled dataset presents a fraction of positive labels of approximately 0.5 for reuse and 0.2 for one-day return. Importantly, since this evaluation is done on a subset of users, the dataset characteristic's do not necessarily represent real production traffic. The joke corpus in this dataset contains thousands of unique jokes of different categories (sci-fi, sports, etc) and types (puns, limerick, etc). The dataset was split timewise into training/validation/test sets, and hyperparameters were optimized to maximize the AUC-ROC on the validation set. As a benchmark, we also consider two additional methods: a non-personalized popularity model and one that follows BIBREF16, replacing the transformer joke encoder with a CNN network (the specialized loss and other characteristics of the DL model are kept).\n\nQuestion: What evaluation metrics were used?\n\nExplanation:  According to [Document 1], AUC-ROC was used to evaluate the models.\n\nAnswer: AUC-ROC\n\nExample 3:\n\n[Document 1]: Our experimental results are shown in Table TABREF21. The first half of the table contains results for task-oriented dialogue with the Sequicity framework with two scenarios for training data preparation. For each experiment, we run our models for 3 times and their scores are averaged as the final score. The mixed training scenario performs the mixing of both the training data, development data and the test data as described in the previous subsection. The non-mixed training scenario performs the mixing only on the development and test data, keeps the training data unmixed as in the original KVRET dataset. As in the Sequicity framework, we report entity match rate, BLEU score and Success F1 score. Entity match rate evaluates task completion, it determines if a system can generate all correct constraints to search the indicated entities of the user. BLEU score evaluates the language quality of generated responses. Success F1 balances the recall and precision rates of slot answers. For further details on these metrics, please refer to BIBREF8.\n\nQuestion: What were the evaluation metrics used?\n\nExplanation:  The evaluation metrics used were entity match rate, BLEU score, and Success F1 score. This information is found in [Document 1].\n\nAnswer: entity match rate, BLEU score, Success F1 score\n\nExample 4:\n\n[Document 1]: Evaluation Metrics: We study the variants of the same model by training with different proportions of the negotiation seen, namely, $f \\in \\lbrace 0.0, 0.2, 0.4, 0.6, 0.8, 1.0\\rbrace $. We compare the models on two evaluation metrics: MAE: Mean Absolute Error between the predicted and ground-truth agreed prices along with Accuracy$\\pm k$: the percentage of cases where the predicted price lies within $k$ percent of the ground-truth. We use $k=5$ and $k=10$ in our experiments.\n\nQuestion: what evaluation metrics were used?\n\nExplanation:"}
{"question_id": "3cc0d773085dc175b85955e95911a2cfaab2cdc4", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German.\n\nQuestion: what language pairs are explored?\n\nExplanation: \n\nAccording to [Document 1], the language pairs explored are German-English, Turkish-English, and English-German.\n\nAnswer: German-English, Turkish-English, English-German\n\nExample 2:\n\nQuestion: Are the semantic analysis findings for Italian language similar to English language version?\n\nExplanation:  The documents do not provide enough information to answer the question.\n\nAnswer: Unanswerable\n\nExample 3:\n\nQuestion: Which language-pair had the better performance?\n\nExplanation: \n\n[Document 1]: We evaluate the system on two language pairs: English-French and English-German.\n\n[Document 2]: The system was trained on a parallel corpus of 3.2 million sentence pairs.\n\n[Document 3]: We use the BLEU score BLEU1 , BLEU2 as our primary evaluation metric.\n\n[Document 4]:\n\nFor the English-French translation task, the system achieves a BLEU score of 34.8.\n\nFor the English-German translation task, the system achieves a BLEU score of 27.1.\n\nFrom [Document 4], we can see that the system had a better performance on the English-French translation task than on the English-German translation task.\n\nAnswer: French-English\n\nExample 4:\n\n[Document 1]: In Fig. FIGREF14 , the cosine similarities of the segment pairs get smaller as the edit distances increase, and the trend is observed in all languages. The gap between each edit distance groups, i.e. (0,1), (1,2), (2,3), (3,4), is obvious. This means that INLINEFORM0 learned from English can successfully encode the sequential phonetic structures into fixed-length vector for the target languages to some good extend even though it has never seen any audio data of the target languages. Another interesting fact is the corresponding variance between languages. In the source language, English, the variances of the five edit distance groups are fixed at 0.030, which means that the cosine similarity in each edit distance group is centralized. However, the variances of the groups in the target languages vary. In French and German, the variance grows from 0.030 to 0.060 as the edit distance increases from 0 to 4. For Czech/Spanish, the variance starts at a larger value of 0.040/0.050 and increases to 0.050/0.073. We suspect that the fluctuating variance is related to the similarity between languages. English, German and French are more similar compared with Czech and Spanish. Among the four target languages, German has the highest lexical similarity with English (0.60) and the second highest is French (0.27), while for Czech and Spanish, the lexical similarity scores is 0 BIBREF48 .\n\nQuestion: Which pairs of languages do they consider similar enough to capture phonetic structure?\n\nExplanation:"}
{"question_id": "e752dc4d721a2cf081108b6bd71e3d10b4644354", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: The aim of this section is to validate the applicability of our theoretical results\u2014which state that self-attention can perform convolution\u2014and to examine whether self-attention layers in practice do actually learn to operate like convolutional layers, when being trained on standard image classification tasks. In particular, we study the relationship between self-attention and convolution with quadratic and learned relative positional encodings. We find that for both cases, the attention probabilities learned tend to respect the conditions of Lemma UNKREF15, corroborating our hypothesis.\n\n[Document 2]: We study a fully attentional model consisting of six multi-head self-attention layers. As it has already been shown by BIBREF9 that combining attention features with convolutional features improves performance on Cifar-100 and ImageNet, we do not focus on attaining state-of-the-art performance. Nevertheless, to validate that our model learns a meaningful classifier we compare it to the standard ResNet18 BIBREF14 on the CIFAR-10 dataset BIBREF15. In all experiments, we use a $2\\times 2$ invertible down-sampling BIBREF16 on the input to reduce the size of the image as storing the attention coefficient tensor requires a large amount of GPU memory. The fixed size representation of the input image is computed as the average pooling of the last layer representations and given to a linear classifier.\n\nQuestion: What numerical experiments they perform?\n\nExplanation: \n\nAccording to [Document 1], the authors perform experiments to validate the applicability of their theoretical results. In particular, they study the relationship between self-attention and convolution with quadratic and learned relative positional encodings. They find that for both cases, the attention probabilities learned tend to respect the conditions of Lemma UNKREF15, corroborating their hypothesis.\n\nAccording to [Document 2], the authors study a fully attentional model consisting of six multi-head self-attention layers. They compare it to the standard ResNet18 on the CIFAR-10 dataset in order to validate that their model learns a meaningful classifier.\n\nAnswer: attention probabilities learned tend to respect the conditions of Lemma UNKREF15, corroborating our hypothesis, validate that our model learns a meaningful classifier we compare it to the standard ResNet18\n\nExample 2:\n\n[Document 1]: The BLEU score is an indication of how close the automated translation is to the reference translation, but does not tell us what exactly changed concerning the gender and number properties we attempt to control. We perform a finer-grained analysis focusing on the relation between the injected speaker and audience information, and the morphological realizations of the corresponding elements. We parse the translations and the references using a Hebrew dependency parser. In addition to the parse structure, the parser also performs morphological analysis and tagging of the individual tokens. We then perform the following analysis.\n\n[Document 2]: Speaker's Gender Effects: We search for first-person singular pronouns with subject case (ani, unmarked for gender, corresponding to the English I), and consider the gender of its governing verb (or adjectives in copular constructions such as `I am nice'). The possible genders are `masculine', `feminine' and `both', where the latter indicates a case where the none-diacriticized written form admits both a masculine and a feminine reading. We expect the gender to match the ones requested in the prefix.\n\n[Document 3]: Interlocutors' Gender and Number Effects: We search for second-person pronouns and consider their gender and number. For pronouns in subject position, we also consider the gender and number of their governing verbs (or adjectives in copular constructions). For a singular audience, we expect the gender and number to match the requested ones. For a plural audience, we expect the masculine-plural forms.\n\nQuestion: What type of syntactic analysis is performed?\n\nExplanation:  [Document 1] mentions that the parser \"also performs morphological analysis and tagging of the individual tokens.\" [Document 2] and [Document 3] both mention specific types of syntactic analysis that are performed.\n\nAnswer: Speaker's Gender Effects, Interlocutors' Gender and Number Effects\n\nExample 3:\n\n[Document 1]: More specifically, we observe the impact of: (i) pre-trained word embeddings BIBREF11, BIBREF12, recurrent BIBREF13 and transformer-based sentence encoders BIBREF14 as question representation strategies; (ii) distinct convolutional neural networks used for visual feature extraction BIBREF15, BIBREF16, BIBREF17; and (iii) standard fusion strategies, as well as the importance of two main attention mechanisms BIBREF18, BIBREF19. We notice that even using a relatively simple baseline architecture, our best models are competitive to the (maybe overly-complex) state-of-the-art models BIBREF20, BIBREF21. Given the experimental nature of this work, we have trained over 130 neural network models, accounting for more than 600 GPU processing hours. We expect our findings to be useful as guidelines for training novel VQA models, and that they serve as a basis for the development of future architectures that seek to maximize predictive performance.\n\nQuestion: What type of experiments are performed?\n\nExplanation:  The types of experiments performed are listed in [Document 1].\n\nAnswer: pre-trained word embeddings BIBREF11, BIBREF12, recurrent BIBREF13, transformer-based sentence encoders BIBREF14, distinct convolutional neural networks, standard fusion strategies,  two main attention mechanisms BIBREF18, BIBREF19\n\nExample 4:\n\n[Document 1]: Four sets of experiments are conducted. The first experiment compares DUPMN with other sentiment analysis methods. The second experiment evaluates the effectiveness of different hop size INLINEFORM0 of memory network. The third experiment evaluates the effectiveness of UMN and PMN in different datasets. The fourth set of experiment examines the effect of memory size INLINEFORM1 on the performance of DUPMN. Performance measures include Accuracy (ACC), Root-Mean-Square-Error (RMSE), and Mean Absolute Error (MAE) for our model. For other baseline methods in Group 2 and Group 3, their reported results are used. We also show the p-value by comparing the result of 10 random tests for both our model and the state-of-the-art model in the t-test .\n\nQuestion: What statistical test is performed?\n\nExplanation:"}
{"question_id": "108f99fcaf620fab53077812e8901870896acf36", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We conducted our experiments on the CSJ BIBREF25, which is one of the most widely used evaluation sets for Japanese speech recognition. The CSJ consists of more than 600 hrs of Japanese recordings.\n\n[Document 2]: While most of the content is lecture recordings by a single speaker, CSJ also contains 11.5 hrs of 54 dialogue recordings (average 12.8 min per recording) with two speakers, which were the main target of ASR and speaker diarization in this study. During the dialogue recordings, two speakers sat in two adjacent sound proof chambers divided by a glass window. They could talk with each other over voice connection through a headset for each speaker. Therefore, speech was recorded separately for each speaker, and we generated mixed monaural recordings by mixing the corresponding speeches of two speakers. When mixing two recordings, we did not apply any normalization of speech volume. Due to this recording procedure, we were able to use non-overlapped speech to evaluate the oracle WERs.\n\nQuestion: How long are dialogue recordings used for evaluation?\n\nExplanation:  [Document 2] states that the average dialogue recording is 12.8 minutes long.\n\nAnswer: average 12.8 min per recording\n\nExample 2:\n\n[Document 1]: We evaluate the proposed models on three conversationet dataset such as MojiTalk BIBREF16, PersonaChat BIBREF11, Empathetic-Dialogues BIBREF26.\n\nQuestion: What three conversational datasets are used for evaluation?\n\nExplanation: \n\nAccording to [Document 1], the three conversational datasets used for evaluation are MojiTalk , PersonaChat , and Empathetic-Dialogues .\n\nAnswer: MojiTalk , PersonaChat , Empathetic-Dialogues\n\nExample 3:\n\n[Document 1]: The most common approach is to treat the task as a document-level classification problem. Given a set of evaluation documents, each having a known correct label from a closed set of labels (often referred to as the \u201cgold-standard\u201d), and a predicted label for each document from the same set, the document-level accuracy is the proportion of documents that are correctly labeled over the entire evaluation collection. This is the most frequently reported metric and conveys the same information as the error rate, which is simply the proportion of documents that are incorrectly labeled (i.e. INLINEFORM0 ).\n\n[Document 2]: Authors sometimes provide a per-language breakdown of results. There are two distinct ways in which results are generally summarized per-language: (1) precision, in which documents are grouped according to their predicted language; and (2) recall, in which documents are grouped according to what language they are actually written in. Earlier work has tended to only provide a breakdown based on the correct label (i.e. only reporting per-language recall). This gives us a sense of how likely a document in any given language is to be classified correctly, but does not give an indication of how likely a prediction for a given language is of being correct. Under the monolingual assumption (i.e. each document is written in exactly one language), this is not too much of a problem, as a false negative for one language must also be a false positive for another language, so precision and recall are closely linked. Nonetheless, authors have recently tended to explicitly provide both precision and recall for clarity. It is also common practice to report an F-score INLINEFORM0 , which is the harmonic mean of precision and recall. The F-score (also sometimes called F1-score or F-measure) was developed in IR to measure the effectiveness of retrieval with respect to a user who attaches different relative importance to precision and recall BIBREF376 . When used as an evaluation metric for classification tasks, it is common to place equal weight on precision and recall (hence \u201cF1\u201d-score, in reference to the INLINEFORM1 hyper-parameter, which equally weights precision and recall when INLINEFORM2 ).\n\nQuestion: what evaluation methods are discussed?\n\nExplanation:  [Document 1] discusses document-level accuracy, while [Document 2] discusses precision, recall, and F-score.\n\nAnswer: document-level accuracy, precision, recall, F-score\n\nExample 4:\n\n[Document 1]: Evaluating open-domain chit-chat models is challenging, especially in multiple languages and at the dialogue-level. Hence, we evaluate our models using both automatic and human evaluation. In both cases, human-annotated dialogues are used, which show the importance of the provided dataset.\n\n[Document 2]: Experiments ::: Evaluation Metrics ::: Automatic\n\n[Document 3]: For each language, we evaluate responses generated by the models using perplexity (ppl.) and BLEU BIBREF67 with reference to the human-annotated responses. Although these automatic measures are not perfect BIBREF68, they help to roughly estimate the performance of different models under the same test set. More recently, BIBREF69 has shown the correlation between perplexity and human judgment in open-domain chit-chat models.\n\n[Document 4]: Following ACUTE-EVAL, the annotator is provided with two full dialogues made by self-chat or human-dialogue. The annotator is asked to choose which of the two dialogues is better in terms of engagingness, interestingness, and humanness. For each comparison, we sample 60\u2013100 conversations from both models. In Appendix C, we report the exact questions and instructions given to the annotators, and the user interface used in the evaluation. We hired native speakers annotators for all six considered languages. The annotators were different from the dataset collection annotators to avoid any possible bias.\n\nQuestion: What kind of evaluations do use to evaluate dialogue?\n\nExplanation:"}
{"question_id": "22c125c461f565f5437dac74bf19c2ef317bad86", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: For each speaker, we divide all available utterances into disjoint train, development, and test sets. Using the force-aligned phone boundaries, we extract the mid-phone frame for each example across the four classes, which leads to a data imbalance. Therefore, for all utterances in the training set, we randomly sample additional examples within a window of 5 frames around the center phone, to at least 50 training examples per class per speaker. It is not always possible to reach the target of 50 examples, however, if no more data is available to sample from. This process gives a total of INLINEFORM0 10700 training examples with roughly 2000 to 3000 examples per class, with each speaker having an average of 185 examples. Because the amount of data varies per speaker, we compute a sampling score, which denotes the proportion of sampled examples to the speaker's total training examples. We expect speakers with high sampling scores (less unique data overall) to underperform when compared with speakers with more varied training examples.\n\nQuestion: How many instances does their dataset have?\n\nExplanation:  The dataset has 10700 training examples. This information is found in the first sentence of the first paragraph.\n\nAnswer: 10700\n\nExample 2:\n\n[Document 1]: We evaluate our approaches on two public Chinese span-extraction machine reading comprehension datasets: CMRC 2018 (simplified Chinese) BIBREF8 and DRCD (traditional Chinese) BIBREF9. The statistics of the two datasets are listed in Table TABREF29.\n\n[Document 2]: Note that, since the test and challenge sets are preserved by CMRC 2018 official to ensure the integrity of the evaluation process, we submitted our best-performing systems to the organizers to get these scores. The resource in source language was chosen as SQuAD BIBREF4 training data. The settings of the proposed approaches are listed below in detail.\n\n[Document 3]: Translation: We use Google Neural Machine Translation (GNMT) system for translation. We evaluated GNMT system on NIST MT02/03/04/05/06/08 Chinese-English set and achieved an average BLEU score of 43.24, compared to previous best work (43.20) BIBREF17, yielding state-of-the-art performance.\n\nQuestion: How big are the datasets used?\n\nExplanation: \n\n[Document 1] provides the sizes of the evaluation datasets used. [Document 2] does not specify the size of the SQuAD dataset, but it is likely to be similar in size to the evaluation datasets. [Document 3] does not specify the size of the NIST MT02/03/04/05/06/08 Chinese-English dataset.\n\nAnswer: Evaluation datasets used:\nCMRC 2018 - 18939 questions, 10 answers\nDRCD - 33953 questions, 5 answers\nNIST MT02/03/04/05/06/08 Chinese-English - Not specified\n\nSource language train data:\nSQuAD - Not specified\n\nExample 3:\n\n[Document 1]: We need good amount of data to try deep learning state-of-the-art algorithms. There are lot of open datasets available for names, locations, organisations, but not for topics as defined in Abstract above. Also defining and inferring topics is an individual preference and there are no fix set of rules for its definition. But according to our definition, we can use wikipedia titles as our target topics. English wikipedia dataset has more than 18 million titles if we consider all versions of them till now. We had to clean up the titles to remove junk titles as wikipedia title almost contains all the words we use daily. To remove such titles, we deployed simple rules as follows -\n\n[Document 2]: After doing some more cleaning we were left with 10 million titles. We have a dump of 15 million English news articles published in past 4 years. Further, we reduced number of articles by removing duplicate and near similar articles. We used our pre-trained doc2vec models and cosine similarity to detect almost similar news articles. Then selected minimum articles required to cover all possible 2-grams to 5-grams. This step is done to save some training time without loosing accuracy. Do note that, in future we are planning to use whole dataset and hope to see gains in F1 and Recall further. But as per manual inspection, our dataset contains enough variations of sentences with rich vocabulary which contains names of celebrities, politicians, local authorities, national/local organisations and almost all locations, India and International, mentioned in the news text, in last 4 years.\n\nQuestion: How large is the dataset they used?\n\nExplanation: \n\nAccording to [Document 1], the English wikipedia dataset has more than 18 million titles. This information is further supported by [Document 2], which states that the dataset contains a dump of 15 million English news articles.\n\nAnswer: English wikipedia dataset has more than 18 million, a dump of 15 million English news articles \n\nExample 4:\n\n[Document 1]: We begin with a hate speech lexicon containing words and phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%. We use the majority decision for each tweet to assign a label. Some tweets were not assigned labels as there was no majority class. This results in a sample of 24,802 labeled tweets.\n\nQuestion: How many users does their dataset have?\n\nExplanation:"}
{"question_id": "283d358606341c399e369f2ba7952cd955326f73", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: The biggest difference between discourse parsing for well-written document and dialogues is that discourse relations can exist on two nonadjacent utterances in dialogues. When we annotate dialogues, we should read dialogues from begin to the end. For each utterance, we should find its one parent node at least from all its previous utterances. We assume that the discourse structure is a connected graph and no utterance is isolated.\n\n[Document 2]: We propose three questions for eache dialogue and annotate the span of answers in the input dialogue. As we know, our dataset is the first corpus for multi-party dialogues reading comprehension.\n\n[Document 3]: We construct following questions and answers for the dialogue in Example 1:\n\n[Document 4]: Q1: When does Bdale leave?\n\n[Document 5]: A1: Fri morning\n\n[Document 6]: Q2: How to get people love Mark in Mjg59's opinion.\n\n[Document 7]: A2: Hire people to work on reverse-engineering closed drivers.\n\n[Document 8]: On the other hand, to improve the difficulty of the task, we propose $ \\frac{1}{6}$ to $ \\frac{1}{3}$ unanswerable questions in our dataset. We annotate unanswerable questions and their plausible answers (PA). Each plausible answer comes from the input dialogue, but is not the answer for the plausible question.\n\n[Document 9]: Q1: Whis is the email of daniels?\n\n[Document 10]: PA: +61 403 505 896\n\nQuestion: Is annotation done manually?\n\nExplanation:  [Document 1], [Document 2], [Document 3], [Document 4], [Document 5], [Document 6], [Document 7], [Document 8], [Document 9], and [Document 10] all suggest that annotation is done manually.\n\nAnswer: Yes\n\nExample 2:\n\n[Document 1]: In this work, we use the datasets released by BIBREF1 and HEOT dataset provided by BIBREF0 . The datasets obtained pass through these steps of processing: (i) Removal of punctuatios, stopwords, URLs, numbers, emoticons, etc. This was then followed by transliteration using the Xlit-Crowd conversion dictionary and translation of each word to English using Hindi to English dictionary. To deal with the spelling variations, we manually added some common variations of popular Hinglish words. Final dictionary comprised of 7200 word pairs. Additionally, to deal with profane words, which are not present in Xlit-Crowd, we had to make a profanity dictionary (with 209 profane words) as well. Table TABREF3 gives some examples from the dictionary.\n\n[Document 2]: Both the HEOT and BIBREF1 datasets contain tweets which are annotated in three categories: offensive, abusive and none (or benign). Some examples from the dataset are shown in Table TABREF4 . We use a LSTM based classifier model for training our model to classify these tweets into these three categories. An overview of the model is given in the Figure FIGREF12 . The model consists of one layer of LSTM followed by three dense layers. The LSTM layer uses a dropout value of 0.2. Categorical crossentropy loss was used for the last layer due to the presence of multiple classes. We use Adam optimizer along with L2 regularisation to prevent overfitting. As indicated by the Figure FIGREF12 , the model was initially trained on the dataset provided by BIBREF1 , and then re-trained on the HEOT dataset so as to benefit from the transfer of learned features in the last stage. The model hyperparameters were experimentally selected by trying out a large number of combinations through grid search.\n\nQuestion: Do they perform some annotation?\n\nExplanation:  There is no mention of any annotation being performed in either document.\n\nAnswer: No\n\nExample 3:\n\n[Document 1]: With the gathered comments, we reconstructed the original conversation trees, from the original post, the root, to the leaves, when they were available and selected a subset to annotated. For annotation purposes, we created snippets of conversations as the ones shown in Example 1 and Example 2 consisting of the parent of the suspected trolling event, the suspected trolling event comment, and all of the direct responses to the suspected trolling event. We added an extra constraint that the parent of the suspected trolling event should also be part of the direct responses, we hypothesize that if the suspected trolling event is indeed trolling, its parent should be the object of its trolling and would have a say about it. We recognize that this limited amount of information is not always sufficient to recover the original message conveyed by all of the participants in the snippet, and additional context would be beneficial. However, the trade off is that snippets like this allow us to make use of Amazon Mechanical Turk (AMT) to have the dataset annotated, because it is not a big burden for a \u201cturker\u201d to work on an individual snippet in exchange for a small pay, and expedites the annotation process by distributing it over dozens of people. Specifically, for each snippet, we requested three annotators to label the four aspects previously described. Before annotating, we set up a qualification test along with borderline examples to guide them in process and align them with our criteria. The qualification test turned out to be very selective since only 5% of all of the turkers that attempted it passed the exam. Our dataset consists of 1000 conversations with 5868 sentences and 71033 tokens. The distribution over the classes per trolling aspect is shown in the table TABREF24 in the column \u201cSize\u201d.\n\nQuestion: how was annotation done?\n\nExplanation: \n\nAccording to [Document 1], annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations. This information is further supported by [Document 2], [Document 3], and [Document 5].\n\nAnswer: Annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations\n\nExample 4:\n\n[Document 1]: Let INLINEFORM0 be a set of Chinese movie reviews with no categorical information. The ultimate task of movie review classification is to label them into different predefined categories as INLINEFORM1 . Starting from scratch, we need to collect such review set INLINEFORM2 from an online review website and then manually label them into generic categories INLINEFORM3 . Based on the collected dataset, we can apply natural language processing techniques to get raw text features and further learn the classifiers. In the following subsections, we will go through and elaborate all the subtasks shown in Figure FIGREF5 .\n\nQuestion: Is manual annotation performed?\n\nExplanation:"}
{"question_id": "a926d71e6e58066d279d9f7dc3210cd43f410164", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We carry out experiments on multiple language pairs including German-English, French-English, and Japanese-English. The experimental results show that the proposed multi-lingual caption model not only achieves better caption performance than independent mono-lingual models for data-scarce languages, but also can induce the two types of features, linguistic and visual features, for different languages in joint spaces. Our proposed method consistently outperforms previous state-of-the-art vision-based bilingual word induction approaches on different languages. The contributions of this paper are as follows:\n\nQuestion: Which languages are used in the multi-lingual caption model?\n\nExplanation:  The languages used in the multi-lingual caption model are German-English, French-English, and Japanese-English according to [Document 1].\n\nAnswer: German-English, French-English, and Japanese-English\n\nExample 2:\n\n[Document 1]: We extract data from the WMT'14 English-French (En-Fr) and English-German (En-De) datasets. To create a larger discrepancy between the tasks, so that there is a clear dataset size imbalance, the En-De data is artificially restricted to only 1 million parallel sentences, while the full En-Fr dataset, comprising almost 40 million parallel sentences, is used entirely. Words are split into subwords units with a joint vocabulary of 32K tokens. BLEU scores are computed on the tokenized output with multi-bleu.perl from Moses BIBREF10.\n\nQuestion: Are this techniques used in training multilingual models, on what languages?\n\nExplanation:  The WMT'14 English-French (En-Fr) and English-German (En-De) datasets are used to train multilingual models.\n\nAnswer: English to French and English to German\n\nExample 3:\n\n[Document 1]: We use the previous CNN architecture with bilingual embedding and the RF model with surface features (e.g., use of personal pronoun, presence of interjections, emoticon or specific punctuation) to verify which pair of the three languages: (a) has similar ironic pragmatic devices, and (b) uses similar text-based pattern in the narrative of the ironic tweets. As continuous word embedding spaces exhibit similar structures across (even distant) languages BIBREF35, we use a multilingual word representation which aims to learn a linear mapping from a source to a target embedding space. Many methods have been proposed to learn this mapping such as parallel data supervision and bilingual dictionaries BIBREF35 or unsupervised methods relying on monolingual corpora BIBREF36, BIBREF37, BIBREF38. For our experiments, we use Conneau et al 's approach as it showed superior results with respect to the literature BIBREF36. We perform several experiments by training on one language ($lang_1$) and testing on another one ($lang_2$) (henceforth $lang_1\\rightarrow lang_2$). We get 6 configurations, plus two others to evaluate how irony devices are expressed cross-culturally, i.e. in European vs. non European languages. In each experiment, we took 20% from the training to validate the model before the testing process. Table TABREF11 presents the results.\n\nQuestion: What multilingual word representations are used?\n\nExplanation: \n\nAccording to [Document 1], the multilingual word representation used is one which aims to learn a linear mapping from a source to a target embedding space.\n\nAnswer:  a multilingual word representation which aims to learn a linear mapping from a source to a target embedding space\n\nExample 4:\n\nQuestion: What languages were included in this multilingual population?\n\nExplanation:"}
{"question_id": "d4b84f48460517bc0a6d4e0c38f6853c58081166", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: As the table shows, terms extracted from Yahoo! Answers tend to be more related, in terms of the number of correlated terms, to attributes related to religion or ethnicity compared to terms from Twitter. However, for two particular attributes (i.e., Price and Buddhist), the number of correlated terms from Twitter is higher than the ones from Yahoo! Answers . These results collectively suggest that there is a wealth of terms, both in Yahoo! Answers and Twitter, which can be used to predict the population demographics.\n\nQuestion: On Twitter, do the demographic attributes and answers show more correlations than on Yahoo! Answers?\n\nExplanation:  The table in [Document 1] shows that, for two particular attributes, the number of correlated terms from Twitter is higher than the ones from Yahoo! Answers. However, this is not the case for all demographic attributes, and overall, there are more correlations between demographic attributes and answers on Yahoo! Answers than on Twitter.\n\nAnswer: No\n\nExample 2:\n\nQuestion: Do they specify which countries they collected twitter data from?\n\nExplanation:  There is no mention of any specific countries in the documents.\n\nAnswer: No\n\nExample 3:\n\n[Document 1]: We used the Twitter API to gather real-time tweets from September 2018 until February 2019, selecting the ones containing any of the 51 Hatebase terms that are more common in hate speech tweets, as studied in BIBREF9. We filtered out retweets, tweets containing less than three words and tweets containing porn related terms. From that selection, we kept the ones that included images and downloaded them. Twitter applies hate speech filters and other kinds of content control based on its policy, although the supervision is based on users' reports. Therefore, as we are gathering tweets from real-time posting, the content we get has not yet passed any filter.\n\nQuestion: How is data collected, manual collection or Twitter api?\n\nExplanation: \n\nAccording to [Document 1], the data was collected using the Twitter API.\n\nAnswer: Twitter API\n\nExample 4:\n\n[Document 1]: To overcome this limitation we combined our Twitter data with the socioeconomic maps of INSEE by assigning each geolocated Twitter user to a patch closest to their estimated home location (within 1 km). This way we obtained for all $110,369$ geolocated users their dynamical linguistic data, their egocentric social network as well as a set of SES indicators.\n\nQuestion: How do they combine the socioeconomic maps with Twitter data? \n\nExplanation:"}
{"question_id": "028d0d9b7a71133e51a14a32cd09dea1e2f39f05", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: BIBREF7 investigated the learning capabilities of simple RNNs to process and formalize a context-free grammar containing hierarchical (recursively embedded) dependencies: He observed that distinct parts of the networks were able to learn some complex representations to encode certain grammatical structures and dependencies of the context-free grammar. Later, BIBREF8 introduced an RNN with an external stack memory to learn simple context-free languages, such as $a^n b^m$ , $a^nb^ncb^ma^m$ , and $a^{n+m} b^n c^m$ . Similar studies BIBREF15 , BIBREF16 , BIBREF17 , BIBREF10 , BIBREF11 have explored the existence of stable counting mechanisms in simple RNNs, which would enable them to learn various context-free and context-sensitive languages, but none of the RNN architectures proposed in the early days were able to generalize the training set to longer (or more complex) test samples with substantially high accuracy.\n\n[Document 2]: BIBREF9 , on the other hand, proposed a variant of Long Short-Term Memory (LSTM) networks to learn two context-free languages, $a^n b^n$ , $a^n b^m B^m A^n$ , and one strictly context-sensitive language, $a^n b^n c^n$ . Given only a small fraction of samples in a formal language, with values of $n$ (and $m$ ) ranging from 1 to a certain training threshold $N$ , they trained an LSTM model until its full convergence on the training set and then tested it on a more generalized set. They showed that their LSTM model outperformed the previous approaches in capturing and generalizing the aforementioned formal languages. By analyzing the cell states and the activations of the gates in their LSTM model, they further demonstrated that the network learns how to count up and down at certain places in the sample sequences to encode information about the underlying structure of each of these formal languages.\n\nQuestion: How do they get the formal languages?\n\nExplanation:  [Document 1] and [Document 2] both mention that the formal languages used in the study are well-known formal languages that have been used in the past to evaluate the learning capabilities of RNNs.\n\nAnswer: These are well-known formal languages some of which was used in the literature to evaluate the learning capabilities of RNNs.\n\nExample 2:\n\n[Document 1]: We acquired the data in two rounds of annotation. In the first one, we were looking for original and uncommon sentence change suggestions. In the second one, we collected sentence alternations using ideas from the first round. The first and second rounds of annotation could be broadly called as collecting ideas and collecting data, respectively.\n\nQuestion: How do they introduce language variation?\n\nExplanation:  The first round is for collecting ideas and the second round is for collecting data.\n\nAnswer:  we were looking for original and uncommon sentence change suggestions\n\nExample 3:\n\n[Document 1]: In this paper, we propose an improved RNN-T model with language bias to alleviate the problem. The model is trained to predict language IDs as well as the subwords. To ensure the model can learn CS information, we add language IDs in the CS point of transcription, as illustrated in Fig. 1. In the figure, we use the arrangements of different geometric icons to represent the CS distribution. Compared with normal text, the tagged data can bias the RNN-T to predict language IDs in CS points. So our method can model the CS distribution directly, no additional LID model is needed. Then we constrain the input word embedding with its corresponding language ID, which is beneficial for model to learn the language identity information from transcription. In the inference process, the predicted language IDs are used to adjust the output posteriors. The experiment results on CS corpus show that our proposed method outperforms the RNN-T baseline (without language bias) significantly. Overall, our best model achieves 16.2% and 12.9% relative error reduction on two test sets, respectively. To our best knowledge, this is the first attempt of using the RNN-T model with language bias as an end-to-end CSSR strategy.\n\nQuestion: How do they obtain language identities?\n\nExplanation: \n\nAccording to [Document 1], the model is trained to predict language IDs as well as the subwords. In order to ensure that the model can learn CS information, the authors add language IDs in the CS point of transcription. This allows the model to learn the language identity information from transcription.\n\nAnswer: model is trained to predict language IDs as well as the subwords, we add language IDs in the CS point of transcriptio\n\nExample 4:\n\n[Document 1]: The basic form of negation in French includes two negative particles: ne (no) before the verb and another particle after the verb that conveys more accurate meaning: pas (not), jamais (never), personne (no one), rien (nothing), etc. Due to this double construction, the first part of the negation (ne) is optional in spoken French, but it is obligatory in standard writing. Sociolinguistic studies have previously observed the realization of ne in corpora of recorded everyday spoken interactions. Although all the studies do not converge, a general trend is that ne realization is more frequent in speakers with higher socioeconomic status than in speakers with lower status BIBREF30 , BIBREF31 . We built upon this research to set out to detect both negation variants in the tweets using regular expressions. We are namely interested in the rate of usage of the standard negation (featuring both negative particles) across users:\n\n[Document 2]: In written French, adjectives and nouns are marked as being plural by generally adding the letters s or x at the end of the word. Because these endings are mute (without counterpart in spoken French), their omission is the most frequent spelling error in adults BIBREF32 . Moreover, studies showed correlations between standard spelling and social status of the writers, in preteens, teens and adults BIBREF33 , BIBREF32 , BIBREF34 . We then set to estimate the use of standard plural across users:\n\nQuestion: How did they define standard language?\n\nExplanation:"}
{"question_id": "1d7b99646a1bc05beec633d7a3beb083ad1e8734", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: How long is the dataset used for training?\n\nExplanation:  There is no mention of the dataset's size in any of the documents.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: The following data sources were used to train the RNN-T and associated RNN-LMs in this study.\n\n[Document 2]: Source-domain baseline RNN-T: approximately 120M segmented utterances (190,000 hours of audio) from YouTube videos, with associated transcripts obtained from semi-supervised caption filtering BIBREF28.\n\n[Document 3]: Source-domain normalizing RNN-LM: transcripts from the same 120M utterance YouTube training set. This corresponds to about 3B tokens of the sub-word units used (see below, Section SECREF30).\n\n[Document 4]: Target-domain RNN-LM: 21M text-only utterance-level transcripts from anonymized, manually transcribed audio data, representative of data from a Voice Search service. This corresponds to about 275M sub-word tokens.\n\n[Document 5]: Target-domain RNN-T fine-tuning data: 10K, 100K, 1M and 21M utterance-level {audio, transcript} pairs taken from anonymized, transcribed Voice Search data. These fine-tuning sets roughly correspond to 10 hours, 100 hours, 1000 hours and 21,000 hours of audio, respectively.\n\nQuestion: How is the training data collected?\n\nExplanation: \n\nAccording to [Document 2] and [Document 3], the training data is collected from YouTube videos, with associated transcripts obtained from semi-supervised caption filtering. This is further supported by [Document 5], which states that the target-domain RNN-T fine-tuning data is collected from a Voice Search service.\n\nAnswer: from YouTube videos, with associated transcripts obtained from semi-supervised caption filtering, from a Voice Search service\n\nExample 3:\n\nQuestion: How big is the impact of training data size on the performance of the multilingual encoder?\n\nExplanation:  The documents provided do not contain any information regarding the impact of training data size on the performance of the multilingual encoder.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: For our machine translation experiments, the result was a modest 7% decrease in steps per second, but we were able to maintain the same model and batch sizes on P100 GPUs as Vaswani et al. vaswani2017.\n\nQuestion: How is the training time compared to the original position encoding? \n\nExplanation:"}
{"question_id": "179bc57b7b5231ea6ad3e93993a6935dda679fa2", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Email classifier using machine learning ::: Machine learning approach ::: Feature selection\n\n[Document 2]: Ngrams are a continuous sequence of n items from a given sample of text. From title, body and OCR text words are selected. Ngrams of 3 nearby words are extracted with Term Frequency-Inverse Document Frequency (TF-IDF) vectorizing, then features are filtered using chi squared the feature scoring method.\n\n[Document 3]: Email classifier using machine learning ::: Machine learning approach ::: Random forest\n\n[Document 4]: Random Forest is a bagging Algorithm, an ensemble learning method for classification that operates by constructing a multitude of decision trees at training time and outputting the class that has highest mean majority vote of the classesBIBREF14.\n\n[Document 5]: Email classifier using machine learning ::: Machine learning approach ::: XGBoost\n\n[Document 6]: XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. It is used commonly in the classification problems involving unstructured dataBIBREF5.\n\n[Document 7]: Email classifier using machine learning ::: Machine learning approach ::: Hierarchical Model\n\n[Document 8]: Since the number of target labels are high, achieving the higher accuracy is difficult, while keeping all the categories under same feature selection method. Some categories performs well with lower TF-IDF vectorizing range and higher n grams features even though they showed lower accuracy in the overall single model. Therefore, hierarchical machine learning models are built to classify 31 categories in the first classification model and remaining categories are named as low-accu and predicted as one category. In the next model, predicted low-accu categories are again classified into 47 categories. Comparatively this hierarchical model works well since various feature selection methods are used for various categoriesBIBREF5.\n\nQuestion: What are all machine learning approaches compared in this work?\n\nExplanation: \n\nAccording to [Document 1], the machine learning approaches that were compared in this work were feature selection, Random Forest, XGBoost, and Hierarchical Model. This information is further supported by [Document 2], [Document 3], [Document 4], [Document 5], [Document 6], [Document 7], and [Document 8].\n\nAnswer: Feature selection, Random forest, XGBoost, Hierarchical Model\n\nExample 2:\n\nQuestion: Do they compare DeepER against other approaches?\n\nExplanation: \n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task\n\nAnswer: Yes\n\nExample 3:\n\n[Document 1]: We conducted cross-validation experiments using various values of $t$ and $m$. Table TABREF26 shows the results for the best values of $t$ and $m$ obtained. The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively. To enable a fair comparison we used the same input features in all systems. These input features combine information from the question and the input sentence and are shown in Fig. FIGREF16. The features are based on BIBREF12, and are the same as in BIBREF1, plus the addition of the position of the input snippet. The best SVC and SVR parameters were determined by grid search.\n\n[Document 2]: The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: \u201cNNC top 5\u201d uses classification labels as described in Section SECREF3, and \u201cNNC SU4 F1\u201d uses the regression labels, that is, the ROUGE-SU4 F1 scores of each sentence. Of interest is the fact that \u201cNNC SU4 F1\u201d outperforms the neural regressor. We have not explored this further and we presume that the relatively good results are due to the fact that ROUGE values range between 0 and 1, which matches the full range of probability values that can be returned by the sigmoid activation of the classifier final layer.\n\nQuestion: What approaches without reinforcement learning have been tried?\n\nExplanation:  [Document 1] discusses classification and regression methods, while [Document 2] discusses neural methods.\n\nAnswer: classification, regression, neural methods\n\nExample 4:\n\n[Document 1]: FLOAT SELECTED: Table 1: Results (F1) on CoNLL 2012 test set. CoNLL is the average of MUC, B3, and CEAFe.\n\nQuestion: Do they compare against Reinforment-Learning approaches?\n\nExplanation:"}
{"question_id": "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Our experiments are run using the English$\\rightarrow $Italian/German portions of the MuST-C corpus BIBREF25, which is extracted from TED talks, using the same train/validation/test split as provided with the corpus (see Table TABREF18). As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De). While our main goal is to verify our hypotheses on a large data condition, thus the need to include proprietary data, for the sake of reproducibility in both languages we also provide results with systems only trained on TED Talks (small data condition). When training on large scale data we use Transformer with layer size of 1024, hidden size of 4096 on feed forward layers, 16 heads in the multi-head attention, and 6 layers in both encoder and decoder. When training only on TED talks, we set layer size of 512, hidden size of 2048 for the feed forward layers, multi-head attention with 8 heads and again 6 layers in both encoder and decoder.\n\nQuestion: What dataset do they use?\n\nExplanation:  [Document 1] states that the English$\\rightarrow $Italian/German portions of the MuST-C corpus are used. It also states that, as additional data, a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De) is used.\n\nAnswer: English$\\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)\n\nExample 2:\n\n[Document 1]: The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type. To make this diversity, after cleaning raw text input, we use multiple types of word tokenizers. Each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words BIBREF5, pre-trained embedding as fasttext (trained on Wiki Vietnamese language) BIBREF6 and sonvx (trained on Vietnamese newspaper) BIBREF7. Each sentence has a set of words corresponding to a set of word vectors, and that set of word vectors is a representation of a sentence. We also make a sentence embedding by using RoBERTa architecture BIBREF8. CBOW and RoBERTa models trained on text from some resources including VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD and text crawled from Facebook. After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13. With the multiply output results, we will use an ensemble method to combine them and output the final result. Ensemble method we use here is Stacking method will be introduced in the section SECREF16.\n\n[Document 2]: The dataset in this HSD task is really imbalance. Clean class dominates with 91.5%, offensive class takes 5% and the rest belongs to hate class with 3.5%. To make model being able to learn with this imbalance data, we inject class weight to the loss function with the corresponding ratio (clean, offensive, hate) is $(0.09, 0.95, 0.96)$. Formular DISPLAY_FORM17 is the loss function apply for all models in our system. $w_i$ is the class weight, $y_i$ is the ground truth and $\\hat{y}_i$ is the output of the model. If the class weight is not set, we find that model cannot adjust parameters. The model tends to output all clean classes.\n\nQuestion: What dataset do they use?\n\nExplanation: \n\nAccording to [Document 1], they used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings. This information is further supported by [Document 2], which states that they used the dataset provided in HSD task to train their model.\n\nAnswer: They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).\n\nExample 3:\n\n[Document 1]: SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language.\n\nQuestion: What dataset do they use?\n\nExplanation:  [Document 1] mentions that they use a parallel corpus for training where the source is English and the target is Python. This is further supported by the fact that the parallel corpus has 18805 aligned data points.\n\nAnswer: A parallel corpus where the source is an English expression of code and the target is Python code.\n\nExample 4:\n\n[Document 1]: For training our classification models, we use a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, provided by BIBREF23. These repositories are split into training, validation, and test splits containing 808, 265, and 264 commits, respectively. In order to minimize the occurrence of duplicate commits in two of these splits (such as in both training and test), commits from no repository belong to more than one split. However, 808 commits may not be sufficient to train deep learning models. Hence, in order to answer RQ4, we augment the training split with commits mined using regular expression matching on the commit messages from the same set of open-source Java projects. This almost doubles the number of commits in the training split to 1493. We then repeat our experiments for the first three research questions on the augmented dataset, and evaluate our trained models on the same validation and test splits.\n\nQuestion: What dataset do they use?\n\nExplanation:"}
{"question_id": "940a16e9db8be5b5f4e67d9c7622b3df99ac10a5", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: In our experiment, we use a conversational Mandarin-English code-switching speech corpus called SEAME Phase II (South East Asia Mandarin-English). The data are collected from spontaneously spoken interviews and conversations in Singapore and Malaysia by bilinguals BIBREF21 . As the data preprocessing, words are tokenized using Stanford NLP toolkit BIBREF22 and all hesitations and punctuations were removed except apostrophe. The split of the dataset is identical to BIBREF9 and it is showed in Table TABREF6 .\n\nQuestion: What languages are explored in this paper?\n\nExplanation: \n\nAccording to [Document 1], the SEAME Phase II corpus contains data from bilingual speakers of Mandarin and English.\n\nAnswer: Mandarin, English\n\nExample 2:\n\nQuestion: What languages are explored in this paper?\n\nExplanation:  This question cannot be answered based on the given documents.\n\nAnswer: Unanswerable\n\nExample 3:\n\n[Document 1]: We use the publicly available dataset KVRET BIBREF5 in our experiments. This dataset is created by the Wizard-of-Oz method BIBREF10 on Amazon Mechanical Turk platform. This dataset includes dialogues in 3 domains: calendar, weather, navigation (POI) which is suitable for our mix-domain dialogue experiments. There are 2,425 dialogues for training, 302 for validation and 302 for testing, as shown in the upper half of Table TABREF12.\n\nQuestion: Which domains did they explored?\n\nExplanation:  The answer can be found in the first sentence of [Document 1].\n\nAnswer: calendar, weather, navigation\n\nExample 4:\n\nQuestion: what domains are explored in this paper?\n\nExplanation:"}
{"question_id": "d922eaa5aa135c1ae211827c6a599b4d69214563", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Conversation Based Features\n\n[Document 2]: These features are devoted to exploit the peculiar characteristics of the dataset, which have a tree structure reflecting the conversation thread.\n\n[Document 3]: Text Similarity to Source Tweet: Jaccard Similarity of each tweet with its source tweet.\n\n[Document 4]: Text Similarity to Replied Tweet: the degree of similarity between the tweet with the previous tweet in the thread (the tweet is a reply to that tweet).\n\n[Document 5]: Tweet Depth: the depth value is obtained by counting the node from sources (roots) to each tweet in their hierarchy.\n\nQuestion: What conversation-based features are used?\n\nExplanation: \n\nAccording to [Document 2], conversation-based features are used to exploit the peculiar characteristics of the dataset, which have a tree structure reflecting the conversation thread. This information is further supported by [Document 3], [Document 4], and [Document 5].\n\nAnswer: Text Similarity to Source Tweet, Text Similarity to Replied Tweet, Tweet Depth\n\nExample 2:\n\n[Document 1]: CNN2-PV. This model (Figure FIGREF10 , left+center) featurizes discourse information into a vector of relation probabilities. In order to derive the discourse features, an entity grid is constructed by feeding the document through an NLP pipeline to identify salient entities. Two flavors of discourse features are created by populating the entity grid with either (i) grammatical relations (GR) or (ii) RST discourse relations (RST). The GR features are represented as grammatical relation transitions derived from the entity grid, e.g., INLINEFORM0 . The RST features are represented as RST discourse relations with their nuclearity, e.g., INLINEFORM1 . The probability vectors are then distributions over relation types. For GR, the vector is a distribution over all the entity role transitions, i.e., INLINEFORM2 (see Table TABREF2 ). For RST, the vector is a distribution over all the RST discourse relations, i.e., INLINEFORM3 Denoting a feature as such with INLINEFORM4 , we construct the pooling vector INLINEFORM5 for the char-bigrams, and concatenate INLINEFORM6 to INLINEFORM7 before feeding the resulting vector to the softmax layer.\n\nQuestion: How are discourse features incorporated into the model?\n\nExplanation:  [Document 1] states that they derive entity grid with grammatical relations and RST discourse relations. The entity grid is then populated with either grammatical relations or RST discourse relations. The probability vectors are then distributions over relation types. The pooling vector is then concatenated with the feature vector before feeding to the softmax layer.\n\nAnswer: They derive entity grid with grammatical relations and RST discourse relations and concatenate them with pooling vector for the char-bigrams before feeding to the resulting vector to the softmax layer.\n\nExample 3:\n\n[Document 1]: Logician is a supervised end-to-end neural learning algorithm which transforms natural language sentences into facts in the SAOKE form. Logician is trained under the attention-based sequence-to-sequence paradigm, with three mechanisms: restricted copy mechanism to ensure literally honestness, coverage mechanism to alleviate the under extraction and over extraction problem, and gated dependency attention mechanism to incorporate dependency information. Experimental results on four types of open information extraction tasks reveal the superiority of the Logician algorithm.\n\nQuestion: How is Logician different from traditional seq2seq models?\n\nExplanation: \n\nAccording to [Document 1], Logician uses a restricted copy mechanism to ensure literal honesty, a coverage mechanism to alleviate the under extraction and over extraction problem, and a gated dependency attention mechanism to incorporate dependency information. These mechanisms are not typically used in traditional seq2seq models, making Logician different from traditional seq2seq models.\n\nAnswer: restricted copy mechanism to ensure literally honestness, coverage mechanism to alleviate the under extraction and over extraction problem, and gated dependency attention mechanism to incorporate dependency information\n\nExample 4:\n\n[Document 1]: Sentiment and objective Information (SOI)- relativity of subjectivity and sentiment with emotion are well studied in the literature. To craft these features we use SentiwordNet BIBREF5 , we create sentiment and subjective score per word in each sentences. SentiwordNet is the result of the automatic annotation of all the synsets of WORDNET according to the notions of positivity, negativity, and neutrality. Each synset s in WORDNET is associated to three numerical scores Pos(s), Neg(s), and Obj(s) which indicate how positive, negative, and objective (i.e., neutral) the terms contained in the synset are. Different senses of the same term may thus have different opinion-related properties. These scores are presented per sentence and their lengths are equal to the length of each sentence. In case that the score is not available, we used a fixed score 0.001.\n\n[Document 2]: Emotion Lexicon feature (emo)- presence of emotion words is the first flag for a sentence to be emotional. We use NRC Emotion Lexicon BIBREF6 with 8 emotion tags (e.i. joy, trust, anticipation, surprise, anger, fear, sadness, disgust). We demonstrate the presence of emotion words as an 8 dimension feature, presenting all 8 emotion categories of the NRC lexicon. Each feature represent one emotion category, where 0.001 indicates of absent of the emotion and 1 indicates the presence of the emotion. The advantage of this feature is their portability in transferring emotion learning across genres.\n\nQuestion: Do they treat differerent turns of conversation differently when modeling features?\n\nExplanation:"}
{"question_id": "342ada55bd4d7408e1fcabf1810b92d84c1dbc41", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: By how much did the results improve?\n\nExplanation:  The question is unanswerable with the given documents. [Document 1] describes the experiments conducted, [Document 2] describes the Stanford NER algorithm, [Document 3] describes the spaCy 2.0 algorithm, [Document 4] describes the dataset used, and [Document 5] describes the recurrent model with a CRF top layer. None of the documents provide information on the results of the experiments.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: Adversarial training can often improve model robustness BIBREF25, BIBREF27. We also fine-tune the target models using adversarial training. At each training step, we train the model with a batch of original examples along with adversarial examples with balanced labels. The adversarial examples account for around 10% in a batch. During training, we generate adversarial examples with the current model as the target and update the model parameters with the hybrid batch iteratively. The beam size for generation is set to 1 to reduce the computation cost, since the generation success rate is minor in adversarial training. We evaluate the adversarially trained models, as shown in Table TABREF18.\n\n[Document 2]: After adversarial training, the performance of all the target models raises significantly, while that on the original examples remain comparable. Note that since the focus of this paper is on model robustness which can hardly be reflected in original data, we do not expect performance improvement on original data. The results demonstrate that adversarial training with our adversarial examples can significantly improve the robustness we focus on without remarkably hurting the performance on original data. Moreover, although the adversarial example generation is constrained by a BERT language model, BiMPM and DIIN which do not use the BERT language model can also significantly benefit from the adversarial examples, further demonstrating the effectiveness of our method.\n\nQuestion: How much in experiments is performance improved for models trained with generated adversarial examples?\n\nExplanation: \n\nAccording to [Document 1], the performance of all the target models raises significantly, while that on the original examples remain comparable. This information is further supported by [Document 2].\n\nAnswer: Answer with content missing: (Table 1) The performance of all the target models raises significantly, while that on the original\nexamples remain comparable (e.g. the overall accuracy of BERT on modified examples raises from 24.1% to 66.0% on Quora)\n\nExample 3:\n\n[Document 1]: As shown in Table TABREF24, MUSE outperforms all previously models on En-De and En-Fr translation, including both state-of-the-art models of stand alone self-attention BIBREF0, BIBREF13, and convolutional models BIBREF11, BIBREF15, BIBREF10. This result shows that either self-attention or convolution alone is not enough for sequence to sequence learning. The proposed parallel multi-scale attention improves over them both on En-De and En-Fr.\n\n[Document 2]: Compared to Evolved Transformer BIBREF19 which is constructed by NAS and also mixes convolutions of different kernel size, MUSE achieves 2.2 BLEU gains in En-Fr translation.\n\nQuestion: How big is improvement in performance over Transformers?\n\nExplanation:  [Document 1] and [Document 2] both mention that MUSE outperforms Transformers by 2.2 BLEU gains.\n\nAnswer: 2.2 BLEU gains\n\nExample 4:\n\n[Document 1]: FLOAT SELECTED: Table 1: Summarization performance.\n\nQuestion: by how much did the performance improve?\n\nExplanation:"}
{"question_id": "84765903b8c7234ca2919d0a40e3c6a5bcedf45d", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We make two main contributions. First, we introduce new strategies for interpreting the behavior of deep models in their intermediate layers, specifically, by examining the saliency of the attention and the gating signals. Second, we provide an extensive analysis of the state-of-the-art model for the NLI task and show that our methods reveal interesting insights not available from traditional methods of inspecting attention and word saliency.\n\nQuestion: Did they use the state-of-the-art model to analyze the attention?\n\nExplanation: \n\nAccording to [Document 1], the authors provide an extensive analysis of the state-of-the-art model. This means that they did use the state-of-the-art model to analyze the attention.\n\nAnswer: we provide an extensive analysis of the state-of-the-art model\n\nExample 2:\n\n[Document 1]: Our application is the escalation of Internet chats. To maintain quality of service, users are transferred to human representatives when their conversations with an intelligent virtual assistant (IVA) fail to progress. These transfers are known as escalations. We apply Han to such conversations in a sequential manner by feeding each user turn to Han as they occur, to determine if the conversation should escalate. If so, the user will be transferred to a live chat representative to continue the conversation. To help the human representative quickly determine the cause of the escalation, we generate a visualization of the user's turns using the attention weights to highlight the turns influential in the escalation decision. This helps the representative quickly scan the conversation history and determine the best course of action based on problematic turns.\n\nQuestion: Do they explain model predictions solely on attention weights?\n\nExplanation:  The passage mentions that the application of Han is done in a sequential manner, and that the visualization of the user's turns uses the attention weights to highlight the turns influential in the escalation decision. This suggests that the model predictions are based solely on the attention weights.\n\nAnswer: Yes\n\nExample 3:\n\n[Document 1]: The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254.\n\nQuestion: How many attention layers are there in their model?\n\nExplanation:  The model has one attention layer, as stated in [Document 1].\n\nAnswer: one\n\nExample 4:\n\n[Document 1]: After a small grid search we decided to inherit most of the hyperparameters of the model from the best results achieved in BIBREF3 where -to-Mizar translation is learned. We used relatively small LSTM cells consisting of 2 layers with 128 units. The \u201cscaled Luong\u201d version of the attention mechanism was used, as well as dropout with rate equal $0.2$. The number of training steps was 10000. (This setting was used for all our experiments described below.)\n\nQuestion: Do any of the models use attention?\n\nExplanation:"}
{"question_id": "df01e98095ba8765d9ab0d40c9e8ef34b64d3700", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We tested the system on two datasets, different in size and complexity of the addressed language.\n\n[Document 2]: Experimental Evaluation ::: Datasets ::: NLU-Benchmark dataset\n\n[Document 3]: The first (publicly available) dataset, NLU-Benchmark (NLU-BM), contains $25,716$ utterances annotated with targeted Scenario, Action, and involved Entities. For example, \u201cschedule a call with Lisa on Monday morning\u201d is labelled to contain a calendar scenario, where the set_event action is instantiated through the entities [event_name: a call with Lisa] and [date: Monday morning]. The Intent is then obtained by concatenating scenario and action labels (e.g., calendar_set_event). This dataset consists of multiple home assistant task domains (e.g., scheduling, playing music), chit-chat, and commands to a robot BIBREF7.\n\n[Document 4]: Experimental Evaluation ::: Datasets ::: ROMULUS dataset\n\n[Document 5]: The second dataset, ROMULUS, is composed of $1,431$ sentences, for each of which dialogue acts, semantic frames, and corresponding frame elements are provided. This dataset is being developed for modelling user utterances to open-domain conversational systems for robotic platforms that are expected to handle different interaction situations/patterns \u2013 e.g., chit-chat, command interpretation. The corpus is composed of different subsections, addressing heterogeneous linguistic phenomena, ranging from imperative instructions (e.g., \u201center the bedroom slowly, turn left and turn the lights off \u201d) to complex requests for information (e.g., \u201cgood morning I want to buy a new mobile phone is there any shop nearby?\u201d) or open-domain chit-chat (e.g., \u201cnope thanks let's talk about cinema\u201d). A considerable number of utterances in the dataset is collected through Human-Human Interaction studies in robotic domain ($\\approx $$70\\%$), though a small portion has been synthetically generated for balancing the frame distribution.\n\nQuestion: Which publicly available NLU dataset is used?\n\nExplanation: \n\nAccording to [Document 1], the system was tested on two datasets, one of which is publicly available. This is further supported by [Document 2] and [Document 4]. [Document 3] and [Document 5] provide information on the NLU-Benchmark dataset and the ROMULUS dataset, respectively.\n\nAnswer: ROMULUS dataset, NLU-Benchmark dataset\n\nExample 2:\n\n[Document 1]: To test the difficulty of our dataset, we checked the majority class label and the accuracies of five state-of-the-art NLI models adopting different approaches: BiMPM (Bilateral Multi-Perspective Matching Model; BIBREF31 , BIBREF31 ), ESIM (Enhanced Sequential Inference Model; BIBREF32 , BIBREF32 ), Decomposable Attention Model BIBREF33 , KIM (Knowledge-based Inference Model; BIBREF34 , BIBREF34 ), and BERT (Bidirectional Encoder Representations from Transformers model; BIBREF35 , BIBREF35 ). Regarding BERT, we checked the performance of a model pretrained on Wikipedia and BookCorpus for language modeling and trained with SNLI and MultiNLI. For other models, we checked the performance trained with SNLI. In agreement with our dataset, we regarded the prediction label contradiction as non-entailment.\n\nQuestion: What NLI models do they analyze?\n\nExplanation:  The document states that the BiMPM, ESIM, Decomposable Attention Model, KIM, and BERT models were analyzed.\n\nAnswer: BiMPM, ESIM, Decomposable Attention Model, KIM, BERT\n\nExample 3:\n\n[Document 1]: The recently introduced How2 dataset BIBREF2 has stimulated research around multimodal language understanding through the availability of 300h instructional videos, English subtitles and their Portuguese translations. For example, BIBREF3 successfully demonstrates that semantically rich action-based visual features are helpful in the context of machine translation (MT), especially in the presence of input noise that manifests itself as missing source words. Therefore, we hypothesize that a speech-to-text translation (STT) system may also benefit from the visual context, especially in the traditional cascaded framework BIBREF4, BIBREF5 where noisy automatic transcripts are obtained from an automatic speech recognition system (ASR) and further translated into the target language using a machine translation (MT) component. The dataset enables the design of such multimodal STT systems, since we have access to a bilingual corpora as well as the corresponding audio-visual stream. Hence, in this paper, we propose a cascaded multimodal STT with two components: (i) an English ASR system trained on the How2 dataset and (ii) a transformer-based BIBREF0 visually grounded MMT system.\n\nQuestion: What dataset was used in this work?\n\nExplanation:  [Document 1] mentions that the recently introduced How2 dataset was used, which is further supported by [Document 2].\n\nAnswer: How2\n\nExample 4:\n\n[Document 1]: The Stanford Natural Language Inference BIBREF7 dataset contains 570k human annotated hypothesis/premise pairs. This is the most widely used entailment dataset for natural language inference.\n\nQuestion: What NLI dataset was used?\n\nExplanation:"}
{"question_id": "f77d7cddef3e021d70e16b9e16cecfd4b8ee80d3", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Character-level language modeling (or character prediction) consists in predicting the next character while reading a document one character at a time. It is a common benchmark for rnn because of the heightened need for shared parametrization when compared to word-level models. We test mgru on two well-known datasets, the Penn Treebank and Text8.\n\nQuestion: Which dataset do they train their models on?\n\nExplanation:  [Document 1] mentions that the models are trained on the Penn Treebank and Text8 datasets.\n\nAnswer: Penn Treebank, Text8\n\nExample 2:\n\n[Document 1]: We evaluate performance of our models on the SQUAD BIBREF16 dataset (denoted $\\mathcal {S}$ ). We use the same split as that of BIBREF4 , where a random subset of 70,484 instances from $\\mathcal {S}\\ $ are used for training ( ${\\mathcal {S}}^{tr}$ ), 10,570 instances for validation ( ${\\mathcal {S}}^{val}$ ), and 11,877 instances for testing ( ${\\mathcal {S}}^{te}$ ).\n\nQuestion: Which datasets are used to train this model?\n\nExplanation: \n\nAccording to [Document 1], the SQUAD dataset is used to train the model.\n\nAnswer: SQUAD\n\nExample 3:\n\n[Document 1]: Clean-labeled Datasets. We use three clean labeled datasets. The first one is the movie sentence polarity dataset from BIBREF19. The other two datasets are laptop and restaurant datasets collected from SemEval-2016 . The former consists of laptop review sentences and the latter consists of restaurant review sentences. The original datasets (i.e., Laptop and Restaurant) were annotated with aspect polarity in each sentence. We used all sentences with only one polarity (positive or negative) for their aspects. That is, we only used sentences with aspects having the same sentiment label in each sentence. Thus, the sentiment of each aspect gives the ground-truth as the sentiments of all aspects are the same.\n\n[Document 2]: Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source. We extracted sentences from each review and assigned review's label to its sentences. Like previous work, we treat 4 or 5 stars as positive and 1 or 2 stars as negative. The data is noisy because a positive (negative) review can contain negative (positive) sentences, and there are also neutral sentences. This gives us three noisy-labeled training datasets. We still use the same test sets as those for the clean-labeled datasets. Summary statistics of all the datasets are shown in Table TABREF9.\n\nQuestion: What is the dataset used to train the model?\n\nExplanation: \n\nAccording to [Document 1], the movie sentence polarity dataset from BIBREF19 is used to train the model. This information is further supported by [Document 2], which states that the laptop and restaurant datasets collected from SemEval-2016 are used to train the model.\n\nAnswer:  movie sentence polarity dataset from BIBREF19, laptop and restaurant datasets collected from SemEval-201, we collected 2,000 reviews for each domain from the same review source\n\nExample 4:\n\n[Document 1]: To test out these approaches, we conduct two sets of NMT experiments: high resource (English INLINEFORM0 German) and low resource (Thai INLINEFORM1 English).\n\n[Document 2]: The English-German data comes from WMT 2014. The training set has 4m sentences and we take newstest2012/newstest2013 as the dev set and newstest2014 as the test set. We keep the top 50k most frequent words, and replace the rest with UNK. The teacher model is a INLINEFORM0 LSTM (as in Luong2015) and we train two student models: INLINEFORM1 and INLINEFORM2 . The Thai-English data comes from IWSLT 2015. There are 90k sentences in the training set and we take 2010/2011/2012 data as the dev set and 2012/2013 as the test set, with a vocabulary size is 25k. Size of the teacher model is INLINEFORM3 (which performed better than INLINEFORM4 , INLINEFORM5 models), and the student model is INLINEFORM6 . Other training details mirror Luong2015.\n\nQuestion: Which dataset do they train on?\n\nExplanation:"}
{"question_id": "7ba6330d105f49c7f71dba148bb73245a8ef2966", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: As a convention, the metric of joint goal accuracy is used to compare our model to previous work. The joint goal accuracy only regards the model making a successful belief state prediction if all of the slots and values predicted are exactly matched with the labels provided. This metric gives a strict measurement that tells how often the DST module will not propagate errors to the downstream modules in a dialogue system. In this work, the model with the highest joint accuracy on the validation set is evaluated on the test set for the test joint accuracy measurement.\n\nQuestion: What are the performance metrics used?\n\nExplanation: \n\nAccording to [Document 1], the joint goal accuracy is used to compare the model to previous work. This metric gives a strict measurement of how often the DST module will not propagate errors to the downstream modules in a dialogue system.\n\nAnswer: joint goal accuracy\n\nExample 2:\n\n[Document 1]: We use a standard set of supervised as well as unsupervised benchmark tasks from the literature to evaluate our trained models, following BIBREF16 . The breadth of tasks allows to fairly measure generalization to a wide area of different domains, testing the general-purpose quality (universality) of all competing sentence embeddings. For downstream supervised evaluations, sentence embeddings are combined with logistic regression to predict target labels. In the unsupervised evaluation for sentence similarity, correlation of the cosine similarity between two embeddings is compared to human annotators.\n\n[Document 2]: Downstream Supervised Evaluation. Sentence embeddings are evaluated for various supervised classification tasks as follows. We evaluate paraphrase identification (MSRP) BIBREF25 , classification of movie review sentiment (MR) BIBREF26 , product reviews (CR) BIBREF27 , subjectivity classification (SUBJ) BIBREF28 , opinion polarity (MPQA) BIBREF29 and question type classification (TREC) BIBREF30 . To classify, we use the code provided by BIBREF22 in the same manner as in BIBREF16 . For the MSRP dataset, containing pairs of sentences INLINEFORM0 with associated paraphrase label, we generate feature vectors by concatenating their Sent2Vec representations INLINEFORM1 with the component-wise product INLINEFORM2 . The predefined training split is used to tune the L2 penalty parameter using cross-validation and the accuracy and F1 scores are computed on the test set. For the remaining 5 datasets, Sent2Vec embeddings are inferred from input sentences and directly fed to a logistic regression classifier. Accuracy scores are obtained using 10-fold cross-validation for the MR, CR, SUBJ and MPQA datasets. For those datasets nested cross-validation is used to tune the L2 penalty. For the TREC dataset, as for the MRSP dataset, the L2 penalty is tuned on the predefined train split using 10-fold cross-validation, and the accuracy is computed on the test set.\n\n[Document 3]: Unsupervised Similarity Evaluation. We perform unsupervised evaluation of the learnt sentence embeddings using the sentence cosine similarity, on the STS 2014 BIBREF31 and SICK 2014 BIBREF32 datasets. These similarity scores are compared to the gold-standard human judgements using Pearson's INLINEFORM0 BIBREF33 and Spearman's INLINEFORM1 BIBREF34 correlation scores. The SICK dataset consists of about 10,000 sentence pairs along with relatedness scores of the pairs. The STS 2014 dataset contains 3,770 pairs, divided into six different categories on the basis of the origin of sentences/phrases, namely Twitter, headlines, news, forum, WordNet and images.\n\nQuestion: What metric is used to measure performance?\n\nExplanation: \n\nAccording to [Document 1], the performance of sentence embeddings is measured using a standard set of supervised and unsupervised benchmark tasks. This information is further supported by [Document 2] and [Document 3].\n\nIn [Document 2], it is stated that the accuracy and F1 score are used to measure the performance of the sentence embeddings on the MSRP dataset, while for the remaining 5 datasets, the Sent2Vec embeddings are inferred from input sentences and directly fed to a logistic regression classifier, and accuracy scores are obtained using 10-fold cross-validation.\n\nIn [Document 3], it is stated that for the STS 2014 and SICK 2014 datasets, the sentence cosine similarity is used to measure the performance of the sentence embeddings, and the similarity scores are compared to the gold-standard human judgements using Pearson's and Spearman's correlation scores.\n\nAnswer: Accuracy and F1 score for supervised tasks, Pearson's and Spearman's correlation for unsupervised tasks\n\nExample 3:\n\n[Document 1]: BPRA: Belief Per-Response Accuracy (BPRA) tests the ability to generate the correct user intents during the dialogue. This metric is used to evaluate the accuracy of dialogue belief tracker BIBREF1.\n\n[Document 2]: APRA: Action Per-Response Accuracy (APRA) evaluates the per-turn accuracy of the dialogue actions generated by dialogue policy maker. For baselines, APRA evaluates the classification accuracy of the dialogue policy maker. But our model actually generates each individual token of actions, and we consider a prediction to be correct only if every token of the model output matches the corresponding token in the ground truth.\n\n[Document 3]: BLEU BIBREF19: The metric evaluates the quality of the final response generated by natural language generator. The metric is usually used to measure the performance of the task-oriented dialogue system.\n\nQuestion: What metrics are used to measure performance of models?\n\nExplanation: \n\nAccording to [Document 1], BPRA is used to measure the performance of dialogue belief tracker. According to [Document 2], APRA is used to measure the per-turn accuracy of the dialogue policy maker. According to [Document 3], BLEU is used to measure the quality of the final response generated by natural language generator.\n\nAnswer: BPRA, APRA, BLEU\n\nExample 4:\n\n[Document 1]: BIBREF6 published SumRepo, a repository of summaries for the DUC2004 dataset generated by several baseline and state-of-the-art methods . We evaluate summaries generated by a selection of these methods on the same data that we use for testing. We calculate Rouge scores with the Rouge toolkit BIBREF9 . In order to compare our results to BIBREF6 we use the same Rouge settings as they do and report results for Rouge-1, Rouge-2 and Rouge-4 recall. The baselines include a basic centroid-based model without an anti-redundancy filter and feature reduction.\n\nQuestion: what are the performance metrics?\n\nExplanation:"}
{"question_id": "1c2d4dc1e842b962c6407d6436f3dc73dd44ce55", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: As a basis for our experiments, we reimplemented 12 existing AV approaches, which have shown their potentials in the previous PAN-AV competitions BIBREF11 , BIBREF12 as well as in a number of AV studies. The methods are listed in Table TABREF33 together with their classifications regarding the AV characteristics, which we proposed in Section SECREF3 .\n\nQuestion: What are the 12 AV approaches which are examined?\n\nExplanation:  The 12 AV approaches which are examined are MOCC, OCCAV, COAV, AVeer, GLAD, DistAV, Unmasking, Caravel, GenIM, ImpGI, SPATIUM and NNCD. This information is found in [Document 1], Table TABREF33.\n\nAnswer: MOCC, OCCAV, COAV, AVeer, GLAD, DistAV, Unmasking, Caravel, GenIM, ImpGI, SPATIUM and NNCD\n\nExample 2:\n\n[Document 1]: However, recent advances in Deep Learning techniques have shown that the NER task can benefit from the use of neural architectures, such as biLSTM-networks BIBREF3 , BIBREF4 . We use the implementation proposed in BIBREF24 for conducting three different experiments. In the first, we train the model using only the word embeddings as feature. In the second, together with the word embeddings we use the POS and chunk tag. In the third, all the features previously defined are included, in addition to the word embeddings. For every experiment, we use both the pre-trained embeddings and the ones that we created with our Twitter corpora. In section 4, results obtained from the several experiments are reported.\n\nQuestion: Which machine learning algorithms did the explore?\n\nExplanation:  [Document 1] states that the authors used biLSTM-networks for their experiments.\n\nAnswer: biLSTM-networks\n\nExample 3:\n\n[Document 1]: We conducted cross-validation experiments using various values of $t$ and $m$. Table TABREF26 shows the results for the best values of $t$ and $m$ obtained. The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively. To enable a fair comparison we used the same input features in all systems. These input features combine information from the question and the input sentence and are shown in Fig. FIGREF16. The features are based on BIBREF12, and are the same as in BIBREF1, plus the addition of the position of the input snippet. The best SVC and SVR parameters were determined by grid search.\n\n[Document 2]: The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: \u201cNNC top 5\u201d uses classification labels as described in Section SECREF3, and \u201cNNC SU4 F1\u201d uses the regression labels, that is, the ROUGE-SU4 F1 scores of each sentence. Of interest is the fact that \u201cNNC SU4 F1\u201d outperforms the neural regressor. We have not explored this further and we presume that the relatively good results are due to the fact that ROUGE values range between 0 and 1, which matches the full range of probability values that can be returned by the sigmoid activation of the classifier final layer.\n\nQuestion: What approaches without reinforcement learning have been tried?\n\nExplanation:  [Document 1] discusses classification and regression methods, while [Document 2] discusses neural methods.\n\nAnswer: classification, regression, neural methods\n\nExample 4:\n\n[Document 1]: Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{I}_{Base}$@!END@\n\n[Document 2]: This is a simple but effective model for multi-domain learning, in which all domains are aggregated together and will be further used for training a set of shared parameters. Notably, domains in this model are not explicitly informed of their differences.\n\n[Document 3]: We achieve this by pre-training our basic model $Model^{I}_{Base}$ with BERT BIBREF28, which is one of the most successful learning frameworks. Then we investigate if BERT can provide domain information and bring the model good domain adaptability. To avoid introducing new structures, we use the feature-based BERT with its parameters fixed.\n\n[Document 4]: Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{III}_{Tag}$@!END@\n\n[Document 5]: The domain type can also be introduced directly as a feature vector, which can augment learned representations with domain-aware ability.\n\n[Document 6]: Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{IV}_{Meta}$@!END@\n\n[Document 7]: In order to overcome the above limitations, we try to bridge the communication gap between different domains when updating shared parameters via meta-learning BIBREF33, BIBREF34, BIBREF35.\n\nQuestion: what four learning strategies are investigated?\n\nExplanation:"}
{"question_id": "4a91432abe3f54fcbdd00bb85dc0df95b16edf42", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We focus primarily on the smaller data set of 645 hand-labeled articles provided to task participants, both for training and for validation. We take the first 80% of this data set for our training set and the last 20% for the validation set. Since the test set is also hand-labeled we found that the 645 articles are much more representative of the final test set than the articles labeled by publisher. The model's performance on articles labeled by publisher was not much above chance level.\n\n[Document 2]: Our first experiment was checking the importance of pre-training. We pre-trained BERT-base on the 600,000 articles without labels by using the same Cloze task BIBREF5 that BERT had originally used for pre-training. We then trained the model on sequence lengths of 100, 250 and 500. The accuracy for each sequence length after 100 epochs is shown in TABREF7 and is labeled as UP (unsupervised pre-training). The other column shows how well BERT-base trained without pre-training. We found improvements for lower sequence lengths, but not at 500 word pieces. Since the longer chunk should have been more informative, and since our hand-labeled training set only contained 516 articles, this likely indicates that BERT experiences training difficulty when dealing with long sequences on such a small dataset. As the cost to do pre-training was only a one time cost all of our remaining experiments use a pre-trained model.\n\nQuestion: How long is the dataset?\n\nExplanation:  [Document 1] states that the dataset contains 645 articles. [Document 2] states that the dataset contains 600,000 articles.\n\nAnswer: 645, 600000\n\nExample 2:\n\n[Document 1]: Data set. For the cross-domain polarity classification experiments, we use the second version of Multi-Domain Sentiment Dataset BIBREF0 . The data set contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). Reviews contain star ratings (from 1 to 5) which are converted into binary labels as follows: reviews rated with more than 3 stars are labeled as positive, and those with less than 3 stars as negative. In each domain, there are 1000 positive and 1000 negative reviews.\n\nQuestion: How long is the dataset?\n\nExplanation:  The dataset contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). In each domain, there are 1000 positive and 1000 negative reviews. This means that there are a total of 8000 reviews in the dataset.\n\nAnswer: 8000\n\nExample 3:\n\n[Document 1]: The corpus includes 2000 sentences. Theses sentence are random renderings of 200 sentences, which we have taken from Sorani Kurdish books of the grades one to three of the primary school in the Kurdistan Region of Iraq. The reason that we have taken only 200 sentences is to have a smaller dictionary and also to increase the repetition of each word in the narrated speech. We transformed the corpus sentences, which are in Persian-Arabic script, into the format which complies with the suggested phones for the related Sorani letters (see Section SECREF6).\n\nQuestion: How long is the dataset?\n\nExplanation:  The dataset consists of 2000 sentences, as stated in [Document 1].\n\nAnswer: 2000\n\nExample 4:\n\n[Document 1]: We begin with a hate speech lexicon containing words and phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%. We use the majority decision for each tweet to assign a label. Some tweets were not assigned labels as there was no majority class. This results in a sample of 24,802 labeled tweets.\n\nQuestion: How long is their dataset?\n\nExplanation:"}
{"question_id": "0a70af6ba334dfd3574991b1dd06f54fc6a700f2", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: How do the authors define fake news?\n\nExplanation:  The documents do not provide a definition for fake news.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: We plotted the distribution of harassment incidents in each categorization dimension (Figure FIGREF19). It displays statistics that provide important evidence as to the scale of harassment and that can serve as the basis for more effective interventions to be developed by authorities ranging from advocacy organizations to policy makers. It provides evidence to support some commonly assumed factors about harassment: First, we demonstrate that harassment occurred more frequently during the night time than the day time. Second, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives.\n\n[Document 2]: Furthermore, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) (Figure FIGREF20). The significance of the correlation is tested by chi-square independence with p value less than 0.05. Identifying these patterns will enable interventions to be differentiated for and targeted at specific populations. For instance, the young harassers often engage in harassment activities as groups. This points to the influence of peer pressure and masculine behavioral norms for men and boys on these activities. We also found that the majority of young perpetrators engaged in harassment behaviors on the streets. These findings suggest that interventions with young men and boys, who are readily influenced by peers, might be most effective when education is done peer-to-peer. It also points to the locations where such efforts could be made, including both in schools and on the streets. In contrast, we found that adult perpetrators of sexual harassment are more likely to act alone. Most of the adult harassers engaged in harassment on public transportation. These differences in adult harassment activities and locations, mean that interventions should be responsive to these factors. For example, increasing the security measures on transit at key times and locations.\n\n[Document 3]: In addition, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location (Figure FIGREF21). For example, young harassers are more likely to engage in behaviors of verbal harassment, rather than physical harassment as compared to adults. It was a single perpetrator that engaged in touching or groping more often, rather than groups of perpetrators. In contrast, commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers. The nature and location of the harassment are particularly significant in developing strategies for those who are harassed or who witness the harassment to respond and manage the everyday threat of harassment. For example, some strategies will work best on public transport, a particular closed, shared space setting, while other strategies might be more effective on the open space of the street.\n\nQuestion: What patterns were discovered from the stories?\n\nExplanation:  [Document 1], [Document 2], and [Document 3] all present different patterns that were discovered from the stories. [Document 1] presents the evidence that harassment occurred more frequently during the night time and that conductors and drivers are top the list of identified types of harassers. [Document 2] presents the evidence of the strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s). [Document 3] presents the evidence that the majority of young perpetrators engaged in harassment behaviors on the streets, that adult perpetrators of sexual harassment are more likely to act alone, and that commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers.\n\nAnswer: we demonstrate that harassment occurred more frequently during the night time than the day time, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) , We also found that the majority of young perpetrators engaged in harassment behaviors on the streets, we found that adult perpetrators of sexual harassment are more likely to act alone, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location , commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers.\n\nExample 3:\n\n[Document 1]: We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. Following the previous work BIBREF0 , we tokenize all text with the popular python package Jieba, and filter out short articles with less than 30 words in content and those with less than 20 comments. The dataset is split into training/validation/test sets, and they contain 191,502/5,000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words.\n\nQuestion: What news comment dataset was used?\n\nExplanation:  The Chinese dataset BIBREF0 was used, as stated in [Document 1].\n\nAnswer: Chinese dataset BIBREF0\n\nExample 4:\n\n[Document 1]: We addressed the challenge of identifying nuances between fake news and satire. Inspired by the humor and social message aspects of satire articles, we tested two classification approaches based on a state-of-the-art contextual language model, and linguistic features of textual coherence. Evaluation of our methods pointed to the existence of semantic and linguistic differences between fake news and satire. In particular, both methods achieved a significantly better performance than the baseline language-based method. Lastly, we studied the feature importance of our linguistic-based method to help shed light on the nuances between fake news and satire. For instance, we observed that satire articles are more sophisticated, or less easy to read, than fake news articles.\n\nQuestion: What nuances between fake news and satire were discovered?\n\nExplanation:"}
{"question_id": "8d1b6c88f06ee195d75af32ede85dbd6477c8497", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We begin by filtering the corpus to keep only those tweets where the user's self-declared language and the tweet's detected language correspond; that language becomes the tweet's correct language label. This operation cuts out roughly half the tweets, and leaves us with a corpus of about 900 million tweets in 54 different languages. Table TABREF6 shows the distribution of languages in that corpus. Unsurprisingly, it is a very imbalanced distribution of languages, with English and Japanese together accounting for 60% of all tweets. This is consistent with other studies and statistics of language use on Twitter, going as far back as 2013. It does however make it very difficult to use this corpus to train a LID system for other languages, especially for one of the dozens of seldom-used languages. This was our motivation for creating a balanced Twitter dataset.\n\nQuestion: What languages are represented in the dataset?\n\nExplanation:  [Document 1] provides a list of languages represented in the dataset.\n\nAnswer: EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO\n\nExample 2:\n\n[Document 1]: For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter. It contains tweets collected based on keyword search that were posted between July 17, 2014 (the day of the plane crash) and December 9, 2016.\n\n[Document 2]: BIBREF4 provide annotations for a subset of the English tweets contained in the dataset. A tweet is annotated with one of three classes that indicate the framing of the tweet with respect to responsibility for the plane crash. A tweet can either be pro-Russian (Ukrainian authorities, NATO or EU countries are explicitly or implicitly held responsible, or the tweet states that Russia is not responsible), pro-Ukrainian (the Russian Federation or Russian separatists in Ukraine are explicitly or implicitly held responsible, or the tweet states that Ukraine is not responsible) or neutral (neither Ukraine nor Russia or any others are blamed). Example tweets for each category can be found in Table TABREF9. These examples illustrate that the framing annotations do not reflect general polarity, but polarity with respect to responsibility to the crash. For example, even though the last example in the table is in general pro-Ukrainian, as it displays the separatists in a bad light, the tweet does not focus on responsibility for the crash. Hence the it is labeled as neutral. Table TABREF8 shows the label distribution of the annotated portion of the data as well as the total amount of original tweets, and original tweets plus their retweets/duplicates in the network. A retweet is a repost of another user's original tweet, indicated by a specific syntax (RT @username: ). We consider as duplicate a tweet with text that is identical to an original tweet after preprocessing (see Section SECREF18). For our classification experiments, we exclusively consider original tweets, but model predictions can then be propagated to retweets and duplicates.\n\nQuestion: What languages are included in the dataset?\n\nExplanation:  The dataset only contains English tweets, as stated in [Document 2].\n\nAnswer: English\n\nExample 3:\n\n[Document 1]: The data stems from a joint ADAPT-Microsoft project. An overview of the provided dataset is given in Table TABREF16 . Notice that the available amount of data differs per language.\n\nQuestion: is the dataset balanced across the four languages?\n\nExplanation:  [Document 1] provides a table that shows the number of data points per language. The dataset is not balanced across the four languages.\n\nAnswer: No\n\nExample 4:\n\nQuestion: What countries and languages are represented in the datasets?\n\nExplanation:"}
{"question_id": "1f2952cd1dc0c891232fa678b6c219f6b4d31958", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: Which language-pair had the better performance?\n\nExplanation: \n\n[Document 1]: We evaluate the system on two language pairs: English-French and English-German.\n\n[Document 2]: The system was trained on a parallel corpus of 3.2 million sentence pairs.\n\n[Document 3]: We use the BLEU score BLEU1 , BLEU2 as our primary evaluation metric.\n\n[Document 4]:\n\nFor the English-French translation task, the system achieves a BLEU score of 34.8.\n\nFor the English-German translation task, the system achieves a BLEU score of 27.1.\n\nFrom [Document 4], we can see that the system had a better performance on the English-French translation task than on the English-German translation task.\n\nAnswer: French-English\n\nExample 2:\n\nQuestion: What is the size of the open vocabulary?\n\nExplanation:  The question cannot be answered with the given documents. [Document 1] discusses the training data used for the model, [Document 2] discusses the model architecture, [Document 3] discusses the results of the model, and [Document 4] discusses the evaluation data used for the model. None of the documents discuss the size of the open vocabulary.\n\nAnswer: Unanswerable\n\nExample 3:\n\nQuestion: Which of the classifiers showed the best performance?\n\nExplanation: \n\n[Document 1]: We compared the performance of four different classifiers: logistic regression, support vector machines, decision trees, and random forests.\n\n[Document 2]: Logistic regression is a statistical model that is used to predict the probability of a binary outcome.\n\n[Document 3]: Support vector machines are a type of supervised machine learning algorithm that can be used for both classification and regression tasks.\n\n[Document 4]: Decision trees are a type of machine learning algorithm that can be used for both classification and regression tasks.\n\n[Document 5]: Random forests are a type of machine learning algorithm that can be used for both classification and regression tasks.\n\nAccording to [Document 1], the four classifiers that were compared were logistic regression, support vector machines, decision trees, and random forests. This information is further supported by [Document 2], [Document 3], [Document 4], and [Document 5].\n\nAnswer: Logistic regression\n\nExample 4:\n\n[Document 1]: FLOAT SELECTED: Table 2: BLEU scores from German-English (De-En) and English-German (En-De) experiments on NewsTest2013 (NT13) and NewsTest2014 (NT14) along with their vocabulary sizes, class imbalance D, and mean sequence length \u00b5.\n\nQuestion: Which vocabulary size was the better performer?\n\nExplanation:"}
{"question_id": "b6e97d1b1565732b1b3f1d74e6d2800dd21be37a", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We first compared the performance of Transformer Transducer (T-T) models with full attention on audio to an RNN-T model using a bidirectional LSTM audio encoder. As shown in Table TABREF12, the T-T model significantly outperforms the LSTM-based RNN-T baseline. We also observed that T-T models can achieve competitive recognition accuracy with existing wordpiece-based end-to-end models with similar model size. To compare with systems using shallow fusion BIBREF18, BIBREF25 with separately trained LMs, we also trained a Transformer-based LM with the same architecture as the label encoder used in T-T, using the full 810M word token dataset. This Transformer LM (6 layers; 57M parameters) had a perplexity of $2.49$ on the dev-clean set; the use of dropout, and of larger models, did not improve either perplexity or WER. Shallow fusion was then performed using that LM and both the trained T-T system and the trained bidirectional LSTM-based RNN-T baseline, with scaling factors on the LM output and on the non-blank symbol sequence length tuned on the LibriSpeech dev sets. The results are shown in Table TABREF12 in the \u201cWith LM\u201d column. The shallow fusion result for the T-T system is competitive with corresponding results for top-performing existing systems.\n\nQuestion: What was previous state of the art model?\n\nExplanation:  The table in [Document 1] shows that the T-T model outperforms the LSTM-based RNN-T baseline, which was the previous state of the art model.\n\nAnswer: LSTM-based RNN-T\n\nExample 2:\n\n[Document 1]: Table TABREF48 shows the BLEU score of all three models based on English-Hindi, Hindi-English on CFILT's test dataset respectively. From the results which we get, it is evident that the transformer model achieves higher BLEU score than both Attention encoder-decoder and sequence-sequence model. Attention encoder-decoder achieves better BLEU score and sequence-sequence model performs the worst out of the three which further consolidates the point that if we are dealing with long source and target sentences then attention mechanism is very much required to capture long term dependencies and we can solely rely on the attention mechanism, overthrowing recurrent cells completely for the machine translation task.\n\nQuestion: How were their results compared to state-of-the-art?\n\nExplanation: \n\nAccording to [Document 1], the transformer model achieves higher BLEU score than both Attention encoder-decoder and sequence-sequence model.\n\nAnswer: transformer model achieves higher BLEU score than both Attention encoder-decoder and sequence-sequence model\n\nExample 3:\n\n[Document 1]: We also evaluate our Joint-BiLSTM structure by comparing with several other state-of-the-art baseline approaches, which exploit either local or global temporal structure. As shown in Table TABREF20 , our Joint-BiLSTM reinforced model outperforms all of the baseline methods. The result of \u201cLSTM\u201d in first row refer from BIBREF15 and the last row but one denotes the best model combining local temporal structure using C3D with global temporal structure utilizing temporal attention in BIBREF17 . From the first two rows, our unidirectional joint LSTM shows rapid improvement, and comparing with S2VT-VGG model in line 3, it also demonstrates some superiority. Even LSTM-E jointly models video and descriptions representation by minimizing the distance between video and corresponding sentence, our Joint-BiLSTM reinforced obtains better performance from bidirectional encoding and separated visual and language models.\n\nQuestion: what are the state of the art methods?\n\nExplanation:  [Document 1] compares the Joint-BiLSTM structure with several other state-of-the-art baseline approaches. The state-of-the-art methods are listed in the table in [Document 1].\n\nAnswer: S2VT, RGB (VGG), RGB (VGG)+Flow (AlexNet), LSTM-E (VGG), LSTM-E (C3D) and Yao et al.\n\nExample 4:\n\n[Document 1]: We compare models on three datasets. The smallest of them is the Penn Treebank corpus by BIBREF13 with preprocessing from BIBREF14 . We also include another word level corpus: Wikitext-2 by BIBREF15 . It is about twice the size of Penn Treebank with a larger vocabulary and much lighter preprocessing. The third corpus is Enwik8 from the Hutter Prize dataset BIBREF16 . Following common practice, we use the first 90 million characters for training, and the remaining 10 million evenly split between validation and test.\n\n[Document 2]: We tested LSTMs of various depths and an RHN of depth 5 with parameter budgets of 10 and 24 million matching the sizes of the Medium and Large LSTMs by BIBREF18 . The results are summarised in Table TABREF9 .\n\n[Document 3]: Notably, in our experiments even the RHN with only 10M parameters has better perplexity than the 24M one in the original publication. Our 24M version improves on that further. However, a shallow LSTM-based model with only 10M parameters enjoys a very comfortable margin over that, with deeper models following near the estimated noise range. At 24M, all depths obtain very similar results, reaching exp(4.065) [fixed,zerofill,precision=1] at depth 4. Unsurprisingly, NAS whose architecture was chosen based on its performance on this dataset does almost equally well, even better than in BIBREF1 .\n\n[Document 4]: Wikitext-2 is not much larger than Penn Treebank, so it is not surprising that even models tuned for Penn Treebank perform reasonably on this dataset, and this is in fact how results in previous works were produced. For a fairer comparison, we also tune hyperparameters on the same dataset. In Table TABREF14 , we report numbers for both approaches. All our results are well below the previous state of the are for models without dynamic evaluation or caching. That said, our best result, exp(4.188) [fixed,zerofill,precision=1] compares favourably even to the Neural Cache BIBREF6 whose innovations are fairly orthogonal to the base model.\n\nQuestion: what was their newly established state of the art results?\n\nExplanation:"}
{"question_id": "4d887ce7dc43528098e7a3d9cd13c6c36f158c53", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Across all 26 datasets, RCRN outperforms not only standard BiLSTMs but also 3L-BiLSTMs which have approximately equal parameterization. 3L-BiLSTMs were overall better than BiLSTMs but lose out on a minority of datasets. RCRN outperforms a wide range of competitive baselines such as DiSAN, Bi-SRUs, BCN and LSTM-CNN, etc. We achieve (close to) state-of-the-art performance on SST, TREC question classification and 16 Amazon review datasets.\n\nQuestion: Does their model have more parameters than other models?\n\nExplanation:  [Document 1] states that RCRN has approximately equal parameterization to 3L-BiLSTMs.\n\nAnswer: approximately equal parameterization\n\nExample 2:\n\n[Document 1]: Encoders with induced latent structures have been shown to benefit several tasks including document classification, natural language inference BIBREF12, BIBREF13, and machine translation BIBREF11. Building on this motivation, our latent structure attention module builds upon BIBREF12 to model the dependencies between sentences in a document. It uses a variant of Kirchhoff\u2019s matrix-tree theorem BIBREF14 to model such dependencies as non-projective tree structures(\u00a7SECREF3). The explicit attention module is linguistically-motivated and aims to incorporate sentence-level structures from externally annotated document structures. We incorporate a coreference based sentence dependency graph, which is then combined with the output of the latent structure attention module to produce a hybrid structure-aware sentence representation (\u00a7SECREF5).\n\nQuestion: Is there any evidence that encoders with latent structures work well on other tasks?\n\nExplanation: \n\nAccording to [Document 1], encoders with latent structures have been shown to benefit several tasks, including document classification, natural language inference, and machine translation. This provides evidence that encoders with latent structures work well on other tasks.\n\nAnswer: Yes\n\nExample 3:\n\n[Document 1]: Traditional text-to-speech (TTS) systems are composed of complex pipelines BIBREF0 , these often include accoustic frontends, duration model, acoustic prediction model and vocoder models. The complexity of the TTS problem coupled with the requirement for deep domain expertise means these systems are often brittle in design and results in un-natural synthesized speech.\n\n[Document 2]: The architecture of our model utilizes RNN-based Seq2Seq model for generating mel spectrogram from text. The architecture is similar to that of Tacotron 2 BIBREF4 . The generated mel spetrogram can either be inverted via iterative algorithms such as Griffin Lim, or through more complicated neural vocoder networks such as a mel spectrogram conditioned Wavenet BIBREF11 .\n\n[Document 3]: Direct comparison of model parameters between ours and the open-source tacotron 2, our model contains 4.5 million parameters, whereas the Tacotron 2 contains around 13 million parameters with default setting. By helping our model learn attention alignment faster, we can afford to use a smaller overall model to achieve similar quality speech quality.\n\nQuestion: Do they reduce the number of parameters in their architecture compared to other direct text-to-speech models?\n\nExplanation:  [Document 3] states that their model contains 4.5 million parameters, whereas the Tacotron 2 contains around 13 million parameters. This means that their model has reduced the number of parameters.\n\nAnswer: Yes\n\nExample 4:\n\n[Document 1]: The edge between input elements $x_i$ and $x_j$ is represented by vectors $a^V_{ij}, a^K_{ij} \\in \\mathbb {R}^{d_a}$ . The motivation for learning two distinct edge representations is that $a^V_{ij}$ and $a^K_{ij}$ are suitable for use in eq. ( 6 ) and eq. ( 7 ), respectively, without requiring additional linear transformations. These representations can be shared across attention heads. We use $d_a = d_z$ .\n\nQuestion: Does the new relative position encoder require more parameters?\n\nExplanation:"}
{"question_id": "98aa86ee948096d6fe16c02c1e49920da00e32d4", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: In this way, an attention vector sequence is segmented into several subsequences and each subsequence represents the attention of one word. Then, motivated by the psycholinguistic finding that readers are likely to pay approximate attention to each character in one Chinese word, we devise an appropriate aggregation module to fuse the inner-word character attention. Concretely, we first transform $\\lbrace \\textbf {a}_c^s,..., \\textbf {a}_c^{s+l-1}\\rbrace $ into one attention vector $\\textbf {a}_w^i$ for $w_i$ with the mixed pooling strategy BIBREF11. Then we execute the piecewise up- mpling operation over each $\\textbf {a}_w^i$ to keep input and output dimensions unchanged for the sake of plug and play. The detailed process can be summarized as follows:\n\nQuestion: How does the fusion method work?\n\nExplanation: \n\nAccording to [Document 1], the attention vector sequence is segmented into several subsequences and each subsequence represents the attention of one word. Then, we devise an appropriate aggregation module to fuse the inner-word character attention. This information is further supported by [Document 2], [Document 3], and [Document 5].\n\nAnswer: ttention vector sequence is segmented into several subsequences and each subsequence represents the attention of one word, we devise an appropriate aggregation module to fuse the inner-word character attention\n\nExample 2:\n\n[Document 1]: Proposed Fusion Techniques ::: Step-Wise Decoder Fusion\n\n[Document 2]: Our first proposed technique is the step-wise decoder fusion of visual features during every prediction step i.e. we concatenate the visual encoding as context at each step of the decoding process. This differs from the usual practice of passing the visual feature only at the beginning of the decoding process BIBREF5.\n\n[Document 3]: Proposed Fusion Techniques ::: Multimodal Attention Modulation\n\n[Document 4]: Similar to general attention BIBREF8, wherein a variable-length alignment vector $a_{th}(s)$, whose size equals the number of time steps on the source side, is derived by comparing the current target hidden state $h_{t}$ with each source hidden state $\\overline{h_{s}}$; we consider a variant wherein the visual encoding $v_{t}$ is used to calculate an attention distribution $a_{tv}(s)$ over the source encodings as well. Then, the true attention distribution $a_{t}(s)$ is computed as an interpolation between the visual and text based attention scores. The score function is a content based scoring mechanism as usual.\n\n[Document 5]: Proposed Fusion Techniques ::: Visual-Semantic (VS) Regularizer\n\n[Document 6]: In terms of leveraging the visual modality for supervision, BIBREF1 use multi-task learning to learn grounded representations through image representation prediction. However, to our knowledge, visual-semantic supervision hasn't been much explored for multimodal translation in terms of loss functions.\n\nQuestion: What are 3 novel fusion techniques that are proposed?\n\nExplanation:  [Document 1], [Document 3], and [Document 5] each introduce one of the three proposed fusion techniques. [Document 2] and [Document 4] provide additional details about Step-Wise Decoder Fusion and Multimodal Attention Modulation, respectively. [Document 6] mentions Visual-Semantic (VS) Regularizer in the context of multimodal translation.\n\nAnswer: Step-Wise Decoder Fusion, Multimodal Attention Modulation, Visual-Semantic (VS) Regularizer\n\nExample 3:\n\n[Document 1]: As shown in Table TABREF46, consensus dropout fusion improves the score of NDCG by around 1.0 from the score of the joint model while still yielding comparable scores for other metrics. Unlike ensemble way, consensus dropout fusion does not require much increase in the number of model parameters.\n\n[Document 2]: As also shown in Table TABREF46, the ensemble model seems to take the best results from each model. Specifically, the NDCG score of the ensemble model is comparable to that of the image-only model and the scores of other metrics are comparable to those of the image-history joint model. From this experiment, we can confirm that the two models are in complementary relation.\n\nQuestion: Which method for integration peforms better ensemble or consensus dropout fusion with shared parameters?\n\nExplanation:  [Document 1] and [Document 2] both compare the performance of the ensemble model and the consensus dropout fusion model. According to both tables, the ensemble model outperforms the consensus dropout fusion model.\n\nAnswer: ensemble model\n\nExample 4:\n\n[Document 1]: FLOAT SELECTED: FIGURE 1 | Representation of our processing pipeline. Existing methods refers to our previous work described in Papegnies et al. (2017b) (content-based method) and Papegnies et al. (2019) (graph-based method), whereas the contribution presented in this article appears on the right side (fusion strategies). Figure available at 10.6084/m9.figshare.7442273 under CC-BY license.\n\nQuestion: What fusion methods are applied?\n\nExplanation:"}
{"question_id": "16cc37e4f8e2db99eaf89337a3d9ada431170d5b", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We built a dataset for the task of hate speech detection in memes with 5,020 images that were weakly labeled into hate or non-hate memes, depending on their source. Hate memes were retrieved from Google Images with a downloading tool. We used the following queries to collect a total of 1,695 hate memes: racist meme (643 memes), jew meme (551 memes), and muslim meme (501 Memes). Non-hate memes were obtained from the Reddit Memes Dataset . We assumed that all memes in the dataset do not contain any hate message, as we considered that average Reddit memes do not belong to this class. A total of 3,325 non-hate memes were collected. We split the dataset into train (4266 memes) and validation (754 memes) subsets. The splits were random and the distribution of classes in the two subsets is the same. We didn't split the dataset into three subsets because of the small amount of data we had and decided to rely on the validation set metrics.\n\nQuestion: How is each instance of the dataset annotated?\n\nExplanation:  [Document 1] states that the dataset is weakly labeled into hate or non-hate memes, depending on their source.\n\nAnswer: weakly labeled into hate or non-hate memes, depending on their source\n\nExample 2:\n\n[Document 1]: For each speaker, we divide all available utterances into disjoint train, development, and test sets. Using the force-aligned phone boundaries, we extract the mid-phone frame for each example across the four classes, which leads to a data imbalance. Therefore, for all utterances in the training set, we randomly sample additional examples within a window of 5 frames around the center phone, to at least 50 training examples per class per speaker. It is not always possible to reach the target of 50 examples, however, if no more data is available to sample from. This process gives a total of INLINEFORM0 10700 training examples with roughly 2000 to 3000 examples per class, with each speaker having an average of 185 examples. Because the amount of data varies per speaker, we compute a sampling score, which denotes the proportion of sampled examples to the speaker's total training examples. We expect speakers with high sampling scores (less unique data overall) to underperform when compared with speakers with more varied training examples.\n\nQuestion: How many instances does their dataset have?\n\nExplanation:  The dataset has 10700 training examples. This information is found in the first sentence of the first paragraph.\n\nAnswer: 10700\n\nExample 3:\n\n[Document 1]: We observe that merely adding more tasks cannot provide much improvement on the target task. Thus, we propose two MTL training algorithms to improve the performance. The first method simply adopts a sampling scheme, which randomly selects training data from the auxiliary tasks controlled by a ratio hyperparameter; The second algorithm incorporates recent ideas of data selection in machine translation BIBREF7 . It learns the sample weights from the auxiliary tasks automatically through language models. Prior to this work, many studies have used upstream datasets to augment the performance of MRC models, including word embedding BIBREF5 , language models (ELMo) BIBREF8 and machine translation BIBREF1 . These methods aim to obtain a robust semantic encoding of both passages and questions. Our MTL method is orthogonal to these methods: rather than enriching semantic embedding with external knowledge, we leverage existing MRC datasets across different domains, which help make the whole comprehension process more robust and universal. Our experiments show that MTL can bring further performance boost when combined with contextual representations from pre-trained language models, e.g., ELMo BIBREF8 .\n\n[Document 2]: We develop a novel re-weighting method to resolve these problems, using ideas inspired by data selection in machine translation BIBREF26 , BIBREF7 . We use $(Q^{k},P^{k},A^{k})$ to represent a data point from the $k$ -th task for $1\\le k\\le K$ , with $k=1$ being the target task. Since the passage styles are hard to evaluate, we only evaluate data points based on $Q^{k}$ and $A^k$ . Note that only data from auxiliary task ( $2\\le k\\le K$ ) is re-weighted; target task data always have weight 1.\n\nQuestion: What is the data selection paper in machine translation\n\nExplanation:  [Document 2] cites BIBREF7 and BIBREF26 as the data selection papers in machine translation that inspired the re-weighting method.\n\nAnswer: BIBREF7, BIBREF26 \n\nExample 4:\n\n[Document 1]: Therefore, in order to target only those tweets that are rich in human activity content, we formulate a set of queries that allows us to use the Twitter Search API to find instances of users tweeting about common human activities. Each query contains a first-person, past-tense verb within a phrase that describes a common activity that people do. Using this approach, we are able to retrieve a set of tweets that contains a high concentration of human activity content, and we also find that users who wrote these tweets are much more likely to have written other tweets that describe human activities (Table TABREF1 ). We build our set of human activity queries from two sources: the Event2Mind dataset BIBREF15 and a set of short activity surveys, which we collect ourselves, to obtain nearly 30K queries (Table TABREF2 ) .\n\nQuestion: how were the data instances chosen?\n\nExplanation:"}
{"question_id": "7c2d6bc913523d77e8fdc82c60598ee95b445d84", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: The stance towards vaccination was categorized into `Negative\u2019, `Neutral\u2019, `Positive\u2019 and `Not clear\u2019. The latter category was essential, as some posts do not convey enough information about the stance of the writer. In addition to the four-valued stance classes we included separate classes grouped under relevance, subject and sentiment as annotation categories. With these additional categorizations we aimed to obtain a precise grasp of all possibly relevant tweet characteristics in relation to vaccination, which could help in a machine learning setting.\n\nQuestion: Do they allow for messages with vaccination-related key terms to be of neutral stance?\n\nExplanation:  The \"Not clear\" category is for posts that do not have enough information to determine the writer's stance. This means that a post can be about vaccination but have a neutral stance.\n\nAnswer: Yes\n\nExample 2:\n\n[Document 1]: We manually reviewed 1,177 pairs of entities and referring expressions generated by the system. We find that 92.2% of the generated referring expressions refer to the correct entity.\n\n[Document 2]: From the generated expressions, 325 (27.6%) were pronouns, 192 (16.3%) are repeating a one-token entity as is, and 505 (42.9%) are generating correct shortening of a long entity. In 63 (5.6%) of the cases the system did not find a good substitute and kept the entire entity intact. Finally, 92 (7.82%) are wrong referrals. Overall, 73.3% of the non-first mentions of entities were replaced with suitable shorter and more fluent expressions.\n\nQuestion: How is fluency of generated text evaluated?\n\nExplanation:  [Document 1] states that the fluency of the generated text was manually reviewed. This is further supported by [Document 2], which states that the fluency of the generated text was manually reviewed and that 73.3% of the non-first mentions of entities were replaced with suitable shorter and more fluent expressions.\n\nAnswer: manually reviewed\n\nExample 3:\n\n[Document 1]: The addition of output verification resulted in negligible changes in BLEU, reinforcing that automatic metrics are not sensitive enough to output accuracy. We thus performed manual analysis, following the procedure in BIBREF0. We manually inspect 148 samples from the seen part of the test set, containing 440 relations, counting expressed, omitted, wrong and over-generated (hallucinated) facts. We compare to the StrongNeural and BestPlan systems from BIBREF0. Results in Table indicate that the effectiveness of the verification process in ensuring correct output, reducing the already small number of ommited and overgenerated facts to 0 (with the exhaustive planner) and keeping it small (with the fast neural planner).\n\nQuestion: How is faithfulness of the resulting text evaluated?\n\nExplanation:  The faithfulness of the resulting text is evaluated by manually inspecting 148 samples from the seen part of the test set, containing 440 relations, counting expressed, omitted, wrong and over-generated (hallucinated) facts.\n\nAnswer: manually inspect\n\nExample 4:\n\n[Document 1]: This work presents data and algorithms for automatically reducing bias in text. We focus on a particular kind of bias: inappropriate subjectivity (\u201csubjective bias\u201d). Subjective bias occurs when language that should be neutral and fair is skewed by feeling, opinion, or taste (whether consciously or unconsciously). In practice, we identify subjective bias via the method of BIBREF2: using Wikipedia's neutral point of view (NPOV) policy. This policy is a set of principles which includes \u201cavoiding stating opinions as facts\u201d and \u201cpreferring nonjudgemental language\u201d.\n\n[Document 2]: We aim to debias text by suggesting edits that would make it more neutral. This contrasts with prior research which has debiased representations of text by removing dimensions of prejudice from word embeddings BIBREF3, BIBREF4 and the hidden states of predictive models BIBREF5, BIBREF6. To avoid overloading the definition of \u201cdebias,\u201d we refer to our kind of text debiasing as neutralizing that text. Figure FIGREF1 gives an example.\n\nQuestion: How is subjective text automatically neutralized?\n\nExplanation:"}
{"question_id": "48ff9645a506aa2c17810d2654d1f0f0d9e609ee", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Many NLP tasks utilize POS as features, but human annotated POS sequences are difficult and expensive to obtain. Thus, it is important to know if we can learn sentences-level syntactic embeddings for low-sources languages without treebanks.\n\n[Document 2]: We performed zero-shot transfer of the syntactic embeddings for French, Portuguese and Indonesian. French and Portuguese are simulated low-resource languages, while Indonesian is a true low-resource language. We reported the 1-NN and 5-NN accuracies for all languages using the same evaluation setting as described in the previous section. The results are shown in Table TABREF31 (top).\n\nQuestion: Do they evaluate on downstream tasks?\n\nExplanation:  The results are shown in Table TABREF31 (top) of [Document 2], which indicates that they evaluate on downstream tasks.\n\nAnswer: Yes\n\nExample 2:\n\n[Document 1]: Finally, we fed these obtained word vectors to a BiLSTM with max-pooling and evaluated the final sentence representations in 11 downstream transfer tasks BIBREF13 , BIBREF18 .\n\n[Document 2]: table:sentence-eval-datasets lists the sentence-level evaluation datasets used in this paper. The provided URLs correspond to the original sources, and not necessarily to the URLs where SentEval got the data from.\n\nQuestion: Which downstream sentence-level tasks do they evaluate on?\n\nExplanation:  The answer can be found in [Document 1].\n\nAnswer: BIBREF13 , BIBREF18\n\nExample 3:\n\n[Document 1]: To measure the effect of adding Bertram to BERT on downstream tasks, we apply the procedure described in Section SECREF4 to a commonly used textual entailment dataset as well as two text classification datasets: MNLI BIBREF21, AG's News BIBREF22 and DBPedia BIBREF23. For all three datasets, we use BERT$_\\text{base}$ as a baseline model and create the substitution dictionary $S$ using the synonym relation of WordNet BIBREF20 and the pattern library BIBREF34 to make sure that all synonyms have consistent parts of speech. As an additional source of word substitutions, we make use of the misspellings dataset of BIBREF25, which is based on query logs of a search engine. To prevent misspellings from dominating the resulting dataset, we only assign misspelling-based substitutes to randomly selected 10% of the words contained in each sentence. Motivated by the results on WNLaMPro-medium, we consider every word that occurs less than 100 times in the WWC and our BooksCorpus replica combined as being rare. Some examples of entries in the resulting datasets can be seen in Table TABREF35.\n\nQuestion: What are three downstream task datasets?\n\nExplanation: \n\nAccording to [Document 1], the three downstream task datasets are MNLI BIBREF21, AG's News BIBREF22, and DBPedia BIBREF23.\n\nAnswer: MNLI BIBREF21, AG's News BIBREF22, DBPedia BIBREF23\n\nExample 4:\n\n[Document 1]: Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - BIBREF13) and a question answering task (SQuAD v1.1 - BIBREF14).\n\nQuestion: What downstream tasks are analyzed?\n\nExplanation:"}
{"question_id": "7c2c15ea3f1b1375b8aaef1103a001069d9915bb", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: The 10-fold cross validation with this setting gave a token-level accuracy of roughly 71%. This result is not satisfactory on an absolute scale, however it is more than encouraging taking into account the small size of our data. This result means that less than 3 tokens, on average, out of 10, must be corrected to increase the size of our corpus. With this model we automatically transcribed into Arabic morphemes, roughly, 5,000 additional tokens, corresponding to the second annotation block. This can be manually annotated in at least 7,5 days, but thanks to the automatic annotation accuracy, it was manually corrected into 3 days. The accuracy of the model on the annotation of the second block was roughly 70%, which corresponds to the accuracy on the test set. The manually-corrected additional tokens were added to the training data of our neural model, and a new block was automatically annotated and manually corrected. Both accuracy on the test set and on the annotation block remained at around 70%. This is because the block added to the training data was significantly different from the previous and from the third. Adding the third block to the training data and annotating a fourth block with the new trained model gave in contrast an accuracy of roughly 80%. This incremental, semi-automatic transcription procedure is in progress for the remaining blocks, but it is clear that it will make the corpus annotation increasingly easier and faster as the amount of training data will grow up.\n\nQuestion: Does the paper report translation accuracy for an automatic translation model for Tunisian to Arabish words?\n\nExplanation:  The paper reports that the accuracy of the model on the annotation of the second block was roughly 70%, which corresponds to the accuracy on the test set. The accuracy of the model on the annotation of the second block was roughly 70%, which corresponds to the accuracy on the test set.\n\nAnswer: Yes\n\nExample 2:\n\n[Document 1]: To achieve a finer-grained understanding of what errors the evaluated translations exhibit, we perform a categorisation of 150 randomly sampled sentences based on the classification used by BIBREF3. We expand the classification with a Context category, which we use to mark errors that are only apparent in larger context (e. g., regarding poor register choice, or coreference errors), and which do not clearly fit into one of the other categories. BIBREF3 perform this classification only for the machine-translated outputs, and thus the natural question of whether the mistakes that humans and computers make are qualitatively different is left unanswered. Our analysis was performed by one of the co-authors who is a bi-lingual native Chinese/English speaker. Sentences were shown in the context of the document, to make it easier to determine whether the translations were correct based on the context. The analysis was performed on one machine translation (MT$_1$) and two human translation outputs (H$_A$, H$_B$), using the same 150 sentences, but blinding their origin by randomising the order in which the documents were presented. We show the results of this analysis in Table TABREF32.\n\nQuestion: What percentage fewer errors did professional translations make?\n\nExplanation:  In [Document 1], it is stated that \"the natural question of whether the mistakes that humans and computers make are qualitatively different is left unanswered.\" However, in Table TABREF32, it is shown that professional translations made 36% fewer errors.\n\nAnswer: 36%\n\nExample 3:\n\nQuestion: What is the worst performing translation granularity?\n\nExplanation:  The documents do not provide enough information to answer the question.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: (1) Excessive focus on English and European languages as one of the involved languages in MT approaches and poor research on low-resource language pairs such as African and/or South American languages. (2) The limitations of SMT approaches for translating across domains. Most MT systems exhibit good performance on law and the legislative domains due to the large amount of data provided by the European Union. In contrast, translations performed on sports and life-hacks commonly fail, because of the lack of training data. (3) How to translate the huge amount of data from social networks that uniquely deal with no-standard speech texts from users (e.g., tweets). (4) The difficult translations among morphologically rich languages. This challenge shares the same problem with the first one, namely that most research work focuses on English as one of the involved languages. Therefore, MT systems which translate content between, for instance, Arabic and Spanish are rare. (5) For the speech translation task, the parallel data for training differs widely from real user speech.\n\nQuestion: What are the other obstacles to automatic translations which are not mentioned in the abstract?\n\nExplanation:"}
{"question_id": "230ff86b7b90b87c33c53014bb1e9c582dfc107f", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: This step entails counting occurrences of all words in the training corpus and sorting them in order of decreasing occurrence. As mentioned, the vocabulary is taken to be the INLINEFORM0 most frequently occurring words, that occur at least some number INLINEFORM1 times. It is implemented in Spark as a straight-forward map-reduce job.\n\nQuestion: Do they perform any morphological tokenization?\n\nExplanation:  There is no mention of any morphological tokenization in [Document 1].\n\nAnswer: No\n\nExample 2:\n\n[Document 1]: In order to address the aforementioned problems we redesign the neural decoder in three different scenarios. In the first scenario we equip the decoder with an additional morphology table including target-side affixes. We place an attention module on top of the table which is controlled by the decoder. At each step, as the decoder samples a character, it searches the table to find the most relevant information which can enrich its state. Signals sent from the table can be interpreted as additional constraints. In the second scenario we share the decoder between two output channels. The first one samples the target character and the other one predicts the morphological annotation of the character. This multi-tasking approach forces the decoder to send morphology-aware information to the final layer which results in better predictions. In the third scenario we combine these two models. Section \"Proposed Architecture\" provides more details on our models.\n\nQuestion: What type of morphological information is contained in the \"morphology table\"?\n\nExplanation:  The answer can be found in [Document 1], which states that the \"morphology table\" includes target-side affixes.\n\nAnswer: target-side affixes\n\nExample 3:\n\n[Document 1]: Inflectional realization defines the inflected forms of a lexeme/lemma. As a computational task, often referred to as simply \u201cmorphological inflection,\" inflectional realization is framed as a mapping from the pairing of a lemma with a set of morphological tags to the corresponding word form. For example, the inflectional realization of SJQ Chatino verb forms entails a mapping of the pairing of the lemma lyu1 `fall' with the tag-set 1;SG;PROG to the word form nlyon32.\n\n[Document 2]: Morphological analysis is the task of creating a morphosyntactic description for a given word. It can be framed in a context-agnostic manner (as in our case) or within a given context, as for instance for the SIGMORPHON 2019 second shared task BIBREF11. We trained an encoder-decoder model that receives the form as character-level input, encodes it with a BiLSTM encoder, and then an attention enhanced decoder BIBREF14 outputs the corresponding sequence of morphological tags, implemented in DyNet. The baseline results are shown in Table . The exact-match accuracy of 67% is lower than the average accuracy that context-aware systems can achieve, and it highlights the challenge that the complexity of the tonal system of SJQ Chatino can pose.\n\nQuestion: How does morphological analysis differ from morphological inflection?\n\nExplanation: \n\nAccording to [Document 1], inflectional realization is the process of mapping a lemma with a set of morphological tags to a corresponding word form. [Document 2] states that morphological analysis is the task of creating a morphosyntactic description for a given word. These two tasks are different because inflectional realization only deals with the forms of words, while morphological analysis deals with the meaning of words.\n\nAnswer: Morphological analysis is the task of creating a morphosyntactic description for a given word,  inflectional realization is framed as a mapping from the pairing of a lemma with a set of morphological tags to the corresponding word form\n\nExample 4:\n\n[Document 1]: The biggest improvement over the word baseline is achieved by the models that have access to morphology for all languages (except for English) as expected. Character trigrams consistently outperformed characters by a small margin. Same pattern is observed on the results of the development set. IOW has the values between 0% to 38% while IOC values range between 2%-10% dependending on the properties of the language and the dataset. We analyze the results separately for agglutinative and fusional languages and reveal the links between certain linguistic phenomena and the IOC, IOW values.\n\nQuestion: What morphological typologies are considered?\n\nExplanation:"}
{"question_id": "65b39676db60f914f29f74b7c1264422ee42ad5c", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We evaluate the model's performance on the bAbI tasks BIBREF18 , a collection of 20 question answering tasks which have become a benchmark for evaluating memory-augmented neural networks. We compare the performance with the Recurrent Entity Networks model (EntNet) BIBREF17 . Performance is measured in terms of mean percentage error on the tasks.\n\n[Document 2]: The results are shown in Table 1 . The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model BIBREF17 . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks.\n\nQuestion: What methods is RelNet compared to?\n\nExplanation:  [Document 1] states that the performance of RelNet is compared to the Recurrent Entity Networks model (EntNet). This is supported by [Document 2], which states that the RelNet model is compared to the EntNet model.\n\nAnswer: We compare the performance with the Recurrent Entity Networks model (EntNet) BIBREF17\n\nExample 2:\n\nQuestion: how well this method is compared to other method?\n\nExplanation: \n\nThis question is unanswerable based on the given documents.\n\nAnswer: Unanswerable\n\nExample 3:\n\nQuestion: Did they compare with gradient-based methods?\n\nExplanation:  The question cannot be answered with the given documents.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: Overall prediction accuracy can be calculated by subtracting one with the average result of error rate division on each party by number of its remaining candidates. We achieve 0.548 prediction accuracy, which is not good enough BIBREF1 . The model accuracy is mainly affected by the large error rate on Democratic candidates (1.33 from 2 candidates).\n\nQuestion: what are the other methods they compare to?\n\nExplanation:"}
{"question_id": "da077b385d619305033785af5b204696d6145bd8", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: Do all teams use neural networks for their models?\n\nExplanation:  The documents do not mention anything about whether or not all teams use neural networks for their models.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: Grammatical error correction (GEC) is a challenging task due to the variability of the type of errors and the syntactic and semantic dependencies of the errors on the surrounding context. Most of the grammatical error correction systems use classification and rule-based approaches for correcting specific error types. However, these systems use several linguistic cues as features. The standard linguistic analysis tools like part-of-speech (POS) taggers and parsers are often trained on well-formed text and perform poorly on ungrammatical text. This introduces further errors and limits the performance of rule-based and classification approaches to GEC. As a consequence, the phrase-based statistical machine translation (SMT) approach to GEC has gained popularity because of its ability to learn text transformations from erroneous text to correct text from error-corrected parallel corpora without any additional linguistic information. They are also not limited to specific error types. Currently, many state-of-the-art GEC systems are based on SMT or use SMT components for error correction BIBREF0 , BIBREF1 , BIBREF2 . In this paper, grammatical error correction includes correcting errors of all types, including word choice errors and collocation errors which constitute a large class of learners' errors.\n\n[Document 2]: We model our GEC system based on the phrase-based SMT approach. However, traditional phrase-based SMT systems treat words and phrases as discrete entities. We take advantage of continuous space representation by adding two neural network components that have been shown to improve SMT systems BIBREF3 , BIBREF4 . These neural networks are able to capture non-linear relationships between source and target sentences and can encode contextual information more effectively. Our experiments show that the addition of these two neural networks leads to significant improvements over a strong baseline and outperforms the current state of the art.\n\n[Document 3]: To train NNJM, we use the publicly available implementation, Neural Probabilistic Language Model (NPLM) BIBREF14 . The latest version of Moses can incorporate NNJM trained using NPLM as a feature while decoding. Similar to NNGLM, we use the parallel text used for training the translation model in order to train NNJM. We use a source context window size of 5 and a target context window size of 4. We select a source context vocabulary of 16,000 most frequent words from the source side. The target context vocabulary and output vocabulary is set to the 32,000 most frequent words. We use a single hidden layer to speed up training and decoding with an input embedding dimension of 192 and 512 hidden layer nodes. We use rectified linear units (ReLU) as the activation function. We train NNJM with noise contrastive estimation with 100 noise samples per training instance, which are obtained from a unigram distribution. The neural network is trained for 30 epochs using stochastic gradient descent optimization with a mini-batch size of 128 and learning rate of 0.1.\n\nQuestion: Do they use pretrained word representations in their neural network models?\n\nExplanation:  There is no mention of pretrained word representations in any of the documents.\n\nAnswer: No\n\nExample 3:\n\n[Document 1]: Figure 1 presents architecture of the WSD system. As one may observe, no human labor is used to learn interpretable sense representations and the corresponding disambiguation models. Instead, these are induced from the input text corpus using the JoBimText approach BIBREF8 implemented using the Apache Spark framework, enabling seamless processing of large text collections. Induction of a WSD model consists of several steps. First, a graph of semantically related words, i.e. a distributional thesaurus, is extracted. Second, word senses are induced by clustering of an ego-network of related words BIBREF9 . Each discovered word sense is represented as a cluster of words. Next, the induced sense inventory is used as a pivot to generate sense representations by aggregation of the context clues of cluster words. To improve interpretability of the sense clusters they are labeled with hypernyms, which are in turn extracted from the input corpus using Hearst:92 patterns. Finally, the obtained WSD model is used to retrieve a list of sentences that characterize each sense. Sentences that mention a given word are disambiguated and then ranked by prediction confidence. Top sentences are used as sense usage examples. For more details about the model induction process refer to BIBREF10 . Currently, the following WSD models induced from a text corpus are available: Word senses based on cluster word features. This model uses the cluster words from the induced word sense inventory as sparse features that represent the sense.\n\n[Document 2]: Word senses based on context word features. This representation is based on a sum of word vectors of all cluster words in the induced sense inventory weighted by distributional similarity scores.\n\n[Document 3]: Super senses based on cluster word features. To build this model, induced word senses are first globally clustered using the Chinese Whispers graph clustering algorithm BIBREF9 . The edges in this sense graph are established by disambiguation of the related words BIBREF11 , BIBREF12 . The resulting clusters represent semantic classes grouping words sharing a common hypernym, e.g. \u201canimal\u201d. This set of semantic classes is used as an automatically learned inventory of super senses: There is only one global sense inventory shared among all words in contrast to the two previous traditional \u201cper word\u201d models. Each semantic class is labeled with hypernyms. This model uses words belonging to the semantic class as features.\n\n[Document 4]: Super senses based on context word features. This model relies on the same semantic classes as the previous one but, instead, sense representations are obtained by averaging vectors of words sharing the same class.\n\nQuestion: Do they use a neural model for their task?\n\nExplanation:  There is no mention of a neural model in any of the documents.\n\nAnswer: No\n\nExample 4:\n\n[Document 1]: In the following subsections, we will introduce our proposed Query-bag Matching (QBM) model which output is the matching probability indicating whether the query and bag are asking the same questions. The basic Q-Q (query-question) matching model hybrid CNN (hCNN) BIBREF5 is presented as the background. Then we will show the base model and its two components designed to promote the performance: Mutual Coverage and Bag Representation. For better understanding, the whole model is shown in Figure FIGREF2.\n\n[Document 2]: After getting the Q-Q matching representation $r_i$, we combine the $\\lbrace r_1, \\dots , r_n\\rbrace $ by element-wise max and mean pooling in order to get $r_p$ to represent the query-bag matching representation: rp = [ max_pooling { r1, ..., rn }; mean_pooling { r1, ..., rn } ] where [;] denotes concatenation. After that, an MLP with softmax is applied to predict whether the query and the bag is asking the same question. Finally, the loss function minimizes the cross entropy of the training data. Due to the out-of-order of the bag, we do not model the bag representation by CNN or LSTM, and experiments show the pooling-based method works well.\n\nQuestion: Does the query-bag matching model use a neural network?\n\nExplanation:"}
{"question_id": "9aabcba3d44ee7d0bbf6a2c019ab9e0f02fab244", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We use two basic features:\n\n[Document 2]: Parts of Speech (POS) tags: We use the POS tagger of NLTK to tag the tweet texts BIBREF0 . We use counts of noun, adjective, adverb, verb words in a tweet as POS features.\n\n[Document 3]: Prior polarity of the words: We use a polarity dictionary BIBREF3 to get the prior polarity of words. The dictionary contains positive, negative and neutral words along with their polarity strength (weak or strong). The polarity of a word is dependent on its POS tag. For example, the word `excuse' is negative when used as `noun' or `adjective', but it carries a positive sense when used as a `verb'. We use the tags produced by NLTK postagger while selecting the prior polarity of a word from the dictionary. We also employ stemming (Porter Stemmer implementation from NLTK) while performing the dictionary lookup to increase number of matches. We use the counts of weak positive words, weak negative words, strong positive words and strong negative words in a tweet as features.\n\n[Document 4]: We have also explored some advanced features that helps improve detecting sentiment of tweets.\n\n[Document 5]: Emoticons: We use the emoticon dictionary from BIBREF2 , and count the positive and negtive emocicons for each tweet.\n\n[Document 6]: The sentiment of url: Since almost all the articles are written in well-formatted english, we analyze the sentiment of the first paragraph of the article using Standford Sentiment Analysis tool BIBREF4 . It predicts sentiment for each sentence within the article. We calculate the fraction of sentences that are negative, positive, and neutral and use these three values as features.\n\n[Document 7]: Hashtag: We count the number of hashtags in each tweet.\n\n[Document 8]: Capitalization: We assume that capitalization in the tweets has some relationship with the degree of sentiment. We count the number of words with capitalization in the tweets.\n\n[Document 9]: Retweet: This is a boolean feature indicating whether the tweet is a retweet or not.\n\n[Document 10]: User Mention: A boolean feature indicating whether the tweet contains a user mention.\n\n[Document 11]: Negation: Words like `no', `not', `won't' are called negation words since they negate the meaning of the word that is following it. As for example `good' becomes `not good'. We detect all the negation words in the tweets. If a negation word is followed by a polarity word, then we negate the polarity of that word. For example, if `good' is preceeded by a `not', we change the polarity from `weak positive' to `weak negative'.\n\n[Document 12]: Text Feature: We use tf-idf based text features to predict the sentiment of a tweet. We perform tf-idf based scoring of words in a tweet and the hashtags present in the tweets. We use the tf-idf vectors to train a classifier and predict the sentiment. This is then used as a stacked prediction feature in the final classifier.\n\nQuestion: What linguistic features are used?\n\nExplanation:  [Document 2], [Document 3], [Document 8], [Document 11], and [Document 12] all mention different linguistic features that are used. POS tags are used in [Document 2], Prior polarity of the words is used in [Document 3], Capitalization is used in [Document 8], Negation is used in [Document 11], and Text Feature is used in [Document 12].\n\nAnswer: Parts of Speech (POS) tags, Prior polarity of the words, Capitalization, Negation, Text Feature\n\nExample 2:\n\n[Document 1]: Exploiting psycho-linguistic features with basic linguistic features as meta-data. The main aim is to minimize the direct dependencies on in-depth grammatical structure of the language (i.e., to support code-mixed data). We have also included emoticons, and punctuation features with it. We use the term \"NLP Features\" to represent it in the entire paper.\n\n[Document 2]: We have identified a novel combination of features which are highly effective in aggression classification when applied in addition to the features obtained from the deep learning classifier at the classification layer. We have introduced two new features in addition to the previously available features. The first one is the Emotion Sensor Feature which use a statistical model to classify the words into 7 different classes based on the sentences obtained from twitter and blogs which contain total 1,185,540 words. The second one is the collection of selected topical signal from text collected using Empath (see Table 1.).\n\nQuestion: Which psycholinguistic and basic linguistic features are used?\n\nExplanation:  [Document 1] mentions that the study uses \"psycho-linguistic features with basic linguistic features as meta-data\". [Document 2] provides more specific information about which features are used, including the Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, and TF-IDF Emoticon features.\n\nAnswer: Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, TF-IDF Emoticon features\n\nExample 3:\n\n[Document 1]: IOCs in cybersecurity articles are often described in a predictable way: being connected to a set of contextual keywords BIBREF16 , BIBREF1 . For example, a human user can infer that the word \u201cntdll.exe\u201d is the name of a malicious file on the basis of the words \u201cdownload\u201d and \u201ccompromised\u201d from the text shown in Fig. FIGREF1 . By analyzing the whole corpus, it is interesting that malicious file names tends to co-occur with words such as \"download\", \"malware\", \"malicious\", etc. In this work, we consider words that can indicate the characteristics of the neighbor words as contextual keywords and develop an approach to generate features from the automatically extracted contextual keywords.\n\nQuestion: What contextual features are used?\n\nExplanation:  [Document 1] states that contextual keywords are used to generate features. This is further supported by the example in Figure 1.\n\nAnswer: The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.\n\nExample 4:\n\n[Document 1]: This paper is structured as follows. In the next sections we will review related work on sentiment classification, followed by a detailed explanation of our approach and our data collection, annotation and processing efforts. After that, we describe our baseline n-gram sentiment classifier model, followed by the explanation of how the baseline model is extended to incorporate contextual information. Next, we describe our analysis of the variation of sentiment within each of the contextual categories. We then evaluate our models and finally summarize our findings and contributions and discuss possible paths for future work.\n\n[Document 2]: There have been several works that do sentiment classification on Twitter using standard sentiment classification techniques, with variations of n-gram and bag of words being the most common. There have been attempts at using more advanced syntactic features as is done in sentiment classification for other domains BIBREF1 , BIBREF2 , however the 140 character limit imposed on tweets makes this hard to do as each article in the Twitter training set consists of sentences of no more than several words, many of them with irregular form BIBREF3 .\n\nQuestion: Are there any other standard linguistic features used, other than ngrams?\n\nExplanation:"}
{"question_id": "1128a600a813116cba9a2cf99d8568ae340f327a", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Our experiments are run using the English$\\rightarrow $Italian/German portions of the MuST-C corpus BIBREF25, which is extracted from TED talks, using the same train/validation/test split as provided with the corpus (see Table TABREF18). As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De). While our main goal is to verify our hypotheses on a large data condition, thus the need to include proprietary data, for the sake of reproducibility in both languages we also provide results with systems only trained on TED Talks (small data condition). When training on large scale data we use Transformer with layer size of 1024, hidden size of 4096 on feed forward layers, 16 heads in the multi-head attention, and 6 layers in both encoder and decoder. When training only on TED talks, we set layer size of 512, hidden size of 2048 for the feed forward layers, multi-head attention with 8 heads and again 6 layers in both encoder and decoder.\n\nQuestion: What dataset do they use for experiments?\n\nExplanation:  The dataset used for experiments is described in [Document 1].\n\nAnswer: English$\\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)\n\nExample 2:\n\n[Document 1]: We extract data from the WMT'14 English-French (En-Fr) and English-German (En-De) datasets. To create a larger discrepancy between the tasks, so that there is a clear dataset size imbalance, the En-De data is artificially restricted to only 1 million parallel sentences, while the full En-Fr dataset, comprising almost 40 million parallel sentences, is used entirely. Words are split into subwords units with a joint vocabulary of 32K tokens. BLEU scores are computed on the tokenized output with multi-bleu.perl from Moses BIBREF10.\n\nQuestion: What datasets are used for experiments?\n\nExplanation:  [Document 1] states that the WMT'14 English-French (En-Fr) and English-German (En-De) datasets are used for experiments.\n\nAnswer: the WMT'14 English-French (En-Fr) and English-German (En-De) datasets.\n\nExample 3:\n\n[Document 1]: Due to the reduction of computational time complexity we expect our method to yield performance gains especially for longer sequences and tasks where the source can be compactly represented in a fixed-size memory matrix. To investigate the trade-off between speed and performance, we compare our technique to standard models with and without attention on a Sequence Copy Task of varying length like in BIBREF14 . We generated 4 training datasets of 100,000 examples and a validation dataset of 1,000 examples. The vocabulary size was 20. For each dataset, the sequences had lengths randomly chosen between 0 to $L$ , for $L\\in \\lbrace 10, 50, 100, 200\\rbrace $ unique to each dataset.\n\n[Document 2]: Next, we explore if the memory-based attention mechanism is able to fit complex real-world datasets. For this purpose we use 4 large machine translation datasets of WMT'17 on the following language pairs: English-Czech (en-cs, 52M examples), English-German (en-de, 5.9M examples), English-Finish (en-fi, 2.6M examples), and English-Turkish (en-tr, 207,373 examples). We used the newly available pre-processed datasets for the WMT'17 task. Note that our scores may not be directly comparable to other work that performs their own data pre-processing. We learn shared vocabularies of 16,000 subword units using the BPE algorithm BIBREF19 . We use newstest2015 as a validation set, and report BLEU on newstest2016.\n\nQuestion: Which datasets are used in experiments?\n\nExplanation: \n\n[Document 1] mentions the Sequence Copy Task dataset, while [Document 2] mentions the WMT'17 dataset.\n\nAnswer: Sequence Copy Task and WMT'17\n\nExample 4:\n\n[Document 1]: For classification task, we test our model on 16 classification datasets, the first 14 datasets are product reviews that collected based on the dataset, constructed by BIBREF27 , contains Amazon product reviews from different domains: Books, DVDs, Electronics and Kitchen and so on. The goal in each domain is to classify a product review as either positive or negative. The datasets in each domain are partitioned randomly into training data, development data and testing data with the proportion of 70%, 10% and 20% respectively. The detailed statistics are listed in Table 1 .\n\n[Document 2]: The remaining two datasets are two sub-datasets about movie reviews.\n\n[Document 3]: IMDB The movie reviews with labels of subjective or objective BIBREF28 .\n\n[Document 4]: MR The movie reviews with two classes BIBREF29 .\n\n[Document 5]: For sequence tagging task, we use the Wall Street Journal(WSJ) portion of Penn Treebank (PTB) BIBREF33 , CoNLL 2000 chunking, and CoNLL 2003 English NER datasets. The statistics of these datasets are described in Table 4 .\n\nQuestion: What datasets do they use in the experiment?\n\nExplanation:"}
{"question_id": "6e3e9818551fc2f8450bbf09b0fe82ac2506bc7a", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: In this section, we empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method) in ROOT. We show that AD can drastically improve accuracy and performance of derivative evaluation, compared to ND.\n\nQuestion: How is correctness of automatic derivation proved?\n\nExplanation: \n\nAccording to [Document 1], the correctness of automatic derivation is proved by empirically comparing automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method).\n\nAnswer: empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method)\n\nExample 2:\n\n[Document 1]: The theorem is proven constructively by selecting the parameters of the multi-head self-attention layer so that the latter acts like a convolutional layer. In the proposed construction, the attention scores of each self-attention head should attend to a different relative shift within the set $\\Delta \\!\\!\\!\\!\\Delta _K = \\lbrace -\\lfloor K/2 \\rfloor , \\dots , \\lfloor K/2 \\rfloor \\rbrace ^2$ of all pixel shifts in a $K\\times K$ kernel. The exact condition can be found in the statement of Lemma UNKREF15.\n\nQuestion: How they prove that multi-head self-attention is at least as powerful as convolution layer? \n\nExplanation: \n\nAccording to [Document 1], the theorem is proven constructively by selecting the parameters of the multi-head self-attention layer so that the latter acts like a convolutional layer. This information is further supported by the statement of Lemma UNKREF15.\n\nAnswer: constructively by selecting the parameters of the multi-head self-attention layer so that the latter acts like a convolutional layer\n\nExample 3:\n\nQuestion: How much better were results of the proposed models than base LSTM-RNN model?\n\nExplanation: \n\n[Document 1]: The proposed models were evaluated on two standard tasks: the task of diversity and the task of relevance.\n\n[Document 2]: The results show that the proposed models outperform the base LSTM-RNN model by a significant margin on both tasks. On the task of diversity, the proposed models improve the results by 6.87 points. On the task of relevance, the proposed models improve the results by 4.6 points.\n\nAnswer: on diversity 6.87 and on relevance 4.6 points higher\n\nExample 4:\n\n[Document 1]: Theorem 1.11. For any context-free language INLINEFORM0 , suppose we relabel and write INLINEFORM1 for some regular language INLINEFORM2 , whose corresponding minimum-size DFA has INLINEFORM3 states. Then there exists a simple RNN with a hidden layer of size INLINEFORM4 whose INLINEFORM5 -language is INLINEFORM6 .\n\n[Document 2]: Proof. Consider the simple RNN with INLINEFORM0 as its INLINEFORM1 -language described in the proof of Theorem 1.1 and the simple RNN with INLINEFORM2 as its INLINEFORM3 -language constructed to prove Theorem 1.10. Merge the INLINEFORM4 nodes in the input layer corresponding to the input and merge the single output nodes of both RNNs. Stack the two hidden layers, and add no new edges. There were INLINEFORM5 hidden nodes in the first RNN and INLINEFORM6 in the second, so altogether the new RNN has INLINEFORM7 hidden nodes.\n\n[Document 3]: The output of the new RNN is equal to the summed output of the two original RNNs, and from the proofs of Theorems 1.1 and 1.10 these outputs are always nonnegative. Thus the output of the new RNN is INLINEFORM0 if and only if the outputs of both old RNNs were INLINEFORM1 , immediately proving the theorem. INLINEFORM2\n\n[Document 4]: Discussion 1.12. This result shows that simple RNNs with arbitrary precision are at least as computationally powerful as PDAs.\n\nQuestion: How do they prove that RNNs with arbitrary precision are as powerful as a pushdown automata?\n\nExplanation:"}
{"question_id": "014a3aa07686ee18a86c977bf0701db082e8480b", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: In this section, we evaluate the proposed method intrinsically in terms of whether the co-occurrence matrix after the low-rank approximation is able to capture similar concepts on student response data sets, and also extrinsically in terms of the end task of summarization on all corpora. In the following experiments, summary length is set to be the average number of words in human summaries or less. For the matrix completion algorithm, we perform grid search (on a scale of [0, 5] with stepsize 0.5) to tune the hyper-parameter INLINEFORM0 (Eq. EQREF10 ) with a leave-one-lecture-out (for student responses) or leave-one-task-out (for others) cross-validation.\n\n[Document 2]: The results are shown in Table TABREF25 . INLINEFORM0 significantly on all three courses. That is, a bigram does receive a higher partial score in a sentence that contains similar bigram(s) to it than a sentence that does not. Therefore, H1.a holds. For H1.b, we only observe INLINEFORM1 significantly on Stat2016 and there is no significant difference between INLINEFORM2 and INLINEFORM3 on the other two courses. First, the gold-standard data set is still small in the sense that only a limited portion of bigrams in the entire data set are evaluated. Second, the assumption that phrases annotated by different colors are not necessarily unrelated is too strong. For example, \u201chypothesis testing\" and \u201cH1 and Ho conditions\" are in different colors in the example of Table TABREF15 , but one is a subtopic of the other. An alternative way to evaluate the hypothesis is to let humans judge whether two bigrams are similar or not, which we leave for future work. Third, the gold standards are pairs of semantically similar bigrams, while matrix completion captures bigrams that occurs in a similar context, which is not necessarily equivalent to semantic similarity. For example, the sentence \u201cgraphs make it easier to understand concepts\" in Table TABREF25 is associated with \u201chard to\".\n\nQuestion: Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?\n\nExplanation:  [Document 1] mentions that they will be evaluating \"intrinsically in terms of whether the co-occurrence matrix after the low-rank approximation is able to capture similar concepts\". This is a quantitative evaluation. [Document 2] mentions that they are using a \"leave-one-lecture-out (for student responses) or leave-one-task-out (for others) cross-validation\" which is also a quantitative evaluation.\n\nAnswer: They evaluate quantitatively.\n\nExample 2:\n\n[Document 1]: Our approach consists of structuring input to the transformer network to use and guide the self-attention of the transformers, conditioning it on the entity. Our main mode of encoding the input, the entity-first method, is shown in Figure FIGREF4. The input sequence begins with a [START] token, then the entity under consideration, then a [SEP] token. After each sentence, a [CLS] token is used to anchor the prediction for that sentence. In this model, the transformer can always observe the entity it should be primarily \u201cattending to\u201d from the standpoint of building representations. We also have an entity-last variant where the entity is primarily observed just before the classification token to condition the [CLS] token's self-attention accordingly. These variants are naturally more computationally-intensive than post-conditioned models, as we need to rerun the transformer for each distinct entity we want to make a prediction for.\n\n[Document 2]: As an additional variation, we can either run the transformer once per document with multiple [CLS] tokens (a document-level model as shown in Figure FIGREF4) or specialize the prediction to a single timestep (a sentence-level model). In a sentence level model, we formulate each pair of entity $e$ and process step $t$ as a separate instance for our classification task. Thus, for a process with $T$ steps and $m$ entities we get $T \\times m$ input sequences for fine tuning our classification task.\n\nQuestion: In what way is the input restructured?\n\nExplanation: \n\nAccording to [Document 1], the input is restructured in four entity-centric ways - entity-first, entity-last, document-level and sentence-level. This information is further supported by [Document 2].\n\nAnswer: In four entity-centric ways - entity-first, entity-last, document-level and sentence-level\n\nExample 3:\n\n[Document 1]: In the TCM prescription generation task, the textual symptom descriptions can be seen as the question and the aim of the task is to produce a set of TCM herbs that form a prescription as the answer to the question. However, the set of herbs is different from the textual answers to a question in the QA task. A difference that is most evident is that there will not be any duplication of herbs in the prescription. However, the basic seq2seq model sometimes produces the same herb tokens repeatedly when applied to the TCM prescription generation task. This phenomenon can hurt the performance of recall rate even after applying a post-process to eliminate repetitions. Because in a limited length of the prescription , the model would produce the same token over and over again, rather than real and novel ones. Furthermore, the basic seq2seq assumes a strict order between generated tokens, but in reality, we should not severely punish the model when it predicts the correct tokens in the wrong order. In this paper, we explore to automatically generate TCM prescriptions based on textual symptoms. We propose a soft seq2seq model with coverage mechanism and a novel soft loss function. The coverage mechanism is designed to make the model aware of the herbs that have already been generated while the soft loss function is to relieve the side effect of strict order assumption. In the experiment results, our proposed model beats all the baselines in professional evaluations, and we observe a large increase in both the recall rate and the F1 score compared with the basic seq2seq model.\n\nQuestion: Do they impose any grammatical constraints over the generated output?\n\nExplanation:  There is no mention of grammatical constraints in [Document 1].\n\nAnswer: No\n\nExample 4:\n\n[Document 1]: Many classifiers can predict a score or confidence about the prediction. Turning this score into the prediction is usually performed by setting a threshold, such as 0 and 0.5, so labels which have a score assigned greater than that are assigned to the sample. This might be not the optimal threshold in the multi-label classification setup and there are many approaches to set it (BIBREF9). Although these methods concentrate in the sample or label, we have had good results with a much more general approach.\n\n[Document 2]: As described in BIBREF1, Read and Pfahringer BIBREF10 introduce a method (referred hereinafter to as LCA) to estimate the threshold globally. Their method chooses the threshold that minimizes the difference between the label cardinality of the training set and the predicted set.\n\n[Document 3]: where $LCard(D_T)$ denotes the label cardinality of training set and $LCard(H_t(D_S))$ the label cardinality of the predictions on test set if $t$ was applied as the threshold. For that the predictions need to be normalized to unity. We also tested this method not for the label cardinality over all samples and labels but only labelwise. In our implementation, the scores of the SVM were not normalized, which produced slightly different results from a normalized approach.\n\n[Document 4]: For the HMC subtask B, we used a simple threshold based on the results obtained for LCA. Especially, using multiple models per node could cause a different scaling.\n\nQuestion: What does post-processing do to the output?\n\nExplanation:"}
{"question_id": "f3b4e52ba962a0004064132d123fd9b78d9e12e2", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Recently, neural models pre-trained on a language modeling task, such as ELMo BIBREF0 , OpenAI GPT BIBREF1 , and BERT BIBREF2 , have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. The success of BERT can largely be associated to the notion of context-aware word embeddings, which differentiate it from common approaches such as word2vec BIBREF3 that establish a static semantic embedding. Since the introduction of BERT, the NLP community continues to be impressed by the amount of ideas produced on top of this powerful language representation model. However, despite its success, it remains unclear whether the representations produced by BERT can be utilized for tasks such as commonsense reasoning. Particularly, it is not clear whether BERT shed light on solving tasks such as the Pronoun Disambiguation Problem (PDP) and Winograd Schema Challenge (WSC). These tasks have been proposed as potential alternatives to the Turing Test, because they are formulated to be robust to statistics of word co-occurrence BIBREF4 .\n\n[Document 2]: In this paper, we show that the attention maps created by an out-of-the-box BERT can be directly exploited to resolve coreferences in long sentences. As such, they can be simply repurposed for the sake of commonsense reasoning tasks while achieving state-of-the-art results on the multiple task. On both PDP and WSC, our method outperforms previous state-of-the-art methods, without using expensive annotated knowledge bases or hand-engineered features. On a Pronoun Disambiguation dataset, PDP-60, our method achieves 68.3% accuracy, which is better than the state-of-art accuracy of 66.7%. On a WSC dataset, WSC-273, our method achieves 60.3%. As of today, state-of-the-art accuracy on the WSC-273 for single model performance is around 57%, BIBREF14 and BIBREF13 . These results suggest that BERT implicitly learns to establish complex relationships between entities such as coreference resolution. Although this helps in commonsense reasoning, solving this task requires more than employing a language model learned from large text corpora.\n\nQuestion: Which datasets do they evaluate on?\n\nExplanation: \n\nAccording to [Document 2], the PDP-60 and WSC-273 datasets were used for evaluation.\n\nAnswer: PDP-60, WSC-273\n\nExample 2:\n\n[Document 1]: Traditional text-to-speech (TTS) systems are composed of complex pipelines BIBREF0 , these often include accoustic frontends, duration model, acoustic prediction model and vocoder models. The complexity of the TTS problem coupled with the requirement for deep domain expertise means these systems are often brittle in design and results in un-natural synthesized speech.\n\n[Document 2]: The open source LJSpeech Dataset was used to train our TTS model. This dataset contains around 13k <text,audio> pairs of a single female english speaker collect from across 7 different non-fictional books. The total training data time is around 21 hours of audio.\n\n[Document 3]: The architecture of our model utilizes RNN-based Seq2Seq model for generating mel spectrogram from text. The architecture is similar to that of Tacotron 2 BIBREF4 . The generated mel spetrogram can either be inverted via iterative algorithms such as Griffin Lim, or through more complicated neural vocoder networks such as a mel spectrogram conditioned Wavenet BIBREF11 .\n\nQuestion: Which dataset(s) do they evaluate on?\n\nExplanation:  [Document 2] mentions that the LJSpeech dataset was used to train the TTS model. This dataset was used to evaluate the model.\n\nAnswer: LJSpeech\n\nExample 3:\n\n[Document 1]: Datasets\n\n[Document 2]: We conduct experiments on two Chinese question answering datasets from NLPCC-2016 evaluation task BIBREF13 .\n\n[Document 3]: DBQA is a document based question answering dataset. There are 8.8k questions with 182k question-sentence pairs for training and 6k questions with 123k question-sentence pairs in the test set. In average, each question has 20.6 candidate sentences and 1.04 golden answers. The average length for questions is 15.9 characters, and each candidate sentence has averagely 38.4 characters. Both questions and sentences are natural language sentences, possibly sharing more similar word choices and expressions compared to the KBQA case. But the candidate sentences are extracted from web pages, and are often much longer than the questions, with many irrelevant clauses.\n\n[Document 4]: KBRE is a knowledge based relation extraction dataset. We follow the same preprocess as BIBREF14 to clean the dataset and replace entity mentions in questions to a special token. There are 14.3k questions with 273k question-predicate pairs in the training set and 9.4k questions with 156k question-predicate pairs for testing. Each question contains only one golden predicate. Each question averagely has 18.1 candidate predicates and 8.1 characters in length, while a KB predicate is only 3.4 characters long on average. Note that a KB predicate is usually a concise phrase, with quite different word choices compared to the natural language questions, which poses different challenges to solve.\n\nQuestion: Which dataset(s) do they evaluate on?\n\nExplanation:  [Document 2] and [Document 3] both state that the DBQA dataset was used. [Document 4] states that the KBRE dataset was used.\n\nAnswer: DBQA, KBRE\n\nExample 4:\n\n[Document 1]: We use a subset of the data available for NIST OpenMT08 task . The parallel training corpus contains approximate 2 million sentence pairs. We choose NIST 2006 (NIST06) dataset as our development set, and the NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets as our test sets. We will use this dataset to evaluate the performance of our partial decoding and context-aware decoding strategy from the perspective of translation quality and latency.\n\nQuestion: Which datasets do they evaluate on?\n\nExplanation:"}
{"question_id": "1e7e3f0f760cd628f698b73d82c0f946707855ca", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: Which languages do they focus on?\n\nExplanation:  The documents do not mention any languages specifically.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: We investigate both methods, either in isolation or combined, on two translation directions (En-It and En-De) for which the length of the target is on average longer than the length of the source. Our ultimate goal is to generate translations whose length is not longer than that of the source string (see example in Table FIGREF1). While generating translations that are just a few words shorter might appear as a simple task, it actually implies good control of the target language. As the reported examples show, the network has to implicitly apply strategies such as choosing shorter rephrasing, avoiding redundant adverbs and adjectives, using different verb tenses, etc. We report MT performance results under two training data conditions, small and large, which show limited degradation in BLEU score and n-gram precision as we vary the target length ratio of our models. We also run a manual evaluation which shows for the En-It task a slight quality degradation in exchange of a statistically significant reduction in the average length ratio, from 1.05 to 1.01.\n\nQuestion: Which languages do they focus on?\n\nExplanation:  The languages that they focus on are two translation directions (En-It and En-De) according to [Document 1].\n\nAnswer: two translation directions (En-It and En-De)\n\nExample 3:\n\n[Document 1]: Our data are collected from the yelp academic dataset, provided by Yelp.com, a popular restaurant review website. The data set contains three types of objects: business, user, and review, where business objects contain basic information about local businesses (i.e. restaurants), review objects contain review texts and star rating, and user objects contain aggregate information about a single user across all of Yelp. Table TABREF31 illustrates the general statistics of the dataset.\n\nQuestion: Does they focus on any specific product/service domain?\n\nExplanation: \n\nAccording to [Document 1], the data set contains information about local businesses (i.e. restaurants). This means that the data set focuses on the product/service domain of local businesses (i.e. restaurants).\n\nAnswer: local businesses (i.e. restaurants)\n\nExample 4:\n\n[Document 1]: Verb Phrase Data The pattern assignment uses the phrase distribution INLINEFORM0 . To do this, we use the \u201cEnglish All\u201d dataset in Google Syntactic N-Grams. The dataset contains counted syntactic ngrams extracted from the English portion of the Google Books corpus. It contains 22,230 different verbs (without stemming), and 147,056 verb phrases. For a fixed verb, we compute the probability of phrase INLINEFORM1 by: DISPLAYFORM0\n\nQuestion: do they focus on english verbs?\n\nExplanation:"}
{"question_id": "c7d3bccee59ab683e6bf047579bc6eab9de9d973", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: To evaluate the performance of the proposed approach, precision (1), recall (2), f-Measure (3), and prediction accuracy (4) have been used as a performance matrices. The experimental results are shown in Table 1, where it can be seen that autoencoders outperformed MLP and CNN outperformed autoencoders with the highest achieved accuracy of 82.6%. DISPLAYFORM0 DISPLAYFORM1\n\nQuestion: Which deep learning model performed better?\n\nExplanation: \n\nAccording to [Document 1], autoencoders outperformed MLP and CNN.\n\nAnswer: autoencoders\n\nExample 2:\n\n[Document 1]: The library design of Torch-Struct follows the distributions API used by both TensorFlow and PyTorch BIBREF29. For each structured model in the library, we define a conditional random field (CRF) distribution object. From a user's standpoint, this object provides all necessary distributional properties. Given log-potentials (scores) output from a deep network $\\ell $, the user can request samples $z \\sim \\textsc {CRF}(\\ell )$, probabilities $\\textsc {CRF}(z;\\ell )$, modes $\\arg \\max _z \\textsc {CRF}(\\ell )$, or other distributional properties such as $\\mathbb {H}(\\textsc {CRF}(\\ell ))$. The library is agnostic to how these are utilized, and when possible, they allow for backpropagation to update the input network. The same distributional object can be used for standard output prediction as for more complex operations like attention or reinforcement learning.\n\nQuestion: Does API provide ability to connect to models written in some other deep learning framework?\n\nExplanation:  [Document 1] states that the library is agnostic to how the distributional properties are utilized. This means that the API can be used with models written in some other deep learning framework.\n\nAnswer: Yes\n\nExample 3:\n\nQuestion: Is this AD implementation used in any deep learning framework?\n\nExplanation:  There is not enough information in the documents to answer the question.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: Our current language model approach is effective but does not account for out of vocabulary words nor long distance dependencies. CNNs in combination with LSTMs seem to be a particularly promising way to overcome these limitations (e.g., BIBREF12 ) which we will explore and compare to our existing results.\n\n[Document 2]: After evaluating CNNs and LSTMs we will explore how to include domain knowledge in these models. One possibility is to create word embeddings from domain specific materials and provide those to the CNNs along with more general text. Another is to investigate the use of Tree\u2013Structured LSTMs BIBREF13 . These have the potential advantage of preserving non-linear structure in text, which may be helpful in recognizing some of the unusual variations of words and concepts that are characteristic of humor.\n\nQuestion: What deep learning models do they plan to use?\n\nExplanation:"}
{"question_id": "25f699c7a33e77bd552782fb3886b9df9d02abb2", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We compile three Chinese text corpora from online data for three domains, namely, \u201chotel\", \u201cmobile phone (mobile)\", and \u201ctravel\". All texts are about user reviews. Each text sample collected is first partitioned into clauses according to Chinese tokens. Three clause sets are subsequently obtained from the three text corpora.\n\nQuestion: What are the sources of the data?\n\nExplanation: \n\nAccording to [Document 1], the data is from user reviews written in Chinese collected online for hotel, mobile phone, and travel domains.\n\nAnswer: User reviews written in Chinese collected online for hotel, mobile phone, and travel domains\n\nExample 2:\n\n[Document 1]: We test our proposed approach for binary classification on either sarcasm or irony, on seven benchmark datasets retrieved from different media sources. Below we describe each dataset, please see Table TABREF1 below for a summary.\n\n[Document 2]: Reddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol.\n\n[Document 3]: Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 . The dataset was manually annotated using binary labels. We also use the dataset by BIBREF4 , which is manually annotated for sarcasm. Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag.\n\n[Document 4]: Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC). Compared to other datasets in our selection, these differ mainly in text length and structure complexity BIBREF22 .\n\nQuestion: What are the three different sources of data?\n\nExplanation: \n\nAccording to [Document 1], the data was collected from seven different sources. This information is further supported by [Document 2], [Document 3], and [Document 4].\n\nAnswer: Twitter, Reddit, Online Dialogues\n\nExample 3:\n\nQuestion: Which data sources do they use?\n\nExplanation: \n\n[Document 1]: We use a number of data sources for training our models. For machine translation, we use the En-Fr (WMT14) and En-De (WMT15) datasets. For language modeling, we use the Skipthought (BookCorpus) dataset. For natural language inference, we use the AllNLI (SNLI + MultiNLI) dataset. For parsing, we use the PTB + 1-billion word dataset.\n\nAnswer: - En-Fr (WMT14)\n- En-De (WMT15)\n- Skipthought (BookCorpus)\n- AllNLI (SNLI + MultiNLI)\n- Parsing (PTB + 1-billion word)\n\nExample 4:\n\n[Document 1]: FLOAT SELECTED: Table 2: Data sources, prevalence and category of text\n\nQuestion: What sources did they get the data from?\n\nExplanation:"}
{"question_id": "62edffd051d056cf60e17deafcc55a8c9af398cb", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We use the word embeddings for Vietnamese that created by Kyubyong Park and Edouard Grave at al:\n\n[Document 2]: Kyubyong Park: In his project, he uses two methods including fastText and word2vec to generate word embeddings from wikipedia database backup dumps. His word embedding is the vector of 100 dimension and it has about 10k words.\n\n[Document 3]: Edouard Grave et al BIBREF11: They use fastText tool to generate word embeddings from Wikipedia. The format is the same at Kyubyong's, but their embedding is the vector of 300 dimension, and they have about 200k words\n\nQuestion: What word embeddings were used?\n\nExplanation: \n\nAccording to [Document 1], the word embeddings used were created by Kyubyong Park and Edouard Grave et al. This information is further supported by [Document 2] and [Document 3].\n\nAnswer: Kyubyong Park, Edouard Grave et al BIBREF11\n\nExample 2:\n\n[Document 1]: Finally, a comparison was made between the Skip-gram model W10N20 obtained at the 50th epoch and the other two W2V in Italian present in the literature (BIBREF9 and BIBREF10). The first test (Table TABREF15) was performed considering all the analogies present, and therefore evaluating as an error any analogy that was not executable (as it related to one or more words absent from the vocabulary).\n\n[Document 2]: As it can be seen, regardless of the metric used, our model has significantly better results than the other two models, both overall and within the two macro-areas. Furthermore, the other two models seem to be more subject to the metric used, perhaps due to a stabilization not yet reached for the few training epochs.\n\nQuestion: Are the word embeddings evaluated?\n\nExplanation:  [Document 1] states that a comparison was made between the Skip-gram model and the other two W2V in Italian. This comparison can be seen as an evaluation of the word embeddings. This is further supported by [Document 2], which states that the Skip-gram model had significantly better results than the other two models.\n\nAnswer: Yes\n\nExample 3:\n\n[Document 1]: In BIBREF14 , several word embedding models trained in a large Portuguese corpus are evaluated. Within the Word2Vec model, two training strategies were used. In the first, namely Skip-Gram, the model is given the word and attempts to predict its neighboring words. The second, Continuous Bag-of-Words (CBOW), the model is given the sequence of words without the middle one and attempts to predict this omitted word. The latter was chosen for application in the present proposal.\n\nQuestion: Which word embeddings are analysed?\n\nExplanation:  [Document 1] states that the Continuous Bag-of-Words (CBOW) model was chosen for application in the present proposal.\n\nAnswer: Continuous Bag-of-Words (CBOW)\n\nExample 4:\n\n[Document 1]: Multilingual word embeddings. We use the MUSE BIBREF17 pre-trained multilingual word embeddings and keep them fixed during training.\n\nQuestion: What is the source of the crosslingual word embeddings?\n\nExplanation:"}
{"question_id": "81e101b2c803257492d67a00e8a1d9a07cbab136", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. In that version, Stacked DeBERT achieves 82$\\%$ accuracy against BERT's 76$\\%$, an improvement of 6$\\%$. In the last case (Inc+Corr), we consider both incorrect and correct tweets as input to the models in hopes of improving performance. However, the accuracy was similar to the first aforementioned version, 80$\\%$ for our model and 74$\\%$ for the second highest performing model. Since the first and last corpus gave similar performances with our model, we conclude that the Twitter dataset does not require complete sentences to be given as training input, in addition to the original naturally incorrect tweets, in order to better model the noisy sentences.\n\nQuestion: Should their approach be applied only when dealing with incomplete data?\n\nExplanation: \n\nAccording to [Document 1], the approach should not be applied only when dealing with incomplete data. This is supported by the fact that the model's performance was similar in the first and last corpus, which suggests that the approach does not require complete sentences to be given as training input.\n\nAnswer: No\n\nExample 2:\n\n[Document 1]: The dataset MSParS is published by NLPCC 2019 evaluation task. The whole dataset consists of 81,826 samples annotated by native English speakers. 80% of them are used as training set. 10% of them are used as validation set while the rest is used as test set. 3000 hard samples are selected from the test set. Metric for this dataset is the exactly matching accuracy on both full test set and hard test subset. Each sample is composed of the question, the logical form, the parameters(entity/value/type) and question type as the Table TABREF3 demonstrates.\n\nQuestion: Does the training dataset provide logical form supervision?\n\nExplanation:  [Document 1] states that the training dataset consists of 81,826 samples annotated by native English speakers. These annotations include logical form supervision.\n\nAnswer: Yes\n\nExample 3:\n\n[Document 1]: To better exploit such existing data sources, we propose an end-to-end (E2E) model based on pointer networks with attention, which can be trained end-to-end on the input/output pairs of human IE tasks, without requiring token-level annotations.\n\n[Document 2]: Since our model does not need token-level labels, we create an E2E version of each data set without token-level labels by chunking the BIO-labeled words and using the labels as fields to extract. If there are multiple outputs for a single field, e.g. multiple destination cities, we join them with a comma. For the ATIS data set, we choose the 10 most common labels, and we use all the labels for the movie and restaurant corpus. The movie data set has 12 fields and the restaurant has 8. See Table 2 for an example of the E2E ATIS data set.\n\nQuestion: Do they assume sentence-level supervision?\n\nExplanation:  The model proposed in [Document 1] does not require token-level annotations, as mentioned in the first sentence of [Document 2]. This means that the model can be trained on data that only has sentence-level labels, without needing token-level labels.\n\nAnswer: No\n\nExample 4:\n\n[Document 1]: The supervision is described under each task, e.g., there are three (conflicting) sources for the Intent task. A task requires labels at the appropriate granularity (singleton, sequence, or set) and type (multiclass or bitvector). The labels are tagged by the source that produced them: these labels may be incomplete and even contradictory. Overton models the sources of these labels, which may come human annotators, or from engineer-defined heuristics such as data augmentation or heuristic labelers. Overton learns the accuracy of these sources using ideas from the Snorkel project BIBREF1. In particular, it estimates the accuracy of these sources and then uses these accuracies to compute a probability that each training point is correct BIBREF9. Overton incorporates this information into the loss function for a task; this also allows Overton to automatically handle common issues like rebalancing classes.\n\nQuestion: How does Overton handles contradictory or incomplete supervision data?\n\nExplanation:"}
{"question_id": "a7313c29b154e84b571322532f5cab08e9d49e51", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: In this case, the decoding function is a linear projection, which is $= f_{\\text{de}}()=+ $ , where $\\in ^{d_\\times d_}$ is a trainable weight matrix and $\\in ^{d_\\times 1}$ is the bias term.\n\n[Document 2]: A family of bijective transformation was designed in NICE BIBREF17 , and the simplest continuous bijective function $f:^D\\rightarrow ^D$ and its inverse $f^{-1}$ is defined as:\n\n[Document 3]: $$h: \\hspace{14.22636pt} _1 &= _1, & _2 &= _2+m(_1) \\nonumber \\\\ h^{-1}: \\hspace{14.22636pt} _1 &= _1, & _2 &= _2-m(_1) \\nonumber $$ (Eq. 15)\n\n[Document 4]: where $_1$ is a $d$ -dimensional partition of the input $\\in ^D$ , and $m:^d\\rightarrow ^{D-d}$ is an arbitrary continuous function, which could be a trainable multi-layer feedforward neural network with non-linear activation functions. It is named as an `additive coupling layer' BIBREF17 , which has unit Jacobian determinant. To allow the learning system to explore more powerful transformation, we follow the design of the `affine coupling layer' BIBREF24 :\n\n[Document 5]: $$h: \\hspace{5.69046pt} _1 &= _1, & _2 &= _2 \\odot \\text{exp}(s(_1)) + t(_1) \\nonumber \\\\ h^{-1}: \\hspace{5.69046pt} _1 &= _1, & _2 &= (_2-t(_1)) \\odot \\text{exp}(-s(_1)) \\nonumber $$ (Eq. 16)\n\n[Document 6]: where $s:^d\\rightarrow ^{D-d}$ and $t:^d\\rightarrow ^{D-d}$ are both neural networks with linear output units.\n\n[Document 7]: The requirement of the continuous bijective transformation is that, the dimensionality of the input $$ and the output $$ need to match exactly. In our case, the output $\\in ^{d_}$ of the decoding function $f_{\\text{de}}$ has lower dimensionality than the input $\\in ^{d_}$ does. Our solution is to add an orthonormal regularised linear projection before the bijective function to transform the vector representation of a sentence to the desired dimension.\n\nQuestion: What are the two decoding functions?\n\nExplanation:  [Document 1] explains that one decoding function is a linear projection. [Document 4] and [Document 5] explain that the other decoding function is a bijective function with continuous transformation though  \u2018affine coupling layer\u2019 of (Dinh et al.,2016).\n\nAnswer: a linear projection and a bijective function with continuous transformation though  \u2018affine coupling layer\u2019 of (Dinh et al.,2016). \n\nExample 2:\n\n[Document 1]: In order to address the aforementioned problems we redesign the neural decoder in three different scenarios. In the first scenario we equip the decoder with an additional morphology table including target-side affixes. We place an attention module on top of the table which is controlled by the decoder. At each step, as the decoder samples a character, it searches the table to find the most relevant information which can enrich its state. Signals sent from the table can be interpreted as additional constraints. In the second scenario we share the decoder between two output channels. The first one samples the target character and the other one predicts the morphological annotation of the character. This multi-tasking approach forces the decoder to send morphology-aware information to the final layer which results in better predictions. In the third scenario we combine these two models. Section \"Proposed Architecture\" provides more details on our models.\n\nQuestion: How are the auxiliary signals from the morphology table incorporated in the decoder?\n\nExplanation: \n\nAccording to [Document 1], the decoder is equipped with an additional morphology table including target-side affixes. This information is further supported by [Document 2], [Document 3], and [Document 5].\n\nAnswer: an additional morphology table including target-side affixes., We inject the decoder with morphological properties of the target language.\n\nExample 3:\n\n[Document 1]: Our baseline decoder is a standard beam search decoder BIBREF5 with several straightforward performance optimizations:\n\nQuestion: What baseline decoder do they use?\n\nExplanation: \n\n[Document 1] mentions that the baseline decoder is a standard beam search decoder with several straightforward performance optimizations.\n\nAnswer: a standard beam search decoder BIBREF5 with several straightforward performance optimizations\n\nExample 4:\n\n[Document 1]: The decoder consists of $M$ layer decoder blocks. The inputs of the decoder are the output of the encoder $H_e^M$ and the output of the previous step of the decoder $\\lbrace y_1,...,y_{t-1} \\rbrace $. The output through the $M$ layer Transformer decoder blocks is defined as\n\n[Document 2]: In each step $t$, the $h_{dt}^M$ is projected to blackthe vocabulary space and the decoder outputs the highest probability token as the next token. The Transformer decoder block consists of a self-attention module, a context-attention module, and a two-layer feed-forward network.\n\nQuestion: What is the architecture of the decoder?\n\nExplanation:"}
{"question_id": "d976c22e9d068e4e31fb46e929023459f8290a63", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: In this section, we compare our model with state-of-the-art systems, including those with different degrees of supervision. The baselines include: (1) Procrustes BIBREF11, which learns a linear mapping through Procrustes Analysis BIBREF36. (2) GPA BIBREF37, an extension of Procrustes Analysis. (3) GeoMM BIBREF38, a geometric approach which learn a Mahalanobis metric to refine the notion of similarity. (4) GeoMM$_{semi}$, iterative GeoMM with weak supervision. (5) Adv-C-Procrustes BIBREF11, which refines the mapping learned by Adv-C with iterative Procrustes, which learns the new mapping matrix by constructing a bilingual lexicon iteratively. (6) Unsup-SL BIBREF13, which integrates a weak unsupervised mapping with a robust self-learning. (7) Sinkhorn-BT BIBREF28, which combines sinkhorn distance BIBREF29 and back-translation. For fair comparison, we integrate our model with two iterative refinement methods (Procrustes and GeoMM$_{semi}$).\n\nQuestion: What are current state-of-the-art methods that consider the two tasks independently?\n\nExplanation:  [Document 1] lists seven current state-of-the-art methods that consider the two tasks independently.\n\nAnswer: Procrustes, GPA, GeoMM, GeoMM$_{semi}$, Adv-C-Procrustes, Unsup-SL, Sinkhorn-BT\n\nExample 2:\n\nQuestion: Is the word segmentation method independently evaluated?\n\nExplanation: \n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task\n\nAnswer: No\n\nExample 3:\n\n[Document 1]: Two-talker overlapped speech is artificially generated by mixing these waveform segments. To maximize the speech overlap, we developed a procedure to mix similarly sized segments at around 0dB. First, we sort the speech segments by length. Then, we take segments in pairs, zero-padding the shorter segment so both have the same length. These pairs are then mixed together to create the overlapped speech data. The overlapping procedure is similar to BIBREF13 except that we make no modification to the signal levels before mixing . After overlapping, there's 150 hours data in the training, called 150 hours dataset, and 915 utterances in the test set. After decoding, there are 1830 utterances for evaluation, and the shortest utterance in the hub5e-swb dataset is discarded. Additionally, we define a small training set, the 50 hours dataset, as a random 50 hour subset of the 150 hours dataset. Results are reported using both datasets.\n\nQuestion: How are the two datasets artificially overlapped?\n\nExplanation:  [Document 1] explains that the two datasets are artificially overlapped by sorting the speech segments by length, taking segments in pairs, zero-padding the shorter segment so both have the same length, and then mixing the pairs together.\n\nAnswer: we sort the speech segments by length, we take segments in pairs, zero-padding the shorter segment so both have the same length, These pairs are then mixed together\n\nExample 4:\n\n[Document 1]: In this method, paragraphs are encoded separately, and the concatenation of the resulted encoding is going through the classifier. First, each paragraph is encoded with LSTM. The hidden state at the end of each sentence is extracted, and the resulting matrix is going through gated CNN BIBREF1 for extraction of single encoding for each paragraph. The accuracy is barely above $50\\%$ , which depicts that this method is not very promising.\n\nQuestion: Are the two paragraphs encoded independently?\n\nExplanation:"}
{"question_id": "e6bc11bd6cfd4b2138c29602b9b56fc5378a4293", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We perform our experiments using ActivityNet Captions dataset BIBREF2 that is considered as the standard benchmark for dense video captioning task. The dataset contains approximately 20k videos from YouTube and split into 50/25/25 % parts for training, validation, and testing, respectively. Each video, on average, contains 3.65 temporally localized captions, around 13.65 words each, and two minutes long. In addition, each video in the validation set is annotated twice by different annotators. We report all results using the validation set (no ground truth is provided for the test set).\n\nQuestion: What domain does the dataset fall into?\n\nExplanation:  The dataset contains videos from YouTube, so the domain is YouTube videos.\n\nAnswer: YouTube videos\n\nExample 2:\n\n[Document 1]: We further evaluate our approach on our main evaluation corpus. The method is tested on both in-domain and out-of-domain parsing. Our DLM-based approach achieved large improvement on all five domains evaluated (Conll, Weblogs, Newsgroups, Reviews, Answers). We achieved the labelled and unlabelled improvements of up to 0.91% and 0.82% on Newsgroups domain. On average we achieved 0.6% gains for both labelled and unlabelled scores on four out-of-domain test sets. We also improved the in-domain accuracy by 0.36% (LAS) and 0.4% (UAS).\n\nQuestion: Which English domains do they evaluate on?\n\nExplanation:  The domains are listed in [Document 1].\n\nAnswer: Conll, Weblogs, Newsgroups, Reviews, Answers\n\nExample 3:\n\n[Document 1]: With unsupervised domain adaptation, one has access to labeled sentence specificity in one source domain, and unlabeled sentences in all target domains. The goal is to predict the specificity of target domain data. Our source domain is news, the only domain with publicly available labeled data for training BIBREF1 . We crowdsource sentence specificity for evaluation for three target domains: Twitter, Yelp reviews and movie reviews. The data is described in Section SECREF4 .\n\nQuestion: What domains do they experiment with?\n\nExplanation:  The domains that they experiment with are Twitter, Yelp reviews and movie reviews. This information is found in [Document 1].\n\nAnswer: Twitter, Yelp reviews and movie reviews\n\nExample 4:\n\n[Document 1]: We used Bing Liu's dataset BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 . Aspects in these review datasets were annotated manually.\n\nQuestion: what domain do the opinions fall under?\n\nExplanation:"}
{"question_id": "98b97d24f31e9c535997e9b6cb126eb99fc72a90", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Once the KryptoOracle engine was bootstrapped with historical data, the real time streamer was started. The real-time tweets scores were calculated in the same way as the historical data and summed up for a minute and sent to the machine learning model with the Bitcoin price in the previous minute and the rolling average price. It predicted the next minute's Bitcoin price from the given data. After the actual price arrived, the RMS value was calculated and the machine learning model updated itself to predict with better understanding the next value. All the calculated values were then stored back to the Spark training RDD for storage. The RDD persisted all the data while training and check-pointed itself to the Hive database after certain period of time.\n\nQuestion: What experimental evaluation is used?\n\nExplanation: \n\nAccording to [Document 1], the experimental evaluation used is the root mean square error between the actual and the predicted price of Bitcoin for every minute.\n\nAnswer: root mean square error between the actual and the predicted price of Bitcoin for every minute\n\nExample 2:\n\n[Document 1]: The clustering performance is evaluated by comparing the clustering results of texts with the tags/labels provided by the text corpus. Two metrics, the accuracy (ACC) and the normalized mutual information metric (NMI), are used to measure the clustering performance BIBREF38 , BIBREF48 . Given a text INLINEFORM0 , let INLINEFORM1 and INLINEFORM2 be the obtained cluster label and the label provided by the corpus, respectively. Accuracy is defined as: DISPLAYFORM0\n\nQuestion: What were the evaluation metrics used?\n\nExplanation: \n\nAccording to [Document 1], accuracy and normalized mutual information were used to measure the clustering performance.\n\nAnswer: accuracy, normalized mutual information\n\nExample 3:\n\n[Document 1]: Our experimental results are shown in Table TABREF21. The first half of the table contains results for task-oriented dialogue with the Sequicity framework with two scenarios for training data preparation. For each experiment, we run our models for 3 times and their scores are averaged as the final score. The mixed training scenario performs the mixing of both the training data, development data and the test data as described in the previous subsection. The non-mixed training scenario performs the mixing only on the development and test data, keeps the training data unmixed as in the original KVRET dataset. As in the Sequicity framework, we report entity match rate, BLEU score and Success F1 score. Entity match rate evaluates task completion, it determines if a system can generate all correct constraints to search the indicated entities of the user. BLEU score evaluates the language quality of generated responses. Success F1 balances the recall and precision rates of slot answers. For further details on these metrics, please refer to BIBREF8.\n\nQuestion: What were the evaluation metrics used?\n\nExplanation:  The evaluation metrics used were entity match rate, BLEU score, and Success F1 score. This information is found in [Document 1].\n\nAnswer: entity match rate, BLEU score, Success F1 score\n\nExample 4:\n\n[Document 1]: With regard to research question RQ2 on the understanding of semantic and linguistic nuances between fake news and satire - a key advantage of studying the coherence metrics is explainability. While the pre-trained model of BERT gives the best result, it is not easily interpretable. The coherence metrics allow us to study the differences between fake news and satire in a straightforward manner.\n\nQuestion: What empirical evaluation was used?\n\nExplanation:"}
{"question_id": "3d662fb442d5fc332194770aac835f401c2148d9", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Three different datasets have been used to train our models: the Toronto book corpus, Wikipedia sentences and tweets. The Wikipedia and Toronto books sentences have been tokenized using the Stanford NLP library BIBREF10 , while for tweets we used the NLTK tweets tokenizer BIBREF11 . For training, we select a sentence randomly from the dataset and then proceed to select all the possible target unigrams using subsampling. We update the weights using SGD with a linearly decaying learning rate.\n\n[Document 2]: Unsupervised Similarity Evaluation Results. In Table TABREF19 , we see that our Sent2Vec models are state-of-the-art on the majority of tasks when comparing to all the unsupervised models trained on the Toronto corpus, and clearly achieve the best averaged performance. Our Sent2Vec models also on average outperform or are at par with the C-PHRASE model, despite significantly lagging behind on the STS 2014 WordNet and News subtasks. This observation can be attributed to the fact that a big chunk of the data that the C-PHRASE model is trained on comes from English Wikipedia, helping it to perform well on datasets involving definition and news items. Also, C-PHRASE uses data three times the size of the Toronto book corpus. Interestingly, our model outperforms C-PHRASE when trained on Wikipedia, as shown in Table TABREF21 , despite the fact that we use no parse tree information. Official STS 2017 benchmark. In the official results of the most recent edition of the STS 2017 benchmark BIBREF35 , our model also significantly outperforms C-PHRASE, and in fact delivers the best unsupervised baseline method.\n\nQuestion: Do they report results only on English data?\n\nExplanation:  [Document 1] and [Document 2] both report results on English data from the Toronto book corpus, Wikipedia sentences, and tweets.\n\nAnswer: Yes\n\nExample 2:\n\n[Document 1]: A serious challenge in the field of AV is the lack of publicly available (and suitable) corpora, which are required to train and evaluate AV methods. Among the few publicly available corpora are those that were released by the organizers of the well-known PAN-AV competitions BIBREF11 , BIBREF5 , BIBREF12 . In regard to our experiments, however, we cannot use these corpora, due to the absence of relevant meta-data such as the precise time spans where the documents have been written as well as the topic category of the texts. Therefore, we decided to compile our own corpora based on English documents, which we crawled from different publicly accessible sources. In what follows, we describe our three constructed corpora, which are listed together with their statistics in Table TABREF23 . Note that all corpora are balanced such that verification cases with matching (Y) and non-matching (N) authorships are evenly distributed.\n\nQuestion: Do they report results only on English data?\n\nExplanation:  [Document 1] states that the corpora are based on English documents.\n\nAnswer: Yes\n\nExample 3:\n\nQuestion: Do they report results only on English data?\n\nExplanation: \n\n[Document 1] reports results only on English data. This is supported by [Document 2], which states that the spaCy 2.0 algorithm performs within 1% of the current state-of-the-art for English.\n\nAnswer: Yes\n\nExample 4:\n\n[Document 1]: We use 303 sub areas from Stack Exchange data dumps. The full list of area names is in the appendix. We do not include Stack Overflow because it is too specific to programming related questions. We also exclude all questions under the following language sub areas: Chinese, German, Spanish, Russian, Japanese, Korean, Latin, Ukrainian. This ensures that the questions in MQR are mostly English sentences. Having questions from 303 Stack Exchange sites makes the MQR dataset cover a broad range of domains.\n\nQuestion: Do they report results only on English data?\n\nExplanation:"}
{"question_id": "d9949dd4865e79c53284932d868ca8fd10d55e70", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: In this study, we constructed the first Japanese word similarity dataset. It contains various parts of speech and includes rare words in addition to common words. Crowdsourced annotators assigned similarity to word pairs during the word similarity task. We gave examples of similarity in the task request sent to annotators, so that we reduced the variance of each word pair. However, we did not restrict the attributes of words, such as the level of feeling, during annotation. Error analysis revealed that the notion of similarity should be carefully defined when constructing a similarity dataset.\n\nQuestion: did they use a crowdsourcing platform for annotations?\n\nExplanation:  [Document 1] states that \"Crowdsourced annotators assigned similarity to word pairs during the word similarity task.\" This indicates that a crowdsourcing platform was used for annotations.\n\nAnswer: Yes\n\nExample 2:\n\n[Document 1]: Reddit is popular website that allows registered users (without identity verification) to participate in fora grouped by topic or interest. Participation consists of posting stories that can be seen by other users, voting stories and comments, and comments in the story's comment section, in the form of a forum. The forums are arranged in the form of a tree, allowing nested conversations, where the replies to a comment are its direct responses. We collected all comments in the stories' conversation in Reddit that were posted in August 2015. Since it is infeasible to manually annotate all of the comments, we process this dataset with the goal of extracting threads that involve suspected trolling attempts and the direct responses to them. To do so, we used Lucene to create an inverted index from the comments and queried it for comments containing the word \u201ctroll\u201d with an edit distance of 1 in order to include close variations of this word, hypothesizing that such comments would be reasonable candidates of real trolling attempts. We did observe, however, that sometimes people use the word troll to point out that another user is trolling. Other times, people use the term to express their frustration about a particular user, but there is no trolling attempt. Yet other times people simply discuss trolling and trolls without actually observing one. Nonetheless, we found that this search produced a dataset in which 44.3% of the comments are real trolling attempts. Moreover, it is possible for commenters to believe that they are witnessing a trolling attempt and respond accordingly even where there is none due to misunderstanding. Therefore, the inclusion of comments that do not involve trolling would allow us to learn what triggers a user's interpretation of trolling when it is not present and what kind of response strategies are used.\n\n[Document 2]: We had two human annotators who were trained on snippets (i.e., (suspected trolling attempt, responses) pairs) taken from 200 conversations and were allowed to discuss their findings. After this training stage, we asked them to independently label the four aspects for each snippet. We recognize that this limited amount of information is not always sufficient to recover the four aspects we are interested in, so we give the annotators the option to discard instances for which they couldn't determine the labels confidently. The final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens. The distribution over the classes per trolling aspect is shown in the table TABREF19 in the column \u201cSize\u201d.\n\nQuestion: Do they use a crowdsourcing platform for annotation?\n\nExplanation:  [Document 1] mentions that they used two human annotators who were trained on snippets taken from 200 conversations. [Document 2] mentions that the final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens. There is no mention of a crowdsourcing platform.\n\nAnswer: No\n\nExample 3:\n\n[Document 1]: To collect article-level labels, we utilized a platform in the company that has been used by the market research team to collect surveys from the subscribers of different news publishers. The survey works as follows: The user is first presented with a set of selected pages (usually 4 pages and around 20 articles) from the print paper the day before. The user can select an article each time that he or she has read, and answer some questions about it. We added 3 questions to the existing survey that asked the level of partisanship, the polarity of partisanship, and which pro- or anti- entities the article presents. We also asked the political standpoint of the user. The complete survey can be found in Appendices.\n\nQuestion: Did they crowdsource the annotations?\n\nExplanation:  [Document 1] mentions that the company utilized a platform to collect article-level labels. This platform is a crowdsourcing platform that has been used by the market research team to collect surveys from the subscribers of different news publishers.\n\nAnswer: Yes\n\nExample 4:\n\n[Document 1]: Based on explicit annotation guidelines written in Greek and our proposal of the definition of offensive language, a team of three volunteers were asked to classify each tweet found in the dataset with one of the following tags: Offensive, Not Offensive and Spam, which was introduced to filter out spam from the dataset. Inter-annotator agreement was subsequently calculated and labels with 100% agreement were deemed acceptable annotations. In cases of disagreement, labels with majority agreement above 66% were selected as the actual annotations of the tweets in question. For labels with complete disagreement between annotators, one of the authors of this paper reviewed the tweets with two extra human judges, to get the desired majority agreement above 66%. Figure FIGREF6 is a confusion matrix that shows the inter-annotator agreement or reliability, statistically measured by Cohen's kappa coefficient. The benchmark annotated dataset produced contained 4,779 tweets, containing over 29% offensive content. The final distribution of labels in the new Offensive Greek Tweet Dataset (OGTD), along with the breakdown of the data into training and testing, is showing in Table TABREF5.\n\nQuestion: Did they use crowdsourcing for the annotations?\n\nExplanation:"}
{"question_id": "cf82251a6a5a77e29627560eb7c05c3eddc20825", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We first compare the perplexity of CWVAE with baseline methods. Perplexity measures the probability of model to regenerate the exact targets, which is particular suitable for evaluating the model performance on one-to-many problem BIBREF20. Further, we employ BLEU score to evaluate the accuracy of generations BIBREF21, and the number of distinct n-gram to evaluate the diversity of generations BIBREF6. The distinct is normalized to $[0, 1]$ by dividing the total number of generated tokens.\n\nQuestion: By how much do they improve the accuracy of inferences over state-of-the-art methods?\n\nExplanation: \n\nAccording to [Document 1], the proposed method improves the accuracy of inferences over state-of-the-art methods by an absolute BLUE score of 2.9, 10.87, 1.79 for xIntent, xReact and oReact respectively on the Event2Mind dataset, and by an absolute BLUE score of 3.95, 4.11, 4.49 for xIntent, xReact and oReact respectively on the Atomic dataset.\n\nAnswer: ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.\nOn Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively.\n\nExample 2:\n\n[Document 1]: We employ BIBREF73 's weighted lexicon of terms that uses the dataset of 75,394 Facebook users who shared their status, age and gender. The predictive power of this lexica was evaluated on Twitter, blog, and Facebook, showing promising results BIBREF73 . Utilizing these two weighted lexicon of terms, we are predicting the demographic information (age or gender) of INLINEFORM0 (denoted by INLINEFORM1 ) using following equation: INLINEFORM2\n\n[Document 2]: where INLINEFORM0 is the lexicon weight of the term, and INLINEFORM1 represents the frequency of the term in the user generated INLINEFORM2 , and INLINEFORM3 measures total word count in INLINEFORM4 . As our data is biased toward young people, we report age prediction performance for each age group separately (Table TABREF42 ). Moreover, to measure the average accuracy of this model, we build a balanced dataset (keeping all the users above 23 -416 users), and then randomly sampling the same number of users from the age ranges (11,19] and (19,23]. The average accuracy of this model is 0.63 for depressed users and 0.64 for control class. Table TABREF44 illustrates the performance of gender prediction for each class. The average accuracy is 0.82 on INLINEFORM5 ground-truth dataset.\n\nQuestion: How do this framework facilitate demographic inference from social media?\n\nExplanation:  [Document 1] explains that the lexicon is used to predict demographic information, and [Document 2] provides examples of how the lexicon is used to predict age and gender.\n\nAnswer: Demographic information is predicted using weighted lexicon of terms.\n\nExample 3:\n\n[Document 1]: Table TABREF34 shows the inference speed of our method when implementing the sequnece modeling layer with the LSTM-based, CNN-based, and Transformer-based architecture, respectively. The speed was evaluated by average sentences per second using a GPU (NVIDIA TITAN X). For a fair comparison with Lattice-LSTM and LR-CNN, we set the batch size of our method to 1 at inference time. From the table, we can see that our method has a much faster inference speed than Lattice-LSTM when using the LSTM-based sequence modeling layer, and it was also much faster than LR-CNN, which used an CNN architecture to implement the sequence modeling layer. And as expected, our method with the CNN-based sequence modeling layer showed some advantage in inference speed than those with the LSTM-based and Transformer-based sequence model layer.\n\nQuestion:  What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?\n\nExplanation: \n\nAccording to [Document 1], the proposed method with the CNN-based sequence modeling layer achieved an average of 363% improvement over the state of the art method (LR-CNN).\n\nAnswer: Across 4 datasets, the best performing proposed model (CNN) achieved an average of 363% improvement over the state of the art method (LR-CNN)\n\nExample 4:\n\n[Document 1]: We further show that regularised adaptation with EWC can reduce bias while limiting degradation in general translation quality. We also present a lattice rescoring procedure in which initial hypotheses produced by the biased baseline system are transduced to create gender-inflected search spaces which can be rescored by the adapted model. We believe this approach, rescoring with models targeted to remove bias, is novel in NMT. The rescoring procedure improves WinoMT accuracy by up to 30% with no decrease in BLEU on the general test set.\n\nQuestion: How does lattice rescoring improve inference?\n\nExplanation:"}
{"question_id": "b54525a0057aa82b73773fa4dacfd115d8f86f1c", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Feature-based models. We used state-of-the-art features that have shown to be useful in ID: some of them are language-independent (e.g., punctuation marks, positive and negative emoticons, quotations, personal pronouns, tweet's length, named entities) while others are language-dependent relying on dedicated lexicons (e.g., negation, opinion lexicons, opposition words). Several classical machine learning classifiers were tested with several feature combinations, among them Random Forest (RF) achieved the best result with all features. Neural model with monolingual embeddings. We used Convolutional Neural Network (CNN) network whose structure is similar to the one proposed by BIBREF29. For the embeddings, we relied on $AraVec$ BIBREF30 for Arabic, FastText BIBREF31 for French, and Word2vec Google News BIBREF32 for English . For the three languages, the size of the embeddings is 300 and the embeddings were fine-tuned during the training process. The CNN network was tuned with 20% of the training corpus using the $Hyperopt$ library.\n\nQuestion: What neural architectures are used?\n\nExplanation:  [Document 1] states that a Convolutional Neural Network (CNN) was used for the neural model.\n\nAnswer: Convolutional Neural Network (CNN)\n\nExample 2:\n\n[Document 1]: As neural classification model, we use a convolutional neural network (CNN) BIBREF29, which has previously shown good results for tweet classification BIBREF30, BIBREF27. The model performs 1d convolutions over a sequence of word embeddings. We use the same pre-trained fasttext embeddings as for the logistic regression model. We use a model with one convolutional layer and a relu activation function, and one max pooling layer. The number of filters is 100 and the filter size is set to 4.\n\nQuestion: What neural classifiers are used?\n\nExplanation:  [Document 1] mentions that a convolutional neural network (CNN) BIBREF29 is used as a neural classifier.\n\nAnswer:  convolutional neural network (CNN) BIBREF29\n\nExample 3:\n\n[Document 1]: To select the most appropriate sentences in a large number of unlabeled corpora, we propose a scoring model based on information entropy and neural network as the sampling strategy of active learning, which is inspired by Cai and Zhao BIBREF32 . The score of a segmented sentence is computed as follows. First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history. Fig. FIGREF10 illustrates the entire scoring model. A gated neural network is employed over character embeddings to generate distributed representations of candidate words, which are sent to a LSTM model.\n\nQuestion: Which neural network architectures are employed?\n\nExplanation: \n\nAccording to [Document 1], a gated neural network is employed over character embeddings to generate distributed representations of candidate words.\n\nAnswer: gated neural network \n\nExample 4:\n\n[Document 1]: Applications of Neuro-symbolism ::: Application I: Learning a Knowledge Graph Embedding Space for Context Understanding in Automotive Driving Scenes ::: Introduction\n\n[Document 2]: Applications of Neuro-symbolism ::: Application II: Neural Question-Answering using Commonsense Knowledge Bases ::: Introduction\n\nQuestion: What are the two applications of neuro-symbolism?\n\nExplanation:"}
{"question_id": "935873b97872820b7b6100d6a785fba286b94900", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: GANN is novel neural network model for APC task aimed to solve the shortcomings of traditional RNNs and CNNs. The GANN applied the Gate Truncation RNN (GTR) to learn informative aspect-dependent sentiment clue representations. GANN obtained the state-of-the-art APC performance on the Chinese review datasets.\n\nQuestion: What was previous state-of-the-art on four Chinese reviews datasets?\n\nExplanation: \n\nAccording to [Document 1], GANN obtained the state-of-the-art APC performance on the Chinese review datasets.\n\nAnswer: GANN obtained the state-of-the-art APC performance on the Chinese review datasets\n\nExample 2:\n\n[Document 1]: To comprehensive evaluate the performance of the proposed model, the experiments were conducted in three most commonly used ABSA datasets, the Laptops and Restaurant datasets of SemEval-2014 Task4 subtask2 BIBREF0 and an ACL Twitter social dataset BIBREF34. To evaluate our model capability with processing the Chinese language, we also tested the performance of LCF-ATEPC on four Chinese comment datasets BIBREF35, BIBREF36, BIBREF29 (Car, Phone, Notebook, Camera). We preprocessed the seven datasets. We reformatted the origin dataset and annotated each sample with the IOB labels for ATE task and polarity labels for APC tasks, respectively. The polarity of each aspect on the Laptops, Restaurants and datasets may be positive, neutral, and negative, and the conflicting labels of polarity are not considered. The reviews in the four Chinese datasets have been purged, with each aspect may be positive or negative binary polarity. To verify the effectiveness and performance of LCF-ATEPC models on multilingual datasets, we built a multilingual dataset by mixing the 7 datasets. We adopt this dataset to conduct multilingual-oriented ATE and APC experiments.\n\nQuestion: In what four Chinese review datasets does LCF-ATEPC achieves state of the art?\n\nExplanation: \n\nAccording to [Document 1], LCF-ATEPC achieves state of the art in four Chinese review datasets: Car, Phone, Notebook, Camera.\n\nAnswer: Car, Phone, Notebook, Camera\n\nExample 3:\n\n[Document 1]: For each dataset used to evaluate the proposed model, we compare our results with published accuracies of existing syllabification systems. Table TABREF21 shows the performance of well known and state of the art syllabifiers for each dataset. Liang's hyphenation algorithm is commonly known for its usage in . The patgen program was used to learn the rules of syllable boundaries BIBREF39. What we call Entropy CRF is a method particular to Manipuri; a rule-based component estimates the entropy of phones and phone clusters while a data-driven CRF component treats syllabification as a sequence modeling task BIBREF35.\n\nQuestion: Which models achieve state-of-the-art performances?\n\nExplanation: \n\nAccording to [Document 1], the CELEX (Dutch and English) - SVM-HMM, Festival, E-Hitz and OpenLexique - Liang hyphenation, and IIT-Guwahat - Entropy CRF models all achieve state-of-the-art performances.\n\nAnswer: CELEX (Dutch and English) - SVM-HMM\nFestival, E-Hitz and OpenLexique - Liang hyphenation\nIIT-Guwahat - Entropy CRF\n\nExample 4:\n\n[Document 1]: FLOAT SELECTED: Table 3: Performance comparison on various model sizes\n\nQuestion: On which datasets does LadaBERT achieve state-of-the-art?\n\nExplanation:"}
{"question_id": "d1d59bca40b8b308c0a35fed1b4b7826c85bc9f8", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: Do they build a model to recognize discourse relations on their dataset?\n\nExplanation:  [Document 1] mentions that they \"release the dataset\" and [Document 2] says that the \"dataset is in the CoNLL format.\" This suggests that they do not have a model that recognizes discourse relations on their dataset.\n\nAnswer: No\n\nExample 2:\n\n[Document 1]: In this section we detail the discussions we use to test our metric and how we determine the ground truth (i.e. if the discussion is controversial or not). We use thirty different discussions that took place between March 2015 and June 2019, half of them with controversy and half without it. We considered discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. We also studied these discussions taking first 140 characters and then 280 from each tweet to analyze the difference in performance and computing time wrt the length of the posts.\n\nQuestion: How many languages do they experiment with?\n\nExplanation:  The document says that they use \"thirty different discussions that took place between March 2015 and June 2019\" in \"four different languages: English, Portuguese, Spanish and French\".\n\nAnswer: four different languages: English, Portuguese, Spanish and French\n\nExample 3:\n\n[Document 1]: The data stems from a joint ADAPT-Microsoft project. An overview of the provided dataset is given in Table TABREF16 . Notice that the available amount of data differs per language.\n\nQuestion: is the dataset balanced across the four languages?\n\nExplanation:  [Document 1] provides a table that shows the amount of data available for each language. It is clear from the table that the dataset is not balanced across the four languages.\n\nAnswer: No\n\nExample 4:\n\n[Document 1]: FLOAT SELECTED: Figure 2: Numbers of implicit discourse relation instances from different agreements of explicit instances in three back-translations. En-Fr denotes instances that are implicit in English but explicit in back-translation of French, same for En-De and En-Cz. The overlap means they share the same relational arguments. The numbers under \u201cTwo-Votes\u201d and \u201cThree-Votes\u201d are the numbers of discourse relation agreement / disagreement between explicits in back-translations of two or three languages.\n\nQuestion: How many languages do they at most attempt to use to generate discourse relation labelled data?\n\nExplanation:"}
{"question_id": "595fe416a100bc7247444f25b11baca6e08d9291", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: 45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier. These features can be grouped into three categories (See Table TABREF5 for complete list of features):\n\n[Document 2]: Sociodemographics: gender, age, marital status, etc.\n\n[Document 3]: Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc.\n\n[Document 4]: Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc.\n\n[Document 5]: The Current Admission feature group has the most number of features, with 29 features included in this group alone. These features can be further stratified into two groups: `structured' clinical features and `unstructured' clinical features.\n\n[Document 6]: Feature Extraction ::: Structured Features\n\n[Document 7]: Structure features are features that were identified on the EHR using regular expression matching and include rating scores that have been reported in the psychiatric literature as correlated with increased readmission risk, such as Global Assessment of Functioning, Insight and Compliance:\n\n[Document 8]: Global Assessment of Functioning (GAF): The psychosocial functioning of the patient ranging from 100 (extremely high functioning) to 1 (severely impaired) BIBREF13.\n\n[Document 9]: Insight: The degree to which the patient recognizes and accepts his/her illness (either Good, Fair or Poor).\n\n[Document 10]: Compliance: The ability of the patient to comply with medication and to follow medical advice (either Yes, Partial, or None).\n\n[Document 11]: These features are widely-used in clinical practice and evaluate the general state and prognosis of the patient during the patient's evaluation.\n\n[Document 12]: Feature Extraction ::: Unstructured Features\n\n[Document 13]: Unstructured features aim to capture the state of the patient in relation to seven risk factor domains (Appearance, Thought Process, Thought Content, Interpersonal, Substance Use, Occupation, and Mood) from the free-text narratives on the EHR. These seven domains have been identified as associated with readmission risk in prior work BIBREF14.\n\n[Document 14]: These unstructured features include: 1) the relative number of sentences in the admission notes that involve each risk factor domain (out of total number of sentences within the admission) and 2) clinical sentiment scores for each of these risk factor domains, i.e. sentiment scores that evaluate the patient\u2019s psychosocial functioning level (positive, negative, or neutral) with respect to each of these risk factor domain.\n\nQuestion: What features are used?\n\nExplanation: \n\nAccording to [Document 1], 45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier. These features can be grouped into three categories: Sociodemographics, Past medical history, and Information from the current admission. This information is further supported by [Document 2], [Document 3], and [Document 4].\n\nAnswer: Sociodemographics: gender, age, marital status, etc., Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc., Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc.\n\nExample 2:\n\n[Document 1]: In this paper, we present the problem of DAR from the viewpoint of extending richer CRF-attentive structural dependencies along with neural network without abandoning end-to-end training. For simplicity, we call the framework as CRF-ASN (CRF-Attentive Structured Network). Specifically, we propose the hierarchical semantic inference integrated with memory mechanism on the utterance modeling. The memory mechanism is adopted in order to enable the model to look beyond localized features and have access to the entire sequence. The hierarchical semantic modeling learns different levels of granularity including word level, utterance level and conversation level. We then develop internal structured attention network on the linear-chain conditional random field (CRF) to specify structural dependencies in a soft manner. This approach generalizes the soft-selection attention on the structural CRF dependencies and takes into account the contextual influence on the nearing utterances. It is notably that the whole process is differentiable thus can be trained in an end-to-end manner.\n\nQuestion: Which features do they use?\n\nExplanation:  The memory mechanism is adopted in order to enable the model to look beyond localized features and have access to the entire sequence.\n\nAnswer: beyond localized features and have access to the entire sequence\n\nExample 3:\n\n[Document 1]: IOCs in cybersecurity articles are often described in a predictable way: being connected to a set of contextual keywords BIBREF16 , BIBREF1 . For example, a human user can infer that the word \u201cntdll.exe\u201d is the name of a malicious file on the basis of the words \u201cdownload\u201d and \u201ccompromised\u201d from the text shown in Fig. FIGREF1 . By analyzing the whole corpus, it is interesting that malicious file names tends to co-occur with words such as \"download\", \"malware\", \"malicious\", etc. In this work, we consider words that can indicate the characteristics of the neighbor words as contextual keywords and develop an approach to generate features from the automatically extracted contextual keywords.\n\nQuestion: What contextual features are used?\n\nExplanation:  [Document 1] states that contextual keywords are used to generate features. This is further supported by the example in Figure 1.\n\nAnswer: The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.\n\nExample 4:\n\n[Document 1]: Recent years have seen a large increase in the amount of disinformation and fake news spread on social media. False information was used to spread fear and anger among people, which in turn, provoked crimes in some countries. The US in the recent years experienced many similar cases during the presidential elections, such as the one commonly known as \u201cPizzagate\" . Later on, Twitter declared that they had detected a suspicious campaign originated in Russia by an organization named Internet Research Agency (IRA), and targeted the US to affect the results of the 2016 presidential elections. The desired goals behind these accounts are to spread fake and hateful news to further polarize the public opinion. Such attempts are not limited to Twitter, since Facebook announced in mid-2019 that they detected a similar attempt originating from UAE, Egypt and Saudi Arabia and targeting other countries such as Qatar, Palestine, Lebanon and Jordan. This attempt used Facebook pages, groups, and user accounts with fake identities to spread fake news supporting their ideological agendas. The automatic detection of such attempts is very challenging, since the true identity of these suspicious accounts is hidden by imitating the profiles of real persons from the targeted audience; in addition, sometimes they publish their suspicious idea in a vague way through their tweets' messages.\n\n[Document 2]: As Twitter declared, although the IRA campaign was originated in Russia, it has been found that IRA trolls concealed their identity by tweeting in English. Furthermore, for any possibility of unmasking their identity, the majority of IRA trolls changed their location to other countries and the language of the Twitter interface they use. Thus, we propose the following features to identify these users using only their tweets text:\n\n[Document 3]: Native Language Identification (NLI): This feature was inspired by earlier works on identifying native language of essays writers BIBREF22. We aim to detect IRA trolls by identifying their way of writing English tweets. As shown in BIBREF19, English tweets generated by non-English speakers have a different syntactic pattern . Thus, we use state-of-the-art NLI features to detect this unique pattern BIBREF23, BIBREF24, BIBREF25; the feature set consists of bag of stopwords, Part-of-speech tags (POS), and syntactic dependency relations (DEPREL). We extract the POS and the DEPREL information using spaCy, an off-the-shelf POS tagger. We clean the tweets from the special characters and maintained dots, commas, and first-letter capitalization of words. We use regular expressions to convert a sequence of dots to a single dot, and similarly for sequence of characters.\n\n[Document 4]: Stylistic: We extract a set of stylistic features following previous works in the authorship attribution domain BIBREF27, BIBREF28, BIBREF29, such as: the count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions. In addition, we extract the uppercase ratio and the tweet length.\n\nQuestion: What profile features are used?\n\nExplanation:"}
{"question_id": "998fa38634000f2d7b52d16518b9e18e898ce933", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We use a new dataset of GD statements from 1970 to 2016, the UN General Debate Corpus (UNGDC), to examine the international development agenda in the UN BIBREF3 . Our application of NLP to these statements focuses in particular on structural topic models (STMs) BIBREF4 . The paper makes two contributions using this approach: (1) It sheds light on the main international development issues that governments prioritise in the UN; and (2) It identifies the key country-specific factors associated with governments discussing development issues in their GD statements.\n\nQuestion: Is the dataset multilingual?\n\nExplanation:  [Document 1] does not mention anything about the languages used in the UNGDC.\n\nAnswer: No\n\nExample 2:\n\n[Document 1]: The performance of the joint learning architecture was evaluated on the two datasets described in the previous section. The joint learning model requires a paired and an unpaired dataset, so each of the two datasets was split into several parts. E2E NLG challenge Dataset: The training set of the E2E challenge dataset which consists of 42K samples was partitioned into a 10K paired and 32K unpaired datasets by a random process. The unpaired database was composed of two sets, one containing MRs only and the other containing natural texts only. This process resulted in 3 training sets: paired set, unpaired text set and unpaired MR set. The original development set (4.7K) and test set (4.7K) of the E2E dataset have been kept.\n\n[Document 2]: The Wikipedia Company Dataset: The Wikipedia company dataset presented in Section SECREF18 was filtered to contain only companies having abstracts of at least 7 words and at most 105 words. As a result of this process, 43K companies were retained. The dataset was then divided into: a training set (35K), a development set (4.3K) and a test set (4.3K). Of course, there was no intersection between these sets.\n\n[Document 3]: The training set was also partitioned in order to obtain the paired and unpaired datasets. Because of the loose correlation between the MRs and their corresponding text, the paired dataset was selected such that it contained the infobox values with the highest similarity with its reference text. The similarity was computed using \u201cdifflib\u201d library, which is an extension of the Ratcliff and Obershelp algorithm BIBREF19. The paired set was selected in this way (rather than randomly) to get samples as close as possible to a carefully annotated set. At the end of partitioning, the following training sets were obtained: paired set (10.5K), unpaired text set (24.5K) and unpaired MR set (24.5K).\n\nQuestion: What non-annotated datasets are considered?\n\nExplanation:  [Document 1] and [Document 2] both list the E2E NLG challenge Dataset and The Wikipedia Company Dataset as non-annotated datasets.\n\nAnswer: E2E NLG challenge Dataset, The Wikipedia Company Dataset\n\nExample 3:\n\n[Document 1]: We describe our rules for WikiSQL here. We first detect WHERE values, which exactly match to table cells. After that, if a cell appears at more than one column, we choose the column name with more overlapped words with the question, with a constraint that the number of co-occurred words is larger than 1. By default, a WHERE operator is INLINEFORM0 , except for the case that surrounding words of a value contain keywords for INLINEFORM1 and INLINEFORM2 . Then, we deal with the SELECT column, which has the largest number of co-occurred words and cannot be same with any WHERE column. By default, the SELECT AGG is NONE, except for matching to any keywords in Table TABREF8 . The coverage of our rule on training set is 78.4%, with execution accuracy of 77.9%.\n\n[Document 2]: Our rule for KBQA is simple without using a curated mapping dictionary. First, we detect an entity from the question using strict string matching, with the constraint that only one entity from the KB has the same surface string and that the question contains only one entity. After that, we get the connected relations of the detected entity, and assign the relation as the one with maximum number of co-occurred words. The coverage of our rule on training set is 16.0%, with an accuracy of 97.3% for relation prediction.\n\n[Document 3]: The pipeline of rules in SequentialQA is similar to that of WikiSQL. Compared to the grammar of WikiSQL, the grammar of SequentialQA has additional actions including copying the previous-turn logical form, no greater than, no more than, and negation. Table TABREF23 shows the additional word-level mapping table used in SequentialQA. The coverage of our rule on training set is 75.5%, with an accuracy of 38.5%.\n\nQuestion: Are the rules dataset specific?\n\nExplanation:  [Document 1], [Document 2], and [Document 3] all describe rules that are specific to the WikiSQL, KBQA, and SequentialQA datasets, respectively. This is further supported by the fact that the rules have different coverage and accuracy rates for each dataset.\n\nAnswer: Yes\n\nExample 4:\n\n[Document 1]: The next step consists of detecting mentions to entities in the raw text. To do this, we tag character segments that exactly match one of the known names of an entity. For instance, we can tag two different entities in the following text:\n\nQuestion: Does the SESAME dataset include discontiguous entities?\n\nExplanation:"}
{"question_id": "756a8a9125e6984e0ca768b653c6c760efa3db66", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: What accuracy score do they obtain?\n\nExplanation: \n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: The best performing model obtained an accuracy of 0.86\n\nAccording to [Document 1], the accuracy of the best performing model was 0.86.\n\nAnswer: the best performing model obtained an accuracy of 0.86\n\nExample 2:\n\nQuestion: What is the highest accuracy score achieved?\n\nExplanation: \n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: The highest accuracy score achieved was 82.0%.\n\n[Document 3]: The highest accuracy score achieved was 82.0%.\n\n[Document 4]: The highest accuracy score achieved was 82.0%.\n\n[Document 5]: The highest accuracy score achieved was 82.0%.\n\nAccording to [Document 1], the highest accuracy score achieved was 82.0%. This information is further supported by [Document 2], [Document 3], [Document 4], and [Document 5].\n\nAnswer: 82.0%\n\nExample 3:\n\nQuestion: What were their accuracy results on the task?\n\nExplanation: \n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: The best model was the one that used 100-dimensional GloVe embeddings and achieved an accuracy of 97.32% on the test set.\n\nThe answer can be found in [Document 2]. The best model was the one that used 100-dimensional GloVe embeddings and achieved an accuracy of 97.32% on the test set.\n\nAnswer: 97.32%\n\nExample 4:\n\n[Document 1]: This section provides a summary of the evaluation of KALM and KALM-QA, where KALM is evaluated for knowledge authoring and KALM-QA is evaluated for question answering. We have created a total of 50 logical frames, mostly derived from FrameNet but also some that FrameNet is missing (like Restaurant, Human_Gender) for representing the meaning of English sentences. Based on the 50 frames, we have manually constructed 250 sentences that are adapted from FrameNet exemplar sentences and evaluate these sentences on KALM, SEMAFOR, SLING, and Stanford KBP system. KALM achieves an accuracy of 95.6%\u2014much higher than the other systems.\n\n[Document 2]: For KALM-QA, we evaluate it on two datasets. The first dataset is manually constructed general questions based on the 50 logical frames. KALM-QA achieves an accuracy of 95% for parsing the queries. The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy\u2014much higher than the state-of-the-art machine learning approach BIBREF14 . Details of the evaluations can be found in BIBREF5 and BIBREF6 .\n\nQuestion: What was their accuracy score?\n\nExplanation:"}
{"question_id": "55612e92791296baf18013d2c8dd0474f35af770", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We find that Cohen $\\kappa $ agreement ranges from .84 for Uneasiness in the English data, .81 for Humor and Nostalgia, down to German Suspense (.65), Awe/Sublime (.61) and Vitality for both languages (.50 English, .63 German). Both annotators have a similar emotion frequency profile, where the ranking is almost identical, especially for German. However, for English, Annotator 2 annotates more Vitality than Uneasiness. Figure FIGREF18 shows the confusion matrices of labels between annotators as heatmaps. Notably, Beauty/Joy and Sadness are confused across annotators more often than other labels. This is topical for poetry, and therefore not surprising: One might argue that the beauty of beings and situations is only beautiful because it is not enduring and therefore not to divorce from the sadness of the vanishing of beauty BIBREF48. We also find considerable confusion of Sadness with Awe/Sublime and Vitality, while the latter is also regularly confused with Beauty/Joy.\n\nQuestion: How is the annotation experiment evaluated?\n\nExplanation: \n\nAccording to [Document 1], the evaluation of the annotation experiment is shown in the confusion matrices of labels between annotators.\n\nAnswer: confusion matrices of labels between annotators\n\nExample 2:\n\n[Document 1]: With the gathered comments, we reconstructed the original conversation trees, from the original post, the root, to the leaves, when they were available and selected a subset to annotated. For annotation purposes, we created snippets of conversations as the ones shown in Example 1 and Example 2 consisting of the parent of the suspected trolling event, the suspected trolling event comment, and all of the direct responses to the suspected trolling event. We added an extra constraint that the parent of the suspected trolling event should also be part of the direct responses, we hypothesize that if the suspected trolling event is indeed trolling, its parent should be the object of its trolling and would have a say about it. We recognize that this limited amount of information is not always sufficient to recover the original message conveyed by all of the participants in the snippet, and additional context would be beneficial. However, the trade off is that snippets like this allow us to make use of Amazon Mechanical Turk (AMT) to have the dataset annotated, because it is not a big burden for a \u201cturker\u201d to work on an individual snippet in exchange for a small pay, and expedites the annotation process by distributing it over dozens of people. Specifically, for each snippet, we requested three annotators to label the four aspects previously described. Before annotating, we set up a qualification test along with borderline examples to guide them in process and align them with our criteria. The qualification test turned out to be very selective since only 5% of all of the turkers that attempted it passed the exam. Our dataset consists of 1000 conversations with 5868 sentences and 71033 tokens. The distribution over the classes per trolling aspect is shown in the table TABREF24 in the column \u201cSize\u201d.\n\nQuestion: how was annotation done?\n\nExplanation: \n\nAccording to [Document 1], annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations. This information is further supported by [Document 2], [Document 3], and [Document 5].\n\nAnswer: Annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations\n\nExample 3:\n\n[Document 1]: We provide a hand-curated collection of complete inflection tables for 198 lemmata. The morphological tags follow the guidelines of the UniMorph schema BIBREF6, BIBREF7, in order to allow for the potential of cross-lingual transfer learning, and they are tagged with respect to:\n\n[Document 2]: Person: first (1), second (2), and third (3)\n\n[Document 3]: Number: singular (SG) ad plural (PL)\n\n[Document 4]: Inclusivity (only applicable to first person plural verbs: inclusive (INCL) and exclusive (EXCL)\n\n[Document 5]: Aspect/mood: completive (CPL), progressive (PROG), potential (POT), and habitual (HAB).\n\nQuestion: How was annotation done?\n\nExplanation:  [Document 1] states that the collection was hand-curated, and [Document 2], [Document 3], [Document 4], and [Document 5] provide details on the annotation.\n\nAnswer:  hand-curated collection of complete inflection tables for 198 lemmata\n\nExample 4:\n\n[Document 1]: In the framework of the challenge, we were required to first identify the entities occurring in the dataset and, then, annotate them with of the 13 possible types. Table TABREF12 provides a description for each type of entity that we made available both to the annotators and to the participants of the challenge.\n\nQuestion: What questions were asked in the annotation process?\n\nExplanation:"}
{"question_id": "d5e716c1386b6485e63075e980f80d44564d0aa2", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We plotted the distribution of harassment incidents in each categorization dimension (Figure FIGREF19). It displays statistics that provide important evidence as to the scale of harassment and that can serve as the basis for more effective interventions to be developed by authorities ranging from advocacy organizations to policy makers. It provides evidence to support some commonly assumed factors about harassment: First, we demonstrate that harassment occurred more frequently during the night time than the day time. Second, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives.\n\n[Document 2]: Furthermore, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) (Figure FIGREF20). The significance of the correlation is tested by chi-square independence with p value less than 0.05. Identifying these patterns will enable interventions to be differentiated for and targeted at specific populations. For instance, the young harassers often engage in harassment activities as groups. This points to the influence of peer pressure and masculine behavioral norms for men and boys on these activities. We also found that the majority of young perpetrators engaged in harassment behaviors on the streets. These findings suggest that interventions with young men and boys, who are readily influenced by peers, might be most effective when education is done peer-to-peer. It also points to the locations where such efforts could be made, including both in schools and on the streets. In contrast, we found that adult perpetrators of sexual harassment are more likely to act alone. Most of the adult harassers engaged in harassment on public transportation. These differences in adult harassment activities and locations, mean that interventions should be responsive to these factors. For example, increasing the security measures on transit at key times and locations.\n\n[Document 3]: In addition, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location (Figure FIGREF21). For example, young harassers are more likely to engage in behaviors of verbal harassment, rather than physical harassment as compared to adults. It was a single perpetrator that engaged in touching or groping more often, rather than groups of perpetrators. In contrast, commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers. The nature and location of the harassment are particularly significant in developing strategies for those who are harassed or who witness the harassment to respond and manage the everyday threat of harassment. For example, some strategies will work best on public transport, a particular closed, shared space setting, while other strategies might be more effective on the open space of the street.\n\nQuestion: What patterns were discovered from the stories?\n\nExplanation:  [Document 1], [Document 2], and [Document 3] all present different patterns that were discovered from the stories. [Document 1] presents the evidence that harassment occurred more frequently during the night time and that conductors and drivers are top the list of identified types of harassers. [Document 2] presents the evidence of the strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s). [Document 3] presents the evidence that the majority of young perpetrators engaged in harassment behaviors on the streets, that adult perpetrators of sexual harassment are more likely to act alone, and that commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers.\n\nAnswer: we demonstrate that harassment occurred more frequently during the night time than the day time, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) , We also found that the majority of young perpetrators engaged in harassment behaviors on the streets, we found that adult perpetrators of sexual harassment are more likely to act alone, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location , commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers.\n\nExample 2:\n\n[Document 1]: Additionally, important patterns can emerge from analysis of the fine-grained acts in a dialogue in a post-prediction setting. For example, if an agent does not follow-up with certain actions in response to a customer's question dialogue act, this could be found to be a violation of a best practice pattern. By analyzing large numbers of dialogue act sequences correlated with specific outcomes, various rules can be derived, i.e. \"Continuing to request information late in a conversation often leads to customer dissatisfaction.\" This can then be codified into a best practice pattern rules for automated systems, such as \"A request for information act should be issued early in a conversation, followed by an answer, informative statement, or apology towards the end of the conversation.\"\n\n[Document 2]: Our analysis helps zone in on how the use of certain dialogue acts may be likely to result in different outcomes. The weights we observe vary in the amount of insight provided: for example, offering extra help at the end of a conversation, or thanking the customer yields more satisfied customers, and more resolved problems (with ratios of above 6:1). However, some outcomes are much more subtle: for example, asking yes-no questions early-on in a conversation is highly associated with problem resolution (ratio 3:1), but asking them at the end of a conversation has as similarly strong association with unsatisfied customers. Giving elaborate answers that are not a simple affirmative, negative, or response acknowledgement (i.e. Answer (Other)) towards the middle of a conversation leads to satisfied customers that are not frustrated. Likewise, requesting information towards the end of a conversation (implying that more information is still necessary at the termination of the dialogue) leads to unsatisfied and unresolved customers, with ratios of at least 4:1.\n\nQuestion: Which patterns and rules are derived?\n\nExplanation: \n\nAccording to [Document 1], patterns and rules can be derived from the analysis of dialogue act sequences. This information is further supported by [Document 2], which provides specific examples of patterns and rules that have been derived.\n\nAnswer: A request for information act should be issued early in a conversation, followed by an answer, informative statement, or apology towards the end of the conversation,  offering extra help at the end of a conversation, or thanking the customer yields more satisfied customers, and more resolved problems , asking yes-no questions early-on in a conversation is highly associated with problem resolution (ratio 3:1), but asking them at the end of a conversation has as similarly strong association with unsatisfied customers, Giving elaborate answers that are not a simple affirmative, negative, or response acknowledgement (i.e. Answer (Other)) towards the middle of a conversation leads to satisfied customers that are not frustrated, requesting information towards the end of a conversation (implying that more information is still necessary at the termination of the dialogue) leads to unsatisfied and unresolved customers\n\nExample 3:\n\nQuestion: What types of facts can be extracted from QA pairs that can't be extracted from general text?\n\nExplanation:  There is not enough information provided to answer the question.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: Table TABREF13 provides examples of patterns learned for each class that are characteristic of that class. We observe that patterns associated with factual arguments often include topic-specific terminology, explanatory language, and argument phrases. In contrast, the patterns associated with feeling based arguments are often based on the speaker's own beliefs or claims, perhaps assuming that they themselves are credible BIBREF20 , BIBREF24 , or they involve assessment or evaluations of the arguments of the other speaker BIBREF29 . They are typically also very creative and diverse, which may be why it is hard to get higher accuracies for feeling classification, as shown by Table TABREF11 .\n\nQuestion: What patterns were extracted which were correlated with factual arguments?\n\nExplanation:"}
{"question_id": "b9a3836cff16af7454c7a8b0e5ff90206d0db1f5", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Tables and summarize the results from the link-prediction experiments on all three datasets, where a different ratio of edges are used for training. Results from models other than GANE are collected from BIBREF9 , BIBREF10 and BIBREF34 . We have also repeated these experiments on our own, and the results are consistent with the ones reported. Note that BIBREF34 did not report results on DMTE. Both GANE variants consistently outperform competing solutions. In the low-training-sample regime our solutions lead by a large margin, and the performance gap closes as the number of training samples increases. This indicates that our OT-based mutual attention framework can yield more informative textual representations than other methods. Note that GANE-AP delivers better results compared with GANE-OT, suggesting the attention parsing mechanism can further improve the low-level mutual attention matrix. More results on Cora and Hepth are provided in the SM.\n\nQuestion: Which of their proposed attention methods works better overall?\n\nExplanation: \n\nAccording to [Document 1], the attention parsing method works better overall. This is further supported by the fact that GANE-AP delivers better results compared with GANE-OT.\n\nAnswer: attention parsing\n\nExample 2:\n\n[Document 1]: Paraphrase Identification (PI) is the task of determining whether two sentences are paraphrase or not. It is considered a binary classification task. The best mono-lingual methods often achieve about 85% accuracy over this corpus BIBREF14, BIBREF18. Filice et al. BIBREF14 extended the tree kernels described in the previous section to operate on text pairs. The underlying idea is that this task is characterized by several syntactic/semantic patterns that a kernel machine can automatically capture from the training material. We can assess a text pair as a paraphrase if it shows a valid transformation rule that we observed in the training data. The following example can clarify this concept. A simple paraphrase rewriting rule is the active-passive transformation, such as in \u201cFederer beat Nadal\u201d and \u201cNadal was defeated by Federer\u201d. The same transformation can be observed in other paraphrases, such as in \u201cMark studied biology\u201d and \u201cBiology was learned by Mark\u201d. Although these two pairs of paraphrases have completely different topics, they have a very similar syntactic structure.\n\n[Document 2]: In this section, the experimental analysis of the proposed models is presented. We have implemented the cross-lingual variant of kernel functions for PI and RE tasks as described in section SECREF3 and measured the accuracy of models by testing them on the parallel data set.\n\nQuestion: What classification task was used to evaluate the cross-lingual adaptation method described in this work?\n\nExplanation: \n\nAccording to [Document 1], Paraphrase Identification is the task of determining whether two sentences are paraphrase or not. This information is further supported by [Document 2], which states that the accuracy of the models was measured by testing them on the parallel data set.\n\nAnswer: Paraphrase Identification\n\nExample 3:\n\n[Document 1]: Table 2 shows the ablation performances of various Q-code on the development set. Note that since the testset is hidden from us, we can only perform such an analysis on the development set. Our baseline model using no Q-code achieved a 68.00% and 77.36% EM and F1 scores, respectively. When we added the explicit question type T-code into the baseline model, the performance was improved slightly to 68.16%(EM) and 77.58%(F1). We then used TreeLSTM introduce syntactic parses for question representation and understanding (replacing simple question type as question understanding Q-code), which consistently shows further improvement. We further incorporated the soft adaptation. When letting the number of hidden question types ( $K$ ) to be 20, the performance improves to 68.73%/77.74% on EM and F1, respectively, which corresponds to the results of our model reported in Table 1 . Furthermore, after submitted our result, we have experimented with a large value of $K$ and found that when $K=100$ , we can achieve a better performance of 69.10%/78.38% on the development set.\n\nQuestion: how much of improvement the adaptation model can get?\n\nExplanation:  The answer can be found in the last paragraph of [Document 1].\n\nAnswer:  69.10%/78.38%\n\nExample 4:\n\n[Document 1]: We ran our Evo-Devo mechanism with the 486 ASR sentences (see Table TABREF14 ) and measured the accuracy after each repair. On an average we have achieved about 5 to 10% improvements in the accuracy of the sentences. Fine-tuning the repair and fitness functions, namely Equation (), would probably yield much better performance accuracies. However, experimental results confirm that the proposed Evo-Devo mechanism is an approach that is able to adapt INLINEFORM0 to get closer to INLINEFORM1 . We present a snapshot of the experiments with Google ASR (Ga) and calculate accuracy with respect to the user spoken question as shown in Table TABREF16 .\n\n[Document 2]: In the machine learning technique of adaptation, we considers INLINEFORM0 pairs as the predominant entity and tests the accuracy of classification of errors.\n\n[Document 3]: The combination of features INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , INLINEFORM3 , INLINEFORM4 namely, (bag of consonants, bag of vowels, left context, number of words, right context) gave the best results with INLINEFORM5 % improvement in accuracy in classification over 10-fold validation.\n\nQuestion: Which of their proposed domain adaptation methods proves best overall?\n\nExplanation:"}
{"question_id": "622efbecd9350a0f4487bdff2b8b362ef2541f3c", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: General lexical features are often used in natural language processing as they are somewhat task-independent and reasonably effective in terms of classification accuracy. In our experiments, we used TF.IDF-based features over the title and over the content of the article we wanted to classify. We had these features twice \u2013 once for the title and once for the the content of the article, as we wanted to have two different representations of the same article. Thus, we used a total of 1,100 TF.IDF-weighted features (800 content + 300 title), limiting the vocabulary to the top 800 and 300 words, respectively (which occurred in more than five articles). We should note that TF.IDF features should be used with caution as they may not remain relevant over time or in different contexts without retraining.\n\nQuestion: what lexical features did they experiment with?\n\nExplanation:  [Document 1] mentions that the experiment used TF.IDF-based features.\n\nAnswer: TF.IDF-based features\n\nExample 2:\n\n[Document 1]: TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. We use the concatenation of the title and the body as the query to retrieve the candidate comment set by means of the similarity of the tf-idf value. The model is trained on unpaired articles and comments, which is the same as our proposed model.\n\n[Document 2]: NVDM (Lexical, Neural) is a VAE-based approach for document modeling BIBREF10 . We compare our model with this baseline to demonstrate the effect of modeling topic.\n\nQuestion: Which lexicon-based models did they compare with?\n\nExplanation:  [Document 1] and [Document 2] both mention that TF-IDF and NVDM are lexicon-based models.\n\nAnswer: TF-IDF, NVDM\n\nExample 3:\n\n[Document 1]: Our contribution leverages pre-trained word embeddings (GloVe, trained on wikipedia+gigaword corpus), the DepecheMood affective lexicon, and convolutional neural networks.\n\nQuestion: What is the source of their lexicon?\n\nExplanation:  [Document 1] mentions that the DepecheMood affective lexicon is used.\n\nAnswer: DepecheMood\n\nExample 4:\n\nQuestion: did the top teams experiment with lexicons?\n\nExplanation:"}
{"question_id": "c8b2fb9e0d5fb9014a25b88d559d93b6dceffbc0", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: For each speaker, we divide all available utterances into disjoint train, development, and test sets. Using the force-aligned phone boundaries, we extract the mid-phone frame for each example across the four classes, which leads to a data imbalance. Therefore, for all utterances in the training set, we randomly sample additional examples within a window of 5 frames around the center phone, to at least 50 training examples per class per speaker. It is not always possible to reach the target of 50 examples, however, if no more data is available to sample from. This process gives a total of INLINEFORM0 10700 training examples with roughly 2000 to 3000 examples per class, with each speaker having an average of 185 examples. Because the amount of data varies per speaker, we compute a sampling score, which denotes the proportion of sampled examples to the speaker's total training examples. We expect speakers with high sampling scores (less unique data overall) to underperform when compared with speakers with more varied training examples.\n\nQuestion: How many instances does their dataset have?\n\nExplanation:  The dataset has 10700 training examples. This information is found in the first sentence of the first paragraph.\n\nAnswer: 10700\n\nExample 2:\n\n[Document 1]: A. As of June 2019, AA had $\\sim $50K entries, however, this includes some number of entries that are not truly research publications (for example, forewords, prefaces, table of contents, programs, schedules, indexes, calls for papers/participation, lists of reviewers, lists of tutorial abstracts, invited talks, appendices, session information, obituaries, book reviews, newsletters, lists of proceedings, lifetime achievement awards, erratum, and notes). We discard them for the analyses here. (Note: CL journal includes position papers like squibs, letter to editor, opinion, etc. We do not discard them.) We are then left with 44,896 articles. Figure FIGREF6 shows a graph of the number of papers published in each of the years from 1965 to 2018.\n\nQuestion: How many papers are used in experiment?\n\nExplanation:  [Document 1] states that after discarding non-research publications, there are 44,896 articles remaining.\n\nAnswer: 44,896 articles\n\nExample 3:\n\n[Document 1]: We demonstrated the utility of Katecheo by deploying the system for question answering in two topics, Medical Sciences and Christianity. These topics are diverse enough that they would warrant different curated sets of knowledge base articles, and we can easily retrieve knowledge base articles for each of these subjects from the Medical Sciences and Christianity Stack Exchange sites, respectively.\n\nQuestion: how many domains did they experiment with?\n\nExplanation:  The question asks how many domains the authors experimented with. This information is found in [Document 1], which states that the authors experimented with two domains: Medical Sciences and Christianity.\n\nAnswer: 2\n\nExample 4:\n\nQuestion: How many instances are explored in the few-shot experiments?\n\nExplanation:"}
{"question_id": "90eeb1b27f84c83ffcc8a88bc914a947c01a0c8b", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: In our paper, we opt for a state-of-the-art character bigram CNN classifier BIBREF4 , and investigate various ways in which the discourse information can be featurized and integrated into the CNN. Specifically,\n\nQuestion: What was the previous state-of-the-art?\n\nExplanation:  [Document 1] mentions that the character bigram CNN classifier was the state-of-the-art method.\n\nAnswer: character bigram CNN classifier\n\nExample 2:\n\n[Document 1]: Table TABREF19 shows the comparison of our proposed system with the existing state-of-the-art system of SemEval 2016 Task 6 for the sentiment dataset. BIBREF7 used feature-based SVM, BIBREF39 used keyword rules, LitisMind relied on hashtag rules on external data, BIBREF38 utilized a combination of sentiment classifiers and rules, whereas BIBREF37 used a maximum entropy classifier with domain-specific features. Our system comfortably surpasses the existing best system at SemEval. Our system manages to improve the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis.\n\n[Document 2]: We also compare our system with the state-of-the-art systems proposed by BIBREF15 on the emotion dataset. The comparison is demonstrated in Table TABREF22. Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN were the five individual systems used by BIBREF15. Overall, our proposed system achieves an improvement of 5 F-Score points over the existing state-of-the-art system for emotion analysis. Individually, the proposed system improves the existing F-scores for all the emotions except surprise. The findings of BIBREF15 also support this behavior (i.e. worst result for the surprise class). This could be attributed to the data scarcity and a very low agreement between the annotators for the emotion surprise.\n\nQuestion: What is the previous state-of-the-art model?\n\nExplanation: \n\nAccording to [Document 1], the previous state-of-the-art model was BIBREF7, BIBREF39, BIBREF37, LitisMind. This information is further supported by [Document 2], which states that the previous state-of-the-art model was BIBREF15, which used Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN.\n\nAnswer: BIBREF7, BIBREF39, BIBREF37, LitisMind, Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN\n\nExample 3:\n\n[Document 1]: We first compared the performance of Transformer Transducer (T-T) models with full attention on audio to an RNN-T model using a bidirectional LSTM audio encoder. As shown in Table TABREF12, the T-T model significantly outperforms the LSTM-based RNN-T baseline. We also observed that T-T models can achieve competitive recognition accuracy with existing wordpiece-based end-to-end models with similar model size. To compare with systems using shallow fusion BIBREF18, BIBREF25 with separately trained LMs, we also trained a Transformer-based LM with the same architecture as the label encoder used in T-T, using the full 810M word token dataset. This Transformer LM (6 layers; 57M parameters) had a perplexity of $2.49$ on the dev-clean set; the use of dropout, and of larger models, did not improve either perplexity or WER. Shallow fusion was then performed using that LM and both the trained T-T system and the trained bidirectional LSTM-based RNN-T baseline, with scaling factors on the LM output and on the non-blank symbol sequence length tuned on the LibriSpeech dev sets. The results are shown in Table TABREF12 in the \u201cWith LM\u201d column. The shallow fusion result for the T-T system is competitive with corresponding results for top-performing existing systems.\n\nQuestion: What was previous state of the art model?\n\nExplanation:  The table in [Document 1] shows that the T-T model outperforms the LSTM-based RNN-T baseline, which was the previous state of the art model.\n\nAnswer: LSTM-based RNN-T\n\nExample 4:\n\n[Document 1]: The state-of-the-art methods for this task can be roughly categorized into two streams. The first is based on semantic parsing BIBREF3 , BIBREF4 , which typically learns a grammar that can parse natural language to a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem BIBREF4 , BIBREF5 , BIBREF6 .\n\n[Document 2]: On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 or distributed representations BIBREF11 , BIBREF12 . Designing large training datasets for these methods is relatively easy BIBREF7 , BIBREF13 , BIBREF14 . These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North America because of the lack of sophisticated representation for the mathematical function highest. To select the correct answer, one has to retrieve all the heights of the mountains, and sort them in descending order, and then pick the first entry. We propose a method based on textual evidence which can answer such questions without solving the mathematic functions implicitly.\n\n[Document 3]: We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answering BIBREF16 , BIBREF12 , and the success of syntactic dependencies for relation extraction BIBREF17 , BIBREF18 , we propose a Multi-Channel Convolutional Neural Network (MCCNN) which could exploit both syntactic and sentential information for relation extraction.\n\n[Document 4]: FLOAT SELECTED: Table 1: Results on the test set.\n\nQuestion: What is the previous state-of-the-art?\n\nExplanation:"}
{"question_id": "8cf52ba480d372fc15024b3db704952f10fdca27", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: What are the baselines?\n\nExplanation: \n\n[Document 1]: We compare our proposed method with several baselines: (1) a vanilla CNN model; (2) an LSTM model; and (3) the BERT model BIBREF1 .\n\n[Document 2]: The vanilla CNN model is a simple convolutional neural network that takes word embeddings as input and outputs a classification for each word.\n\n[Document 3]: The LSTM model is a recurrent neural network that takes word embeddings as input and outputs a classification for each word.\n\n[Document 4]: The BERT model is a transformer-based model that takes word embeddings as input and outputs a classification for each word.\n\nAnswer: CNN, LSTM, BERT\n\nExample 2:\n\n[Document 1]: We compare our method with the models trained using Adobe internal NLU tool, Pytext BIBREF18 and Rasa BIBREF19 NLU tools. Table TABREF22 shows the results on Test set. Our model improves the F1 score by almost 2%, which corresponds to a 12.3% error rate reduction. This improvements proves the effectiveness of using contextual information for the task of slot filling.\n\nQuestion: What are the baselines?\n\nExplanation: \n\nAccording to [Document 1], the baselines are Adobe internal NLU tool, Pytext, and Rasa.\n\nAnswer: Adobe internal NLU tool, Pytext, Rasa\n\nExample 3:\n\n[Document 1]: We show the final results for opinion recommendation, comparing our proposed model with the following state-of-the-art baseline systems:\n\n[Document 2]: RS-Average is the widely-adopted baseline (e.g., by Yelp.com), using the averaged review scores as the final score.\n\n[Document 3]: RS-Linear estimates the rating score that a user would give by INLINEFORM0 BIBREF49 , where INLINEFORM1 and INLINEFORM2 are the the training deviations of the user INLINEFORM3 and the product INLINEFORM4 , respectively.\n\n[Document 4]: RS-Item applies INLINEFORM0 NN to estimate the rating score BIBREF50 . We choose the cosine similarity between INLINEFORM1 to measure the distance between product.\n\n[Document 5]: RS-MF is a state-of-the-art recommendation model, which uses matrix factorisation to predict rating score BIBREF8 , BIBREF41 , BIBREF25 .\n\n[Document 6]: Sum-Opinosis uses a graph-based framework to generate abstractive summarisation given redundant opinions BIBREF51 .\n\n[Document 7]: Sum-LSTM-Att is a state-of-the-art neural abstractive summariser, which uses an attentional neural model to consolidate information from multiple text sources, generating summaries using LSTM decoding BIBREF44 , BIBREF3 .\n\n[Document 8]: All the baseline models are single-task models, without considering rating and summarisation prediction jointly. The results are shown in Table TABREF46 . Our model (\u201c Joint\u201d) significantly outperforms both \u201cRS-Average\u201d and \u201cRS-Linear\u201d ( INLINEFORM0 using INLINEFORM1 -test), which demonstrates the strength of opinion recommendation, which leverages user characteristics for calculating a rating score for the user.\n\nQuestion: What are the baselines?\n\nExplanation: \n\nAccording to [Document 1], the baselines are RS-Average, RS-Linear, RS-Item, RS-MF, Sum-Opinosis, and Sum-LSTM-Att. This information is further supported by [Document 2], [Document 3], [Document 4], [Document 5], [Document 6], and [Document 7].\n\nAnswer: RS-Average , RS-Linear, RS-Item, RS-MF, Sum-Opinosis, Sum-LSTM-Att\n\nExample 4:\n\n[Document 1]: In this paper, we generate word embedding vectors on the training corpus based on word2vec BIBREF9 . Then we propose an algorithm to combine the generated one with the pre-trained word embedding vectors on a large general text corpus based on vector concatenation. The new word representation maintains information learned from both general text corpus and task-domain. The nice property of the algorithm is simplicity and little extra computational cost will be added. It can address word out-of-vocabulary issue effectively. This method can be applied to most NLP deep neural network models and is language-independent. We integrated our methods with ESIM(baseline model) BIBREF10 . The experimental results have shown that the proposed method has significantly improved the performance of original ESIM model and obtained state-of-the-art results on both Ubuntu Dialogue Corpus and Douban Conversation Corpus BIBREF11 . On Ubuntu Dialogue Corpus (V2), the improvement to the previous best baseline model (single) on INLINEFORM0 is 3.8% and our ensemble model on INLINEFORM1 is 75.9%. On Douban Conversation Corpus, the improvement to the previous best model (single) on INLINEFORM2 is 3.6%.\n\nQuestion: what are the baselines?\n\nExplanation:"}
{"question_id": "fa800a21469a70fa6490bfc67cabdcc8bf086fb5", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: To pretrain BERT and GPT-2 language models, as well as an AWD-LSTM language model for use in ULMFiT, a large unlabeled training corpora is needed. For this purpose, we construct a corpus of 172,815 articles from Tagalog Wikipedia which we call WikiText-TL-39 BIBREF18. We form training-validation-test splits of 70%-15%-15% from this corpora.\n\nQuestion: What other datasets are used?\n\nExplanation: \n\nAccording to [Document 1], the WikiText-TL-39 dataset is used.\n\nAnswer: WikiText-TL-39\n\nExample 2:\n\n[Document 1]: By combining both NSA and GSA, we obtain an enhanced SA module. We then construct our Normalized and Geometry-aware Self-Attention Network, namely NG-SAN, by replacing the vanilla SA modules in the encoder of the self-attention network with the proposed one. Extensive experiments on MS-COCO validates the effectiveness of our proposals. In particular, our NG-SAN establishes a new state-of-the-art on the MS-COCO evaluation sever, improving the best single-model result in terms of CIDEr from 125.5 to 128.6. To demonstrate the generality of NSA, we further present video captioning, machine translation, and visual question answering experiments on the VATEX, WMT 2014 English-to-German, and VQA-v2 datasets, respectively. On top of the strong Transformer-based baselines, our methods can consistently increase accuracies on all tasks at a negligible extra computational cost.\n\nQuestion: What datasets are used for experiments on three other tasks?\n\nExplanation: \n\nAccording to [Document 1], the VATEX, WMT 2014 English-to-German, and VQA-v2 datasets were used for experiments on three other tasks.\n\nAnswer: VATEX, WMT 2014 English-to-German, and VQA-v2 datasets\n\nExample 3:\n\n[Document 1]: The recently introduced How2 dataset BIBREF2 has stimulated research around multimodal language understanding through the availability of 300h instructional videos, English subtitles and their Portuguese translations. For example, BIBREF3 successfully demonstrates that semantically rich action-based visual features are helpful in the context of machine translation (MT), especially in the presence of input noise that manifests itself as missing source words. Therefore, we hypothesize that a speech-to-text translation (STT) system may also benefit from the visual context, especially in the traditional cascaded framework BIBREF4, BIBREF5 where noisy automatic transcripts are obtained from an automatic speech recognition system (ASR) and further translated into the target language using a machine translation (MT) component. The dataset enables the design of such multimodal STT systems, since we have access to a bilingual corpora as well as the corresponding audio-visual stream. Hence, in this paper, we propose a cascaded multimodal STT with two components: (i) an English ASR system trained on the How2 dataset and (ii) a transformer-based BIBREF0 visually grounded MMT system.\n\nQuestion: What dataset was used in this work?\n\nExplanation:  [Document 1] mentions that the recently introduced How2 dataset was used, which is further supported by [Document 2].\n\nAnswer: How2\n\nExample 4:\n\n[Document 1]: We experiment with the dataset of Waseem and Hovy c53cecce142c48628b3883d13155261c, containing tweets manually annotated for hate speech. The authors retrieved around $136k$ tweets over a period of two months. They bootstrapped their collection process with a search for commonly used slurs and expletives related to religious, sexual, gender and ethnic minorities. From the results, they identified terms and references to entities that frequently showed up in hateful tweets. Based on this sample, they used a public Twitter api to collect the entire corpus of ca. $136k$ tweets. After having manually annotated a randomly sampled subset of $16,914$ tweets under the categories racism, sexism or none themselves, they asked an expert to review their annotations in order to mitigate against any biases. The inter-annotator agreement was reported at $\\kappa =0.84$ , with a further insight that $85\\%$ of all the disagreements occurred in the sexism class.\n\n[Document 2]: The dataset was released as a list of $16,907$ tweet IDs along with their corresponding annotations. Using python's Tweepy library, we could only retrieve $16,202$ of the tweets since some of them have now been deleted or their visibility limited. Of the ones retrieved, 1,939 (12%) are labelled as racism, 3,148 (19.4%) as sexism, and the remaining 11,115 (68.6%) as none; this distribution follows the original dataset very closely (11.7%, 20.0%, 68.3%).\n\nQuestion: Is the dataset used in other work?\n\nExplanation:"}
{"question_id": "88d1bd21b53b8be4f9d3cb26ecc3cbcacffcd63e", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Statistical characterization of languages has been a field of study for decadesBIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5. Even simple quantities, like letter frequency, can be used to decode simple substitution cryptogramsBIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10. However, probably the most surprising result in the field is Zipf's law, which states that if one ranks words by their frequency in a large text, the resulting rank frequency distribution is approximately a power law, for all languages BIBREF0, BIBREF11. These kind of universal results have long piqued the interest of physicists and mathematicians, as well as linguistsBIBREF12, BIBREF13, BIBREF14. Indeed, a large amount of effort has been devoted to try to understand the origin of Zipf's law, in some cases arguing that it arises from the fact that texts carry information BIBREF15, all the way to arguing that it is the result of mere chance BIBREF16, BIBREF17. Another interesting characterization of texts is the Heaps-Herdan law, which describes how the vocabulary -that is, the set of different words- grows with the size of a text, the number of which, empirically, has been found to grow as a power of the text size BIBREF18, BIBREF19. It is worth noting that it has been argued that this law is a consequence Zipf's law. BIBREF20, BIBREF21\n\nQuestion: How do Zipf and Herdan-Heap's laws differ?\n\nExplanation:  [Document 1] states that Zipf's law describes how, if you rank words by their frequency in a large text, the resulting rank frequency distribution is approximately a power law. Furthermore, it is stated that Heaps-Herdan law describes how the vocabulary -that is, the set of different words- grows with the size of a text.\n\nAnswer: Zipf's law describes change of word frequency rate, while Heaps-Herdan describes different word number in large texts (assumed that Hepas-Herdan is consequence of Zipf's)\n\nExample 2:\n\n[Document 1]: We use two different unsupervised approaches for word sense disambiguation. The first, called `sparse model', uses a straightforward sparse vector space model, as widely used in Information Retrieval, to represent contexts and synsets. The second, called `dense model', represents synsets and contexts in a dense, low-dimensional space by averaging word embeddings.\n\n[Document 2]: In the synset embeddings model approach, we follow SenseGram BIBREF14 and apply it to the synsets induced from a graph of synonyms. We transform every synset into its dense vector representation by averaging the word embeddings corresponding to each constituent word: DISPLAYFORM0\n\n[Document 3]: We observe that the SenseGram-based approach for word sense disambiguation yields substantially better results in every case (Table TABREF25 ). The primary reason for that is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words. Thus, we recommend using the dense approach in further studies. Although the AdaGram approach trained on a large text corpus showed better results according to the weighted average, this result does not transfer to languages with less available corpus size.\n\nQuestion: Do the authors offer any hypothesis about why the dense mode outperformed the sparse one?\n\nExplanation:  The authors offer a hypothesis about why the dense mode outperformed the sparse one in [Document 3]. They state that the primary reason for that is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words.\n\nAnswer: Yes\n\nExample 3:\n\nQuestion: Do the authors offer any hypothesis as to why the transformations sometimes disimproved performance?\n\nExplanation:  The authors do not offer any hypothesis as to why the transformations sometimes disimproved performance. This is evident from a close reading of [Document 1], [Document 2], [Document 3], [Document 4], and [Document 5].\n\nAnswer: No\n\nExample 4:\n\nQuestion: Do they authors offer any hypothesis for why the parameters of Zipf's law and Heaps' law differ on Twitter?\n\nExplanation:"}
{"question_id": "3f717e6eceab0a066af65ddf782c1ebc502c28c0", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Recently, neural models pre-trained on a language modeling task, such as ELMo BIBREF0 , OpenAI GPT BIBREF1 , and BERT BIBREF2 , have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. The success of BERT can largely be associated to the notion of context-aware word embeddings, which differentiate it from common approaches such as word2vec BIBREF3 that establish a static semantic embedding. Since the introduction of BERT, the NLP community continues to be impressed by the amount of ideas produced on top of this powerful language representation model. However, despite its success, it remains unclear whether the representations produced by BERT can be utilized for tasks such as commonsense reasoning. Particularly, it is not clear whether BERT shed light on solving tasks such as the Pronoun Disambiguation Problem (PDP) and Winograd Schema Challenge (WSC). These tasks have been proposed as potential alternatives to the Turing Test, because they are formulated to be robust to statistics of word co-occurrence BIBREF4 .\n\n[Document 2]: In this paper, we show that the attention maps created by an out-of-the-box BERT can be directly exploited to resolve coreferences in long sentences. As such, they can be simply repurposed for the sake of commonsense reasoning tasks while achieving state-of-the-art results on the multiple task. On both PDP and WSC, our method outperforms previous state-of-the-art methods, without using expensive annotated knowledge bases or hand-engineered features. On a Pronoun Disambiguation dataset, PDP-60, our method achieves 68.3% accuracy, which is better than the state-of-art accuracy of 66.7%. On a WSC dataset, WSC-273, our method achieves 60.3%. As of today, state-of-the-art accuracy on the WSC-273 for single model performance is around 57%, BIBREF14 and BIBREF13 . These results suggest that BERT implicitly learns to establish complex relationships between entities such as coreference resolution. Although this helps in commonsense reasoning, solving this task requires more than employing a language model learned from large text corpora.\n\nQuestion: Which datasets do they evaluate on?\n\nExplanation: \n\nAccording to [Document 2], the PDP-60 and WSC-273 datasets were used for evaluation.\n\nAnswer: PDP-60, WSC-273\n\nExample 2:\n\n[Document 1]: Traditional text-to-speech (TTS) systems are composed of complex pipelines BIBREF0 , these often include accoustic frontends, duration model, acoustic prediction model and vocoder models. The complexity of the TTS problem coupled with the requirement for deep domain expertise means these systems are often brittle in design and results in un-natural synthesized speech.\n\n[Document 2]: The open source LJSpeech Dataset was used to train our TTS model. This dataset contains around 13k <text,audio> pairs of a single female english speaker collect from across 7 different non-fictional books. The total training data time is around 21 hours of audio.\n\n[Document 3]: The architecture of our model utilizes RNN-based Seq2Seq model for generating mel spectrogram from text. The architecture is similar to that of Tacotron 2 BIBREF4 . The generated mel spetrogram can either be inverted via iterative algorithms such as Griffin Lim, or through more complicated neural vocoder networks such as a mel spectrogram conditioned Wavenet BIBREF11 .\n\nQuestion: Which dataset(s) do they evaluate on?\n\nExplanation:  [Document 2] mentions that the LJSpeech dataset was used to train the TTS model. This dataset was used to evaluate the model.\n\nAnswer: LJSpeech\n\nExample 3:\n\n[Document 1]: Datasets\n\n[Document 2]: We conduct experiments on two Chinese question answering datasets from NLPCC-2016 evaluation task BIBREF13 .\n\n[Document 3]: DBQA is a document based question answering dataset. There are 8.8k questions with 182k question-sentence pairs for training and 6k questions with 123k question-sentence pairs in the test set. In average, each question has 20.6 candidate sentences and 1.04 golden answers. The average length for questions is 15.9 characters, and each candidate sentence has averagely 38.4 characters. Both questions and sentences are natural language sentences, possibly sharing more similar word choices and expressions compared to the KBQA case. But the candidate sentences are extracted from web pages, and are often much longer than the questions, with many irrelevant clauses.\n\n[Document 4]: KBRE is a knowledge based relation extraction dataset. We follow the same preprocess as BIBREF14 to clean the dataset and replace entity mentions in questions to a special token. There are 14.3k questions with 273k question-predicate pairs in the training set and 9.4k questions with 156k question-predicate pairs for testing. Each question contains only one golden predicate. Each question averagely has 18.1 candidate predicates and 8.1 characters in length, while a KB predicate is only 3.4 characters long on average. Note that a KB predicate is usually a concise phrase, with quite different word choices compared to the natural language questions, which poses different challenges to solve.\n\nQuestion: Which dataset(s) do they evaluate on?\n\nExplanation:  [Document 2] and [Document 3] both state that the DBQA dataset was used. [Document 4] states that the KBRE dataset was used.\n\nAnswer: DBQA, KBRE\n\nExample 4:\n\n[Document 1]: We use two datasets: FIGER (GOLD) BIBREF0 and BBN BIBREF5. The sizes of their tag sets are 113 and 47, respectively. FIGER (GOLD) allows mentions to have multiple type paths, but BBN does not. Another commonly used dataset, OntoNotes BIBREF1, is not used since it contains many pronoun and common noun phrase mentions such as \u201cit,\u201d \u201che,\u201d \u201ca thrift institution,\u201d which are not suitable to directly apply entity linking on.\n\nQuestion: Which datasets do they evaluate on?\n\nExplanation:"}
{"question_id": "ebb4db9c24aa36db9954dd65ea079a798df80558", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Appropriate future directions on adversarial attacks and defenses: As an attacker, designing universal perturbations to catch better adversarial examples can be taken into consideration like it works in image BIBREF29 . A universal adversarial perturbation on any text is able to make a model misbehave with high probability. Moreover, more wonderful universal perturbations can fool multi-models or any model on any text. On the other hand, the work of enhancing the transferability of adversarial examples is meaningful in more practical back-box attacks. On the contrary, defenders prefer to completely revamp this vulnerability in DNNs, but it is no less difficult than redesigning a network and is also a long and arduous task with the common efforts of many people. At the moment defender can draw on methods from image area to text for improving the robustness of DNNs, e.g. adversarial training BIBREF107 , adding extra layer BIBREF113 , optimizing cross-entropy function BIBREF114 , BIBREF115 or weakening the transferability of adversarial examples.\n\nQuestion: Which strategies show the most promise in deterring these attacks?\n\nExplanation:  [Document 1] contains a list of possible strategies for deterring adversarial attacks. These include adversarial training, adding extra layer, optimizing cross-entropy function, or weakening the transferability of adversarial examples.\n\nAnswer: At the moment defender can draw on methods from image area to text for improving the robustness of DNNs, e.g. adversarial training BIBREF107 , adding extra layer BIBREF113 , optimizing cross-entropy function BIBREF114 , BIBREF115 or weakening the transferability of adversarial examples.\n\nExample 2:\n\n[Document 1]: Among the baselines, HRL achieves significantly higher F1 scores on the two datasets. We run their model and our models five times and report the median results in Table TABREF15. Scores of other baselines in Table TABREF15 are taken from previous published papers BIBREF6, BIBREF11, BIBREF14. Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. We perform a statistical significance test (t-test) under a bootstrap pairing between HRL and our models and see that the higher F1 scores achieved by our models are statistically significant ($p < 0.001$). Next, we combine the outputs of five runs of our models and five runs of HRL to build ensemble models. For a test instance, we include those tuples which are extracted in the majority ($\\ge 3$) of the five runs. This ensemble mechanism increases the precision significantly on both datasets with a small improvement in recall as well. In the ensemble scenario, compared to HRL, WDec achieves $4.2\\%$ and $3.5\\%$ higher F1 scores and PNDec achieves $4.2\\%$ and $2.9\\%$ higher F1 scores on the NYT29 and NYT24 datasets respectively.\n\nQuestion: Which one of two proposed approaches performed better in experiments?\n\nExplanation: \n\nAccording to [Document 1], the WordDecoding (WDec) model achieved higher F1 scores than the HRL model on the NYT29 and NYT24 datasets. This information is further supported by the table in [Document 1].\n\nAnswer: WordDecoding (WDec) model\n\nExample 3:\n\nQuestion: What is the optimal trading strategy based on reinforcement learning?\n\nExplanation:  There is not enough information provided to answer the question.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: We further investigate the performance gain from two different patient teacher designs: PKD-Last vs. PKD-Skip. Results of both PKD variants on the GLUE benchmark (with BERT$_6$ as the student) are summarized in Table TABREF23. Although both strategies achieved improvement over the vanilla KD baseline (see Table TABREF16), PKD-Skip performs slightly better than PKD-Last. Presumably, this might be due to the fact that distilling information across every $k$ layers captures more diverse representations of richer semantics from low-level to high-level, while focusing on the last $k$ layers tends to capture relatively homogeneous semantic information.\n\nQuestion: What of the two strategies works best?\n\nExplanation:"}
{"question_id": "8c288120139615532838f21094bba62a77f92617", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We compile three Chinese text corpora from online data for three domains, namely, \u201chotel\", \u201cmobile phone (mobile)\", and \u201ctravel\". All texts are about user reviews. Each text sample collected is first partitioned into clauses according to Chinese tokens. Three clause sets are subsequently obtained from the three text corpora.\n\n[Document 2]: In the second stage, five users are invited to label each text sample in the three raw data sets. The average score of the five users on each sample is calculated. Samples with average scores located in [0.6, 1] are labeled as \u201cpositive\". Samples with average scores located in [0, 0.4] are labeled as \u201cnegative\". Others are labeled as \u201cneutral\". The details of the labeling results are shown in Table 1.\n\nQuestion: How long are the datasets?\n\nExplanation:  [Document 1] contains the number of raw samples and clauses for the three corpora. [Document 2] contains the number of raw samples for the three corpora.\n\nAnswer: Travel dataset contains 4100 raw samples, 11291 clauses, Hotel dataset contains 3825 raw samples, 11264 clauses, and the Mobile dataset contains 3483 raw samples and 8118 clauses\n\nExample 2:\n\n[Document 1]: We focus primarily on the smaller data set of 645 hand-labeled articles provided to task participants, both for training and for validation. We take the first 80% of this data set for our training set and the last 20% for the validation set. Since the test set is also hand-labeled we found that the 645 articles are much more representative of the final test set than the articles labeled by publisher. The model's performance on articles labeled by publisher was not much above chance level.\n\n[Document 2]: Our first experiment was checking the importance of pre-training. We pre-trained BERT-base on the 600,000 articles without labels by using the same Cloze task BIBREF5 that BERT had originally used for pre-training. We then trained the model on sequence lengths of 100, 250 and 500. The accuracy for each sequence length after 100 epochs is shown in TABREF7 and is labeled as UP (unsupervised pre-training). The other column shows how well BERT-base trained without pre-training. We found improvements for lower sequence lengths, but not at 500 word pieces. Since the longer chunk should have been more informative, and since our hand-labeled training set only contained 516 articles, this likely indicates that BERT experiences training difficulty when dealing with long sequences on such a small dataset. As the cost to do pre-training was only a one time cost all of our remaining experiments use a pre-trained model.\n\nQuestion: How long is the dataset?\n\nExplanation:  [Document 1] states that the dataset contains 645 articles. [Document 2] states that the dataset contains 600,000 articles.\n\nAnswer: 645, 600000\n\nExample 3:\n\n[Document 1]: Data set. For the cross-domain polarity classification experiments, we use the second version of Multi-Domain Sentiment Dataset BIBREF0 . The data set contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). Reviews contain star ratings (from 1 to 5) which are converted into binary labels as follows: reviews rated with more than 3 stars are labeled as positive, and those with less than 3 stars as negative. In each domain, there are 1000 positive and 1000 negative reviews.\n\nQuestion: How long is the dataset?\n\nExplanation:  The dataset contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). In each domain, there are 1000 positive and 1000 negative reviews. This means that there are a total of 8000 reviews in the dataset.\n\nAnswer: 8000\n\nExample 4:\n\n[Document 1]: We used the Mboshi5K corpus BIBREF13 as a test set for all the experiments reported here. Mboshi (Bantu C25) is a typical Bantu language spoken in Congo-Brazzaville. It is one of the languages documented by the BULB (Breaking the Unwritten Language Barrier) project BIBREF14 . This speech dataset was collected following a real language documentation scenario, using Lig_Aikuma, a mobile app specifically dedicated to fieldwork language documentation, which works both on android powered smartphones and tablets BIBREF15 . The corpus is multilingual (5130 Mboshi speech utterances aligned to French text) and contains linguists' transcriptions in Mboshi (in the form of a non-standard graphemic form close to the language phonology). It is also enriched with automatic forced-alignment between speech and transcriptions. The dataset is made available to the research community. More details on this corpus can be found in BIBREF13 .\n\nQuestion: How long are the datasets?\n\nExplanation:"}
{"question_id": "d5d48b812576470edbf978fc18c00bd24930a7b7", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space. We do this by estimating the language centroid as the mean of the mBERT representations for a set of sentences in that language and subtracting the language centroid from the contextual embeddings.\n\nQuestion: Are language-specific and language-neutral components disjunctive?\n\nExplanation: \n\nAccording to [Document 1], the language-specific and language-neutral components are not disjunctive. This is because the language-specific information is removed from the representations by centering the representations of sentences in each language.\n\nAnswer: No\n\nExample 2:\n\n[Document 1]: Using a set of semantically oriented tasks that require explicit semantic cross-lingual representations, we showed that mBERT contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks.\n\nQuestion: What challenges this work presents that must be solved to build better language-neutral representations?\n\nExplanation: \n\nAccording to [Document 1], contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks.\n\nAnswer: contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks\n\nExample 3:\n\n[Document 1]: Table TABREF13 shows the inconsistency rates of back translation between Adv-C and our method on MUSE. Compared with Adv-C, our model significantly reduces the inconsistency rates on all language pairs, which explains the overall improvement in Table TABREF12. Table TABREF14 gives several word translation examples. In the first three cases, our regularizer successfully fixes back translation errors. In the fourth case, ensuring cycle consistency does not lead to the correct translation, which explains some errors by our system. In the fifth case, our model finds a related word but not the same word in the back translation, due to the use of cosine similarity for regularization.\n\nQuestion: What 6 language pairs is experimented on?\n\nExplanation: \n\nAccording to [Document 1], the inconsistency rates of back translation were compared between Adv-C and our method on MUSE for six language pairs: EN<->ES, EN<->DE, EN<->IT, EN<->EO, EN<->MS, and EN<->FI.\n\nAnswer: EN<->ES\nEN<->DE\nEN<->IT\nEN<->EO\nEN<->MS\nEN<->FI\n\nExample 4:\n\n[Document 1]: FLOAT SELECTED: Table 3: Templates used to infer gender biases in the translation of job occupations and adjectives to the English language.\n\nQuestion: How many different sentence constructions are translated in gender neutral languages?\n\nExplanation:"}
{"question_id": "5206b6f40a91fc16179829041c1139a6c6d91ce7", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Each annotator annotated 90 reference sentences (i.e. from the training corpus) with which style they thought the sentence was from. The accuracy on this baseline task for annotators A1, A2, and A3 was 80%, 88%, and 80% respectively, giving us an upper expected bound on the human evaluation.\n\nQuestion: How they perform manual evaluation, what is criteria?\n\nExplanation: \n\nAccording to [Document 1], the accuracy is the criterion for manual evaluation.\n\nAnswer: accuracy\n\nExample 2:\n\n[Document 1]: Table 2 lists some example definitions generated with different models. For each word-sememes pair, the generated three definitions are ordered according to the order: Baseline, AAM and SAAM. For AAM and SAAM, we use the model that incorporates sememes. These examples show that with sememes, the model can generate more accurate and concrete definitions. For example, for the word \u201c\u65c5\u9986\u201d (hotel), the baseline model fails to generate definition containing the token \u201c\u65c5\u884c\u8005\u201d(tourists). However, by incoporating sememes' information, especially the sememe \u201c\u65c5\u6e38\u201d (tour), AAM and SAAM successfully generate \u201c\u65c5\u884c\u8005\u201d(tourists). Manual inspection of others examples also supports our claim.\n\nQuestion: Do they perform manual evaluation?\n\nExplanation:  The document provides an example of how the definitions generated by the different models are evaluated. The definitions are ordered according to the order: Baseline, AAM and SAAM. This shows that the definitions are evaluated by someone other than the model itself, in this case, by a human.\n\nAnswer: Yes\n\nExample 3:\n\n[Document 1]: We compare the performance of our model (Table 2 ) with traditional Bag of Words (BoW), TF-IDF, and n-grams features based classifiers. We also compare against averaged Skip-Gram BIBREF29 , Doc2Vec BIBREF30 , CNN BIBREF23 , Hierarchical Attention (HN-ATT) BIBREF24 and hierarchical network (HN) models. HN it is similar to our model HN-SA but without any self attention.\n\n[Document 2]: Analysis: As is evident from the experiments on both the versions of SWBD, our model (HN-SA) outperforms traditional feature based topic spotting models and deep learning based document classification models. It is interesting to see that simple BoW and n-gram baselines are quite competitive and outperform some of the deep learning based document classification model. Similar observation has also been reported by BIBREF31 ( BIBREF31 ) for the task of sentiment analysis. The task of topic spotting is arguably more challenging than document classification. In the topic spotting task, the number of output classes (66/42 classes) is much more than those in document classification (5/6 classes), which is done mainly on the texts from customer reviews. Dialogues in SWBD have on an average 200 utterances and are much longer texts than customer reviews. Additionally, the number of dialogues available for training the model is significantly lesser than customer reviews. We further investigated the performance on SWBD2 by examining the confusion matrix of the model. Figure 2 shows the heatmap of the normalized confusion matrix of the model on SWBD2. For most of the classes the classifier is able to predict accurately. However, the model gets confused between the classes which are semantically close (w.r.t. terms used) to each other, for example, the model gets confused between pragmatically similar topics e.g. HOBBIES\u0080\u0099 vs \u0080\u0098GARDENING\u0080\u0099, \u0080\u0098MOVIES vs \u0080\u0098TV PROGRAMS\u00e2\u0080\u0099, \u0080\u0098RIGHT TO PRIVACY vs\u0080\u0098 DRUG TESTING\u0080\u0099.\n\nQuestion: Do the authors do manual evaluation?\n\nExplanation:  There is no mention of manual evaluation in [Document 1] or [Document 2].\n\nAnswer: No\n\nExample 4:\n\n[Document 1]: In this section we present the results from our experiments. Table TABREF15 shows the results from the manual evaluation. Note that each criterion scores between 0 and 3 which leads to a maximum total score of 12. The achieved total score range from 5 to 10 with an average of 8.1. In particular, the grammatical correctness and the sentence transitions were very good. Each of them scored on average 2.3 out of 3. The speech content yielded the lowest scores. This indicates that the topic model may need some improvement.\n\n[Document 2]: FLOAT SELECTED: Table 6: Results from manual evaluation\n\nQuestion: what manual evaluation is presented?\n\nExplanation:"}
{"question_id": "9baca9bdb8e7d5a750f8cbe3282beb371347c164", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: How is tweet subjectivity measured?\n\nExplanation:  The documents do not mention how tweet subjectivity is measured.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: The classification of racist insults presents us with the problem of giving an adequate definition of racism. More so than in other domains, judging whether an utterance is an act of racism is highly personal and does not easily fit a simple definition. The Belgian anti-racist law forbids discrimination, violence and crime based on physical qualities (like skin color), nationality or ethnicity, but does not mention textual insults based on these qualities. Hence, this definition is not adequate for our purposes, since it does not include the racist utterances one would find on social media; few utterances that people might perceive as racist are actually punishable by law, as only utterances which explicitly encourage the use of violence are illegal. For this reason, we use a common sense definition of racist language, including all negative utterances, negative generalizations and insults concerning ethnicity, nationality, religion and culture. In this, we follow paolo2015racist, bonilla2002linguistics and razavi2010offensive, who show that racism is no longer strictly limited to physical or ethnic qualities, but can also include social and cultural aspects.\n\nQuestion: how did they ask if a tweet was racist?\n\nExplanation:  [Document 1] states that the researchers used a common sense definition of racist language, which included negative utterances, negative generalizations, and insults concerning ethnicity, nationality, religion, and culture.\n\nAnswer: if it includes  negative utterances, negative generalizations and insults concerning ethnicity, nationality, religion and culture.\n\nExample 3:\n\n[Document 1]: Sentence classification combines natural language processing (NLP) with machine learning to identify trends in sentence structure, BIBREF14 , BIBREF15 . Each tweet is converted to a numeric word vector in order to identify distinguishing features by training an NLP classifier on a validated set of relevant tweets. The classifier acts as a tool to sift through ads, news, and comments not related to patients. Our scheme combines a logistic regression classifier, BIBREF16 , with a Convolutional Neural Network (CNN), BIBREF17 , BIBREF18 , to identify self-reported diagnostic tweets.\n\n[Document 2]: It is important to be wary of automated accounts (e.g. bots, spam) whose large output of tweets pollute relevant organic content, BIBREF19 , and can distort sentiment analyses, BIBREF20 . Prior to applying sentence classification, we removed tweets containing hyperlinks to remove automated content (some organic content is necessarily lost with this strict constraint).\n\n[Document 3]: Our goal was to analyze content authored only by patients. To help ensure this outcome we removed posts containing a URL for classification, BIBREF19 . Twitter allows users to spread content from other users via `retweets'. We also removed these posts prior to classification to isolate tweets authored by patients. We also accounted for non-relevant astrological content by removing all tweets containing any of the following horoscope indicators: `astrology',`zodiac',`astronomy',`horoscope',`aquarius',`pisces',`aries',`taurus',`leo',`virgo',`libra', and `scorpio'. We preprocessed tweets by lowercasing and removing punctuation. We also only analyzed tweets for which Twitter had identified `en' for the language English.\n\nQuestion: What machine learning and NLP methods were used to sift tweets relevant to breast cancer experiences?\n\nExplanation: \n\nAccording to [Document 1], a logistic regression classifier was used in combination with a Convolutional Neural Network (CNN) to identify self-reported diagnostic tweets. This information is further supported by [Document 2] and [Document 3]. In [Document 2], it is mentioned that tweets containing hyperlinks were removed in order to remove automated content, and in [Document 3], it is mentioned that tweets containing URLs were removed in order to isolate tweets authored by patients. Additionally, [Document 3] mentions that tweets containing horoscope indicators were removed in order to remove astrological content. Finally, [Document 1] mentions that tweets were converted to numeric word vectors, and [Document 3] mentions that tweets were lowercased and punctuation was removed.\n\nAnswer: ML  logistic regression classifier combined with a Convolutional Neural Network (CNN) to identify self-reported diagnostic tweets.\nNLP methods:  tweet conversion to numeric word vector,  removing tweets containing hyperlinks, removing \"retweets\", removing all tweets containing horoscope indicators,  lowercasing and  removing punctuation.\n\nExample 4:\n\n[Document 1]: To obtain meaningful linguistic data we pre-processed the incoming tweet streams in several ways. As our central question here deals with language semantics of individuals, re-tweets do not bring any additional information to our study, thus we removed them by default. We also removed any expressions considered to be semantically meaningless like URLs, emoticons, mentions of other users (denoted by the @ symbol) and hashtags (denoted by the # symbol) to simplify later post-processing. In addition, as a last step of textual pre-processing, we downcased and stripped the punctuation from the text of every tweet.\n\nQuestion: How do they preprocess Tweets?\n\nExplanation:"}
{"question_id": "f8bba20d1781ce2b14fad28d6eff024e5a6c2c02", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: As a first step towards developing an automated system for detecting the features of student talk that lead to high quality discussions, we propose a new annotation scheme for student talk during ELA \u201ctext-based\" discussions - that is, discussions that center on a text or piece of literature (e.g., book, play, or speech). The annotation scheme was developed to capture three aspects of classroom talk that are theorized in the literature as important to discussion quality and learning opportunities: argumentation (the process of systematically reasoning in support of an idea), specificity (the quality of belonging or relating uniquely to a particular subject), and knowledge domain (area of expertise represented in the content of the talk). We demonstrate the reliability and validity of our scheme via an annotation study of five transcripts of classroom discussion.\n\nQuestion: how do they measure discussion quality?\n\nExplanation:  The annotation scheme captures three aspects of classroom talk: argumentation, specificity, and knowledge domain. These three aspects are important to discussion quality and learning opportunities, according to the literature.\n\nAnswer: Measuring three aspects: argumentation, specificity and knowledge domain.\n\nExample 2:\n\nQuestion: How is the quality of the discussion evaluated?\n\nExplanation:  There is not enough information provided to answer the question.\n\nAnswer: Unanswerable\n\nExample 3:\n\n[Document 1]: Table 2 shows that our ensemble model, controlled with the NLG and Q&A styles, achieved state-of-the-art performance on the NLG and Q&A tasks in terms of Rouge-L. In particular, for the NLG task, our single model outperformed competing models in terms of both Rouge-L and Bleu-1. The capability of creating abstractive summaries from the question and passages contributed to its improvements over the state-of-the-art extractive approaches BIBREF6 , BIBREF7 .\n\nQuestion: How do they measure the quality of summaries?\n\nExplanation: \n\nAccording to [Document 1], the quality of summaries is measured by Rouge-L and Bleu-1.\n\nAnswer: Rouge-L, Bleu-1\n\nExample 4:\n\n[Document 1]: We further evaluate the semantic coherence of the words in a topic learnt by LDA, PTM, DMR, LF-LDA, WF-LDA, GPU-DMM and MetaLDA. Here we use the Normalised Pointwise Mutual Information (NPMI) BIBREF31 , BIBREF32 to calculate topic coherence score for topic INLINEFORM0 with top INLINEFORM1 words: INLINEFORM2 , where INLINEFORM3 is the probability of word INLINEFORM4 , and INLINEFORM5 is the joint probability of words INLINEFORM6 and INLINEFORM7 that co-occur together within a sliding window. Those probabilities were computed on an external large corpus, i.e., a 5.48GB Wikipedia dump in our experiments. The NPMI score of each topic in the experiments is calculated with top 10 words ( INLINEFORM8 ) by the Palmetto package. Again, we report the average scores and the standard deviations over 5 random runs.\n\nQuestion: How do they measure topic quality?\n\nExplanation:"}
{"question_id": "8e9561541f2e928eb239860c2455a254b5aceaeb", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German.\n\nQuestion: what language pairs are explored?\n\nExplanation: \n\nAccording to [Document 1], the language pairs explored are German-English, Turkish-English, and English-German.\n\nAnswer: German-English, Turkish-English, English-German\n\nExample 2:\n\n[Document 1]: We perform experiments (\u00a7 SECREF5 ) on two English-Japanese translation corpora to evaluate the method's utility in improving translation accuracy and reducing the time required for training.\n\nQuestion: What language pairs did they experiment with?\n\nExplanation:  The question can be answered directly from [Document 1].\n\nAnswer: English-Japanese\n\nExample 3:\n\nQuestion: Which language-pair had the better performance?\n\nExplanation: \n\n[Document 1]: We evaluate the system on two language pairs: English-French and English-German.\n\n[Document 2]: The system achieves a new state-of-the-art for English-French translation, with a BLEU score of 41.2.\n\n[Document 3]: For English-German translation, the system achieves a BLEU score of 28.4.\n\nAccording to [Document 1], the system was evaluated on two language pairs: English-French and English-German. According to [Document 2], the system achieved a new state-of-the-art for English-French translation. This information is further supported by [Document 3], which states that the system achieved a BLEU score of 28.4 for English-German translation.\n\nAnswer: French-English\n\nExample 4:\n\n[Document 1]: M-Bert's ability to transfer between languages that are written in different scripts, and thus have effectively zero lexical overlap, is surprising given that it was trained on separate monolingual corpora and not with a multilingual objective. To probe deeper into how the model is able to perform this generalization, Table TABREF14 shows a sample of pos results for transfer across scripts.\n\n[Document 2]: Among the most surprising results, an M-Bert model that has been fine-tuned using only pos-labeled Urdu (written in Arabic script), achieves 91% accuracy on Hindi (written in Devanagari script), even though it has never seen a single pos-tagged Devanagari word. This provides clear evidence of M-Bert's multilingual representation ability, mapping structures onto new vocabularies based on a shared representation induced solely from monolingual language model training data.\n\n[Document 3]: However, cross-script transfer is less accurate for other pairs, such as English and Japanese, indicating that M-Bert's multilingual representation is not able to generalize equally well in all cases. A possible explanation for this, as we will see in section SECREF18 , is typological similarity. English and Japanese have a different order of subject, verb and object, while English and Bulgarian have the same, and M-Bert may be having trouble generalizing across different orderings.\n\n[Document 4]: Generalizing across typological features \n\n[Document 5]: Table TABREF20 shows macro-averaged pos accuracies for transfer between languages grouped according to two typological features: subject/object/verb order, and adjective/noun order BIBREF11 . The results reported include only zero-shot transfer, i.e. they do not include cases training and testing on the same language. We can see that performance is best when transferring between languages that share word order features, suggesting that while M-Bert's multilingual representation is able to map learned structures onto new vocabularies, it does not seem to learn systematic transformations of those structures to accommodate a target language with different word order.\n\n[Document 6]: Our results show that M-Bert is able to perform cross-lingual generalization surprisingly well. More importantly, we present the results of a number of probing experiments designed to test various hypotheses about how the model is able to perform this transfer. Our experiments show that while high lexical overlap between languages improves transfer, M-Bert is also able to transfer between languages written in different scripts\u2014thus having zero lexical overlap\u2014indicating that it captures multilingual representations. We further show that transfer works best for typologically similar languages, suggesting that while M-Bert's multilingual representation is able to map learned structures onto new vocabularies, it does not seem to learn systematic transformations of those structures to accommodate a target language with different word order.\n\nQuestion: What language pairs are affected?\n\nExplanation:"}
{"question_id": "eafea4a24d103fdecf8f347c7d84daff6ef828a3", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We use data from Mandarin Chinese and Cantonese. For each language, the data consists of a list of spoken words, recorded by the same speaker. The Mandarin dataset is from a female speaker and is provided by Shtooka, and the Cantonese dataset is from a male speaker and is downloaded from Forvo, an online crowd-sourced pronunciation dictionary. We require all samples within each language to be from the same speaker to avoid the difficulties associated with channel effects and inter-speaker variation. We randomly sample 400 words from each language, which are mostly between 2 and 4 syllables; to reduce the prosody effects with longer utterances, we exclude words longer than 4 syllables.\n\nQuestion: What dataset is used for training?\n\nExplanation: \n\nAccording to [Document 1], the Mandarin dataset is from a female speaker and is provided by Shtooka, and the Cantonese dataset is from a male speaker and is downloaded from Forvo, an online crowd-sourced pronunciation dictionary.\n\nAnswer: Mandarin dataset, Cantonese dataset\n\nExample 2:\n\n[Document 1]: Training and Validation Sets We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned. We further split the two corpora into a training set and a validation set.\n\nQuestion: What dataset is used for training?\n\nExplanation:  [Document 1] states that the training set is composed of a corpus of poems and a corpus of vernacular literature.\n\nAnswer: We collected a corpus of poems and a corpus of vernacular literature from online resources\n\nExample 3:\n\n[Document 1]: We evaluate our proposed approach for joint sentiment and emotion analysis on the benchmark dataset of SemEval 2016 Task 6 BIBREF7 and Stance Sentiment Emotion Corpus (SSEC) BIBREF15. The SSEC corpus is an annotation of the SemEval 2016 Task 6 corpus with emotion labels. The re-annotation of the SemEval 2016 Task 6 corpus helps to bridge the gap between the unavailability of a corpus with sentiment and emotion labels. The SemEval 2016 corpus contains tweets which are classified into positive, negative or other. It contains 2,914 training and 1,956 test instances. The SSEC corpus is annotated with anger, anticipation, disgust, fear, joy, sadness, surprise and trust labels. Each tweet could belong to one or more emotion classes and one sentiment class. Table TABREF15 shows the data statistics of SemEval 2016 task 6 and SSEC which are used for sentiment and emotion analysis, respectively.\n\nQuestion: What are the datasets used for training?\n\nExplanation: \n\nAccording to [Document 1], the SemEval 2016 Task 6 BIBREF7 and Stance Sentiment Emotion Corpus (SSEC) BIBREF15 were used for training. This information is further supported by [Document 2], [Document 3], and [Document 5].\n\nAnswer: SemEval 2016 Task 6 BIBREF7, Stance Sentiment Emotion Corpus (SSEC) BIBREF15\n\nExample 4:\n\n[Document 1]: To assess the performance of binary paragraph vectors, we carried out experiments on three datasets: 20 Newsgroups, a cleansed version (also called v2) of Reuters Corpus Volume 1 (RCV1) and English Wikipedia. As paragraph vectors can be trained with relatively large vocabularies, we did not perform any stemming of the source text. However, we removed stop words as well as words shorter than two characters and longer than 15 characters. Results reported by BIBREF15 indicate that performance of PV-DBOW can be improved by including n-grams in the model. We therefore evaluated two variants of Binary PV-DBOW: one predicting words in documents and one predicting words and bigrams. Since 20 Newsgroups is a relatively small dataset, we used all words and bigrams from its documents. This amounts to a vocabulary with slightly over one million elements. For the RCV1 dataset we used words and bigrams with at least 10 occurrences in the text, which gives a vocabulary with approximately 800 thousands elements. In case of English Wikipedia we used words and bigrams with at least 100 occurrences, which gives a vocabulary with approximately 1.5 million elements.\n\n[Document 2]: In this work we present Binary Paragraph Vector models, an extensions to PV-DBOW and PV-DM that learn short binary codes for text documents. One inspiration for binary paragraph vectors comes from a recent work by BIBREF11 on learning binary codes for images. Specifically, we introduce a sigmoid layer to the paragraph vector models, and train it in a way that encourages binary activations. We demonstrate that the resultant binary paragraph vectors significantly outperform semantic hashing codes. We also evaluate binary paragraph vectors in transfer learning settings, where training and inference are carried out on unrelated text corpora. Finally, we study models that simultaneously learn short binary codes for document filtering and longer, real-valued representations for ranking. While BIBREF11 employed a supervised criterion to learn image codes, binary paragraph vectors remain unsupervised models: they learn to predict words in documents.\n\nQuestion: Which training dataset do they use?\n\nExplanation:"}
{"question_id": "3e4e415e346a313f5a7c3764fe0f51c11f51b071", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: What language(s) is the model trained/tested on?\n\nExplanation:  There is not enough information in the documents to answer this question.\n\nAnswer: Unanswerable\n\nExample 2:\n\nQuestion: Is the model tested for language identification?\n\nExplanation: \n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task\n\nAnswer: No\n\nExample 3:\n\n[Document 1]: The dataset at HASOC 2019 were given in three languages: Hindi, English, and German. Dataset in Hindi and English had three subtasks each, while German had only two subtasks. We participated in all the tasks provided by the organisers and decided to develop a single model that would be language agnostic. We used the same model architecture for all the three languages.\n\n[Document 2]: In the results of subtask A, models are mainly affected by imbalance of the dataset. The training dataset of Hindi dataset was more balanced than English or German dataset. Hence, the results were around 0.78. As the dataset in German language was highly imbalanced, the results drops to 0.62. In subtask B, the highest F1 score reached was by the profane class for each language in table TABREF20. The model got confused between OFFN, HATE and PRFN labels which suggests that these models are not able to capture the context in the sentence. The subtask C was again a case of imbalanced dataset as targeted(TIN) label gets the highest F1 score in table TABREF21.\n\nQuestion: What are the languages used to test the model?\n\nExplanation: \n\nAccording to [Document 1], the model was tested on Hindi, English and German. This information is further supported by [Document 2].\n\nAnswer: Hindi, English and German (German task won)\n\nExample 4:\n\nQuestion: What language is the model tested on?\n\nExplanation:"}
{"question_id": "3a25f82512d56d9e1ffba72f977f515ae3ba3cca", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: Do they experiment with language modeling on large datasets?\n\nExplanation: \n\nThere is no mention of language modeling or large datasets in any of the documents.\n\nAnswer: No\n\nExample 2:\n\n[Document 1]: The dataset needed to train the W2V was obtained using the information extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila). The dataset (composed of 2.6 GB of raw text) includes $421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences.\n\nQuestion: How big is dataset used to train Word2Vec for the Italian Language?\n\nExplanation:  The answer to the question can be found directly in [Document 1].\n\nAnswer: $421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences\n\nExample 3:\n\nQuestion: Is proposed abstractive dialog summarization dataset open source?\n\nExplanation:  The question cannot be answered with the given documents. [Document 1] discusses the abstractive dialog summarization dataset, but does not mention whether it is open source. [Document 2] is a paper that uses the dataset, but again, there is no mention of whether it is open source. [Document 3] is a paper that discusses a different dataset.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: As mentioned above, a number of language resources have been made available at the repository m\u00e1lf\u00f6ng. Most of these are now also available at the CLARIN-IS website and will be integrated into the CLARIN Virtual Language Observatory. Below we give a brief and non-exhaustive overview of language resources for Icelandic which will be developed in the programme.\n\n[Document 2]: We will update the IGC with new data from more sources and continue collecting data from rights holders who have given their permission for using their material. A new version will be released each year during the five-year programme.\n\n[Document 3]: Treebanks. The largest of the syntactically parsed treebanks that exist is the Icelandic Parsed Historical Corpus (IcePaHC; Wallenberg et al., 2011; R\u00f6gnvaldsson et al., 2011, 2012), which contains one million words from the 12th to the 21st century. The scheme used for the syntactic annotation is based on the Penn Parsed Corpora of Historical English BIBREF24, BIBREF25. On the other hand, no Universal Dependencies (UD)-treebanks are available for Icelandic. Within the programme, a UD-treebank will by built, based on IcePaHC, and extended with new material.\n\n[Document 4]: Morphological database. The Database of Icelandic Morphology (DIM; Bjarnad\u00f3ttir et al., 2019) contains inflectional paradigms of about 287,000 lemmas. A part of the database, DMII-Core, only includes data in a prescriptive context and is suited for language learners, creating teaching material and other prescriptive uses. It consists of the inflection of approx. 50,000 words. We will extend it by reviewing ambiguous inflection forms. We will define format for data publication as the core will be available for use by a third party. For the sake of simplifying the process of adding material to the database and its maintenance, we will take advantage of the lexicon acquisition tool described in Section SECREF16 and adapt it for DIM.\n\nQuestion: Does programme plans gathering and open sourcing some large dataset for Icelandic language?\n\nExplanation:"}
{"question_id": "57ee20f494d8ce3fae46028c3f3551d180dba3e0", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: What languages are the model evaluated on?\n\nExplanation:  The documents do not mention any languages specifically.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: We evaluate the proposed methods in 8 languages, showing a significant ability to learn from partial data. We additionally experiment with initializing CBL with domain-specific instance-weighting schemes, showing mixed results. In the process, we use weighted variants of popular NER models, showing strong performance in both non-neural and neural settings. Finally, we show experiments in a real-world setting, by employing non-speakers to manually annotate romanized Bengali text. We show that a small amount of non-speaker annotation combined with our method can outperform previous methods.\n\n[Document 2]: We experiment on 8 languages. Four languages \u2013 English, German, Spanish, Dutch \u2013 come from the CoNLL 2002/2003 shared tasks BIBREF21, BIBREF22. These are taken from newswire text, and have labelset of Person, Organization, Location, Miscellaneous.\n\n[Document 3]: The remaining four languages come from the LORELEI project BIBREF23. These languages are: Amharic (amh: LDC2016E87), Arabic (ara: LDC2016E89), Hindi (hin: LDC2017E62), and Somali (som: LDC2016E91). These come from a variety of sources including discussion forums, newswire, and social media. The labelset is Person, Organization, Location, Geo-political entity. We define train/development/test splits, taking care to keep a similar distribution of genres in each split. Data statistics for all languages are shown in Table TABREF25.\n\nQuestion: Which languages are evaluated?\n\nExplanation:  According to [Document 1], the proposed methods are evaluated in 8 languages. This information is further supported by [Document 2] and [Document 3].\n\nAnswer: Bengali, English, German, Spanish, Dutch, Amharic, Arabic, Hindi, Somali \n\nExample 3:\n\nQuestion: Do they evaluate in other language appart from English?\n\nExplanation:  The documents do not mention any other languages besides English.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: trusted resources: We randomly collected 200 news articles about broad topics commonly reported by news wires over our target time period. These range from news about celebrities and disasters to financial and political affairs as seen in table 1 . Since we operate on Chinese social media, we gathered news articles from Xinhua News Agency, the leading news-wire in China. To ensure a fair evaluation, we collected the news articles before judging rumours, not knowing which rumours we would find later on. We also only consider news articles published before the timestamps of the social media messages.\n\n[Document 2]: For our social media stream, we chose Sina Weibo, a Chinese social media service with more than 200 million active users. Micro-blogs from Sina Weibo are denoted as 'weibos'.\n\nQuestion: What languages do they evaluate their methods on?\n\nExplanation:"}
{"question_id": "2ddfb40a9e73f382a2eb641c8e22bbb80cef017b", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Our experiments on a goal-oriented dialog corpus, the personalized bAbI dialog dataset, show that leveraging personal information can significantly improve the performance of dialog systems. The Personalized MemN2N outperforms current state-of-the-art methods with over 7% improvement in terms of per-response accuracy. A test with real human users also illustrates that the proposed model leads to better outcomes, including higher task completion rate and user satisfaction.\n\nQuestion: What datasets did they use?\n\nExplanation:  [Document 1] mentions that experiments were conducted on the personalized bAbI dialog dataset.\n\nAnswer: the personalized bAbI dialog dataset\n\nExample 2:\n\n[Document 1]: Single-Turn: This dataset consists of single-turn instances of statements and responses from the MiM chatbot developed at Constellation AI BIBREF5. The responses provided were then evaluated using Amazon Mechanical Turk (AMT) workers. A total of five AMT workers evaluated each of these pairs. The mean of the five evaluations is then used as the target variable. A sample can be seen in Table TABREF3. This dataset was used during experiments with results published in the Results section.\n\n[Document 2]: Multi-Turn: This dataset is taken from the ConvAI2 challenge and consists of various types of dialogue that have been generated by human-computer conversations. At the end of each dialogue, an evaluation score has been given, for each dialogue, between 1\u20134.\n\nQuestion: what datasets did they use?\n\nExplanation: \n\nAccording to [Document 1], the Single-Turn dataset was used in the experiments. This information is further supported by [Document 2], which states that the Multi-Turn dataset was used in the experiments.\n\nAnswer: Single-Turn, Multi-Turn\n\nExample 3:\n\n[Document 1]: We present an extensive comparison of the differences in recognition accuracy for eight languages (Sec. SECREF5 ) and compare the accuracy of models trained on publicly available datasets where available (Sec. SECREF4 ). In addition, we propose a new standard experimental protocol for the IBM-UB-1 dataset BIBREF25 (Sec. SECREF50 ) to enable easier comparison between approaches in the future.\n\n[Document 2]: The IAM-OnDB dataset BIBREF42 is probably the most used evaluation dataset for online handwriting recognition. It consists of 298 523 characters in 86 272 word instances from a dictionary of 11 059 words written by 221 writers. We use the standard IAM-OnDB dataset separation: one training set, two validations sets and a test set containing 5 363, 1 438, 1 518 and 3 859 written lines, respectively. We tune the decoder weights using the validation set with 1 438 items and report error rates on the test set.\n\n[Document 3]: We provide an evaluation of our production system trained on our in-house datasets applied to a number of publicly available benchmark datasets from the literature. Note that for all experiments presented in this section we evaluate our current live system without any tuning specifec to the tasks at hand.\n\n[Document 4]: The ICDAR-2013 Competition for Online Handwriting Chinese Character Recognition BIBREF45 introduced a dataset for classifying the most common Chinese characters. We report the error rates in comparison to published results from the competition and more recent work done by others in Table TABREF56 .\n\n[Document 5]: In the ICFHR2018 Competition on Vietnamese Online Handwritten Text Recognition using VNOnDB BIBREF50 , our production system was evaluated against other systems. The system used in the competition is the one reported and described in this paper. Due to licensing restrictions we were unable to do any experiments on the competition training data, or specific tuning for the competition, which was not the case for the other systems mentioned here.\n\nQuestion: What datasets did they use?\n\nExplanation: \n\nAccording to [Document 1], the IBM-UB-1 dataset BIBREF25 was used. This information is further supported by [Document 2], [Document 3], [Document 4], and [Document 5].\n\nAnswer: IBM-UB-1 dataset BIBREF25, IAM-OnDB dataset BIBREF42, The ICDAR-2013 Competition for Online Handwriting Chinese Character Recognition BIBREF45, ICFHR2018 Competition on Vietnamese Online Handwritten Text Recognition using VNOnDB BIBREF50\n\nExample 4:\n\n[Document 1]: We perform ner experiments on two datasets: the publicly available CoNLL-2002 and -2003 sets, containing Dutch, Spanish, English, and German BIBREF5 , BIBREF6 ; and an in-house dataset with 16 languages, using the same CoNLL categories. Table TABREF4 shows M-Bert zero-shot performance on all language pairs in the CoNLL data.\n\n[Document 2]: We perform pos experiments using Universal Dependencies (UD) BIBREF7 data for 41 languages. We use the evaluation sets from BIBREF8 . Table TABREF7 shows M-Bert zero-shot results for four European languages. We see that M-Bert generalizes well across languages, achieving over INLINEFORM0 accuracy for all pairs.\n\n[Document 3]: We sample 5000 pairs of sentences from WMT16 BIBREF14 and feed each sentence (separately) to M-Bert with no fine-tuning. We then extract the hidden feature activations at each layer for each of the sentences, and average the representations for the input tokens except [cls] and [sep], to get a vector for each sentence, at each layer INLINEFORM0 , INLINEFORM1 . For each pair of sentences, e.g. INLINEFORM2 , we compute the vector pointing from one to the other and average it over all pairs: INLINEFORM3 , where INLINEFORM4 is the number of pairs. Finally, we translate each sentence, INLINEFORM5 , by INLINEFORM6 , find the closest German sentence vector, and measure the fraction of times the nearest neighbour is the correct pair, which we call the \u201cnearest neighbor accuracy\u201d.\n\nQuestion: What datasets did they use?\n\nExplanation:"}
{"question_id": "641fe5dc93611411582e6a4a0ea2d5773eaf0310", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: To study the lexical and semantic diversities of responses, we performed three analyses. First, we aggregated all worker responses to a particular question into a single list corresponding to that question. Across all questions, we found that the number of unique responses was higher for the AUI than for the Control (Fig. FIGREF19 A), implying higher diversity for AUI than for Control.\n\n[Document 2]: Second, we compared the diversity of individual responses between Control and AUI for each question. To measure diversity for a question, we computed the number of responses divided by the number of unique responses to that question. We call this the response density. A set of responses has a response density of 1 when every response is unique but when every response is the same, the response density is equal to the number of responses. Across the ten questions, response density was significantly lower for AUI than for Control (Wilcoxon signed rank test paired on questions: INLINEFORM0 , INLINEFORM1 , INLINEFORM2 ) (Fig. FIGREF19 B).\n\nQuestion: How was lexical diversity measured?\n\nExplanation: \n\nAccording to [Document 1], lexical diversity was measured by aggregating all worker responses to a particular question into a single list corresponding to that question. The number of unique responses was then found.\n\nAccording to [Document 2], lexical diversity was measured by computing the number of responses divided by the number of unique responses to that question.\n\nAnswer: By computing number of unique responses and number of responses divided by the number of unique responses to that question for each of the questions\n\nExample 2:\n\n[Document 1]: Two-talker overlapped speech is artificially generated by mixing these waveform segments. To maximize the speech overlap, we developed a procedure to mix similarly sized segments at around 0dB. First, we sort the speech segments by length. Then, we take segments in pairs, zero-padding the shorter segment so both have the same length. These pairs are then mixed together to create the overlapped speech data. The overlapping procedure is similar to BIBREF13 except that we make no modification to the signal levels before mixing . After overlapping, there's 150 hours data in the training, called 150 hours dataset, and 915 utterances in the test set. After decoding, there are 1830 utterances for evaluation, and the shortest utterance in the hub5e-swb dataset is discarded. Additionally, we define a small training set, the 50 hours dataset, as a random 50 hour subset of the 150 hours dataset. Results are reported using both datasets.\n\nQuestion: How are the two datasets artificially overlapped?\n\nExplanation:  [Document 1] explains that the two datasets are artificially overlapped by sorting the speech segments by length, taking segments in pairs, zero-padding the shorter segment so both have the same length, and then mixing the pairs together.\n\nAnswer: we sort the speech segments by length, we take segments in pairs, zero-padding the shorter segment so both have the same length, These pairs are then mixed together\n\nExample 3:\n\nQuestion: Do they evaluate their framework on content of low lexical variety?\n\nExplanation: \n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task\n\nAnswer: No\n\nExample 4:\n\n[Document 1]: Explanations for a given question here take the form of a list of sentences, where each sentence is a reference to a specific table row in the table store. To increase their utility for knowledge and inference analyses, we require that each sentence in an explanation be explicitly lexically connected (i.e. share words) with either the question, answer, or other sentences in the explanation. We call this lexically-connected set of sentences an explanation graph.\n\nQuestion: What does it mean for sentences to be \"lexically overlapping\"?\n\nExplanation:"}
{"question_id": "c25014b7e57bb2949138d64d49f356d69838bc25", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We compare the results of our model to a baseline multi-task architecture inspired by yang2016multi. In our baseline model there are no explicit connections between tasks - the only shared parameters are in the hidden layer.\n\nQuestion: What is the baseline?\n\nExplanation:  [Document 1] states that the baseline is a multi-task architecture inspired by yang2016multi.\n\nAnswer: The baseline is a multi-task architecture inspired by another paper.\n\nExample 2:\n\n[Document 1]: Emotions have been used in many natural language processing tasks and they showed their efficiency BIBREF35. We aim at investigating their efficiency to detect false information. In addition to EIN, we created a model (Emotion-based Model) that uses emotional features only and compare it to two baselines. Our aim is to investigate if the emotional features independently can detect false news. The two baselines of this model are Majority Class baseline (MC) and the Random selection baseline (RAN).\n\nQuestion: What is the baseline?\n\nExplanation:  The baselines for the Emotion-based Model are the Majority Class baseline (MC) and the Random selection baseline (RAN), as stated in [Document 1].\n\nAnswer: Majority Class baseline (MC) , Random selection baseline (RAN)\n\nExample 3:\n\n[Document 1]: Evaluation metric. Since our problem is imbalanced, we use the F1 score as our evaluation metric. For the tagging approach, we average the labels of words with the same stemmed version to obtain a single prediction for the stemmed word. To establish a baseline, we consider a random method that predicts the positive label with 0.15 probability (the base rate of positive instances).\n\n[Document 2]: To examine the utility of our features in a neural framework, we further adapt our word-level task as a tagging task, and use LSTM as a baseline. Specifically, we concatenate an OP and PC with a special token as the separator so that an LSTM model can potentially distinguish the OP from PC, and then tag each word based on the label of its stemmed version. We use GloVe embeddings to initialize the word embeddings BIBREF40. We concatenate our proposed features of the corresponding stemmed word to the word embedding; the resulting difference in performance between a vanilla LSTM demonstrates the utility of our proposed features. We scale all features to $[0, 1]$ before fitting the models. As introduced in Section SECREF3, we split our tuples of (OP, PC, explanation) into training, validation, and test sets, and use the validation set for hyperparameter tuning. Refer to the supplementary material for additional details in the experiment.\n\nQuestion: What is the baseline?\n\nExplanation: \n\nAccording to [Document 1], the baseline is a random method that predicts the positive label with 0.15 probability. This is further supported by [Document 2], which states that LSTM is used as a baseline.\n\nAnswer: random method , LSTM \n\nExample 4:\n\n[Document 1]: We also implement a rule-based unigram matching baseline, which takes the entry with highest unigram overlap with the query string to be the top match. This model only returns the top match, so only top-1 recall is evaluated, and top-3 recall is not applicable. Both neural models outperform the baseline, but by far the best performing model is BERT with fine-tuning, which retrieves the correct match for nearly 90% of queries (Table TABREF21).\n\nQuestion: What is the baseline?\n\nExplanation:"}
{"question_id": "9595fdf7b51251679cd39bc4f6befc81f09c853c", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Self-disclosure clues have been extensively utilized for creating ground-truth data for numerous social media analytic studies e.g., for predicting demographics BIBREF36 , BIBREF41 , and user's depressive behavior BIBREF46 , BIBREF47 , BIBREF48 . For instance, vulnerable individuals may employ depressive-indicative terms in their Twitter profile descriptions. Others may share their age and gender, e.g., \"16 years old suicidal girl\"(see Figure FIGREF15 ). We employ a huge dataset of 45,000 self-reported depressed users introduced in BIBREF46 where a lexicon of depression symptoms consisting of 1500 depression-indicative terms was created with the help of psychologist clinician and employed for collecting self-declared depressed individual's profiles. A subset of 8,770 users (24 million time-stamped tweets) containing 3981 depressed and 4789 control users (that do not show any depressive behavior) were verified by two human judges BIBREF46 . This dataset INLINEFORM0 contains the metadata values of each user such as profile descriptions, followers_count, created_at, and profile_image_url.\n\nQuestion: How is the data annotated?\n\nExplanation:  [Document 1] explains that the data are self-reported by Twitter users and then verified by two human experts. This information is further supported by [Document 2].\n\nAnswer: The data are self-reported by Twitter users and then verified by two human experts.\n\nExample 2:\n\nQuestion: How is the data annotated?\n\nExplanation:  The documents do not provide enough information to answer the question.\n\nAnswer: Unanswerable\n\nExample 3:\n\n[Document 1]: To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each.\n\n[Document 2]: Word are white-space and punctuation separated, and some spelling errors are corrected (1.33% of the total words) to have very clean test cases. Lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization as shown in Figure FIGREF2 .\n\nQuestion: How was the dataset annotated?\n\nExplanation:  [Document 2] states that lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization.\n\nAnswer: Lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization\n\nExample 4:\n\n[Document 1]: Next we aimed to estimate SES from architectural/urban features associated to the home locations. Thus, for each home location we collected two additional satellite views at different resolutions as well as six Street View images, each with a horizontal view of approximately INLINEFORM0 . We randomly selected a sample of INLINEFORM1 locations and involved architects to assign a SES score (from 1 to 9) to a sample set of selected locations based on the satellite and Street View around it (both samples had 333 overlapping locations). For validation, we took users from each annotated SES class and computed the distribution of their incomes inferred from the IRIS census data (see Section SECREF6 ). Violin plots in Fig. FIGREF12 d show that in expert annotated data, as expected, the inferred income values were positively correlated with the annotated SES classes. Labels were then categorized into two socioeconomic classes for comparison purposes. All in all, both annotators assigned the same label to the overlapping locations in INLINEFORM2 of samples.\n\nQuestion: How is the remotely sensed data annotated?\n\nExplanation:"}
{"question_id": "6df57a21ca875e63fb39adece6a9ace5bb2b2cfa", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We address the above issues with a new methodology. First, we introduce a two-stage labeling strategy for sentiment texts. In the first stage, annotators are invited to label a large number of short texts with relatively pure sentiment orientations. Each sample is labeled by only one annotator. In the second stage, a relatively small number of text samples with mixed sentiment orientations are annotated, and each sample is labeled by multiple annotators. Second, we propose a two-level long short-term memory (LSTM) BIBREF4 network to achieve two-level feature representation and classify the sentiment orientations of a text sample to utilize two labeled data sets. Lastly, in the proposed two-level LSTM network, lexicon embedding is leveraged to incorporate linguistic features used in lexicon-based methods.\n\nQuestion: What is the new labeling strategy?\n\nExplanation:  The new labeling strategy is detailed in [Document 1]. In the first stage, annotators label a large number of short texts with relatively pure sentiment orientations. In the second stage, a relatively small number of text samples with mixed sentiment orientations are annotated, and each sample is labeled by multiple annotators.\n\nAnswer: They use a two-stage labeling strategy where in the first stage single annotators label a large number of short texts with relatively pure sentiment orientations and in the second stage multiple annotators label few text samples with mixed sentiment orientations\n\nExample 2:\n\n[Document 1]: We create the graph using all data, and training set tweets have an initial language label distribution. A na\u00efve approach to building the tweet-tweet subgraph requires O( INLINEFORM0 ) comparisons, measuring the similarity of each tweet with all others. Instead, we performed INLINEFORM1 -nearest-neighbour classification on all tweets, represented as a bag of unigrams, and compared each tweet and the top- INLINEFORM2 neighbours. We use Junto (mad) BIBREF5 to propagate labels from labelled to unlabelled nodes. Upon convergence, we renormalise label scores for initially unlabelled nodes to find the value of INLINEFORM4 .\n\nQuestion: How are labels propagated using this approach?\n\nExplanation:  This information is found in [Document 1].\n\nAnswer: We use Junto (mad) BIBREF5 to propagate labels from labelled to unlabelled nodes. \n\nExample 3:\n\n[Document 1]: Secondly, texts go through a cascade of annotation tools, enriching it with the following information:\n\n[Document 2]: Morphosyntactic interpretations (sets of tags), using Morfeusz 0.82 BIBREF25 ,\n\n[Document 3]: Tagging (selection of the most probable interpretation), using a transformation-based learning tagger, PANTERA 0.9.1 BIBREF26 ,\n\n[Document 4]: Syntactic groups (possibly nested) with syntactic and semantic heads, using a rule-based shallow parser Spejd 1.3.7 BIBREF27 with a Polish grammar, including improved version of modifications by BIBREF28 , enabling lemmatisation of nominal syntactic groups,\n\n[Document 5]: Named entities, using two available tools: NERF 0.1 BIBREF29 and Liner2 2.3 BIBREF30 .\n\nQuestion: How is the data in RAFAEL labelled?\n\nExplanation:  [Document 1] states that texts go through a cascade of annotation tools, which [Document 2-5] list as Morfeusz, PANTERA, Spejd, NERF and Liner.\n\nAnswer: Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner\n\nExample 4:\n\n[Document 1]: SE concepts are less ambiguous as compared to generic natural language text. A word usually means one concept. For example, the word `system' usually means the same when referring to a `complex system', `system structure', or `management system' in the SE domain. In generic text, the meaning of terms like `evaluation', `requirement', or `analysis' may contextually differ. We would like domain specific phrases such as `system evaluation', `performance requirement', or `system analysis' to be single entities. Based on the operational and system concepts described in BIBREF0, we carefully construct a set of concept-labels for the SE handbook which is shown in the next section.\n\n[Document 2]: CONCEPT RECOGNITION ::: BIO Labelling Scheme\n\n[Document 3]: abb: represents abbreviations such as TRL representing Technology Readiness Level.\n\n[Document 4]: grp: represents a group of people or an individual such as Electrical Engineers, Systems Engineers or a Project Manager.\n\n[Document 5]: syscon: represents any system concepts such as engineering unit, product, hardware, software, etc. They mostly represent physical concepts.\n\n[Document 6]: opcon: represents operational concepts such as decision analysis process, technology maturity assessment, system requirements review, etc.\n\n[Document 7]: seterm: represents generic terms that are frequently used in SE text and those that do not fall under syscon or opcon such as project, mission, key performance parameter, audit etc.\n\n[Document 8]: event: represents event-like information in SE text such as Pre-Phase A, Phase A, Phase B, etc.\n\n[Document 9]: org: represents an organization such as `NASA', `aerospace industry', etc.\n\n[Document 10]: art: represents names of artifacts or instruments such as `AS1300'\n\n[Document 11]: cardinal: represents numerical values such as `1', `100', 'one' etc.\n\n[Document 12]: loc: represents location-like entities such as component facilities or centralized facility.\n\n[Document 13]: mea: represents measures, features, or behaviors such as cost, risk, or feasibility.\n\nQuestion: How does labeling scheme look like?\n\nExplanation:"}
{"question_id": "9240ee584d4354349601aeca333f1bc92de2165e", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: What is the agreement value for each dataset?\n\nExplanation:  The documents do not contain any information regarding agreement values for datasets.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: We calculated inter-annotator agreement by Krippendorff's Alpha BIBREF23, which accounts for different annotator pairs and empty values. To also zoom in on the particular agreement by category, we calculated mutual F-scores for each of the categories. This metric is typically used to evaluate system performance by category on gold standard data, but could also be applied to annotation pairs by alternating the roles of the two annotators between classifier and ground truth. A summary of the agreement by categorization is given in Table TABREF10. While both the Relevance and Subject categorizations are annotated at a percent agreement of $0.71$ and $0.70$, their agreement scores are only fair, at $\\alpha =0.27$ and $\\alpha =0.29$. The percent agreement on Stance and Sentiment, which carry more categories than the former two, is $0.54$ for both. Their agreement scores are also fair, at $\\alpha =0.35$ and $\\alpha =0.34$. The mutual F-scores show marked differences in agreement by category, where the categories that were annotated most often typically yield a higher score. This holds for the Relevant category ($0.81$), the Vaccine category ($0.79$) and the Positive category ($0.64$). The Negative category yields a mutual F-score of $0.42$, which is higher than the more frequently annotated categories Neutral ($0.23$) and Not clear ($0.31$). We found that these categories are often confused. After combining the annotations of the two, the stance agreement would be increased to $\\alpha =0.43$.\n\nQuestion: What is the agreement score of their annotated dataset?\n\nExplanation:  The answer can be found in [Document 1].\n\nAnswer:  Relevance and Subject categorizations are annotated at a percent agreement of $0.71$ and $0.70$, their agreement scores are only fair, at $\\alpha =0.27$ and $\\alpha =0.29$, Stance and Sentiment, which carry more categories than the former two, is $0.54$ for both. Their agreement scores are also fair, at $\\alpha =0.35$ and $\\alpha =0.34$, This holds for the Relevant category ($0.81$), the Vaccine category ($0.79$) and the Positive category ($0.64$),  The Negative category yields a mutual F-score of $0.42$, which is higher than the more frequently annotated categories Neutral ($0.23$) and Not clear ($0.31$).\n\nExample 3:\n\n[Document 1]: We describe our rules for WikiSQL here. We first detect WHERE values, which exactly match to table cells. After that, if a cell appears at more than one column, we choose the column name with more overlapped words with the question, with a constraint that the number of co-occurred words is larger than 1. By default, a WHERE operator is INLINEFORM0 , except for the case that surrounding words of a value contain keywords for INLINEFORM1 and INLINEFORM2 . Then, we deal with the SELECT column, which has the largest number of co-occurred words and cannot be same with any WHERE column. By default, the SELECT AGG is NONE, except for matching to any keywords in Table TABREF8 . The coverage of our rule on training set is 78.4%, with execution accuracy of 77.9%.\n\n[Document 2]: Our rule for KBQA is simple without using a curated mapping dictionary. First, we detect an entity from the question using strict string matching, with the constraint that only one entity from the KB has the same surface string and that the question contains only one entity. After that, we get the connected relations of the detected entity, and assign the relation as the one with maximum number of co-occurred words. The coverage of our rule on training set is 16.0%, with an accuracy of 97.3% for relation prediction.\n\n[Document 3]: The pipeline of rules in SequentialQA is similar to that of WikiSQL. Compared to the grammar of WikiSQL, the grammar of SequentialQA has additional actions including copying the previous-turn logical form, no greater than, no more than, and negation. Table TABREF23 shows the additional word-level mapping table used in SequentialQA. The coverage of our rule on training set is 75.5%, with an accuracy of 38.5%.\n\nQuestion: Are the rules dataset specific?\n\nExplanation:  [Document 1], [Document 2], and [Document 3] all describe rules that are specific to the WikiSQL, KBQA, and SequentialQA datasets, respectively. This is further supported by the fact that the rules have different coverage and accuracy rates for each dataset.\n\nAnswer: Yes\n\nExample 4:\n\n[Document 1]: The intra-annotator reliability of both datasets $D_\\textrm {\\textit {LR}}$ and $D_\\textrm {\\textit {HR}}$ is shown in Table TABREF21. The reliability was calculated using the Krippendorff's alpha coefficient. Krippendorff's alpha can handle missing values, which in this case was necessary since many of the texts were annotated by only a few annotators.\n\nQuestion: What is the agreement of the dataset?\n\nExplanation:"}
{"question_id": "e8fa4303b36a47a5c87f862458442941bbdff7d9", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Our first goal is to train a sentiment analysis model (on training and validation datasets) in order to perform classification inference on event-based tweets. We experimented with different feature extraction methods and classification models. Feature extractions examined include Tokenizer, Unigram, Bigram, 5-char-gram, and td-idf methods. Models include both neural nets (e.g. RNNs, CNNs) and standard machine learning tools (e.g. Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel). Model accuracies are reported in Table FIGREF3 .\n\nQuestion: Which machine learning models are used?\n\nExplanation: \n\nAccording to [Document 1], the machine learning models used include RNNs, CNNs, Naive Bayes with Laplace Smoothing, k-clustering, and SVM with linear kernel.\n\nAnswer: RNNs, CNNs, Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel\n\nExample 2:\n\n[Document 1]: We ran three groups of experiments, to assess both the effectiveness of our approach when compared with the approaches we found in literature and its capability of extracting features that are relevant for sarcasm in a cross-domain scenario. In both cases, we denote with the word model one of the possible combinations of classic/statistical LSA and a classifier. The used classifiers are Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF) and gradient boosting (XGB).\n\nQuestion: What classical machine learning algorithms are used?\n\nExplanation:  [Document 1] states that the Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF), and gradient boosting (XGB) algorithms were used.\n\nAnswer: Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF), gradient boosting (XGB)\n\nExample 3:\n\n[Document 1]: To enable sentence-level classification we need to obtain a sentence representation from the word vectors INLINEFORM0 . We achieved this by using a BiLSTM with max pooling, which was shown to be a good universal sentence encoding mechanism BIBREF13 .\n\nQuestion: Which model architecture do they use to obtain representations?\n\nExplanation:  The document states that they use a BiLSTM with max pooling to obtain representations.\n\nAnswer: BiLSTM with max pooling\n\nExample 4:\n\n[Document 1]: Classical IMT approaches relay on the statistical formalization of the MT problem. Given a source sentence $\\mathbf {x}$, SMT aims at finding its most likely translation $\\hat{\\mathbf {y}}$ BIBREF18:\n\n[Document 2]: For years, the prevailing approach to compute this expression have been phrase-based models BIBREF19. These models rely on a log-linear combination of different models BIBREF20: namely, phrase-based alignment models, reordering models and language models; among others BIBREF21, BIBREF22. However, more recently, this approach has shifted into neural models (see se:NMT).\n\n[Document 3]: Prefix-based IMT proposed a user\u2013computer collaboration that starts with the system proposing an initial translation $\\mathbf {y}$ of length $I$. Then, the user corrects the leftmost wrong word $y_i$, inherently validating all preceding words. These words form a validated prefix $\\tilde{\\mathbf {y}}_p$, that includes the corrected word $\\tilde{y}_i$. The system reacts to this user feedback, generating a suffix $\\hat{\\mathbf {y}}_s$ that completes $\\tilde{\\mathbf {y}}_p$ to obtain a new translation of $\\mathbf {x}:\\hat{\\mathbf {y}}~=~\\tilde{\\mathbf {y}}_p\\,\\hat{\\mathbf {y}}_s$. This process is repeated until the user accepts the complete system suggestion. fi:IMT illustrates this protocol.\n\n[Document 4]: Interactive Machine Translation ::: Neural Machine Translation\n\n[Document 5]: In NMT, eq:SMT is modeled by a neural network with parameters $\\mathbf {\\Theta }$:\n\n[Document 6]: This neural network usually follows an encoder-decoder architecture, featuring recurrent networks BIBREF23, BIBREF24, convolutional networks BIBREF25 or attention mechanisms BIBREF26. Model parameters are jointly estimated on large parallel corpora, using stochastic gradient descent BIBREF27, BIBREF28. At decoding time, the system obtains the most likely translation using a beam search method.\n\n[Document 7]: Interactive Machine Translation ::: Prefix-based Interactive Neural Machine Translation\n\n[Document 8]: The prefix-based IMT protocol (see se:PBIMT) can be naturally included into NMT systems since sentences are generated from left to right. In order to take into account the user's feedback and generate compatible hypothesis, the search space must be constraint. Given a prefix $\\tilde{\\mathbf {y}}_p$, only a single path accounts for it. The branching of the search process starts once this path has been covered. Introducing the validated prefix $\\tilde{\\mathbf {y}}_p$, eq:NMT becomes:\n\n[Document 9]: which implies a search over the space of translations, but constrained by the validated prefix $\\tilde{\\mathbf {y}}_p$ BIBREF15.\n\nQuestion: What machine learning techniques are used in the model architecture?\n\nExplanation:"}
{"question_id": "95083d486769b9b5e8c57fe2ef1b452fc3ea5012", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: All models outperform previously reported results for mlstm BIBREF8 despite lower parameter counts. This is likely due to our relatively small batch size. However, they perform fairly similarly. Encouraged by these results, we built an mgru with both hidden and intermediate state sizes set to that of the original mlstm (700). This version highly surpasses the previous state of the art while still having fewer parameters than previous work.\n\nQuestion: Do they compare results against state-of-the-art language models?\n\nExplanation:  [Document 1] states that \"all models outperform previously reported results for mlstm.\" This indicates that the results are being compared against state-of-the-art language models.\n\nAnswer: Yes\n\nExample 2:\n\n[Document 1]: In terms of attraction scores in Table TABREF51, we have three findings: (1) The human-written headlines are more attractive than those from NHG, which agrees with our observation in Section SECREF1. (2) Our TitleStylist can generate more attractive headlines over the NHG and Multitask baselines for all three styles, demonstrating that adapting the model to these styles could improve the attraction and specialization of some parameters in the model for different styles can further enhance the attraction. (3) Adapting the model to the \u201cClickbait\u201d style could create the most attractive headlines, even out-weighting the original ones, which agrees with the fact that click-baity headlines are better at drawing readers' attention. To be noted, although we learned the \u201cClickbait\u201d style into our summarization system, we still made sure that we are generating relevant headlines instead of too exaggerated ones, which can be verified by our relevance scores.\n\nQuestion: Which state-of-the-art model is surpassed by 9.68% attraction score?\n\nExplanation: \n\nAccording to [Document 1], the human-written headlines are more attractive than those from NHG. Therefore, the pure summarization model NHG is surpassed by 9.68% attraction score.\n\nAnswer: pure summarization model NHG\n\nExample 3:\n\n[Document 1]: We compare our HR-VAE model with three strong baselines using VAE for text modelling:\n\n[Document 2]: VAE-LSTM-base: A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue BIBREF0;\n\n[Document 3]: VAE-CNN: A variational autoencoder model with a LSTM encoder and a dilated CNN decoder BIBREF7;\n\n[Document 4]: vMF-VAE: A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution BIBREF5.\n\nQuestion: Do they compare against state of the art text generation?\n\nExplanation:  [Document 1] states that the HR-VAE model is being compared to three strong baselines, all of which use VAE for text modelling. This comparison against state of the art text generation is further supported by [Document 2], [Document 3], and [Document 4].\n\nAnswer: Yes\n\nExample 4:\n\n[Document 1]: FLOAT SELECTED: Table 2: Parsing results on PTB \u00a723 (D=discriminative, G=generative, S=semisupervised). ? indicates the (Vinyals et al., 2015) result with trained only on the WSJ corpus without ensembling.\n\n[Document 2]: FLOAT SELECTED: Table 3: Parsing results on CTB 5.1.\n\n[Document 3]: FLOAT SELECTED: Table 4: Language model perplexity results.\n\nQuestion: what state of the art models do they compare to?\n\nExplanation:"}
{"question_id": "ced63053eb631c78a4ddd8c85ec0f3323a631a54", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .\n\n[Document 2]: Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.\n\n[Document 3]: Health Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 .\n\nQuestion: Which three Twitter sentiment classification datasets are used for experiments?\n\nExplanation:  [Document 1], [Document 2], and [Document 3] all mention the three Twitter sentiment classification datasets that are used for experiments.\n\nAnswer: Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, Health Care Reform (HCR)\n\nExample 2:\n\n[Document 1]: We preprocess the visual data by extracting the sequence of images for each interaction from the point where the agent's or the co-driver's question was completely uttered until the driver's response stops. The average length is 16.3 seconds, with the minimum at 2.2s and the maximum at 54.7s. We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions). It delivers frame-by-frame scores ($\\in [0;100]$) for discrete emotional states of joy, anger and fear. While joy corresponds directly to our annotation, we map anger to our label annoyance and fear to our label insecurity. The maximal average score across all frames constitutes the overall classification for the video sequence. Frames where the software is not able to detect the face are ignored.\n\n[Document 2]: We extract the audio signal for the same sequence as described for facial expressions and apply an off-the-shelf tool for emotion recognition. The software delivers single classification scores for a set of 24 discrete emotions for the entire utterance. We consider the outputs for the states of joy, anger, and fear, mapping analogously to our classes as for facial expressions. Low-confidence predictions are interpreted as \u201cno emotion\u201d. We accept the emotion with the highest score as the discrete prediction otherwise.\n\nQuestion: What are the emotion detection tools used for audio and face input?\n\nExplanation:  [Document 1] and [Document 2] both mention that the team used an off-the-shelf tool for emotion recognition. However, the manufacturer of the tool cannot be disclosed due to licensing restrictions.\n\nAnswer: We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions)\n\nExample 3:\n\nQuestion: Which dataset sources to they use to demonstrate moral sentiment through history?\n\nExplanation:  The documents do not mention any specific dataset sources.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: We conduct experiments on a simplified Chinese emotion cause corpus BIBREF31 , the only publicly available dataset on this task to the best of our knowledge. The corpus contains 2,105 documents from SINA city news. Each document has only one emotion word and one or more emotion causes. The documents are segmented into clauses manually. The main task is to identify which clause contains the emotion cause.\n\nQuestion: what emotion cause dataset was used?\n\nExplanation:"}
{"question_id": "c53b036eff430a9d0449fb50b8d2dc9d2679d9fe", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Table 2 shows the ablation performances of various Q-code on the development set. Note that since the testset is hidden from us, we can only perform such an analysis on the development set. Our baseline model using no Q-code achieved a 68.00% and 77.36% EM and F1 scores, respectively. When we added the explicit question type T-code into the baseline model, the performance was improved slightly to 68.16%(EM) and 77.58%(F1). We then used TreeLSTM introduce syntactic parses for question representation and understanding (replacing simple question type as question understanding Q-code), which consistently shows further improvement. We further incorporated the soft adaptation. When letting the number of hidden question types ( $K$ ) to be 20, the performance improves to 68.73%/77.74% on EM and F1, respectively, which corresponds to the results of our model reported in Table 1 . Furthermore, after submitted our result, we have experimented with a large value of $K$ and found that when $K=100$ , we can achieve a better performance of 69.10%/78.38% on the development set.\n\nQuestion: how much of improvement the adaptation model can get?\n\nExplanation:  The answer can be found in the last paragraph of [Document 1].\n\nAnswer:  69.10%/78.38%\n\nExample 2:\n\n[Document 1]: All models are trained and evaluated using the same (w.r.t. data shuffle and split) 10-fold cross-validation (CV) on Trafficking-10k, except for HTDN, whose result is read from the original paper BIBREF9 . During each train-test split, INLINEFORM0 of the training set is further reserved as the validation set for tuning hyperparameters such as L2-penalty in IT, AT and LAD, and learning rate in ORNN. So the overall train-validation-test ratio is 70%-20%-10%. We report the mean metrics from the CV in Table TABREF14 . As previous research has pointed out that there is no unbiased estimator of the variance of CV BIBREF29 , we report the naive standard error treating metrics across CV as independent.\n\n[Document 2]: We can see that ORNN has the best MAE, INLINEFORM0 and Acc. as well as a close 2nd best Wt. Acc. among all models. Its Wt. Acc. is a substantial improvement over HTDN despite the fact that the latter use both text and image data. It is important to note that HTDN is trained using binary labels, whereas the other models are trained using ordinal labels and then have their ordinal predictions converted to binary predictions. This is most likely the reason that even the baseline models except for LAD can yield better Wt. Acc. than HTDN, confirming our earlier claim that polarizing the ordinal labels during training may lead to information loss.\n\nQuestion: By how much do they outperform previous state-of-the-art models?\n\nExplanation:  [Document 1] provides the results of the 10-fold cross-validation for all models. [Document 2] compares the results of the proposed ORNN model with the best state-of-the-art model, HTDN. The results in [Document 2] show that the proposed ORNN model outperforms the HTDN model by 0.769, 1.238, 0.818, 0.772 on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.), respectively.\n\nAnswer: Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)\n\nExample 3:\n\n[Document 1]: Table TABREF11 presents the results on 14 treebanks from the CoNLL shared tasks. Our model yields the best results on both UAS and LAS metrics of all languages except the Japanese. As for Japanese, our model gives unsatisfactory results because the original treebank was written in Roman phonetic characters instead of hiragana, which is used by both common Japanese writing and our pre-trained embeddings. Despite this, our model overall still gives 1.0% higher average UAS and LAS than the previous best parser, BIAF.\n\n[Document 2]: Following BIBREF23, we report results on the test sets of 12 different languages from the UD treebanks along with the current state-of-the-art: BIAF and STACKPTR. Although both BIAF and STACKPTR parsers have achieved relatively high parsing accuracies on the 12 languages and have all UAS higher than 90%, our model achieves state-of-the-art results in all languages for both UAS and LAS. Overall, our model reports more than 1.0% higher average UAS than STACKPTR and 0.3% higher than BIAF.\n\nQuestion: What are performance compared to former models?\n\nExplanation: \n\nAccording to [Document 1], the model gives 1.0% higher average UAS and LAS than the previous best parser, BIAF. This information is further supported by [Document 2], which states that the model reports more than 1.0% higher average UAS than STACKPTR and 0.3% higher than BIAF.\n\nAnswer: model overall still gives 1.0% higher average UAS and LAS than the previous best parser, BIAF, our model reports more than 1.0% higher average UAS than STACKPTR and 0.3% higher than BIAF\n\nExample 4:\n\n[Document 1]: In this section, we report our experiment results on PTB character-level language modeling using our baseline models GRURNN and LSTMRNN as well as our proposed models GRURNTN and LSTMRNTN. Fig. 8 shows performance comparisons from every model based on the validation set's BPC per epoch. In this experiment, GRURNN made faster progress than LSTMRNN, but eventually LSTMRNN converged into a better BPC based on the development set. Our proposed model GRURNTN made faster and quicker progress than LSTMRNTN and converged into a similar BPC in the last epoch. Both proposed models produced lower BPC than our baseline models from the first epoch to the last epoch.\n\n[Document 2]: Table 1 shows PTB test set BPC among our baseline models, our proposed models and several published results. Our proposed model GRURNTN and LSTMRNTN outperformed both baseline models. GRURNTN reduced the BPC from 1.39 to 1.33 (0.06 absolute / 4.32% relative BPC) from the baseline GRURNN, and LSTMRNTN reduced the BPC from 1.37 to 1.34 (0.03 absolute / 2.22% relative BPC) from the baseline LSTMRNN. Overall, GRURNTN slightly outperformed LSTMRNTN, and both proposed models outperformed all of the baseline models on the character-level language modeling task.\n\n[Document 3]: In this section, we report our experiment results on PTB word-level language modeling using our baseline models GRURNN and LSTMRNN and our proposed models GRURNTN and LSTMRNTN. Fig. 9 compares the performance from every models based on the validation set's PPL per epoch. In this experiment, GRURNN made faster progress than LSTMRNN. Our proposed GRURNTN's progress was also better than LSTMRNTN. The best model in this task was GRURNTN, which had a consistently lower PPL than the other models.\n\n[Document 4]: Table 1 shows the PTB test set PPL among our baseline models, proposed models, and several published results. Both our proposed models outperformed their baseline models. GRURNTN reduced the perplexity from 97.78 to 87.38 (10.4 absolute / 10.63% relative PPL) over the baseline GRURNN and LSTMRNTN reduced the perplexity from 108.26 to 96.97 (11.29 absolute / 10.42% relative PPL) over the baseline LSTMRNN. Overall, LSTMRNTN improved the LSTMRNN model and its performance closely resembles the baseline GRURNN. However, GRURNTN outperformed all the baseline models as well as the other models by a large margin.\n\n[Document 5]: We presented a new RNN architecture by combining the gating mechanism and tensor product concepts. Our proposed architecture can learn long-term dependencies from temporal and sequential data using gating units as well as more powerful interaction between the current input and previous hidden layers by introducing tensor product operations. From our experiment on the PennTreeBank corpus, our proposed models outperformed the baseline models with a similar number of parameters in character-level language modeling and word-level language modeling tasks. In a character-level language modeling task, GRURNTN obtained 0.06 absolute (4.32% relative) BPC reduction over GRURNN, and LSTMRNTN obtained 0.03 absolute (2.22% relative) BPC reduction over LSTMRNN. In a word-level language modeling task, GRURNTN obtained 10.4 absolute (10.63% relative) PPL reduction over GRURNN, and LSTMRNTN obtained 11.29 absolute (10.42% relative PPL) reduction over LSTMRNN. In the future, we will investigate the possibility of combining our model with other stacked RNNs architecture, such as Gated Feedback RNN (GFRNN). We would also like to explore other possible tensor operations and integrate them with our RNN architecture. By applying these ideas together, we expect to gain further performance improvement. Last, for further investigation we will apply our proposed models to other temporal and sequential tasks, such as speech recognition and video recognition.\n\nQuestion: How much improvement do the introduced model achieve compared to the previous models?\n\nExplanation:"}
{"question_id": "8cbe3fa4ec0f66071e3d6b829b09b6395b631c44", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We first experiment with a suite of non-neural models, including Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees. We tune the parameters for these models using grid search and 10-fold cross-validation, and obtain results for different combinations of input and features.\n\n[Document 2]: We finally experiment with neural models, although our dataset is relatively small. We train both a two-layer bidirectional Gated Recurrent Neural Network (GRNN) BIBREF20 and Convolutional Neural Network (CNN) (as designed in BIBREF21) with parallel filters of size 2 and 3, as these have been shown to be effective in the literature on emotion detection in text (e.g., BIBREF22, BIBREF23). Because neural models require large amounts of data, we do not cull the data by annotator agreement for these experiments and use all the labeled data we have. We experiment with training embeddings with random initialization as well as initializing with our domain-specific Word2Vec embeddings, and we also concatenate the best feature set from our non-neural experiments onto the representations after the recurrent and convolutional/pooling layers respectively.\n\nQuestion: What supervised methods are used?\n\nExplanation:  [Document 1] and [Document 2] list the supervised methods used: Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees, a two-layer bidirectional Gated Recurrent Neural Network (GRNN) BIBREF20 and Convolutional Neural Network (CNN) (as designed in BIBREF21).\n\nAnswer: Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees, a two-layer bidirectional Gated Recurrent Neural Network (GRNN) BIBREF20 and Convolutional Neural Network (CNN) (as designed in BIBREF21)\n\nExample 2:\n\nQuestion: Does the system trained only using XR loss outperform the fully supervised neural system?\n\nExplanation: \n\n[Document 1]: We compare the performance of our system trained only using XR loss (denoted as XR-only) with the fully supervised neural system (denoted as Full).\n\n[Document 2]: The results in Table 1 show that the XR-only system outperforms the Full system in all three metrics.\n\nAccording to [Document 1], the XR-only system was compared with the Full system. The results in [Document 2] show that the XR-only system outperforms the Full system, which answers the question.\n\nAnswer: Yes\n\nExample 3:\n\n[Document 1]: Support Vector Machine (SVM) ( BIBREF25 ) is a commonly used supervised classification algorithm that has shown good performance over a range of tasks. SVM can be thought of as a binary linear classifier where the goal is to maximize the size of the gap between the class-separating line and the points on either side of the line. This helps avoid over-fitting on the training data. SVMRank is a modification to SVM that assigns scores to each data point and allows the results to be ranked ( BIBREF26 ). We use SVMRank in the experiments below. SVMRank has previously been used in the task of document retrieval in ( BIBREF27 ) for a more traditional short query task and has been shown to be a top-performing system for ranking.\n\nQuestion: what is the supervised model they developed?\n\nExplanation:  [Document 1] mentions that SVMRank is a modification of SVM that assigns scores to each data point and allows the results to be ranked.\n\nAnswer: SVMRank\n\nExample 4:\n\n[Document 1]: Rumour detection on social media is challenging due to the short texts, creative lexical variations and high volume of the streams. The task becomes even harder if we attempt to perform rumour detection on-the-fly, without looking into the future. We provide an effective and highly scalable approach to detect rumours instantly after they were posted with zero delay. We introduce a new features category called novelty based features. Novelty based features compensate the absence of repeated information by consulting additional data sources - news wire articles. We hypothesize that information not confirmed by official news is an indication of rumours. Additionally we introduce pseudo feedback for classification. In a nutshell, documents that are similar to previously detected rumours are considered to be more likely to also be a rumour. The proposed features can be computed in constant time and space allowing us to process high-volume streams in real-time (Muthukrishnan, 2005). Our experiments reveal that novelty based features and pseudo feedback significantly increases detection performance for early rumour detection.\n\nQuestion: Are their methods fully supervised?\n\nExplanation:"}
{"question_id": "4e59808a7f73ac499b9838d3c0ce814196a02473", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We first compare the perplexity of CWVAE with baseline methods. Perplexity measures the probability of model to regenerate the exact targets, which is particular suitable for evaluating the model performance on one-to-many problem BIBREF20. Further, we employ BLEU score to evaluate the accuracy of generations BIBREF21, and the number of distinct n-gram to evaluate the diversity of generations BIBREF6. The distinct is normalized to $[0, 1]$ by dividing the total number of generated tokens.\n\nQuestion: How do they measure the diversity of inferences?\n\nExplanation:  The answer can be found in [Document 1], which states that they use the number of distinct n-grams to evaluate the diversity of generations.\n\nAnswer: by number of distinct n-grams\n\nExample 2:\n\n[Document 1]: Because our approach is specifically intended to yield sentences that are grammatical, we additionally consider the following log ratio (i.e., the grammatical phrase over the ungrammatical phrase): DISPLAYFORM0\n\nQuestion: How do they measure grammaticality?\n\nExplanation:  [Document 1] explains that the grammaticality of a sentence is measured by calculating the log ratio of a grammatical phrase over an ungrammatical phrase.\n\nAnswer: by calculating log ratio of grammatical phrase over ungrammatical phrase\n\nExample 3:\n\n[Document 1]: Just as introduced in sec:introduction, it is intractable to compute similarity exactly, as involving INLINEFORM0 computation. Hence, we consider the monte-carlo approximation: DISPLAYFORM0\n\n[Document 2]: where INLINEFORM0 is a list of entity pairs sampled from INLINEFORM1 . We use sequential sampling to gain INLINEFORM6 , which means we first sample INLINEFORM7 given INLINEFORM8 from INLINEFORM9 , and then sample INLINEFORM10 given INLINEFORM11 and INLINEFORM12 from INLINEFORM13 .\n\nQuestion: Which sampling method do they use to approximate similarity between the conditional probability distributions over entity pairs?\n\nExplanation: \n\n[Document 1] introduces the concept of monte-carlo approximation and [Document 2] explains that sequential sampling is used to gain a more accurate approximation.\n\nAnswer: monte-carlo, sequential sampling\n\nExample 4:\n\n[Document 1]: We further analyze why the decoder is more sensitive by masking the previous tokens, and comparing autoregressive NMT with the non-autoregressive counterpart. We find that the preceding tokens in the decoder provide strong conditional information, which partially explain the previous two observations on the decoder.\n\nQuestion: How do they measure conditional information strength?\n\nExplanation:"}
{"question_id": "272defe245d1c5c091d3bc51399181da2da5e5f0", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We present here a step in this direction: a probabilistic semantic parser that uses a large knowledge base (NELL) to form a prior probability distribution on the meanings of sentences it parses, and that \"understands\" each sentence either by identifying its existing beliefs that correspond to the sentence's meaning, or by creating new beliefs. More precisely, our semantic parser corresponds to a probabilistic generative model that assigns high probability to sentence semantic parses resulting in beliefs it already holds, lower prior probability to parses resulting in beliefs it does not hold but which are consistent with its more abstract knowledge about semantic types of arguments to different relations, and still lower prior probability to parses that contradict its beliefs about which entity types can participate in which relations.\n\nQuestion: What knowledge bases do they use?\n\nExplanation:  The document states that the parser uses a large knowledge base called NELL.\n\nAnswer: NELL\n\nExample 2:\n\n[Document 1]: Much recent work on semantic parsing has been evaluated using the WebQuestions dataset BIBREF3 . This dataset is not suitable for evaluating our model because it was filtered to only questions that are mappable to Freebase queries. In contrast, our focus is on language that is not directly mappable to Freebase. We thus use the dataset introduced by Krishnamurthy and Mitchell krishnamurthy-2015-semparse-open-vocabulary, which consists of the ClueWeb09 web corpus along with Google's FACC entity linking of that corpus to Freebase BIBREF9 . For training data, 3 million webpages from this corpus were processed with a CCG parser to produce logical forms BIBREF10 . This produced 2.1m predicate instances involving 142k entity pairs and 184k entities. After removing infrequently-seen predicates (seen fewer than 6 times), there were 25k categories and 4.2k relations.\n\nQuestion: What knowledge base do they use?\n\nExplanation: \n\nAccording to [Document 1], the WebQuestions dataset was filtered to only questions that are mappable to Freebase queries. This indicates that the knowledge base used is Freebase. This is further supported by the fact that the ClueWeb09 web corpus was processed with a CCG parser to produce logical forms, and that the Google's FACC entity linking of that corpus was used to Freebase.\n\nAnswer: Freebase\n\nExample 3:\n\n[Document 1]: We show the statistics of the dataset we use in tab:statistics, and the construction procedures will be introduced in this section.\n\n[Document 2]: In Wikidata BIBREF8 , facts can be described as (Head item/property, Property, Tail item/property). To construct a dataset suitable for our task, we only consider the facts whose head entity and tail entity are both items. We first choose the most common 202 relations and 120000 entities from Wikidata as our initial data. Considering that the facts containing the two most frequently appearing relations (P2860: cites, and P31: instance of) occupy half of the initial data, we drop the two relations to downsize the dataset and make the dataset more balanced. Finally, we keep the triples whose head and tail both come from the selected 120000 entities as well as its relation comes from the remaining 200 relations.\n\n[Document 3]: ReVerb BIBREF9 is a program that automatically identifies and extracts binary relationships from English sentences. We use the extractions from running ReVerb on Wikipedia. We only keep the relations appear more than 10 times and their corresponding triples to construct our dataset.\n\n[Document 4]: FB15K BIBREF3 is a subset of freebase. TACRED BIBREF10 is a large supervised relation extraction dataset obtained via crowdsourcing. We directly use these two dataset, no extra processing steps were applied.\n\nQuestion: Which knowledge bases do they use?\n\nExplanation: \n\nAccording to [Document 2], the dataset is constructed from Wikidata. According to [Document 3], the dataset is constructed from ReVerb. According to [Document 4], the dataset is constructed from FB15K and TACRED.\n\nAnswer: Wikidata, ReVerb, FB15K, TACRED\n\nExample 4:\n\n[Document 1]: The model performance is evaluated on the QA and Recs tasks of the bAbI Movie Dialog dataset using HITS@k evaluation metric, which is equal to the number of correct answers in the top- INLINEFORM0 results. In particular, the performance for the QA task is evaluated according to HITS@1, while the performance for the Recs task is evaluated according to HITS@100.\n\n[Document 2]: Differently from BIBREF2 , the relevant knowledge base facts, taken from the knowledge base in triple form distributed with the dataset, are retrieved by INLINEFORM0 implemented by exploiting the Elasticsearch engine and not according to an hash lookup operator which applies a strict filtering procedure based on word frequency. In our work, INLINEFORM1 returns at most the top 30 relevant facts for INLINEFORM2 . Each entity in questions and documents is recognized using the list of entities provided with the dataset and considered as a single word of the dictionary INLINEFORM3 .\n\nQuestion: Which knowledge base do they use to retrieve facts?\n\nExplanation:"}
{"question_id": "b3b9d7c8722e8ec41cbbae40e68458485a5ba25c", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Evaluation Task. Our task is bilingual lexicon induction (BLI). It has become the de facto standard evaluation for projection-based CLWEs BIBREF16 , BIBREF17 . In short, after a shared CLWE space has been induced, the task is to retrieve target language translations for a test set of source language words. Its lightweight nature allows us to conduct a comprehensive evaluation across a large number of language pairs. Since BLI is cast as a ranking task, following glavas2019howto we use mean average precision (MAP) as the main evaluation metric: in our BLI setup with only one correct translation for each \u201cquery\u201d word, MAP is equal to mean reciprocal rank (MRR).\n\nQuestion: How does BLI measure alignment quality?\n\nExplanation:  [Document 1] states that \"Since BLI is cast as a ranking task, following glavas2019howto we use mean average precision (MAP) as the main evaluation metric\".\n\nAnswer: we use mean average precision (MAP) as the main evaluation metric\n\nExample 2:\n\n[Document 1]: Table , extracted from BIBREF0, reports WERs obtained on evaluation data sets with a strongly adapted ASR, demonstrating the difficulty of the related speech recognition task for both languages. Refer to BIBREF10 for comparisons with a different non-native children speech data set and to scientific literature BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19 for detailed descriptions of children speech recognition and related issues. Important, although not exhaustive of the topic, references on non-native speech recognition can be found in BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF28, BIBREF29.\n\nQuestion: How is the speech recognition system evaluated?\n\nExplanation: \n\nAccording to [Document 1], the speech recognition system is evaluated using WER metric.\n\nAnswer: Speech recognition system is evaluated using WER metric.\n\nExample 3:\n\nQuestion: How is the quality of the discussion evaluated?\n\nExplanation:  There is not enough information provided to answer the question.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: The evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:\n\n[Document 2]: Wrong alignment\n\n[Document 3]: Partial alignment, some words or sentences may be missing\n\n[Document 4]: Correct alignment, allowing non-spoken syllables at start or end.\n\nQuestion: How is the speech alignment quality evaluated?\n\nExplanation:"}
{"question_id": "654306d26ca1d9e77f4cdbeb92b3802aa9961da1", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: In this paper we presented a way of enriching BERT with knowledge graph embeddings and additional metadata. Exploiting the linked knowledge that underlies Wikidata improves performance for our task of document classification. With this approach we improve the standard BERT models by up to four percentage points in accuracy. Furthermore, our results reveal that with task-specific information such as author names and publication metadata improves the classification task essentially compared a text-only approach. Especially, when metadata feature engineering is less trivial, adding additional task-specific information from an external knowledge source such as Wikidata can help significantly. The source code of our experiments and the trained models are publicly available.\n\nQuestion: By how much do they outperform standard BERT?\n\nExplanation:  The answer to the question can be found in the first sentence of the second paragraph of [Document 1].\n\nAnswer: up to four percentage points in accuracy\n\nExample 2:\n\n[Document 1]: To evaluate our sampling strategy, we compare it with the pretrained model and fine-tuned model on 16 different corpora. The results show that our approach outperforms those baselines on all corpora: it achieves 1.6% lower phone error rate on average. Additionally, we demonstrate that our corpus-level embeddings are able to capture the characteristics of each corpus, especially the language and domain information. The main contributions of this paper are as follows:\n\nQuestion: By how much do they, on average, outperform the baseline multilingual model on 16 low-resource tasks?\n\nExplanation:  The answer to the question can be found in [Document 1], which states that the proposed approach outperforms the baseline multilingual model by 1.6% on average.\n\nAnswer: 1.6% lower phone error rate on average\n\nExample 3:\n\n[Document 1]: Baselines We thoroughly compare our approaches to the following baselines:\n\n[Document 2]: Direct source$\\rightarrow $target: A standard NMT model trained on given source$\\rightarrow $target parallel data.\n\n[Document 3]: Multilingual: A single, shared NMT model for multiple translation directions BIBREF6.\n\n[Document 4]: Many-to-many: Trained for all possible directions among source, target, and pivot languages.\n\n[Document 5]: Many-to-one: Trained for only the directions to target language, i.e., source$\\rightarrow $target and pivot$\\rightarrow $target, which tends to work better than many-to-many systems BIBREF27.\n\nQuestion: What are multilingual models that were outperformed in performed experiment?\n\nExplanation:  [Document 1] introduces the baselines for the performed experiment. [Document 2], [Document 3], [Document 4], and [Document 5] describe these baselines in more detail.\n\nAnswer: Direct source$\\rightarrow $target: A standard NMT model trained on given source$\\rightarrow $target, Multilingual: A single, shared NMT model for multiple translation directions, Many-to-many: Trained for all possible directions among source, target, and pivot languages, Many-to-one: Trained for only the directions to target language\n\nExample 4:\n\n[Document 1]: FLOAT SELECTED: Table 6: Results for POS tagging (standard deviation in parentheses)\n\n[Document 2]: FLOAT SELECTED: Table 8: NER results for in-domain test set (standard deviation in parentheses)\n\n[Document 3]: FLOAT SELECTED: Table 9: NER results for out of domain test set (standard deviation in parentheses)\n\n[Document 4]: FLOAT SELECTED: Table 10: Labeled attachment score (LAS) parsing results for for predicted (p.seg) and gold (g.seg) segmentation. *Best performing combination in the TDT treebank (ELMo + transition-based parser).\n\nQuestion: By how much did the new model outperform multilingual BERT?\n\nExplanation:"}
{"question_id": "cb029240d4dedde74fcafad6a46c1cfc2621b934", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We train our models on Sentiment140 and Amazon product reviews. Both of these datasets concentrates on sentiment represented by a short text. Summary description of other datasets for validation are also as below:\n\n[Document 2]: We have provided baseline results for the accuracy of other models against datasets (as shown in Table 1 ) For training the softmax model, we divide the text sentiment to two kinds of emotion, positive and negative. And for training the tanh model, we convert the positive and negative emotion to [-1.0, 1.0] continuous sentiment score, while 1.0 means positive and vice versa. We also test our model on various models and calculate metrics such as accuracy, precision and recall and show the results are in Table 2 . Table 3 , Table 4 , Table 5 , Table 6 and Table 7 . Table 8 are more detail information with precisions and recall of our models against other datasets.\n\nQuestion: Was the introduced LSTM+CNN model trained on annotated data in a supervised fashion?\n\nExplanation:  [Document 1] mentions that the model was trained on Sentiment140 and Amazon product reviews, which are both annotated datasets. This is further supported by [Document 2], which mentions that the model was trained on various datasets and that the results are shown in various tables.\n\nAnswer: Yes\n\nExample 2:\n\n[Document 1]: We use the LSTM network as follows. The $x$ vector is fed through the LSTM network which outputs a vector $\\overrightarrow{h_i}$ for each time step $i$ from 0 to $n-1$. This is the forward LSTM. As we have access to the complete vector $x$, we can process a backward LSTM as well. This is done by computing a vector $\\overleftarrow{h_i}$ for each time step $i$ from $n-1$ to 0. Finally, we concatenate the backward LSTM with the forward LSTM:\n\n[Document 2]: Both $\\overrightarrow{h_i}$ and $\\overleftarrow{h_i}$ have a dimension of $l$, which is an optimized hyperparameter. The BiLSTM output $h$ thus has dimension $2l\\times n$.\n\nQuestion: Is the LSTM bidirectional?\n\nExplanation:  The LSTM network is bidirectional because it has a forward LSTM and a backward LSTM, as described in [Document 1]. This is further supported by [Document 2], which states that the BiLSTM output $h$ has dimension $2l\\times n$.\n\nAnswer: Yes\n\nExample 3:\n\n[Document 1]: We interpret the HMM and LSTM states in the hybrid algorithm with 10 LSTM state dimensions and 10 HMM states in Figures 3 and 3 , showing which features are identified by the HMM and LSTM components. In Figures 3 and 3 , we color-code the training data with the 10 HMM states. In Figures 3 and 3 , we apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters. The HMM and LSTM states pick up on spaces, indentation, and special characters in the data (such as comment symbols in Linux data). We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data. In Figure 2 , we see that some individual LSTM hidden state dimensions identify similar features, such as comment symbols in the Linux data.\n\nQuestion: What kind of information do the HMMs learn that the LSTMs don't?\n\nExplanation:  In [Document 1], it is stated that the HMM can identify punctuation or pick up on vowels. This is supported by the fact that the HMM and LSTM states complement each other in Figures 3 and 3.\n\nAnswer: The HMM can identify punctuation or pick up on vowels.\n\nExample 4:\n\n[Document 1]: In this paper, we first introduced a PFC layer to involve representations from both language and vision subsystems to form a human-like thinking system (the LGI system). The LGI contains three subsystems: the vision, language, and PFC subsystem, which are trained separately. The development, recognition and learning mechanism is discussed in the cocurrent paper [10]. In the language subsystem, we use an LSTM layer to mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output, instead of traditional softmax classifier. We propose to train the LGI with the NFP loss function, which endows the capacity to describe the image content in form of symbol text and manipulated images according to language commands. LGI shows its ability to learn eight different syntaxes or tasks in a cumulative learning way, and form the first machine thinking loop with the interaction between imagined pictures and language text.\n\nQuestion: In what was does an LSTM mimic the prefrontal cortex?\n\nExplanation:"}
{"question_id": "8d14dd9c67d71494b4468000ff9683afdd11af7e", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: The questions and answers are mainly collected from a large community QA website Baidu Zhidao and a small portion are from hand collected web documents. Therefore, all these questions are indeed asked by real-world users in daily life instead of under controlled conditions. All the questions are of single-entity factoid type, which means (1) each question is a factoid question and (2) its answer involves only one entity (but may have multiple words). The question in Figure FIGREF3 is a positive example, while the question \u201cWho are the children of Albert Enistein?\u201d is a counter example because the answer involves three persons. The type and correctness of all the question answer pairs are verified by at least two annotators.\n\nQuestion: What was the inter-annotator agreement?\n\nExplanation:  [Document 1] states that the correctness of all the question answer pairs are verified by at least two annotators.\n\nAnswer: correctness of all the question answer pairs are verified by at least two annotators\n\nExample 2:\n\n[Document 1]: In order to calculate inter-annotator agreement, a total of 30 stories from 6 scenarios were randomly chosen for parallel annotation by all 4 annotators after the first annotation phase. We checked the agreement on these data using Fleiss' Kappa BIBREF4 . The results are shown in Figure 4 and indicate moderate to substantial agreement BIBREF5 . Interestingly, if we calculated the Kappa only on the subset of cases that were annotated with script-specific event and participant labels by all annotators, results were better than those of the evaluation on all labeled instances (including also unrelated and related non-script events). This indicates one of the challenges of the annotation task: In many cases it is difficult to decide whether a particular event should be considered a central script event, or an event loosely related or unrelated to the script.\n\n[Document 2]: For coreference chain annotation, we calculated the percentage of pairs which were annotated by at least 3 annotators (qualified majority vote) compared to the set of those pairs annotated by at least one person (see Figure 4 ). We take the result of 90.5% between annotators to be a good agreement.\n\nQuestion: Did the annotators agreed and how much?\n\nExplanation:  [Document 1] provides information on the level of agreement for event types and participant types using Fleiss' Kappa, while [Document 2] provides information on the level of agreement for coreference chain annotation.\n\nAnswer: For event types and participant types, there was a moderate to substantial level of agreement using the Fleiss' Kappa. For coreference chain annotation, there was average agreement of 90.5%.\n\nExample 3:\n\n[Document 1]: We calculated inter-annotator agreement by Krippendorff's Alpha BIBREF23, which accounts for different annotator pairs and empty values. To also zoom in on the particular agreement by category, we calculated mutual F-scores for each of the categories. This metric is typically used to evaluate system performance by category on gold standard data, but could also be applied to annotation pairs by alternating the roles of the two annotators between classifier and ground truth. A summary of the agreement by categorization is given in Table TABREF10. While both the Relevance and Subject categorizations are annotated at a percent agreement of $0.71$ and $0.70$, their agreement scores are only fair, at $\\alpha =0.27$ and $\\alpha =0.29$. The percent agreement on Stance and Sentiment, which carry more categories than the former two, is $0.54$ for both. Their agreement scores are also fair, at $\\alpha =0.35$ and $\\alpha =0.34$. The mutual F-scores show marked differences in agreement by category, where the categories that were annotated most often typically yield a higher score. This holds for the Relevant category ($0.81$), the Vaccine category ($0.79$) and the Positive category ($0.64$). The Negative category yields a mutual F-score of $0.42$, which is higher than the more frequently annotated categories Neutral ($0.23$) and Not clear ($0.31$). We found that these categories are often confused. After combining the annotations of the two, the stance agreement would be increased to $\\alpha =0.43$.\n\nQuestion: What is the agreement score of their annotated dataset?\n\nExplanation:  The answer can be found in [Document 1].\n\nAnswer:  Relevance and Subject categorizations are annotated at a percent agreement of $0.71$ and $0.70$, their agreement scores are only fair, at $\\alpha =0.27$ and $\\alpha =0.29$, Stance and Sentiment, which carry more categories than the former two, is $0.54$ for both. Their agreement scores are also fair, at $\\alpha =0.35$ and $\\alpha =0.34$, This holds for the Relevant category ($0.81$), the Vaccine category ($0.79$) and the Positive category ($0.64$),  The Negative category yields a mutual F-score of $0.42$, which is higher than the more frequently annotated categories Neutral ($0.23$) and Not clear ($0.31$).\n\nExample 4:\n\n[Document 1]: Our study focuses on the Twitter ecosystem and a small part of its network. The initial sampling of tweets was based on a machine learning classifier of aggressive English language. This classifier has an F1 score of 0.90 BIBREF35. Even with this filter, only 0.7% of tweets were deemed by a majority of MTurk workers as cyberbullying (Table TABREF17). This extreme class imbalance can disadvantage a wide range of machine learning models. Moreover, the MTurk workers exhibited only moderate inter-annotator agreement (Table TABREF17). We also acknowledge that notions of harmful intent and power imbalance can be subjective, since they may depend on the particular conventions or social structure of a given community. For these reasons, we recognize that cyberbullying still has not been unambiguously defined. Moreover, their underlying constructs are difficult to identify. In this study, we did not train workers to recognize subtle cues for interpersonal popularity, nor the role of anonymity in creating a power imbalance.\n\nQuestion: Do they report the annotation agreement?\n\nExplanation:"}
{"question_id": "b8bbdc3987bb456739544426c6037c78ede01b77", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Following the algorithms above, with the consideration of both the advantages and disadvantages of them, in this project, I am going to use a modified method: sound-class based skip-grams with bipartite networks (BipSkip). The whole procedure is quite straightforward and could be divided into three steps. First step: the word pair and their skip-grams are two sets of the bipartite networks. The second step is optional, which is to refine the bipartite network. Before I run the program, I will be asked to input a threshold, which determines if the program should delete the skip-gram nodes linked to fewer word nodes than the threshold itself. According to the experiment, even though I did not input any threshold as one of the parameters, the algorithm could still give the same answer but with more executing time. In the last step, the final generated bipartite graph would be connected to a monopartite graph and partitioned into cognate sets with the help of graph partitioning algorithms. Here I would use Informap algorithm BIBREF12. To make a comparison to this method, I am using CCM and SCA for distance measurement in this experiment, too. UPGMA algorithm would be used accordingly in these two cases.\n\nQuestion: Is the proposed method compared to previous methods?\n\nExplanation:  The proposed method is compared to previous methods in [Document 1].\n\nAnswer: Yes\n\nExample 2:\n\n[Document 1]: Experiments ::: Baselines\n\n[Document 2]: For comparison, we select several public models as baselines including semantic parsing models:\n\n[Document 3]: BiDAF BIBREF3, an MRC model which utilizes a bi-directional attention flow network to encode the question and passage;\n\n[Document 4]: QANet BIBREF12, which utilizes convolutions and self-attentions as the building blocks of encoders to represent the question and passage;\n\n[Document 5]: BERT BIBREF23, a pre-trained bidirectional Transformer-based language model which achieves state-of-the-art performance on lots of public MRC datasets recently;\n\n[Document 6]: and numerical MRC models:\n\n[Document 7]: NAQANet BIBREF6, a numerical version of QANet model.\n\n[Document 8]: NAQANet+, an enhanced version of NAQANet implemented by ourselves, which further considers real number (e.g. \u201c2.5\u201d), richer arithmetic expression, data augmentation, etc. The enhancements are also used in our NumNet model and the details are given in the Appendix.\n\n[Document 9]: Syn Dep BIBREF6, the neural semantic parsing model (KDG) BIBREF22 with Stanford dependencies based sentence representations;\n\n[Document 10]: OpenIE BIBREF6, KDG with open information extraction based sentence representations;\n\n[Document 11]: SRL BIBREF6, KDG with semantic role labeling based sentence representations;\n\n[Document 12]: and traditional MRC models:\n\nQuestion: what are the existing models they compared with?\n\nExplanation:  [Document 2] lists the semantic parsing models that were used as baselines, which are Syn Dep, OpenIE, and SRL. [Document 3] lists the BiDAF model as a baseline, [Document 4] lists the QANet model as a baseline, and [Document 5] lists the BERT model as a baseline. [Document 7] lists the NAQANet model as a baseline, and [Document 8] lists the NAQANet+ model as a baseline.\n\nAnswer: Syn Dep, OpenIE, SRL, BiDAF, QANet, BERT, NAQANet, NAQANet+\n\nExample 3:\n\n[Document 1]: Tables TABREF15 and TABREF16 compare results from TagLM with previously published state of the art results without additional labeled data or task specific gazetteers. Tables TABREF17 and TABREF18 compare results of TagLM to other systems that include additional labeled data or gazetteers. In both tasks, TagLM establishes a new state of the art using bidirectional LMs (the forward CNN-BIG-LSTM and the backward LSTM-2048-512).\n\nQuestion: what previous systems were compared to?\n\nExplanation:  [Document 1] compares the results of TagLM to other systems, including Chiu and Nichols (2016), Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), Hashimoto et al. (2016), and S\u00f8gaard and Goldberg (2016).\n\nAnswer: Chiu and Nichols (2016), Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), Hashimoto et al. (2016), S\u00f8gaard and Goldberg (2016) \n\nExample 4:\n\n[Document 1]: We also evaluated the coreference resolution model on the test-set of CALOR (Table TABREF11), obtaining an average precision, recall and F-1 of 65.59%, 48.86% and 55.77% respectively. The same model reached a average F-1 of 68.8% for English BIBREF6. Comparable measurements are not available for French. F-1 scores for French are believed to be lower because of the lower amount of annotated data.\n\nQuestion: Is the proposed system compared to existing systems?\n\nExplanation:"}
{"question_id": "7efbd9adbc403de4be6b1fb1999dd5bed9d6262c", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: In Twitter social networking, people express their opinions containing sub-sentences. These sub-sentences using specific PoS particles (Conjunction and Conjunctive adverbs), like \"but, while, however, despite, however\" have different polarities. However, the overall sentiment of tweets often focus on certain sub-sentences. For example:\n\n[Document 2]: @lonedog bwahahah...you are amazing! However, it was quite the letdown.\n\n[Document 3]: @kirstiealley my dentist is great but she's expensive...=(\n\n[Document 4]: In two tweets above, the overall sentiment is negative. However, the main sentiment is only in the sub-sentences following but and however. This inspires a processing step to remove unessential parts in a tweet. Rule-based approach can assists these problems in handling negation and dealing with specific PoS particles led to effectively affect the final output of classification BIBREF11 BIBREF16 . BIBREF11 summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after using the rules. Table TABREF16 illustrates the number of processed sentences on each dataset.\n\nQuestion: What semantic rules are proposed?\n\nExplanation: \n\nAccording to [Document 4], the proposed semantic rules are rules that compute polarity of words after POS tagging or parsing steps. This information is further supported by [Document 1], which states that the proposed rules are designed to effectively affect the final output of classification.\n\nAnswer: rules that compute polarity of words after POS tagging or parsing steps\n\nExample 2:\n\n[Document 1]: Our prediction task is thus a straightforward binary classification task at the word level. We develop the following five groups of features to capture properties of how a word is used in the explanandum (see Table TABREF18 for the full list):\n\n[Document 2]: [itemsep=0pt,leftmargin=*,topsep=0pt]\n\n[Document 3]: Non-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.\n\n[Document 4]: Word usage in an OP or PC (two groups). These features capture how a word is used in an OP or PC. As a result, for each feature, we have two values for the OP and PC respectively.\n\n[Document 5]: How a word connects an OP and PC. These features look at the difference between word usage in the OP and PC. We expect this group to be the most important in our task.\n\n[Document 6]: General OP/PC properties. These features capture the general properties of a conversation. They can be used to characterize the background distribution of echoing.\n\n[Document 7]: Table TABREF18 further shows the intuition for including each feature, and condensed $t$-test results after Bonferroni correction. Specifically, we test whether the words that were echoed in explanations have different feature values from those that were not echoed. In addition to considering all words, we also separately consider stopwords and content words in light of Figure FIGREF8. Here, we highlight a few observations:\n\nQuestion: What are their proposed features?\n\nExplanation:  [Document 1] states that they develop the following five groups of features to capture properties of how a word is used in the explanandum. These five groups are described in more detail in [Document 3], [Document 4], [Document 5], and [Document 6]. Finally, [Document 7] provides a table with the intuition for each feature and the results of a t-test.\n\nAnswer: Non-contextual properties of a word, Word usage in an OP or PC (two groups), How a word connects an OP and PC., General OP/PC properties\n\nExample 3:\n\n[Document 1]: Many practical fake news detection algorithms use a kind of semantic side information, such as whether the generated text is factually correct, in addition to its statistical properties. Although statistical side information would be straightforward to incorporate in the hypothesis testing framework, it remains to understand how to cast such semantic knowledge in a statistical decision theory framework.\n\nQuestion: What semantic features help in detecting whether a piece of text is genuine or generated? of \n\nExplanation:  [Document 1] does not give any specific examples of semantic features that help in detecting whether a piece of text is genuine or generated. The document only discusses that semantic features are used in practice and that it is yet to be discovered how to embed that knowledge into statistical decision theory framework.\n\nAnswer: No feature is given, only discussion that semantic features are use in practice and yet to be discovered how to embed that knowledge into statistical decision theory framework.\n\nExample 4:\n\n[Document 1]: We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic. Some of these features were motivated by various works on speech act classification, while others are novel features. Overall we selected 3313 binary features, composed of 1647 semantic and 1666 syntactic features.\n\n[Document 2]: Opinion Words: We used the \"Harvard General Inquirer\" lexicon BIBREF8 , which is a dataset used commonly in sentiment classification tasks, to identify 2442 strong, negative and positive opinion words (such as robust, terrible, untrustworthy, etc). The intuition here is that these opinion words tend to signal certain speech acts such as expressions and recommendations. One binary feature indicates whether any of these words appear in a tweet.\n\n[Document 3]: Vulgar Words: Similar to opinion words, vulgar words can either signal great emotions or an informality mostly seen in expressions than any other kind of speech act (least seen in assertions). We used an online collection of vulgar words to collect a total of 349 vulgar words. A binary feature indicates the appearance or lack thereof of any of these words.\n\n[Document 4]: Emoticons: Emoticons have become ubiquitous in online communication and so cannot be ignored. Like vulgar words, emoticons can also signal emotions or informality. We used an online collection of text-based emoticons to collect a total of 362 emoticons. A binary feature indicates the appearance or lack thereof of any of these emoticons.\n\n[Document 5]: Speech Act Verbs: There are certain verbs (such as ask, demand, promise, report, etc) that typically signal certain speech acts. Wierzbicka BIBREF9 has compiled a total of 229 English speech act verbs divided into 37 groups. Since this is a collection of verbs, it is crucially important to only consider the verbs in a tweet and not any other word class (since some of these words can appear in multiple part-of-speech categories). In order to do this, we used Owoputi et al.'s BIBREF10 Twitter part-of-speech tagger to identify all the verbs in a tweet, which were then stemmed using Porter Stemming BIBREF11 . The stemmed verbs were then compared to the 229 speech act verbs (which were also stemmed using Porter Stemming). Thus, we have 229 binary features coding the appearance or lack thereof of each of these verbs.\n\n[Document 6]: N-grams: In addition to the verbs mentioned, there are certain phrases and non-verb words that can signal certain speech acts. For example, the phrase \"I think\" signals an expression, the phrase \"could you please\" signals a request and the phrase \"is it true\" signals a question. Similarly, the non-verb word \"should\" can signal a recommendation and \"why\" can signal a question.\n\nQuestion: what are the proposed semantic features?\n\nExplanation:"}
{"question_id": "2d5d0b0c54105717bf48559b914fefd0c94964a6", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We first experiment with a suite of non-neural models, including Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees. We tune the parameters for these models using grid search and 10-fold cross-validation, and obtain results for different combinations of input and features.\n\n[Document 2]: We finally experiment with neural models, although our dataset is relatively small. We train both a two-layer bidirectional Gated Recurrent Neural Network (GRNN) BIBREF20 and Convolutional Neural Network (CNN) (as designed in BIBREF21) with parallel filters of size 2 and 3, as these have been shown to be effective in the literature on emotion detection in text (e.g., BIBREF22, BIBREF23). Because neural models require large amounts of data, we do not cull the data by annotator agreement for these experiments and use all the labeled data we have. We experiment with training embeddings with random initialization as well as initializing with our domain-specific Word2Vec embeddings, and we also concatenate the best feature set from our non-neural experiments onto the representations after the recurrent and convolutional/pooling layers respectively.\n\nQuestion: What supervised methods are used?\n\nExplanation:  [Document 1] and [Document 2] list the supervised methods used: Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees, a two-layer bidirectional Gated Recurrent Neural Network (GRNN) BIBREF20 and Convolutional Neural Network (CNN) (as designed in BIBREF21).\n\nAnswer: Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees, a two-layer bidirectional Gated Recurrent Neural Network (GRNN) BIBREF20 and Convolutional Neural Network (CNN) (as designed in BIBREF21)\n\nExample 2:\n\n[Document 1]: The proposed method can learn words related to places from the utterances of sentences. We use an unsupervised word segmentation method latticelm that can directly segment words from the lattices of the speech recognition results of the uttered sentences BIBREF22 . The lattice can represent to a compact the set of more promising hypotheses of a speech recognition result, such as N-best, in a directed graph format. Unsupervised word segmentation using the lattices of syllable recognition is expected to be able to reduce the variability and errors in phonemes as compared to NPYLM BIBREF13 , i.e., word segmentation using the 1-best speech recognition results.\n\nQuestion: Which method do they use for word segmentation?\n\nExplanation: \n\nAccording to [Document 1], the proposed method uses an unsupervised word segmentation method called latticelm.\n\nAnswer: unsupervised word segmentation method latticelm\n\nExample 3:\n\n[Document 1]: We constructed the ensembled predictions by choosing the answer from the network that had the highest probability and choosing no answer if any of the networks predicted no answer.\n\nQuestion: What ensemble methods are used for best model?\n\nExplanation:  [Document 1] states that the best model is chosen by \"choosing the answer from the network that had the highest probability and choosing no answer if any of the networks predicted no answer.\"\n\nAnswer: choosing the answer from the network that had the highest probability and choosing no answer if any of the networks predicted no answer\n\nExample 4:\n\n[Document 1]: This work is a first attempt towards using contemporary neural machine translation (NMT) techniques to perform machine translation for Nigerian Pidgin, establishing solid baselines that will ease and spur future work. We evaluate the performance of supervised and unsupervised neural machine translation models using word-level and the subword-level tokenization of BIBREF3.\n\nQuestion: What tokenization methods are used?\n\nExplanation:"}
{"question_id": "2974237446d04da33b78ce6d22a477cdf80877b7", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: How do the authors define fake news?\n\nExplanation:  The documents do not provide a definition for fake news.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: In order to verify the reliability of our technique in coverage expansion for infrequent words we did a set of experiments on the Rare Word similarity dataset BIBREF6 . The dataset comprises 2034 pairs of rare words, such as ulcerate-change and nurturance-care, judged by 10 raters on a [0,10] scale. Table TABREF15 shows the results on the dataset for three pre-trained word embeddings (cf. \u00a7 SECREF2 ), in their initial form as well as when enriched with additional words from WordNet.\n\nQuestion: How are rare words defined?\n\nExplanation:  Rare words are defined as those that are judged by 10 raters on a [0,10] scale, according to [Document 1].\n\nAnswer: judged by 10 raters on a [0,10] scale\n\nExample 3:\n\n[Document 1]: In our experiments, the short list is determined according to the word frequency. Concretely, we sort the vocabulary according to the word frequency from high to low. A frequency filter ratio INLINEFORM0 is set to filter out the low-frequency words (rare words) from the lookup table. For example, INLINEFORM1 =0.9 means the least frequent 10% words are replaced with the default UNK notation.\n\nQuestion: how are rare words defined?\n\nExplanation:  Rare words are defined as low-frequency words according to [Document 1].\n\nAnswer: low-frequency words\n\nExample 4:\n\n[Document 1]: To increase instantaneous detection performance, we compensate for the absence of future information by consulting additional data sources. In particular, we make use of news wire articles, which are considered to be of high credibility. This is reasonable as according to Petrovic et. al (2013), in the majority of cases, news wires lead social media for reporting news. When a message arrives from a social media stream, we build features based on its novelty with respect to the confirmed information in the trusted sources. In a nutshell, the presence of information unconfirmed by the official media is construed as an indication of being a rumour. Note that this closely resembles the definition of what a rumour is.\n\nQuestion: How do they define rumors?\n\nExplanation:"}
{"question_id": "256dfa501a71d7784520a527f43aec0549b1afea", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: In Table TABREF39 , our results are comparable to the state-of-the-art methods. Since we do not have the same computational resource used in BIBREF50 to tune hyper-parameters at large scale, we expect that our model could achieve better performance after an aggressive hyperparameter tuning process. As shown in Table TABREF42 , our method outperform baseline methods. It is worth noticing that the continuous cache pointer can also be applied to output of our Predict Network without modification. Visualizations of tree structure generated from learned PTB language model are included in Appendix . In Table TABREF40 , we show the value of test perplexity for different variants of PRPN, each variant remove part of the model. By removing Parsing Network, we observe a significant drop of performance. This stands as empirical evidence regarding the benefit of having structure information to control attention.\n\n[Document 2]: Word-level Language Model\n\nQuestion: How do they measure performance of language model tasks?\n\nExplanation: \n\nAccording to [Document 1], the performance of language model tasks is measured by BPC and perplexity. This information is further supported by [Document 2].\n\nAnswer: BPC, Perplexity\n\nExample 2:\n\nQuestion: Is the model tested for language identification?\n\nExplanation: \n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task\n\nAnswer: No\n\nExample 3:\n\nQuestion: What is their best performance on the largest language direction dataset?\n\nExplanation:  The question asks for the best performance on the largest language direction dataset, but the documents do not provide any information on performance.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: FLOAT SELECTED: TABLE I AVERAGE CROSS SUBJECT (CS) ACCURACY ACROSS ALL TEST SUBJECTS FOR DIFFERENT PROPOSED ARCHITECTURES AND BASELINES. STANDARD DEVIATION ACROSS TEST SUBJECTS\u2019 ACCURACY IS ALSO SHOWN.\n\n[Document 2]: Table TABREF28 shows the comparative results among our proposed architectures and baselines. Overall, we use data from 12 subjects for our experiments which sum up to 13107 sign gesture samples in total. To evaluate model performance on a specific subject (test subject), we adopt cross subject evaluation criteria. Suppose, X is the test subject. We train our networks with all sign samples except those are from subject X. We use subject X's data as test split to evaluate the performance of the networks. Table TABREF28 shows the average test accuracy for all 12 subjects. We can see that 3D CNN network alone performs worse than simpler baselines. But when coupled with AI-LSTM as Max CNN-LSTM, it shows an increase in recognition accuracy by 2% from AI-LSTM alone. This is because some of the signs are confused by the AI-LSTM network because of similar skeletal motion pattern. Incorporating spatial relationship among joints leads to a significant accuracy gain. The Spatial AI-LSTM is trained only on skeletal data but outperforms the combined network by 6%.\n\nQuestion: What is the performance of the best model in the sign language recognition task?\n\nExplanation:"}
{"question_id": "f61268905626c0b2a715282478a5e373adda516c", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: Is the model tested for language identification?\n\nExplanation: \n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task\n\nAnswer: No\n\nExample 2:\n\n[Document 1]: In this section, we consider 3 categories of data augmentation techniques that are effectively applicable to video datasets.\n\n[Document 2]: To further increase training data size and diversity, we can create new audios via superimposing each original audio with additional noisy audios in time domain. To obtain diverse noisy audios, we use AudioSet, which consists of 632 audio event classes and a collection of over 2 million manually-annotated 10-second sound clips from YouTube videos BIBREF28.\n\n[Document 3]: We consider applying the frequency and time masking techniques \u2013 which are shown to greatly improve the performance of end-to-end ASR models BIBREF23 \u2013 to our hybrid systems. Similarly, they can be applied online during each epoch of LF-MMI training, without the need for realignment.\n\n[Document 4]: Consider each utterance (i.e. after the audio segmentation in Section SECREF5), and we compute its log mel spectrogram with $\\nu $ dimension and $\\tau $ time steps:\n\n[Document 5]: Frequency masking is applied $m_F$ times, and each time the frequency bands $[f_0$, $f_0+ f)$ are masked, where $f$ is sampled from $[0, F]$ and $f_0$ is sampled from $[0, \\nu - f)$.\n\n[Document 6]: Time masking is optionally applied $m_T$ times, and each time the time steps $[t_0$, $t_0+ t)$ are masked, where $t$ is sampled from $[0, T]$ and $t_0$ is sampled from $[0, \\tau - t)$.\n\n[Document 7]: Data augmentation ::: Speed and volume perturbation\n\n[Document 8]: Both speed and volume perturbation emulate mean shifts in spectrum BIBREF18, BIBREF19. To perform speed perturbation of the training data, we produce three versions of each audio with speed factors $0.9$, $1.0$, and $1.1$. The training data size is thus tripled. For volume perturbation, each audio is scaled with a random variable drawn from a uniform distribution $[0.125, 2]$.\n\nQuestion: What are the best within-language data augmentation methods?\n\nExplanation: \n\nAccording to [Document 3], frequency masking and time masking are effective data augmentation methods. This is further supported by [Document 5] and [Document 6]. Additionally, [Document 2] suggests that additive noise is also an effective data augmentation method, while [Document 8] suggests that speed and volume perturbation are effective methods.\n\nAnswer: Frequency masking, Time masking, Additive noise, Speed and volume perturbation\n\nExample 3:\n\n[Document 1]: As introduced above, the `One Million Post' corpus provides annotation labels for more than 11,000 user comments. Although there is no directly comparable category capturing `offensive language' as defined in the shared task, there are two closely related categories. From the resource, we extract all those comments in which a majority of the annotators agree that they contain either `inappropriate' or `discriminating' content, or none of the aforementioned. We treat the first two cases as examples of `offense' and the latter case as examples of `other'. This results in 3,599 training examples (519 offense, 3080 other) from on the `One Million Post' corpus. We conduct pre-training of the neural model as a binary classification task (similar to the Task 1 of GermEval 2018)\n\nQuestion: What are the near-offensive language categories?\n\nExplanation:  The document states that the categories of \"inappropriate\" and \"discriminating\" are considered to be examples of \"offense\".\n\nAnswer: inappropriate, discriminating\n\nExample 4:\n\n[Document 1]: The performance of the deep learning models is presented in table TABREF18. As we can see LSTM and GRU with Attention outperformed all the other models in-terms of macro-f1. Notably it outperformed all other classifical models and deep learning models in precision, recall and f1 for Offensive class as well as the Not Offensive class. However, fine tuning BERT-Base Multilingual Cased model did not achieve good results. For this task monolingual Greek word embeddings perform significantly better than the multilingual bert embeddings. LSTM and GRU with Attention can be considered as the best model trained for OGTD.\n\nQuestion: Which method best performs on the offensive language identification task?\n\nExplanation:"}
{"question_id": "301a453abaa3bc15976817fefce7a41f3b779907", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Follow the prior works BIBREF6, BIBREF7, BIBREF9, we adopt the BLEU and the Micro Entity F1 to evaluate our model performance. The experimental results are illustrated in Table TABREF30.\n\n[Document 2]: We provide human evaluation on our framework and the compared models. These responses are based on distinct dialogue history. We hire several human experts and ask them to judge the quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5. In each judgment, the expert is presented with the dialogue history, an output of a system with the name anonymized, and the gold response.\n\nQuestion: What were the evaluation metrics?\n\nExplanation: \n\nAccording to [Document 1], the BLEU and the Micro Entity F1 were used to evaluate the model performance. This information is further supported by [Document 2].\n\nAnswer: BLEU, Micro Entity F1, quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5\n\nExample 2:\n\nQuestion: what are the evaluation metrics?\n\nExplanation: \n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: The models were evaluated using precision, recall, and F1 metrics.\n\nExplanation:\n\nAccording to [Document 1], the models were evaluated using precision, recall, and F1 metrics. This information is further supported by [Document 2].\n\nAnswer: Precision, Recall, F1\n\nExample 3:\n\n[Document 1]: Evaluating the final model, we set as a baseline the prediction of the majority class, i.e., the fake news class. This baseline has an F1 of 41.59% and accuracy of 71.22%. The performance of the built models can be seen in Table TABREF19 . Another stable baseline, apart from just taking the majority class, is the TF.IDF bag-of-words approach, which sets a high bar for the general model score. We then observe how much the attention mechanism embeddings improve the score (AttNN). Finally, we add the hand-crafted features (Feats), which further improve the performance. From the results, we can conclude that both the attention-based task-specific embeddings and the manual features are important for the task of finding fake news.\n\nQuestion: what are their evaluation metrics?\n\nExplanation:  The evaluation metrics are stated in [Document 1].\n\nAnswer: F1, accuracy\n\nExample 4:\n\n[Document 1]: The official evaluation metric for this task was micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1\n\nQuestion: what were the evaluation metrics?\n\nExplanation:"}
{"question_id": "079ca5810060e1cdc12b5935d8c248492f0478b9", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: What languages are the model evaluated on?\n\nExplanation:  The documents do not mention any specific languages that the model was evaluated on.\n\nAnswer: Unanswerable\n\nExample 2:\n\nQuestion: Is the model tested for language identification?\n\nExplanation: \n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task\n\nAnswer: No\n\nExample 3:\n\n[Document 1]: The dataset at HASOC 2019 were given in three languages: Hindi, English, and German. Dataset in Hindi and English had three subtasks each, while German had only two subtasks. We participated in all the tasks provided by the organisers and decided to develop a single model that would be language agnostic. We used the same model architecture for all the three languages.\n\n[Document 2]: In the results of subtask A, models are mainly affected by imbalance of the dataset. The training dataset of Hindi dataset was more balanced than English or German dataset. Hence, the results were around 0.78. As the dataset in German language was highly imbalanced, the results drops to 0.62. In subtask B, the highest F1 score reached was by the profane class for each language in table TABREF20. The model got confused between OFFN, HATE and PRFN labels which suggests that these models are not able to capture the context in the sentence. The subtask C was again a case of imbalanced dataset as targeted(TIN) label gets the highest F1 score in table TABREF21.\n\nQuestion: What are the languages used to test the model?\n\nExplanation: \n\nAccording to [Document 1], the model was tested on Hindi, English and German. This information is further supported by [Document 2].\n\nAnswer: Hindi, English and German (German task won)\n\nExample 4:\n\nQuestion: Can the model be extended to other languages?\n\nExplanation:"}
{"question_id": "fe6181ab0aecf5bc8c3def843f82e530347d918b", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We compared our proposed approaches to a standard CNN that exploits a single set of word embeddings BIBREF3 . We also compared to a baseline of simply concatenating embeddings for each word to form long vector inputs. We refer to this as Concatenation-CNN C-CNN. For all multiple embedding approaches (C-CNN, MG-CNN and MGNC-CNN), we explored two combined sets of embedding: word2vec+Glove, and word2vec+syntactic, and one three sets of embedding: word2vec+Glove+syntactic. For all models, we tuned the l2 norm constraint INLINEFORM0 over the range INLINEFORM1 on a validation set. For instantiations of MGNC-CNN in which we exploited two embeddings, we tuned both INLINEFORM2 , and INLINEFORM3 ; where we used three embedding sets, we tuned INLINEFORM4 and INLINEFORM5 .\n\nQuestion: What are the baseline models?\n\nExplanation: \n\nAccording to [Document 1], the baseline models are MC-CNN, MVCNN, and CNN. This information is further supported by [Document 2], [Document 3], and [Document 5].\n\nAnswer: MC-CNN\nMVCNN\nCNN\n\nExample 2:\n\n[Document 1]: Methods ::: Models Tested ::: Recurrent Neural Network (RNN) Language Models\n\n[Document 2]: are trained to output the probability distribution of the upcoming word given a context, without explicitly representing the structure of the context BIBREF9, BIBREF10. We trained two two-layer recurrent neural language models with long short-term memory architecture BIBREF11 on a relatively small corpus. The first model, referred as `LSTM (PTB)' in the following sections, was trained on the sentences from Penn Treebank BIBREF12. The second model, referred as `LSTM (FTB)', was trained on the sentences from French Treebank BIBREF13. We set the size of input word embedding and LSTM hidden layer of both models as 256.\n\n[Document 3]: Methods ::: Models Tested ::: ActionLSTM\n\n[Document 4]: models the linearized bracketed tree structure of a sentence by learning to predict the next action required to construct a phrase-structure parse BIBREF16. The action space consists of three possibilities: open a new non-terminal node and opening bracket; generate a terminal node; and close a bracket. To compute surprisal values for a given token, we approximate $P(w_i|w_{1\\cdots i-1)}$ by marginalizing over the most-likely partial parses found by word-synchronous beam search BIBREF17.\n\n[Document 5]: Methods ::: Models Tested ::: Generative Recurrent Neural Network Grammars (RNNG)\n\n[Document 6]: jointly model the word sequence as well as the underlying syntactic structure BIBREF18. Following BIBREF19, we estimate surprisal using word-synchronous beam search BIBREF17. We use the same hyper-parameter settings as BIBREF18.\n\nQuestion: What are the baseline models?\n\nExplanation:  [Document 1], [Document 3], and [Document 5] list the baseline models as RNN, ActionLSTM, and RNNG respectively. This information is further supported by [Document 2], [Document 4], and [Document 6].\n\nAnswer: Recurrent Neural Network (RNN), ActionLSTM, Generative Recurrent Neural Network Grammars (RNNG)\n\nExample 3:\n\n[Document 1]: Majority: the text picks the label of the largest size.\n\n[Document 2]: ESA: A dataless classifier proposed in BIBREF0. It maps the words (in text and label names) into the title space of Wikipedia articles, then compares the text with label names. This method does not rely on train.\n\n[Document 3]: We implemented ESA based on 08/01/2019 Wikipedia dump. There are about 6.1M words and 5.9M articles.\n\n[Document 4]: Word2Vec BIBREF23: Both the representations of the text and the labels are the addition of word embeddings element-wisely. Then cosine similarity determines the labels. This method does not rely on train either.\n\n[Document 5]: Binary-BERT: We fine-tune BERT on train, which will yield a binary classifier for entailment or not; then we test it on test \u2013 picking the label with the maximal probability in single-label scenarios while choosing all the labels with \u201centailment\u201d decision in multi-label cases.\n\nQuestion: What are their baseline models?\n\nExplanation:  [Document 1], [Document 2], [Document 4], and [Document 5] all mention different baseline models. [Document 3] provides additional evidence that ESA is a baseline model by describing how it was implemented.\n\nAnswer: Majority, ESA, Word2Vec , Binary-BERT\n\nExample 4:\n\n[Document 1]: We first train an MLE model as our baseline, trained on the Conceptual Captions training split alone. We referred to this model as Baseline. For a baseline approach that utilizes (some of) the Caption-Quality data, we merge positively-rated captions from the Caption-Quality training split with the Conceptual Captions examples and finetune the baseline model. We call this model Baseline$+(t)$, where $t \\in [0,1]$ is the rating threshold for the included positive captions. We train models for two variants, $t\\in \\lbrace 0.5, 0.7\\rbrace $, which results in $\\sim $72K and $\\sim $51K additional (pseudo-)ground-truth captions, respectively. Note that the Baseline$+(t)$ approaches attempt to make use of the same additional dataset as our two reinforced models, OnPG and OffPG, but they need to exclude below-threshold captions due to the constraints in MLE.\n\nQuestion: What are the baseline models?\n\nExplanation:"}
{"question_id": "516b691ef192f136bb037c12c3c9365ef5a6604c", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Table TABREF24 shows performance of ensemble models by combining prediction results of the best context-aware logistic regression model and the best context-aware neural network model. We used two strategies in combining prediction results of two types of models. Specifically, the Max Score Ensemble model made the final decisions based on the maximum of two scores assigned by the two separate models; instead, the Average Score Ensemble model used the average score to make final decisions.\n\nQuestion: How do they combine the models?\n\nExplanation: \n\nAccording to [Document 1], the two models are combined by taking the maximum of the two scores assigned by the two separate models (Max Score Ensemble model) or by taking the average of the two scores (Average Score Ensemble model).\n\nAnswer: maximum of two scores assigned by the two separate models, average score\n\nExample 2:\n\n[Document 1]: Recently, continuous space representations of words and phrases have been incorporated into SMT systems via neural networks. Specifically, addition of monolingual neural network language models BIBREF13 , BIBREF14 , neural network joint models (NNJM) BIBREF4 , and neural network global lexicon models (NNGLM) BIBREF3 have been shown to be useful for SMT. Neural networks have been previously used for GEC as a language model feature in the classification approach BIBREF15 and as a classifier for article error correction BIBREF16 . Recently, a neural machine translation approach has been proposed for GEC BIBREF17 . This method uses a recurrent neural network to perform sequence-to-sequence mapping from erroneous to well-formed sentences. Additionally, it relies on a post-processing step based on statistical word-based translation models to replace out-of-vocabulary words. In this paper, we investigate the effectiveness of two neural network models, NNGLM and NNJM, in SMT-based GEC. To the best of our knowledge, there is no prior work that uses these two neural network models for SMT-based GEC.\n\n[Document 2]: Grammatical error correction (GEC) is a challenging task due to the variability of the type of errors and the syntactic and semantic dependencies of the errors on the surrounding context. Most of the grammatical error correction systems use classification and rule-based approaches for correcting specific error types. However, these systems use several linguistic cues as features. The standard linguistic analysis tools like part-of-speech (POS) taggers and parsers are often trained on well-formed text and perform poorly on ungrammatical text. This introduces further errors and limits the performance of rule-based and classification approaches to GEC. As a consequence, the phrase-based statistical machine translation (SMT) approach to GEC has gained popularity because of its ability to learn text transformations from erroneous text to correct text from error-corrected parallel corpora without any additional linguistic information. They are also not limited to specific error types. Currently, many state-of-the-art GEC systems are based on SMT or use SMT components for error correction BIBREF0 , BIBREF1 , BIBREF2 . In this paper, grammatical error correction includes correcting errors of all types, including word choice errors and collocation errors which constitute a large class of learners' errors.\n\n[Document 3]: We conduct experiments by incorporating NNGLM and NNJM both independently and jointly into our baseline system. The results of our experiments are described in Section SECREF23 . The evaluation is performed similar to the CoNLL 2014 shared task setting using the the official test data of the CoNLL 2014 shared task with annotations from two annotators (without considering alternative annotations suggested by the participating teams). The test dataset consists of 1,312 error-annotated sentences with 30,144 tokens on the source side. We make use of the official scorer for the shared task, M INLINEFORM0 Scorer v3.2 BIBREF19 , for evaluation. We perform statistical significance test using one-tailed sign test with bootstrap resampling on 100 samples.\n\n[Document 4]: On top of our baseline system described above, we incorporate the two neural network components, neural network global lexicon model (NNGLM) and neural network joint model (NNJM) as features. Both NNGLM and NNJM are trained using the parallel data used to train the translation model of our baseline system.\n\nQuestion: How do they combine the two proposed neural network models?\n\nExplanation: \n\nAccording to [Document 1], the two proposed neural network models are incorporated both independently and jointly into the baseline system. This is further supported by [Document 3] and [Document 4].\n\nAnswer: ncorporating NNGLM and NNJM both independently and jointly into, baseline system\n\nExample 3:\n\n[Document 1]: We use the additional INLINEFORM0 training articles labeled by publisher as an unsupervised data set to further train the BERT model.\n\n[Document 2]: We first investigate the impact of pre-training on BERT-BASE's performance. We then compare the performance of BERT-BASE with BERT-LARGE. For both, we vary the number of word-pieces from each article that are used in training. We perform tests with 100, 250 and 500 word pieces.\n\n[Document 3]: Next, we further explore the impact of sequence length using BERT-LARGE. The model took approximately 3 days to pre-train when using 4 NVIDIA GeForce GTX 1080 Ti. On the same computer, fine tuning the model on the small training set took only about 35 minutes for sequence length 100. The model's training time scaled roughly linearly with sequence length. We did a grid search on sequence length and learning rate.\n\nQuestion: How are the two different models trained?\n\nExplanation:  [Document 1] and [Document 2] state that they use 600000 articles as an unsupervised dataset to pre-train the models. [Document 3] states that they fine-tune the models on a small training set.\n\nAnswer: They pre-train the models using 600000 articles as an unsupervised dataset and then fine-tune the models on small training set.\n\nExample 4:\n\n[Document 1]: Previously in Sections \"Experiment Settings\" and \"Recursive Neural Tensor Network\" , we discussed that the gating mechanism concept can helps RNNs learn long-term dependencies from sequential input data and that adding more powerful interaction between the input and hidden layers simultaneously with the tensor product operation in a bilinear form improves neural network performance and expressiveness. By using tensor product, we increase our model expressiveness by using second-degree polynomial interactions, compared to first-degree polynomial interactions on standard dot product followed by addition in common RNNs architecture. Therefore, in this paper we proposed a Gated Recurrent Neural Tensor Network (GRURNTN) to combine these two advantages into an RNN architecture. In this architecture, the tensor product operation is applied between the current input and previous hidden layer multiplied by the reset gates for calculating the current candidate hidden layer values. The calculation is parameterized by tensor weight. To construct a GRURNTN, we defined the formulation as:\n\n[Document 2]: As with GRURNTN, we also applied the tensor product operation for the LSTM unit to improve its performance. In this architecture, the tensor product operation is applied between the current input and the previous hidden layers to calculate the current memory cell. The calculation is parameterized by the tensor weight. We call this architecture a Long Short Term Memory Recurrent Neural Tensor Network (LSTMRNTN). To construct an LSTMRNTN, we defined its formulation:\n\nQuestion: How does the introduced model combine the both factors?\n\nExplanation:"}
{"question_id": "00c443f8d32d6baf7c7cea8f4ca9fa749532ccfd", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Within this scenario, it is crucial to analyse people attitudes towards rumours in social media and to resolve their veracity as soon as possible. Several approaches have been proposed to check the rumour veracity in social media BIBREF1 . This paper focus on a stance-based analysis of event-related rumours, following the approach proposed at SemEval-2017 in the new RumourEval shared task (Task 8, sub-task A) BIBREF2 . In this task English tweets from conversation threads, each associated to a newsworthy event and the rumours around it, are provided as data. The goal is to determine whether a tweet in the thread is supporting, denying, querying, or commenting the original rumour which started the conversation. It can be considered a stance classification task, where we have to predict the user's stance towards the rumour from a tweet, in the context of a given thread. This task has been defined as open stance classification task and is conceived as a key step in rumour resolution, by providing an analysis of people reactions towards an emerging rumour BIBREF0 , BIBREF3 . The task is also different from detecting stance towards a specific target entity BIBREF4 .\n\nQuestion: Is this an English-language dataset?\n\nExplanation:  The dataset is for the SemEval-2017 shared task, which is specifically for English tweets. This is stated in [Document 1].\n\nAnswer: Yes\n\nExample 2:\n\n[Document 1]: For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter. It contains tweets collected based on keyword search that were posted between July 17, 2014 (the day of the plane crash) and December 9, 2016.\n\n[Document 2]: BIBREF4 provide annotations for a subset of the English tweets contained in the dataset. A tweet is annotated with one of three classes that indicate the framing of the tweet with respect to responsibility for the plane crash. A tweet can either be pro-Russian (Ukrainian authorities, NATO or EU countries are explicitly or implicitly held responsible, or the tweet states that Russia is not responsible), pro-Ukrainian (the Russian Federation or Russian separatists in Ukraine are explicitly or implicitly held responsible, or the tweet states that Ukraine is not responsible) or neutral (neither Ukraine nor Russia or any others are blamed). Example tweets for each category can be found in Table TABREF9. These examples illustrate that the framing annotations do not reflect general polarity, but polarity with respect to responsibility to the crash. For example, even though the last example in the table is in general pro-Ukrainian, as it displays the separatists in a bad light, the tweet does not focus on responsibility for the crash. Hence the it is labeled as neutral. Table TABREF8 shows the label distribution of the annotated portion of the data as well as the total amount of original tweets, and original tweets plus their retweets/duplicates in the network. A retweet is a repost of another user's original tweet, indicated by a specific syntax (RT @username: ). We consider as duplicate a tweet with text that is identical to an original tweet after preprocessing (see Section SECREF18). For our classification experiments, we exclusively consider original tweets, but model predictions can then be propagated to retweets and duplicates.\n\nQuestion: What languages are included in the dataset?\n\nExplanation:  The dataset only contains English tweets, as stated in [Document 2].\n\nAnswer: English\n\nExample 3:\n\n[Document 1]: We begin by filtering the corpus to keep only those tweets where the user's self-declared language and the tweet's detected language correspond; that language becomes the tweet's correct language label. This operation cuts out roughly half the tweets, and leaves us with a corpus of about 900 million tweets in 54 different languages. Table TABREF6 shows the distribution of languages in that corpus. Unsurprisingly, it is a very imbalanced distribution of languages, with English and Japanese together accounting for 60% of all tweets. This is consistent with other studies and statistics of language use on Twitter, going as far back as 2013. It does however make it very difficult to use this corpus to train a LID system for other languages, especially for one of the dozens of seldom-used languages. This was our motivation for creating a balanced Twitter dataset.\n\nQuestion: What languages are represented in the dataset?\n\nExplanation:  [Document 1] provides a list of languages represented in the dataset.\n\nAnswer: EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO\n\nExample 4:\n\n[Document 1]: The Phoenix dataset is an attempt to take both the new advances in event data described above, along with decades of knowledge regarding best practices, in order to create a new iteration of event data. The dataset makes use of 450 English-language news sites, which are each scraped every hour for new content. New data is generated on a daily basis, coded according to the CAMEO event ontology, with an average of 2,200 events generated per day. The full dataset examined here contains 254,060 total events spread across 102 days of generated data. Based on publicly available information, the project also makes use of the most up-to-date actor dictionaries of any available machine-coded event dataset.\n\nQuestion: What language is this dataset in?\n\nExplanation:"}
{"question_id": "87b65b538d79e1218fa19aaac71e32e9b49208df", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We collect utterances from the $\\mathbf {C}$hinese $\\mathbf {A}$rtificial $\\mathbf {I}$ntelligence $\\mathbf {S}$peakers (CAIS), and annotate them with slot tags and intent labels. The training, validation and test sets are split by the distribution of intents, where detailed statistics are provided in the supplementary material. Since the utterances are collected from speaker systems in the real world, intent labels are partial to the PlayMusic option. We adopt the BIOES tagging scheme for slots instead of the BIO2 used in the ATIS, since previous studies have highlighted meaningful improvements with this scheme BIBREF30 in the sequence labeling field.\n\nQuestion: What is the domain of their collected corpus?\n\nExplanation: \n\nAccording to [Document 1], the utterances in the corpus were collected from speaker systems in the real world.\n\nAnswer: speaker systems in the real world\n\nExample 2:\n\n[Document 1]: Previous attempts to annotate QA-SRL initially involved trained annotators BIBREF4 but later resorted to crowdsourcing BIBREF5 to achieve scalability. Naturally, employing crowd workers raises challenges when annotating semantic structures like SRL. As BIBREF5 acknowledged, the main shortage of the large-scale 2018 dataset is the lack of recall, estimated by experts to be in the lower 70s.\n\nQuestion: How was the corpus obtained?\n\nExplanation:  [Document 1] mentions that the corpus was initially annotated by trained annotators, but later resorted to crowdsourcing in order to achieve scalability.\n\nAnswer:  trained annotators BIBREF4, crowdsourcing BIBREF5 \n\nExample 3:\n\n[Document 1]: We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB. For each test set, we use the corresponding training questions $Q_\\mathit {tr}$ to retrieve domain-relevant sentences from S. Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T).\n\n[Document 2]: We consider two knowledge sources. The Sentence corpus (S) consists of domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining. This corpus is used by the IR solver and also used to create the tuple KB T and on-the-fly tuples $T^{\\prime }_{qa}$ . Additionally, TableILP uses $\\sim $ 70 Curated tables (C) designed for 4th grade NY Regents exams.\n\nQuestion: What corpus was the source of the OpenIE extractions?\n\nExplanation:  [Document 1] mentions that the text corpora used to build the tuple KB came from BIBREF6 aristo2016:combining. [Document 2] provides more details about the text corpora, specifically that it consists of domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining.\n\nAnswer: domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining\n\nExample 4:\n\nQuestion: What are all the domains the corpus came from?\n\nExplanation:"}
{"question_id": "97055ab0227ed6ac7a8eba558b94f01867bb9562", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: BLEU: We use the Bilingual Evaluation Understudy (BLEU) BIBREF34 metric which is commonly used in machine translation tasks. The BLEU metric can be used to evaluate dialogue generation models as in BIBREF5, BIBREF35. The BLEU metric is a word-overlap metric which computes the co-occurrence of N-grams in the reference and the generated response and also applies the brevity penalty which tries to penalize far too short responses which are usually not desired in task-oriented chatbots. We compute the BLEU score using all generated responses of our systems.\n\n[Document 2]: Per-turn Accuracy: Per-turn accuracy measures the similarity of the system generated response versus the target response. Eric and Manning eric2017copy used this metric to evaluate their systems in which they considered their response to be correct if all tokens in the system generated response matched the corresponding token in the target response. This metric is a little bit harsh, and the results may be low since all the tokens in the generated response have to be exactly in the same position as in the target response.\n\n[Document 3]: Per-Dialogue Accuracy: We calculate per-dialogue accuracy as used in BIBREF8, BIBREF5. For this metric, we consider all the system generated responses and compare them to the target responses. A dialogue is considered to be true if all the turns in the system generated responses match the corresponding turns in the target responses. Note that this is a very strict metric in which all the utterances in the dialogue should be the same as the target and in the right order.\n\n[Document 4]: F1-Entity Score: Datasets used in task-oriented chores have a set of entities which represent user preferences. For example, in the restaurant domain chatbots common entities are meal, restaurant name, date, time and the number of people (these are usually the required entities which are crucial for making reservations, but there could be optional entities such as location or rating). Each target response has a set of entities which the system asks or informs the user about. Our models have to be able to discern these specific entities and inject them into the generated response. To evaluate our models we could use named-entity recognition evaluation metrics BIBREF36. The F1 score is the most commonly used metric used for the evaluation of named-entity recognition models which is the harmonic average of precision and recall of the model. We calculate this metric by micro-averaging over all the system generated responses.\n\nQuestion: Is human evaluation performed?\n\nExplanation:  There is no mention of human evaluation in any of the documents. The only evaluation methods that are mentioned are BLEU, per-turn accuracy, per-dialogue accuracy, and F1-entity score.\n\nAnswer: No\n\nExample 2:\n\n[Document 1]: We investigate both methods, either in isolation or combined, on two translation directions (En-It and En-De) for which the length of the target is on average longer than the length of the source. Our ultimate goal is to generate translations whose length is not longer than that of the source string (see example in Table FIGREF1). While generating translations that are just a few words shorter might appear as a simple task, it actually implies good control of the target language. As the reported examples show, the network has to implicitly apply strategies such as choosing shorter rephrasing, avoiding redundant adverbs and adjectives, using different verb tenses, etc. We report MT performance results under two training data conditions, small and large, which show limited degradation in BLEU score and n-gram precision as we vary the target length ratio of our models. We also run a manual evaluation which shows for the En-It task a slight quality degradation in exchange of a statistically significant reduction in the average length ratio, from 1.05 to 1.01.\n\nQuestion: Do they conduct any human evaluation?\n\nExplanation:  [Document 1] mentions that they ran a manual evaluation which shows that there is a slight quality degradation in exchange for a statistically significant reduction in the average length ratio.\n\nAnswer: Yes\n\nExample 3:\n\n[Document 1]: Three annotators from a commercial annotation company are recruited to conduct our human evaluation. Responses from different models are shuffled for labeling. 300 test queries are randomly selected out, and annotators are asked to independently score the results of these queries with different points in terms of their quality: (1) Good (3 points): The response is grammatical, semantically relevant to the query, and more importantly informative and interesting; (2) Acceptable (2 points): The response is grammatical, semantically relevant to the query, but too trivial or generic (e.g.,\u201c\u6211\u4e0d\u77e5\u9053(I don't know)\", \u201c\u6211\u4e5f\u662f(Me too)\u201d, \u201c\u6211\u559c\u6b22(I like it)\" etc.); (3) Failed (1 point): The response has grammar mistakes or irrelevant to the query.\n\nQuestion: How is human evaluation performed, what were the criteria?\n\nExplanation:  [Document 1] explains how human evaluation is performed and what the criteria are.\n\nAnswer: (1) Good (3 points): The response is grammatical, semantically relevant to the query, and more importantly informative and interesting, (2) Acceptable (2 points): The response is grammatical, semantically relevant to the query, but too trivial or generic, (3) Failed (1 point): The response has grammar mistakes or irrelevant to the query\n\nExample 4:\n\n[Document 1]: Human evaluation, albeit time- and labor-consuming, conforms to the ultimate goal of open-domain conversation systems. We asked three educated volunteers to annotate the results using a common protocol known as pointwise annotation nbciteacl,ijcai,seq2BF. In other words, annotators were asked to label either \u201c0\u201d (bad), \u201c1\u201d (borderline), or \u201c2\u201d (good) to a query-reply pair. The subjective evaluation was performed in a strict random and blind fashion to rule out human bias.\n\nQuestion: Were human evaluations conducted?\n\nExplanation:"}
{"question_id": "5370a0062aae7fa4e700ae47aa143be5c5fc6b22", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We carry out experiments on multiple language pairs including German-English, French-English, and Japanese-English. The experimental results show that the proposed multi-lingual caption model not only achieves better caption performance than independent mono-lingual models for data-scarce languages, but also can induce the two types of features, linguistic and visual features, for different languages in joint spaces. Our proposed method consistently outperforms previous state-of-the-art vision-based bilingual word induction approaches on different languages. The contributions of this paper are as follows:\n\nQuestion: Which languages are used in the multi-lingual caption model?\n\nExplanation:  The languages used in the multi-lingual caption model are German-English, French-English, and Japanese-English according to [Document 1].\n\nAnswer: German-English, French-English, and Japanese-English\n\nExample 2:\n\n[Document 1]: In this section we detail the discussions we use to test our metric and how we determine the ground truth (i.e. if the discussion is controversial or not). We use thirty different discussions that took place between March 2015 and June 2019, half of them with controversy and half without it. We considered discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. We also studied these discussions taking first 140 characters and then 280 from each tweet to analyze the difference in performance and computing time wrt the length of the posts.\n\nQuestion: How many languages do they experiment with?\n\nExplanation:  The document says that they use \"thirty different discussions that took place between March 2015 and June 2019\" in \"four different languages: English, Portuguese, Spanish and French\".\n\nAnswer: four different languages: English, Portuguese, Spanish and French\n\nExample 3:\n\n[Document 1]: We extract data from the WMT'14 English-French (En-Fr) and English-German (En-De) datasets. To create a larger discrepancy between the tasks, so that there is a clear dataset size imbalance, the En-De data is artificially restricted to only 1 million parallel sentences, while the full En-Fr dataset, comprising almost 40 million parallel sentences, is used entirely. Words are split into subwords units with a joint vocabulary of 32K tokens. BLEU scores are computed on the tokenized output with multi-bleu.perl from Moses BIBREF10.\n\nQuestion: Are this techniques used in training multilingual models, on what languages?\n\nExplanation:  The WMT'14 English-French (En-Fr) and English-German (En-De) datasets are used to train multilingual models.\n\nAnswer: English to French and English to German\n\nExample 4:\n\n[Document 1]: Next, we explore how multilingual annotated data can be used to improve feature extraction for a zero-resource target language. We train multilingual bnfs on between one and ten languages from the GlobalPhone collection and evaluate on six other languages (simulating different zero-resource targets). We show that training on more languages consistently improves performance on word discrimination, and that the improvement is not simply due to more training data: an equivalent amount of data from one language fails to give the same benefit. In fact, we observe the largest gain in performance when adding the second training language, which is already better than adding three times as much data from the same language. Moreover, when compared to our best results from training unsupervised on target language data only, we find that bnfs trained on just a single other language already outperform the target-language-only training, with multilingual bnfs doing better by a wide margin.\n\nQuestion: With how many languages do they experiment in the multilingual setup?\n\nExplanation:"}
{"question_id": "35d2eae3a7c9bed54196334a09344591f9cbb5c8", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Text classification is a core task to many applications, like spam detection, sentiment analysis or smart replies. We used fastText and SVM BIBREF16 for preliminary experiments. We have pre-processed the text removing punctuation's and lowering the case. Facebook developers have developed fastText BIBREF17 which is a library for efficient learning of word representations and sentence classification. The reason we have used fastText is because of its promising results in BIBREF18 .\n\nQuestion: What classification models were used?\n\nExplanation: \n\nAccording to [Document 1], fastText and SVM BIBREF16 were used for preliminary experiments.\n\nAnswer: fastText and SVM BIBREF16\n\nExample 2:\n\n[Document 1]: The first type of classifier we evaluate in this work are feedforward neural networks (DNNs) consisting of 3 hidden layers, each with 512 rectified linear units (ReLUs) with a softmax activation function. The networks are optimized for 40 epochs with a mini-batch of 32 samples using stochastic gradient descent. Based on preliminary experiments on the validation set, hyperparameters such learning rate, decay rate, and L2 weight vary depending on the input format (Raw, PCA, or DCT). Generally, Raw inputs work better with smaller learning rates and heavier regularization to prevent overfitting to the high-dimensional data. As a second classifier to evaluate, we use convolutional neural networks (CNNs) with 2 convolutional and max pooling layers, followed by 2 fully-connected ReLU layers with 512 nodes. The convolutional layers use 16 filters, 8x8 and 4x4 kernels respectively, and rectified units. The fully-connected layers use dropout with a drop probability of 0.2. Because CNN systems take longer to converge, they are optimized over 200 epochs. For all systems, at the end of every epoch, the model is evaluated on the development set, and the best model across all epochs is kept.\n\nQuestion: What type of models are used for classification?\n\nExplanation:  [Document 1] states that feedforward neural networks (DNNs) and convolutional neural networks (CNNs) are used for classification.\n\nAnswer: feedforward neural networks (DNNs), convolutional neural networks (CNNs)\n\nExample 3:\n\n[Document 1]: General lexical features are often used in natural language processing as they are somewhat task-independent and reasonably effective in terms of classification accuracy. In our experiments, we used TF.IDF-based features over the title and over the content of the article we wanted to classify. We had these features twice \u2013 once for the title and once for the the content of the article, as we wanted to have two different representations of the same article. Thus, we used a total of 1,100 TF.IDF-weighted features (800 content + 300 title), limiting the vocabulary to the top 800 and 300 words, respectively (which occurred in more than five articles). We should note that TF.IDF features should be used with caution as they may not remain relevant over time or in different contexts without retraining.\n\nQuestion: what lexical features did they experiment with?\n\nExplanation:  [Document 1] mentions that the experiment used TF.IDF-based features.\n\nAnswer: TF.IDF-based features\n\nExample 4:\n\n[Document 1]: Most notably, we have found that taking into consideration multiple linguistic dimensions when assessing linguistic complexity is especially useful for sentence-level analysis. In our experiments, using only word-frequency features was almost as predictive as a combination of all features for the document level, but the latter made more accurate predictions for sentences, resulting in a 7% difference in accuracy. Besides L2 course book materials, we tested both our document- and sentence-level models also on unseen data with promising results.\n\n[Document 2]: FLOAT SELECTED: Table 2. The complete feature set.\n\nQuestion: what combination of features helped improve the classification?\n\nExplanation:"}
{"question_id": "f5913e37039b9517a323ec700b712e898316161b", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Our study uses WikiHop BIBREF0, as it is an entity-based multi-hop QA dataset and has been actively used. We randomly sampled 10,000 instances from 43,738 training instances and 2,000 instances from 5,129 validation instances (i.e. 36,000 annotation tasks were published on AMT). We manually converted structured WikiHop question-answer pairs (e.g. locatedIn(Macchu Picchu, Peru)) into natural language statements (Macchu Picchu is located in Peru) using a simple conversion dictionary.\n\nQuestion: What dataset was used in the experiment?\n\nExplanation:  The document states that the WikiHop dataset was used in the experiment.\n\nAnswer: WikiHop\n\nExample 2:\n\n[Document 1]: Contribution: We propose a preliminary study of a generic approach allowing any model to benefit from document-level information while translating sentence pairs. The core idea is to augment source data by adding document information to each sentence of a source corpus. This document information corresponds to the belonging document of a sentence and is computed prior to training, it takes every document word into account. Our approach focuses on pre-processing and consider whole documents as long as they have defined boundaries. We conduct experiments using the Transformer base model BIBREF1. For the English-German language pair we use the full WMT 2019 parallel dataset. For the English-French language pair we use a restricted dataset containing the full TED corpus from MUST-C BIBREF10 and sampled sentences from WMT 2019 dataset. We obtain important improvements over the baseline and present evidences that this approach helps to resolve cross-sentence ambiguities.\n\nQuestion: Which datasets were used in the experiment?\n\nExplanation: \n\nAccording to [Document 1], the WMT 2019 parallel dataset, a restricted dataset containing the full TED corpus from MUST-C BIBREF10, and sampled sentences from the WMT 2019 dataset were used in the experiment.\n\nAnswer: WMT 2019 parallel dataset, a restricted dataset containing the full TED corpus from MUST-C BIBREF10, sampled sentences from WMT 2019 dataset\n\nExample 3:\n\n[Document 1]: We extract data from the WMT'14 English-French (En-Fr) and English-German (En-De) datasets. To create a larger discrepancy between the tasks, so that there is a clear dataset size imbalance, the En-De data is artificially restricted to only 1 million parallel sentences, while the full En-Fr dataset, comprising almost 40 million parallel sentences, is used entirely. Words are split into subwords units with a joint vocabulary of 32K tokens. BLEU scores are computed on the tokenized output with multi-bleu.perl from Moses BIBREF10.\n\nQuestion: What datasets are used for experiments?\n\nExplanation:  [Document 1] states that the WMT'14 English-French (En-Fr) and English-German (En-De) datasets are used for experiments.\n\nAnswer: the WMT'14 English-French (En-Fr) and English-German (En-De) datasets.\n\nExample 4:\n\n[Document 1]: Here we compare the efficiency of our system with four widely used annotation tools. We extract 100 sentences from CoNLL 2003 English NER BIBREF8 training data, with each sentence containing at least 4 entities. Two undergraduate students without any experience on those tools are invited to annotate those sentences. Their average annotation time is shown in Figure FIGREF25 , where \u201cYedda+R\u201d suggests annotation using Yedda with the help of system recommendation. The inter-annotator agreements for those tools are closed, which around 96.1% F1-score. As we can see from the figure, our Yedda system can greatly reduce the annotation time. With the help of system recommendation, the annotation time can be further reduced. We notice that \u201cYedda+R\u201d has larger advantage with the increasing numbers of annotated sentences, this is because the system recommendation gives better suggestions when it learns larger annotated sentences. The \u201cYedda+R\u201d gives 16.47% time reduction in annotating 100 sentences.\n\nQuestion: what dataset was used in their experiment?\n\nExplanation:"}
{"question_id": "4738158f92b5b520ceba6207e8029ae082786dbe", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: To test the applicability of the proposed MWA attention, we choose three publicly available Chinese pre-trained models as the basic encoder: BERT, ERNIE, and BERT-wwm. In order to make a fair comparison, we keep the same hyper-parameters (such maximum length, warm-up steps, initial learning rate, etc) as suggested in BERT-wwm BIBREF13 for both baselines and our method on each dataset. We run the same experiment for five times and report the average score to ensure the reliability of results. For detailed hyper-parameter settings, please see Appendix. Besides, three popular CWS tools thulac BIBREF14, ictclas BIBREF15 and hanlp BIBREF16 are employed to segment the Chinese sentences into words.\n\nQuestion: What pre-trained models did they compare to?\n\nExplanation:  [Document 1] states that the authors compared the proposed MWA attention to three publicly available Chinese pre-trained models: BERT, ERNIE, and BERT-wwm.\n\nAnswer: BERT, ERNIE, and BERT-wwm\n\nExample 2:\n\n[Document 1]: Here is a list of architectures for which reference implementations and pretrained weights are currently provided in Transformers. These models fall into two main categories: generative models (GPT, GPT-2, Transformer-XL, XLNet, XLM) and models for language understanding (Bert, DistilBert, RoBERTa, XLM).\n\n[Document 2]: BERT (BIBREF13) is a bi-directional Transformer-based encoder pretrained with a linear combination of masked language modeling and next sentence prediction objectives.\n\n[Document 3]: RoBERTa (BIBREF5) is a replication study of BERT which showed that carefully tuning hyper-parameters and training data size lead to significantly improved results on language understanding.\n\n[Document 4]: DistilBERT (BIBREF32) is a smaller, faster, cheaper and lighter version BERT pretrained with knowledge distillation.\n\n[Document 5]: GPT (BIBREF34) and GPT2 (BIBREF9) are two large auto-regressive language models pretrained with language modeling. GPT2 showcased zero-shot task transfer capabilities on various tasks such as machine translation or reading comprehension.\n\n[Document 6]: Transformer-XL (BIBREF35) introduces architectural modifications enabling Transformers to learn dependency beyond a fixed length without disrupting temporal coherence via segment-level recurrence and relative positional encoding schemes.\n\n[Document 7]: XLNet (BIBREF4) builds upon Transformer-XL and proposes an auto-regressive pretraining scheme combining BERT's bi-directional context flow with auto-regressive language modeling by maximizing the expected likelihood over permutations of the word sequence.\n\n[Document 8]: XLM (BIBREF8) shows the effectiveness of pretrained representations for cross-lingual language modeling (both on monolingual data and parallel data) and cross-lingual language understanding.\n\n[Document 9]: We systematically release the model with the corresponding pretraining heads (language modeling, next sentence prediction for BERT) for adaptation using the pretraining objectives. Some models fine-tuned on downstream tasks such as SQuAD1.1 are also available. Overall, more than 30 pretrained weights are provided through the library including more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) .\n\nQuestion: What state-of-the-art general-purpose pretrained models are made available under the unified API? \n\nExplanation:  [Document 1] provides a list of architectures for which reference implementations and pretrained weights are available in Transformers. These models include BERT, RoBERTa, DistilBERT, GPT, GPT2, Transformer-XL, XLNet, and XLM. This information is further supported by [Document 2], [Document 3], [Document 4], [Document 5], [Document 6], [Document 7], [Document 8], and [Document 9].\n\nAnswer: BERT, RoBERTa, DistilBERT, GPT, GPT2, Transformer-XL, XLNet, XLM\n\nExample 3:\n\n[Document 1]: Our baseline is a GRU network for each of the three tasks. We use the same network architecture across the 3 tasks. For each network, the network contains a layer unidirectional GRU, with 500 units and an output linear layer. The network is trained end-to-end. Our input embedding layer is initialized with a standard normal distribution, with $\\mu =0$, and $\\sigma =1$, i.e., $W \\sim N(0,1)$. We use a maximum sequence length of 50 tokens, and choose an arbitrary vocabulary size of 100,000 types, where we use the 100,000 most frequent words in TRAIN. To avoid over-fitting, we use dropout BIBREF2 with a rate of 0.5 on the hidden layer. For the training, we use the Adam BIBREF3 optimizer with a fixed learning rate of $1e-3$. We employ batch training with a batch size of 32 for this model. We train the network for 15 epochs and save the model at the end of each epoch, choosing the model that performs highest accuracy on DEV as our best model. We present our best result on DEV in Table TABREF7. We report all our results using accuracy. Our best model obtains 42.48% for age, 37.50% for dialect, and 57.81% for gender. All models obtains best results with 2 epochs.\n\nQuestion: Are the models compared to some baseline models?\n\nExplanation: \n\nAccording to [Document 1], the models are compared to a GRU network baseline.\n\nAnswer: Yes\n\nExample 4:\n\n[Document 1]: FLOAT SELECTED: Table 1: ROUGE F1 scores on the test set. Our hierarchical (Hier-NSE) model outperform previous hierarchical and pointer-generator models. Hier-NSE-factor is the factored model and Hier-NSE-SC is the self-critic model.\n\nQuestion: What are previoius similar models authors are referring to?\n\nExplanation:"}
{"question_id": "726c5c1b6951287f4bae22978f9a91d22d9bef61", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: What are the baselines?\n\nExplanation: \n\n[Document 1]: We compare our proposed method with several baselines: (1) a vanilla CNN model; (2) an LSTM model; and (3) the BERT model BIBREF1 .\n\n[Document 2]: The vanilla CNN model is a simple convolutional neural network that takes word embeddings as input and outputs a classification for each word.\n\n[Document 3]: The LSTM model is a recurrent neural network that takes word embeddings as input and outputs a classification for each word.\n\n[Document 4]: The BERT model is a transformer-based model that takes word embeddings as input and outputs a classification for each word.\n\nAnswer: CNN, LSTM, BERT\n\nExample 2:\n\n[Document 1]: We compare our method with the models trained using Adobe internal NLU tool, Pytext BIBREF18 and Rasa BIBREF19 NLU tools. Table TABREF22 shows the results on Test set. Our model improves the F1 score by almost 2%, which corresponds to a 12.3% error rate reduction. This improvements proves the effectiveness of using contextual information for the task of slot filling.\n\nQuestion: What are the baselines?\n\nExplanation: \n\nAccording to [Document 1], the baselines are Adobe internal NLU tool, Pytext, and Rasa.\n\nAnswer: Adobe internal NLU tool, Pytext, Rasa\n\nExample 3:\n\n[Document 1]: We show the final results for opinion recommendation, comparing our proposed model with the following state-of-the-art baseline systems:\n\n[Document 2]: RS-Average is the widely-adopted baseline (e.g., by Yelp.com), using the averaged review scores as the final score.\n\n[Document 3]: RS-Linear estimates the rating score that a user would give by INLINEFORM0 BIBREF49 , where INLINEFORM1 and INLINEFORM2 are the the training deviations of the user INLINEFORM3 and the product INLINEFORM4 , respectively.\n\n[Document 4]: RS-Item applies INLINEFORM0 NN to estimate the rating score BIBREF50 . We choose the cosine similarity between INLINEFORM1 to measure the distance between product.\n\n[Document 5]: RS-MF is a state-of-the-art recommendation model, which uses matrix factorisation to predict rating score BIBREF8 , BIBREF41 , BIBREF25 .\n\n[Document 6]: Sum-Opinosis uses a graph-based framework to generate abstractive summarisation given redundant opinions BIBREF51 .\n\n[Document 7]: Sum-LSTM-Att is a state-of-the-art neural abstractive summariser, which uses an attentional neural model to consolidate information from multiple text sources, generating summaries using LSTM decoding BIBREF44 , BIBREF3 .\n\n[Document 8]: All the baseline models are single-task models, without considering rating and summarisation prediction jointly. The results are shown in Table TABREF46 . Our model (\u201c Joint\u201d) significantly outperforms both \u201cRS-Average\u201d and \u201cRS-Linear\u201d ( INLINEFORM0 using INLINEFORM1 -test), which demonstrates the strength of opinion recommendation, which leverages user characteristics for calculating a rating score for the user.\n\nQuestion: What are the baselines?\n\nExplanation: \n\nAccording to [Document 1], the baselines are RS-Average, RS-Linear, RS-Item, RS-MF, Sum-Opinosis, and Sum-LSTM-Att. This information is further supported by [Document 2], [Document 3], [Document 4], [Document 5], [Document 6], and [Document 7].\n\nAnswer: RS-Average , RS-Linear, RS-Item, RS-MF, Sum-Opinosis, Sum-LSTM-Att\n\nExample 4:\n\n[Document 1]: We evaluate on four categories of architecture:\n\n[Document 2]: reschke2014 proposed several methods for event extraction in this scenario. We compare against three notable examples drawn from this work:\n\n[Document 3]: Reschke CRF: a conditional random field model.\n\n[Document 4]: Reschke Noisy-OR: a sequence tagger with a \"Noisy-OR\" form of aggregation that discourages the model from predicting the same value for multiple slots.\n\n[Document 5]: Reschke Best: a sequence tagger using a cost-sensitive classifier, optimized with SEARN BIBREF17 , a learning-to-search framework.\n\nQuestion: what are the baselines?\n\nExplanation:"}
{"question_id": "b8b79a6123716cb9fabf751b31dff424235a2ee2", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\nQuestion: Do they normalize the calculated intermediate output hypotheses to compensate for the incompleteness?\n\nExplanation:  The question cannot be answered with the given documents. [Document 1] discusses the various stages of the algorithm, [Document 2] discusses the normalization process, and [Document 3] discusses the final stage of the algorithm. However, there is no mention of normalization in [Document 1], so it is unclear if normalization occurs at that stage.\n\nAnswer: Unanswerable\n\nExample 2:\n\n[Document 1]: We plotted the distribution of harassment incidents in each categorization dimension (Figure FIGREF19). It displays statistics that provide important evidence as to the scale of harassment and that can serve as the basis for more effective interventions to be developed by authorities ranging from advocacy organizations to policy makers. It provides evidence to support some commonly assumed factors about harassment: First, we demonstrate that harassment occurred more frequently during the night time than the day time. Second, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives.\n\n[Document 2]: Furthermore, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) (Figure FIGREF20). The significance of the correlation is tested by chi-square independence with p value less than 0.05. Identifying these patterns will enable interventions to be differentiated for and targeted at specific populations. For instance, the young harassers often engage in harassment activities as groups. This points to the influence of peer pressure and masculine behavioral norms for men and boys on these activities. We also found that the majority of young perpetrators engaged in harassment behaviors on the streets. These findings suggest that interventions with young men and boys, who are readily influenced by peers, might be most effective when education is done peer-to-peer. It also points to the locations where such efforts could be made, including both in schools and on the streets. In contrast, we found that adult perpetrators of sexual harassment are more likely to act alone. Most of the adult harassers engaged in harassment on public transportation. These differences in adult harassment activities and locations, mean that interventions should be responsive to these factors. For example, increasing the security measures on transit at key times and locations.\n\n[Document 3]: In addition, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location (Figure FIGREF21). For example, young harassers are more likely to engage in behaviors of verbal harassment, rather than physical harassment as compared to adults. It was a single perpetrator that engaged in touching or groping more often, rather than groups of perpetrators. In contrast, commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers. The nature and location of the harassment are particularly significant in developing strategies for those who are harassed or who witness the harassment to respond and manage the everyday threat of harassment. For example, some strategies will work best on public transport, a particular closed, shared space setting, while other strategies might be more effective on the open space of the street.\n\nQuestion: What patterns were discovered from the stories?\n\nExplanation:  [Document 1], [Document 2], and [Document 3] all present different patterns that were discovered from the stories. [Document 1] presents the distribution of harassment incidents while [Document 2] and [Document 3] present the correlations between different factors and harassment.\n\nAnswer: we demonstrate that harassment occurred more frequently during the night time than the day time, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) , We also found that the majority of young perpetrators engaged in harassment behaviors on the streets, we found that adult perpetrators of sexual harassment are more likely to act alone, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location , commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers.\n\nExample 3:\n\n[Document 1]: Our framework includes three key modules: Retrieve, Fast Rerank, and BiSET. For each source article, Retrieve aims to return a few candidate templates from the training corpus. Then, the Fast Rerank module quickly identifies a best template from the candidates. Finally, BiSET mutually selects important information from the source article and the template to generate an enhanced article representation for summarization.\n\n[Document 2]: This module starts with a standard information retrieval library to retrieve a small set of candidates for fine-grained filtering as cao2018retrieve. To do that, all non-alphabetic characters (e.g., dates) are removed to eliminate their influence on article matching. The retrieval process starts by querying the training corpus with a source article to find a few (5 to 30) related articles, the summaries of which will be treated as candidate templates.\n\n[Document 3]: The above retrieval process is essentially based on superficial word matching and cannot measure the deep semantic relationship between two articles. Therefore, the Fast Rerank module is developed to identify a best template from the candidates based on their deep semantic relevance with the source article. We regard the candidate with highest relevance as the template. As illustrated in Figure FIGREF6 , this module consists of a Convolution Encoder Block, a Similarity Matrix and a Pooling Layer.\n\nQuestion: How are templates discovered from training data?\n\nExplanation:  [Document 1] and [Document 2] both describe how templates are discovered from training data. First, a small set of candidates is retrieved from the training corpus. Then, the Fast Rerank module is used to identify the best template from the candidates.\n\nAnswer: For each source article, Retrieve aims to return a few candidate templates from the training corpus. Then, the Fast Rerank module quickly identifies a best template from the candidates.\n\nExample 4:\n\n[Document 1]: We have randomly selected 150 problems out of the RTE corpus which were marked as \u201cYES\u201d (i.e. entailment holds). The problems were not further selected nor doctored by us. The problems were then re-rated by experts in logic and/or linguistics. For each problem, three experts were consulted, and each expert rated 30 problems. More precisely, the experts were instructed to re-consider each problem and be especially wary of missing hypotheses. If they considered the entailment to hold, we still gave the instruction to optionally mention any additional implicit hypothesis that they would be using. Similarly, if they considered that there was no entailment in the problem, they were given prompted to (optionally) give an argument for their judgement.\n\nQuestion: How were missing hypotheses discovered?\n\nExplanation:"}
{"question_id": "6edef748370e63357a57610b5784204c9715c0b4", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Results form Table TABREF31 may give an idea that INLINEFORM0 is just an scaled INLINEFORM1 . While it is true that they show a linear correlation, INLINEFORM2 may produce a different system ranking than INLINEFORM3 given the integral multi-reference principle it follows. However, what we consider the most profitable about INLINEFORM4 is the twofold inclusion of all available references it performs. First, the construction of INLINEFORM5 to provide a more inclusive reference against to whom be evaluated and then, the computation of INLINEFORM6 , which scales the result depending of the agreement between references.\n\n[Document 2]: In this paper we presented WiSeBE, a semi-automatic multi-reference sentence boundary evaluation protocol based on the necessity of having a more reliable way for evaluating the SBD task. We showed how INLINEFORM0 is an inclusive metric which not only evaluates the performance of a system against all references, but also takes into account the agreement between them. According to your point of view, this inclusivity is very important given the difficulties that are present when working with spoken language and the possible disagreements that a task like SBD could provoke.\n\nQuestion: What makes it a more reliable metric?\n\nExplanation:  [Document 1] explains that one advantage of INLINEFORM0 is that it takes into account the agreement between different systems. This is further supported by [Document 2], which states that this inclusivity is very important given the difficulties that are present when working with spoken language and the possible disagreements that a task like SBD could provoke.\n\nAnswer: It takes into account the agreement between different systems\n\nExample 2:\n\n[Document 1]: When testing across languages, we report accuracy for two setups: average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All). The latter setting is also used for the embeddings model. We report accuracy for all experiments.\n\nQuestion: What are the evaluation metrics used?\n\nExplanation: \n\nAccording to [Document 1], the evaluation metrics used are average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All).\n\nAnswer: average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All)\n\nExample 3:\n\n[Document 1]: We evaluate our approach on two well benchmarked sequence tagging tasks, the CoNLL 2003 NER task BIBREF13 and the CoNLL 2000 Chunking task BIBREF14 . We report the official evaluation metric (micro-averaged INLINEFORM0 ). In both cases, we use the BIOES labeling scheme for the output tags, following previous work which showed it outperforms other options BIBREF15 . Following BIBREF8 , we use the Senna word embeddings BIBREF2 and pre-processed the text by lowercasing all tokens and replacing all digits with 0.\n\nQuestion: what metrics are used in evaluation?\n\nExplanation:  [Document 1] states that the micro-averaged F1 score is used as the evaluation metric for both the CoNLL 2003 NER task and the CoNLL 2000 Chunking task.\n\nAnswer: micro-averaged F1\n\nExample 4:\n\n[Document 1]: Our training sample has vastly over-sampled alerts compared with a typical responses in order to make it easier to train an engine. This also means that a typical test train split would not necessarily be useful in determining the efficacy of our models. The metric we use to evaluate the efficacy of our model is an approximation of the probability that a held-out alert is flagged if a fixed percentage of a typical population were to be flagged as potential alerts.\n\nQuestion: How is validity identified and what metric is used to quantify it?\n\nExplanation:"}
{"question_id": "1d9d7c96c5e826ac06741eb40e89fca6b4b022bd", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: In order to obtain insights about why the more fine-grained bi-sense emoji embedding helps in understanding the complexed sentiments behind tweets, we visualize the attention weights for ATT-E-LSTM and MATT-BiE-LSTM for comparison. The example tweets with corresponding attention weights calculated by word-emoji embedding and senti-emoji embedding are shown in Figure FIGREF27 , where the contexts are presented in the captions. The emojis used are , , and , respectively.\n\n[Document 2]: Therefore, we construct the new input INLINEFORM0 to each LSTM unit by concatenating the original word embedding and the attention vector in Equation EQREF21 to distribute the senti-emoji information to each step. This model is called Multi-level Attention-based LSTM with Bi-sense Emoji Embedding (MATT-BiE-LSTM). We choose the same binary cross-entropy as the loss function with the same network configuration with WATT-BiE-LSTM. DISPLAYFORM0\n\n[Document 3]: Attention-based LSTM with emojis:We also use the word-emoji embedding to calculate the emoji-word attention following Equation EQREF20 and EQREF21 , and the only difference is that we replace the attention-derived senti-emoji embedding with the pre-trained word-emoji embedding by fasttext, denoted as ATT-E-LSTM.\n\n[Document 4]: In Figure FIGREF27 (a), the ATT-E-LSTM model (baseline) assigns relatively more weights on the word \u201cno\u201d and \u201cpressure\u201d, while MATT-BiE-LSTM attends mostly on the word \u201chappy\u201d and \u201clovely\u201d. The different attention distributions suggest that the proposed senti-emoji embedding is capable of recognizing words with strong sentiments that are closely related to the true sentiment even with the presence of words with conflicting sentiments, such as \u201cpressure\u201d and \u201chappy\u201d. while ATT-E-LSTM tends to pick up all sentimental words which could raise confusions. The senti-emoji embedding is capable of extracting representations of complexed semantics and sentiments which help guide the attentions even in cases when the word sentiment and emoji sentiment are somewhat contradictory to each other. From Figure FIGREF27 (b) and (c) we can observe that the ATT-E-LSTM assigns more weights on the sentiment-irrelevant words than the MATT-BiE-LSTM such as \u201choodies\u201d, \u201cwait\u201d and \u201cafter\u201d, indicating that the proposed model is more robust to irrelevant words and concentrates better on important words. Because of the senti-emoji embedding obtained through bi-sense emoji embedding and the sentence-level LSTM encoding on the text input (described in Section SECREF13 ), we are able to construct a more robust embedding based on the semantic and sentiment information from the whole context compared to the word-emoji embedding used in ATT-E-LSTM which takes only word-level information into account.\n\nQuestion: What evidence does visualizing the attention give to show that it helps to obtain a more robust understanding of semantics and sentiments?\n\nExplanation: \n\nAccording to [Document 1], visualizing the attention weights for ATT-E-LSTM and MATT-BiE-LSTM can help to understand the complexed sentiments behind tweets. This information is further supported by [Document 4], which states that the proposed senti-emoji embedding is capable of recognizing words with strong sentiments that are closely related to the true sentiment even with the presence of words with conflicting sentiments.\n\nAnswer: The different attention distributions suggest that the proposed senti-emoji embedding is capable of recognizing words with strong sentiments that are closely related to the true sentiment even with the presence of words with conflicting sentiments\n\nExample 2:\n\n[Document 1]: Street gang members have established online presences coinciding with their physical occupation of neighborhoods. The National Gang Threat Assessment Report confirms that at least tens of thousands of gang members are using social networking websites such as Twitter and video sharing websites such as YouTube in their daily life BIBREF0 . They are very active online; the 2007 National Assessment Center's survey of gang members found that 25% of individuals in gangs use the Internet for at least 4 hours a week BIBREF4 . Gang members typically use social networking sites and social media to develop online respect for their street gang BIBREF5 and to post intimidating, threatening images or videos BIBREF6 . This \u201cCyber-\u201d or \u201cInternet banging\u201d BIBREF7 behavior is precipitated by the fact that an increasing number of young members of the society are joining gangs BIBREF8 , and these young members have become enamored with technology and with the notion of sharing information quickly and publicly through social media. Stronger police surveillance in the physical spaces where gangs congregate further encourages gang members to seek out virtual spaces such as social media to express their affiliation, to sell drugs, and to celebrate their illegal activities BIBREF9 .\n\nQuestion: Do the authors provide evidence that 'most' street gang members use Twitter to intimidate others?\n\nExplanation:  The authors provide evidence that some street gang members use Twitter to intimidate others, but they do not provide evidence that most street gang members use Twitter to intimidate others.\n\nAnswer: No\n\nExample 3:\n\n[Document 1]: We collected COVID-19 related Arabic tweets from January 1, 2020 until April 15, 2020, using Twitter streaming API and the Tweepy Python library. We have collected more than 3,934,610 million tweets so far. In our dataset, we store the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet. We created a list of the most common Arabic keywords associated with COVID-19. Using Twitter\u2019s streaming API, we searched for any tweet containing the keyword(s) in the text of the tweet. Table TABREF1 shows the list of keywords used along with the starting date of tracking each keyword. Furthermore, Table TABREF2 shows the list of hashtags we have been tracking along with the number of tweets collected from each hashtag. Indeed, some tweets were irrelevant, and we kept only those that were relevant to the pandemic.\n\nQuestion: What additional information is found in the dataset?\n\nExplanation:  [Document 1] states that the dataset includes \"the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet.\"\n\nAnswer: the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet\n\nExample 4:\n\n[Document 1]: Knowledge bases like Freebase capture real world facts, and Web resources like Wikipedia provide a large repository of sentences that validate or support these facts. For example, a sentence in Wikipedia says, Denali (also known as Mount McKinley, its former official name) is the highest mountain peak in North America, with a summit elevation of 20,310 feet (6,190 m) above sea level. To answer our example question against a KB using a relation extractor, we can use this sentence as external evidence, filter out wrong answers and pick the correct one.\n\n[Document 2]: Using textual evidence not only mitigates representational issues in relation extraction, but also alleviates the data scarcity problem to some extent. Consider the question, who was queen isabella's mother. Answering this question involves predicting two constraints hidden in the word mother. One constraint is that the answer should be the parent of Isabella, and the other is that the answer's gender is female. Such words with multiple latent constraints have been a pain-in-the-neck for both semantic parsing and relation extraction, and requires larger training data (this phenomenon is coined as sub-lexical compositionality by wang2015). Most systems are good at triggering the parent constraint, but fail on the other, i.e., the answer entity should be female. Whereas the textual evidence from Wikipedia, ...her mother was Isabella of Barcelos ..., can act as a further constraint to answer the question correctly.\n\nQuestion: What additional evidence they use?\n\nExplanation:"}
{"question_id": "c17ece1dad42d92c78fca2e3d8afa9a20ff19598", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: It is also worth to mention that we use a ResNet trained on 1.28 million images for a classification tasks. The features used by the attention mechanism are strongly object-oriented and the machine could miss important information for a multimodal translation task. We believe that the robust architecture of both encoders INLINEFORM0 combined with a GRU layer and word-embeddings took care of the right translation for relationships between objects and time-dependencies. Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations. The translation is then totally mislead. We illustrate with an example:\n\nQuestion: What misbehavior is identified?\n\nExplanation:  The misbehavior is identified in [Document 1].\n\nAnswer: if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations\n\nExample 2:\n\n[Document 1]: This step consists of generating a query out of the claim and querying a search engine (here, we experiment with Google and Bing) in order to retrieve supporting documents. Rather than querying the search engine with the full claim (as on average, a claim is two sentences long), we generate a shorter query following the lessons highlighted in BIBREF0 .\n\n[Document 2]: We rank the words by means of tf-idf. We compute the idf values on a 2015 Wikipedia dump and the English Gigaword. BIBREF0 suggested that a good way to perform high-quality search is to only consider the verbs, the nouns and the adjectives in the claim; thus, we exclude all words in the claim that belong to other parts of speech. Moreover, claims often contain named entities (e.g., names of persons, locations, and organizations); hence, we augment the initial query with all the named entities from the claim's text. We use IBM's AlchemyAPI to identify named entities. Ultimately, we generate queries of 5\u201310 tokens, which we execute against a search engine. We then collect the snippets and the URLs in the results, skipping any result that points to a domain that is considered unreliable. Finally, if our query has returned no results, we iteratively relax it by dropping the final tokens one at a time.\n\nQuestion: How are the potentially relevant text fragments identified?\n\nExplanation:  [Document 1] and [Document 2] both describe the process of identifying potentially relevant text fragments. First, a query is generated from the claim. Then, the words are ranked by means of TF-IDF. Next, IBM's AlchemyAPI is used to identify named entities. Finally, queries of 5-10 tokens are generated and executed against a search engine. The results are then collected, and any results that point to an unreliable domain are skipped.\n\nAnswer:  Generate a query out of the claim and querying a search engine, rank the words by means of TF-IDF, use IBM's AlchemyAPI to identify named entities, generate queries of 5\u201310 tokens, which execute against a search engine, and collect the snippets and the URLs in the results, skipping any result that points to a domain that is considered unreliable.\n\nExample 3:\n\n[Document 1]: Most of the previous works in aspect term extraction have either used conditional random fields (CRFs) BIBREF9 , BIBREF10 or linguistic patterns BIBREF7 , BIBREF11 . Both of these approaches have their own limitations: CRF is a linear model, so it needs a large number of features to work well; linguistic patterns need to be crafted by hand, and they crucially depend on the grammatical accuracy of the sentences. In this chapter, we apply an ensemble of deep learning and linguistics to tackle both the problem of aspect extraction and subjectivity detection.\n\nQuestion: How are aspects identified in aspect extraction?\n\nExplanation: \n\nAccording to [Document 1], an ensemble of deep learning and linguistics is used to identify aspects in aspect extraction.\n\nAnswer: apply an ensemble of deep learning and linguistics t\n\nExample 4:\n\n[Document 1]: Concurrently, it has been argued for mental health research that it would constitute a `valuable critical step' BIBREF10 to analyse first-hand accounts by individuals with lived experience of severe mental health issues in blog posts, tweets, and discussion forums. Several severe mental health difficulties, e.g., bipolar disorder (BD) and schizophrenia are considered as chronic and clinical recovery, defined as being relapse and symptom free for a sustained period of time BIBREF11 , is considered difficult to achieve BIBREF12 , BIBREF13 , BIBREF14 . Moreover, clinically recovered individuals often do not regain full social and educational/vocational functioning BIBREF15 , BIBREF16 . Therefore, research originating from initiatives by people with lived experience of mental health issues has been advocating emphasis on the individual's goals in recovery BIBREF17 , BIBREF18 . This movement gave rise to the concept of personal recovery BIBREF19 , BIBREF20 , loosely defined as a `way of living a satisfying, hopeful, and contributing life even with limitations caused by illness' BIBREF18 . The aspects of personal recovery have been conceptualised in various ways BIBREF21 , BIBREF22 , BIBREF23 . According to the frequently used CHIME model BIBREF24 , its main components are Connectedness, Hope and optimism, Identity, Meaning and purpose, and Empowerment. Here, we focus on BD, which is characterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12 . Bipolar spectrum disorders were estimated to affect approximately 2% of the UK population BIBREF13 with rates ranging from 0.1%-4.4% across 11 other European, American and Asian countries BIBREF26 . Moreover, BD is associated with a high risk of suicide BIBREF27 , making its prevention and treatment important tasks for society. BD-specific personal recovery research is motivated by mainly two facts: First, the pole of positive/elevated mood and ongoing mood instability constitute core features of BD and pose special challenges compared to other mental health issues, such as unipolar depression BIBREF25 . Second, unlike for some other severe mental health difficulties, return to normal functioning is achievable given appropriate treatment BIBREF28 , BIBREF16 , BIBREF29 .\n\nQuestion: How are the individuals with bipolar disorder identified?\n\nExplanation:"}
{"question_id": "77bc886478925c8e9fb369b1ba5d05c42b3cd79a", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: Hence, the contributions of this work are two-fold: (1) we develop a linguistically-infused neural network model to classify reactions in social media posts, and (2) we apply our model to label 10.8M Twitter posts and 6.2M Reddit comments in order to evaluate the speed and type of user reactions to various news sources.\n\n[Document 2]: We develop a neural network architecture that relies on content and other linguistic signals extracted from reactions and parent posts, and takes advantage of a \u201clate fusion\u201d approach previously used effectively in vision tasks BIBREF13 , BIBREF14 . More specifically, we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent.\n\nQuestion: What is the architecture of their model?\n\nExplanation:  The answer can be found in [Document 2].\n\nAnswer: we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent.\n\nExample 2:\n\n[Document 1]: Training: The baseline model was trained using RNNLM BIBREF25 . Then, we trained our LSTM models with different hidden sizes [200, 500]. All LSTMs have 2 layers and unrolled for 35 steps. The embedding size is equal to the LSTM hidden size. A dropout regularization BIBREF26 was applied to the word embedding vector and POS tag embedding vector, and to the recurrent output BIBREF27 with values between [0.2, 0.4]. We used a batch size of 20 in the training. EOS tag was used to separate every sentence. We chose Stochastic Gradient Descent and started with a learning rate of 20 and if there was no improvement during the evaluation, we reduced the learning rate by a factor of 0.75. The gradient was clipped to a maximum of 0.25. For the multi-task learning, we used different loss weights hyper-parameters INLINEFORM0 in the range of [0.25, 0.5, 0.75]. We tuned our model with the development set and we evaluated our best model using the test set, taking perplexity as the final evaluation metric. Where the latter was calculated by taking the exponential of the error in the negative log-form. INLINEFORM1\n\nQuestion: What is the architecture of the model?\n\nExplanation:  The model is an LSTM, as stated in [Document 1].\n\nAnswer: LSTM\n\nExample 3:\n\n[Document 1]: We use the method of BIBREF5 to train neural sequence-to-sequence Spanish-English ST models. As in that study, before training ST, we pre-train the models using English ASR data from the Switchboard Telephone speech corpus BIBREF7, which consists of around 300 hours of English speech and transcripts. This was reported to substantially improve translation quality when the training set for ST was only tens of hours.\n\n[Document 2]: Obtaining gold topic labels for our data would require substantial manual annotation, so we instead use the human translations from the 1K (train20h) training set utterances to train the NMF topic model with scikit-learn BIBREF14, and then use this model to infer topics on the evaluation set. These silver topics act as an oracle: they tell us what a topic model would infer if it had perfect translations. NMF and model hyperparameters are described in Appendix SECREF7.\n\nQuestion: What is the architecture of the model?\n\nExplanation: \n\nAccording to [Document 1], the model is a neural sequence-to-sequence model. This is further supported by [Document 2], which states that the model is an NMF topic model with scikit-learn BIBREF14.\n\nAnswer: BIBREF5 to train neural sequence-to-sequence, NMF topic model with scikit-learn BIBREF14\n\nExample 4:\n\n[Document 1]: We train a Bidirectional Encoding model BIBREF2 , which has achieved state-of-the-art results for Twitter stance detection on our dataset. The model encodes the entity using a bidirectional LSTM (BiLSTM), which is then used to initialize a BiLSTM that encodes the article and produces a prediction. To reduce the sequence length, we use the same context window that was presented to annotators for training the LSTM. We use pretrained GloVe embeddings BIBREF13 and tune hyperparameters on a validation set. The best model achieves a test accuracy of INLINEFORM0 and a macro-averaged test F1 score of INLINEFORM1 . It significantly outperforms baselines such as a bag-of-n-grams (accuracy: INLINEFORM2 ; F1: INLINEFORM3 ).\n\nQuestion: what is the architecture of their model?\n\nExplanation:"}
{"question_id": "74ebfba06f37cc95dfe59c3790ebe6165e6be19c", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We built a dataset for the task of hate speech detection in memes with 5,020 images that were weakly labeled into hate or non-hate memes, depending on their source. Hate memes were retrieved from Google Images with a downloading tool. We used the following queries to collect a total of 1,695 hate memes: racist meme (643 memes), jew meme (551 memes), and muslim meme (501 Memes). Non-hate memes were obtained from the Reddit Memes Dataset . We assumed that all memes in the dataset do not contain any hate message, as we considered that average Reddit memes do not belong to this class. A total of 3,325 non-hate memes were collected. We split the dataset into train (4266 memes) and validation (754 memes) subsets. The splits were random and the distribution of classes in the two subsets is the same. We didn't split the dataset into three subsets because of the small amount of data we had and decided to rely on the validation set metrics.\n\nQuestion: How is each instance of the dataset annotated?\n\nExplanation:  [Document 1] states that the dataset is weakly labeled into hate or non-hate memes, depending on their source.\n\nAnswer: weakly labeled into hate or non-hate memes, depending on their source\n\nExample 2:\n\n[Document 1]: The effectiveness and ubiquity of pretrained sentence embeddings for natural language understanding has grown dramatically in recent years. Recent sentence encoders like OpenAI's Generative Pretrained Transformer BIBREF3 and BERT BIBREF2 achieve the state of the art on the GLUE benchmark BIBREF4 . Among the GLUE tasks, these state-of-the-art systems make their greatest gains on the acceptability task with the Corpus of Linguistic Acceptability BIBREF0 . CoLA contains example sentences from linguistics publications labeled by experts for grammatical acceptability, and written to show subtle grammatical features. Because minimal syntactic differences can separate acceptable sentences from unacceptable ones (What did Bo write a book about? / *What was a book about written by Bo?), and acceptability classifiers are more reliable when trained on GPT and BERT than on recurrent models, it stands to reason that GPT and BERT have better implicit knowledge of syntactic features relevant to acceptability.\n\nQuestion: How is the CoLA grammatically annotated?\n\nExplanation:  This information is directly stated in [Document 1].\n\nAnswer: labeled by experts\n\nExample 3:\n\n[Document 1]: The dataset consists of a set of polysemous words: 20 nouns, 20 verbs, and 10 adjectives and specifies 20 to 100 contexts per word, with the total of 4,664 contexts, drawn from the Open American National Corpus. Given a set of contexts of a polysemous word, the participants of the competition had to divide them into clusters by sense of the word. The contexts are manually labelled with WordNet senses of the target words, the gold standard clustering is generated from this labelling.\n\n[Document 2]: The task allows two setups: graded WSI where participants can submit multiple senses per word and provide the probability of each sense in a particular context, and non-graded WSI where a model determines a single sense for a word in context. In our experiments we performed non-graded WSI. We considered the most suitable sense as the one with the highest cosine similarity with embeddings of the context, as described in Section SECREF9.\n\nQuestion: How are the different senses annotated/labeled? \n\nExplanation:  [Document 1] states that the contexts are manually labelled with WordNet senses of the target words. This information is further supported by [Document 2], which states that the most suitable sense is the one with the highest cosine similarity with embeddings of the context.\n\nAnswer: The contexts are manually labelled with WordNet senses of the target words\n\nExample 4:\n\n[Document 1]: We collected INLINEFORM0 TED Talk transcripts. An example transcription is given in Figure FIGREF4 . The collected transcripts were split into sentences using the Stanford CoreNLP tool BIBREF11 . In this study, sentences containing or immediately followed by `(Laughter)' were used as `Laughter' sentences, as shown in Figure FIGREF4 ; all other sentences were defined as `No-Laughter' sentences. Following BIBREF1 and BIBREF3 , we selected the same numbers ( INLINEFORM1 ) of `Laughter' and `No-Laughter' sentences. To minimize possible topic shifts between positive and negative instances, for each positive instance, we picked one negative instance nearby (the context window was 7 sentences in this study). For example, in Figure FIGREF4 , a negative instance (corresponding to `sent-2') was selected from the nearby sentences ranging from `sent-7' to `sent+7'.\n\nQuestion: How are the positive instances annotated? e.g. by annotators, or by laughter from the audience?\n\nExplanation:"}
{"question_id": "2b021e1486343d503bab26c2282f56cfdab67248", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: An LM's overall performance on *X can be measured simply by taking the proportion of correct predictions across the 67,000 minimal pairs from all paradigms. GPT-2 achieves the highest score and the $n$-gram the lowest. Transformer-XL and the LSTM LM perform in the middle, and at roughly the same level as each other. All models perform well below estimated human agreement (as described in Section SECREF11). The $n$-gram model's poor overall performance confirms *X is not solvable from co-occurrence information alone. Rather, success at *X is driven by the more abstract features learned by neural networks. There are no categories in which the $n$-gram approaches human performance.\n\nQuestion: Which of the model yields the best performance?\n\nExplanation:  The GPT-2 model yields the best performance according to [Document 1].\n\nAnswer: GPT-2\n\nExample 2:\n\nQuestion: What is the best performance achieved by supervised models?\n\nExplanation:  The documents do not provide any information about the best performance achieved by supervised models.\n\nAnswer: Unanswerable\n\nExample 3:\n\n[Document 1]: We compared several summarization methods which can be categorized into three groups: unsupervised, non-neural supervised, and neural supervised methods. For the unsupervised methods, we tested:\n\n[Document 2]: As a baseline, we used Lead-N which selects INLINEFORM0 leading sentences as the summary. For all methods, we extracted 3 sentences as the summary since it is the median number of sentences in the gold summaries that we found in our exploratory analysis.\n\n[Document 3]: Table TABREF26 shows the test INLINEFORM0 score of ROUGE-1, ROUGE-2, and ROUGE-L of all the tested models described previously. The mean and standard deviation (bracketed) of the scores are computed over the 5 folds. We put the score obtained by an oracle summarizer as Oracle. Its summaries are obtained by using the true labels. This oracle summarizer acts as the upper bound of an extractive summarizer on our dataset. As we can see, in general, every scenario of NeuralSum consistently outperforms the other models significantly. The best scenario is NeuralSum with word embedding size of 300, although its ROUGE scores are still within one standard deviation of NeuralSum with the default word embedding size. Lead-3 baseline performs really well and outperforms almost all the other models, which is not surprising and even consistent with other work that for news summarization, Lead-N baseline is surprisingly hard to beat. Slightly lower than Lead-3 are LexRank and Bayes, but their scores are still within one standard deviation of each other so their performance are on par. This result suggests that a non-neural supervised summarizer is not better than an unsupervised one, and thus if labeled data are available, it might be best to opt for a neural summarizer right away. We also want to note that despite its high ROUGE, every NeuralSum scenario scores are still considerably lower than Oracle, hinting that it can be improved further. Moreover, initializing with FastText pre-trained embedding slightly lowers the scores, although they are still within one standard deviation. This finding suggests that the effect of FastText pre-trained embedding is unclear for our case.\n\nQuestion: What was the best performing baseline?\n\nExplanation: \n\nAccording to [Document 3], the Lead-3 baseline performed really well and outperformed almost all the other models.\n\nAnswer: Lead-3\n\nExample 4:\n\n[Document 1]: FLOAT SELECTED: Table 3: EI-reg task results.\n\n[Document 2]: FLOAT SELECTED: Table 4: V-reg task results.\n\n[Document 3]: FLOAT SELECTED: Table 5: EI-oc task results.\n\n[Document 4]: FLOAT SELECTED: Table 6: V-oc task results.\n\nQuestion: What is the best performing model?\n\nExplanation:"}
{"question_id": "9d6b2672b11d49c37a6bfb06172d39742d48aef4", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We calculate the probability of a sentence with a long-short term memory (LSTM, hochreiter1997long) LM, i.e., a special type of RNN LM, which has been trained on a large corpus. More details on LSTM LMs can be found, e.g., in sundermeyer2012lstm. The unigram probabilities for SLOR are estimated using the same corpus.\n\n[Document 2]: We train our LSTM LMs on the English Gigaword corpus BIBREF15 , which consists of news data.\n\nQuestion: what language models do they use?\n\nExplanation:  [Document 1] mentions that they use LSTM LMs, and [Document 2] provides more details about the specific LSTM LM that is trained on the English Gigaword corpus.\n\nAnswer: LSTM LMs\n\nExample 2:\n\n[Document 1]: We compared our models with the following state-of-the-art baselines:\n\n[Document 2]: Sequence to Sequence (Seq2Seq): A simple encoder-decoder model which concatenates four sentences to a long sentence with an attention mechanism BIBREF31 .\n\n[Document 3]: Hierarchical LSTM (HLSTM): The story context is represented by a hierarchical LSTM: a word-level LSTM for each sentence and a sentence-level LSTM connecting the four sentences BIBREF29 . A hierarchical attention mechanism is applied, which attends to the states of the two LSTMs respectively.\n\n[Document 4]: HLSTM+Copy: The copy mechanism BIBREF32 is applied to hierarchical states to copy the words in the story context for generation.\n\n[Document 5]: HLSTM+Graph Attention(GA): We applied multi-source attention HLSTM where commonsense knowledge is encoded by graph attention.\n\n[Document 6]: HLSTM+Contextual Attention(CA): Contextual attention is applied to represent commonsense knowledge.\n\nQuestion: Did they compare to Transformer based large language models?\n\nExplanation:  There is no mention of Transformer based large language models in any of the documents.\n\nAnswer: No\n\nExample 3:\n\nQuestion: Do they experiment with language modeling on large datasets?\n\nExplanation: \n\nThere is no mention of language modeling or large datasets in any of the documents.\n\nAnswer: No\n\nExample 4:\n\n[Document 1]: Our system estimated tweet probabilities using Ngram language models. We created models from two different corpora - a collection of funny tweets from the @midnight program, and a corpus of news data that is freely available for research. We scored tweets by assigning them a probability based on each model. Tweets that have a higher probability according to the funny tweet model are considered funnier since they are more like the humorous training data. However, tweets that have a lower probability according to the news language model are viewed as funnier since they are least like the (unfunny) news corpus. We took a standard approach to language modeling and used bigrams and trigrams as features in our models. We used KenLM BIBREF8 with modified Kneser-Ney smoothing and a back-off technique as our language modeling tool.\n\nQuestion: What type of language models are used? e.g. trigrams, bigrams?\n\nExplanation:"}
{"question_id": "20632fc4d2b693b5aabfbbc99ee5c1e9fc485dea", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: The text and image encodings were combined by concatenation, which resulted in a feature vector of 4,864 dimensions. This multimodal representation was afterward fed as input into a multi-layer perceptron (MLP) with two hidden layer of 100 neurons with a ReLU activation function. The last single neuron with no activation function was added at the end to predict the hate speech detection score.\n\nQuestion: Is the dataset multimodal?\n\nExplanation:  The dataset is multimodal because it contains text and image encodings, as stated in [Document 1].\n\nAnswer: Yes\n\nExample 2:\n\n[Document 1]: In a nutshell, B4MSA starts by applying text-transformations to the messages, then transformed text is represented in a vector space model (see Subsection SECREF13 ), and finally, a Support Vector Machine (with linear kernel) is used as the classifier. B4MSA uses a number of text transformations that are categorized in cross-language features (see Subsection SECREF3 ) and language dependent features (see Subsection SECREF9 ). It is important to note that, all the text-transformations considered are either simple to implement or there is a well-known library (e.g. BIBREF6 , BIBREF7 ) to use them. It is important to note that to maintain the cross-language property, we limit ourselves to not use additional knowledge, this include knowledge from affective lexicons or models based on distributional semantics.\n\nQuestion: What are the components of the multilingual framework?\n\nExplanation: \n\nAccording to [Document 1], the multilingual framework consists of text-transformations to the messages, vector space model, and Support Vector Machine.\n\nAnswer: text-transformations to the messages, vector space model, Support Vector Machine\n\nExample 3:\n\n[Document 1]: To investigate whether our BERT-based model can transfer knowledge beyond language, we consider image features as simple visual tokens that can be presented to the model analogously to textual token embeddings. In order to make the $o_j$ vectors (of dimension $2048+4=2052$) comparable to BERT embeddings (of dimension 768), we use a simple linear cross-modal projection layer $W$ of dimensions $2052\\hspace{-1.00006pt}\\times \\hspace{-1.00006pt}768$. The $N$ object regions detected in an image, are thus represented as $X_{img} = (W.o_1,...,W.o_N)$. Once mapped into the BERT embedding space with $W$, the image is seen by the rest of the model as a sequence of units with no explicit indication if it is a text or an image embedding.\n\nQuestion: How are multimodal representations combined?\n\nExplanation:  [Document 1] explains that the image feature vectors are mapped into BERT embedding dimensions using a linear cross-modal projection layer. These vectors are then treated like a text sequence.\n\nAnswer: The image feature vectors are mapped into BERT embedding dimensions and treated like a text sequence afterwards.\n\nExample 4:\n\n[Document 1]: From the perspective of computational processing, diagrammatic representations present a formidable challenge, as they involve tasks from both computer vision and natural language processing. On the one hand, diagrams have a spatial organisation \u2013 layout \u2013 which needs to be segmented to identify meaningful units and their position. Making sense of how diagrams exploit the 2D layout space falls arguably within the domain of computer vision. On the other hand, diagrams also have a discourse structure, which uses the layout space to set up discourse relations between instances of natural language, various types of images, arrows and lines, thus forming a unified discourse organisation. The need to parse this discourse structure shifts the focus towards the field of natural language processing.\n\nQuestion: What are the parts of the \"multimodal\" resources?\n\nExplanation:"}
{"question_id": "4ddc53afffaf1622d97695347dd1b3190d156dee", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: To enable sentence-level classification we need to obtain a sentence representation from the word vectors INLINEFORM0 . We achieved this by using a BiLSTM with max pooling, which was shown to be a good universal sentence encoding mechanism BIBREF13 .\n\nQuestion: Which model architecture do they use to obtain representations?\n\nExplanation:  The document states that they use a BiLSTM with max pooling to obtain representations.\n\nAnswer: BiLSTM with max pooling\n\nExample 2:\n\n[Document 1]: The model is composed of an encoder (\u00a7SECREF5) and a decoder with the attention mechanism (\u00a7SECREF7) that are both implemented using recurrent neural networks (RNNs); the encoder converts source words into a sequence of vectors, and the decoder generates target language words one-by-one with the attention mechanism based on the conditional probability shown in the equation DISPLAY_FORM2 and DISPLAY_FORM3. The details are described below.\n\nQuestion: Which model architecture do they use to build a model?\n\nExplanation: \n\nAccording to [Document 1], the model is composed of an encoder and a decoder with the attention mechanism, both of which are implemented using recurrent neural networks.\n\nAnswer: model is composed of an encoder (\u00a7SECREF5) and a decoder with the attention mechanism (\u00a7SECREF7) that are both implemented using recurrent neural networks (RNNs)\n\nExample 3:\n\n[Document 1]: We select the following training objectives to learn general-purpose sentence embeddings. Our desiderata for the task collection were: sufficient diversity, existence of fairly large datasets for training, and success as standalone training objectives for sentence representations.\n\n[Document 2]: Multi-task training setup\n\nQuestion: Which model architecture do they for sentence encoding?\n\nExplanation: \n\nAccording to [Document 1], the model architecture for sentence encoding is an RNN. This is further supported by [Document 2], which states that the encoder and decoders are parameterized as separate RNNs.\n\nAnswer: Answer with content missing: (Skip-thought vectors-Natural Language Inference paragraphs) The encoder for the current sentence and the decoders for the previous (STP) and next sentence (STN) are typically parameterized as separate RNNs\n- RNN\n\nExample 4:\n\n[Document 1]: We first measure gender biases in st and abt datasets. We explore three neural models used in previous works on abusive language classification: Convolutional Neural Network (CNN) BIBREF7 , Gated Recurrent Unit (GRU) BIBREF14 , and Bidirectional GRU with self-attention ( INLINEFORM0 -GRU) BIBREF8 , but with a simpler mechanism used in BIBREF15 . Hyperparameters are found using the validation set by finding the best performing ones in terms of original AUC scores. These are the used hyperparameters:\n\nQuestion: What model architectures are used?\n\nExplanation:"}
{"question_id": "80d425258d027e3ca3750375d170debb9d92fbc6", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: The sequence modeling layer models the dependency between characters built on vector representations of the characters. In this work, we explore the applicability of our method to three popular architectures of this layer: the LSTM-based, the CNN-based, and the transformer-based.\n\n[Document 2]: Table TABREF46 shows performance of our method with different sequence modeling architectures. From the table, we can first see that the LSTM-based architecture performed better than the CNN- and transformer- based architectures. In addition, our methods with different sequence modeling layers consistently outperformed their corresponding ExSoftword baselines. This shows that our method is applicable to different neural sequence modeling architectures for exploiting lexicon information.\n\nQuestion: Which are the sequence model architectures this method can be transferred across?\n\nExplanation: \n\nAccording to [Document 1], the method can be transferred to three popular architectures of this layer: the LSTM-based, the CNN-based, and the transformer-based. This information is further supported by [Document 2], which states that the method can be transferred to different neural sequence modeling architectures for exploiting lexicon information.\n\nAnswer: The sequence model architectures which this method is transferred to are: LSTM and Transformer-based models\n\nExample 2:\n\n[Document 1]: The OpenBookQA dataset has a collection of questions and four answer choices for each question. The dataset comes with 1326 facts representing an open book. It is expected that answering each question requires at least one of these facts. In addition it requires common knowledge. To obtain relevant common knowledge we use an IR system BIBREF6 front end to a set of knowledge rich sentences. Compared to reading comprehension based QA (RCQA) setup where the answers to a question is usually found in the given small paragraph, in the OpenBookQA setup the open book part is much larger (than a small paragraph) and is not complete as additional common knowledge may be required. This leads to multiple challenges. First, finding the relevant facts in an open book (which is much bigger than the small paragraphs in the RCQA setting) is a challenge. Then, finding the relevant common knowledge using the IR front end is an even bigger challenge, especially since standard IR approaches can be misled by distractions. For example, Table 1 shows a sample question from the OpenBookQA dataset. We can see the retrieved missing knowledge contains words which overlap with both answer options A and B. Introduction of such knowledge sentences increases confusion for the question answering model. Finally, reasoning involving both facts from open book, and common knowledge leads to multi-hop reasoning with respect to natural language text, which is also a challenge.\n\nQuestion: How is OpenBookQA different from other natural language QA?\n\nExplanation: \n\nAccording to [Document 1], in the OpenBookQA setup the open book part is much larger and is not complete as additional common knowledge may be required. This is different from other natural language QA setups where the answer to a question is usually found in the given small paragraph.\n\nAnswer: in the OpenBookQA setup the open book part is much larger, the open book part is much larger (than a small paragraph) and is not complete as additional common knowledge may be required\n\nExample 3:\n\n[Document 1]: In this paper, we propose a method that enables NMT to translate patent sentences with a large vocabulary of technical terms. We use an NMT model similar to that used by Sutskever et al. Sutskever14, which uses a deep long short-term memories (LSTM) BIBREF7 to encode the input sentence and a separate deep LSTM to output the translation. We train the NMT model on a bilingual corpus in which the technical terms are replaced with technical term tokens; this allows it to translate most of the source sentences except technical terms. Similar to Sutskever et al. Sutskever14, we use it as a decoder to translate source sentences with technical term tokens and replace the tokens with technical term translations using statistical machine translation (SMT). We also use it to rerank the 1,000-best SMT translations on the basis of the average of the SMT and NMT scores of the translated sentences that have been rescored with the technical term tokens. Our experiments on Japanese-Chinese patent sentences show that our proposed NMT system achieves a substantial improvement of up to 3.1 BLEU points and 2.3 RIBES points over a traditional SMT system and an improvement of approximately 0.6 BLEU points and 0.8 RIBES points over an equivalent NMT system without our proposed technique.\n\n[Document 2]: One important difference between our NMT model and the one used by Sutskever et al. Sutskever14 is that we added an attention mechanism. Recently, Bahdanau et al. Bahdanau15 proposed an attention mechanism, a form of random access memory, to help NMT cope with long input sequences. Luong et al. Luong15b proposed an attention mechanism for different scoring functions in order to compare the source and target hidden states as well as different strategies for placing the attention. In this paper, we utilize the attention mechanism proposed by Bahdanau et al. Bahdanau15, wherein each output target word is predicted on the basis of not only a recurrent hidden state and the previously predicted word but also a context vector computed as the weighted sum of the hidden states.\n\n[Document 3]: According to the approach proposed by Dong et al. Dong15b, we identify Japanese-Chinese technical term pairs using an SMT phrase translation table. Given a parallel sentence pair $\\langle S_J, S_C\\rangle $ containing a Japanese technical term $t_J$ , the Chinese translation candidates collected from the phrase translation table are matched against the Chinese sentence $S_C$ of the parallel sentence pair. Of those found in $S_C$ , $t_C$ with the largest translation probability $P(t_C\\mid t_J)$ is selected, and the bilingual technical term pair $\\langle t_J,t_C\\rangle $ is identified.\n\n[Document 4]: For the Japanese technical terms whose Chinese translations are not included in the results of Step UID11 , we then use an approach based on SMT word alignment. Given a parallel sentence pair $\\langle S_J, S_C\\rangle $ containing a Japanese technical term $t_J$ , a sequence of Chinese words is selected using SMT word alignment, and we use the Chinese translation $t_C$ for the Japanese technical term $t_J$ .\n\n[Document 5]: Figure 3 illustrates the procedure for producing Chinese translations via decoding the Japanese sentence using the method proposed in this paper. In the step 1 of Figure 3 , when given an input Japanese sentence, we first automatically extract the technical terms and replace them with the technical term tokens \u201c $TT_{i}$ \u201d ( $i=1,2,\\ldots $ ). Consequently, we have an input sentence in which the technical term tokens \u201c $TT_{i}$ \u201d ( $i=1,2,\\ldots $ ) represent the positions of the technical terms and a list of extracted Japanese technical terms. Next, as shown in the step 2-N of Figure 3 , the source Japanese sentence with technical term tokens is translated using the NMT model trained according to the procedure described in Section \"NMT Training after Replacing Technical Term Pairs with Tokens\" , whereas the extracted Japanese technical terms are translated using an SMT phrase translation table in the step 2-S of Figure 3 . Finally, in the step 3, we replace the technical term tokens \u201c $TT_{i}$ \u201d ( $i=1,2,\\ldots $ ) of the sentence translation with SMT the technical term translations.\n\nQuestion: Can the approach be generalized to other technical domains as well? \n\nExplanation:  There is no mention in any of the documents about whether or not this approach can be generalized to other technical domains. However, because technical terms are replaced with tokens, there is no reason to think that this approach wouldn't also be successful for other technical domains. So long as there is a corresponding process for identifying and replacing technical terms in the new domain this approach could be viable.\n\nAnswer: There is no reason to think that this approach wouldn't also be successful for other technical domains. Technical terms are replaced with tokens, therefore so as long as there is a corresponding process for identifying and replacing technical terms in the new domain this approach could be viable.\n\nExample 4:\n\nQuestion: Can their method be transferred to other Q&A platforms (in other languages)?\n\nExplanation:"}
{"question_id": "11c4071d9d7efeede84f47892b1fa0c6a93667eb", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We work with four individual datasets. The datasets contain addition, subtraction, multiplication, and division word problems.\n\n[Document 2]: AI2 BIBREF2. AI2 is a collection of 395 addition and subtraction problems, containing numeric values, where some may not be relevant to the question.\n\n[Document 3]: CC BIBREF19. The Common Core dataset contains 600 2-step questions. The Cognitive Computation Group at the University of Pennsylvania gathered these questions.\n\n[Document 4]: IL BIBREF4. The Illinois dataset contains 562 1-step algebra word questions. The Cognitive Computation Group compiled these questions also.\n\n[Document 5]: MAWPS BIBREF20. MAWPS is a relatively large collection, primarily from other MWP datasets. We use 2,373 of 3,915 MWPs from this set. The problems not used were more complex problems that generate systems of equations. We exclude such problems because generating systems of equations is not our focus.\n\nQuestion: What datasets do they use?\n\nExplanation:  [Document 1] states that the authors work with four individual datasets. These datasets are further described in [Document 2], [Document 3], [Document 4], and [Document 5].\n\nAnswer: AI2 BIBREF2, CC BIBREF19, IL BIBREF4, MAWPS BIBREF20\n\nExample 2:\n\n[Document 1]: We validate our approach on the Wikipedia toxic comments dataset BIBREF18 . Our fairness experiments show that the classifiers trained with our method achieve the same performance, if not better, on the original task, while improving AUC and fairness metrics on a synthetic, unbiased dataset. Models trained with our technique also show lower attributions to identity terms on average. Our technique produces much better word vectors as a by-product when compared to the baseline. Lastly, by setting an attribution target of 1 on toxic words, a classifier trained with our objective function achieves better performance when only a subset of the data is present.\n\nQuestion: Which datasets do they use?\n\nExplanation:  The Wikipedia toxic comments dataset is used according to [Document 1].\n\nAnswer:  Wikipedia toxic comments\n\nExample 3:\n\n[Document 1]: From a character-level view, natural language is a discrete sequence of data, where discrete symbols form a distinct and shallow tree structure: the sentence is the root, words are children of the root, and characters are leafs. However, compared to word-level language modeling, character-level language modeling requires the model to handle longer-term dependencies. We evaluate a character-level variant of our proposed language model over a preprocessed version of the Penn Treebank (PTB) and Text8 datasets.\n\n[Document 2]: The unsupervised constituency parsing task compares hte tree structure inferred by the model with those annotated by human experts. The experiment is performed on WSJ10 dataset. WSJ10 is the 7422 sentences in the Penn Treebank Wall Street Journal section which contained 10 words or less after the removal of punctuation and null elements. Evaluation was done by seeing whether proposed constituent spans are also in the Treebank parse, measuring unlabeled F1 ( INLINEFORM0 ) of unlabeled constituent precision and recall. Constituents which could not be gotten wrong (those of span one and those spanning entire sentences) were discarded. Given the mechanism discussed in Section SECREF14 , our model generates a binary tree. Although standard constituency parsing tree is not limited to binary tree. Previous unsupervised constituency parsing model also generate binary trees BIBREF11 , BIBREF13 . Our model is compared with the several baseline methods, that are explained in Appendix .\n\nQuestion: Which dataset do they experiment with?\n\nExplanation: \n\nAccording to [Document 1], the character-level variant of the proposed language model is evaluated over a preprocessed version of the Penn Treebank (PTB) and Text8 datasets. This information is further supported by [Document 2], which states that the experiment is performed on the WSJ10 dataset, which is a subset of the Penn Treebank.\n\nAnswer: Penn Treebank, Text8, WSJ10\n\nExample 4:\n\n[Document 1]: We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages. We use 1.8M sentences of this corpus and build models in English to German and vice versa. To segment initial words (i.e. before any subword processing) we use the Moses word tokenizer and detokenizer. We evaluate with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track.\n\nQuestion: What datasets do they look at?\n\nExplanation:"}
{"question_id": "9a7aeecbecf5e30ffa595c233fca31719c9b429f", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: To implement deep neural network models, we utilize the Keras library BIBREF36 with TensorFlow BIBREF37 backend. Each model is run on a single NVIDIA GeForce GTX 1080 Ti GPU. The models are trained by Adam optimization algorithm BIBREF38 whose parameters are the same as the default settings except for learning rate set to $5\\times 10^{-5}$. Batch size is set to 3 or 4 due to the lack of graphical memory. We select BERT-base as the pre-trained language model in this paper. Due to the high cost of pre-training BERT language model, we directly adopt parameters pre-trained by Google in Chinese general corpus. The named entity recognition is applied on both pathology report texts and query texts.\n\nQuestion: What data is the language model pretrained on?\n\nExplanation: \n\nAccording to [Document 1], the language model is pretrained on Chinese general corpus.\n\nAnswer: Chinese general corpus\n\nExample 2:\n\n[Document 1]: In this work, the experiments are conducted using the BABEL speech corpus collected from the IARPA babel program. The corpus is mainly composed of conversational telephone speech (CTS) but some scripted recordings and far field recordings are presented as well. Table TABREF14 presents the details of the languages used in this work for training and evaluation.\n\nQuestion: What data do they train the language models on?\n\nExplanation:  The BABEL speech corpus is composed of conversational telephone speech, scripted recordings, and far field recordings.\n\nAnswer:  BABEL speech corpus \n\nExample 3:\n\n[Document 1]: We calculate the probability of a sentence with a long-short term memory (LSTM, hochreiter1997long) LM, i.e., a special type of RNN LM, which has been trained on a large corpus. More details on LSTM LMs can be found, e.g., in sundermeyer2012lstm. The unigram probabilities for SLOR are estimated using the same corpus.\n\n[Document 2]: We train our LSTM LMs on the English Gigaword corpus BIBREF15 , which consists of news data.\n\nQuestion: what language models do they use?\n\nExplanation:  [Document 1] mentions that they use LSTM LMs, and [Document 2] provides more details about the specific LSTM LM that is trained on the English Gigaword corpus.\n\nAnswer: LSTM LMs\n\nExample 4:\n\n[Document 1]: Once we had the corpora ready, we used the KenLM Toolkit to train the N-gram language models on each corpus. We trained using both bigrams and trigrams on the tweet and news data. Our language models accounted for unknown words and were built both with and without considering sentence or tweet boundaries.\n\nQuestion: What package was used to build the ngram language models?\n\nExplanation:"}
{"question_id": "8861138891669a45de3955c802c55a37be717977", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: We present two use cases on which Seshat was developped: clinical interviews, and daylong child-centered recordings.\n\nQuestion: Did they experiment with the tool?\n\nExplanation: \n\n[Document 1] mentions that Seshat was developed for use with clinical interviews and daylong child-centered recordings. This implies that the tool was experimented with in order to test its effectiveness in these scenarios.\n\nAnswer: Yes\n\nExample 2:\n\n[Document 1]: 2. We proposed joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM) BIBREF9, BIBREF10 as basic units. Our models can automatically extract the key elements from the sexual harassment stories and at the same time categorize the stories in different dimensions. The proposed models outperformed the single task models, and achieved higher than previously reported accuracy in classifications of harassment forms BIBREF6.\n\nQuestion: What model did they use?\n\nExplanation:  The answer can be found in the first sentence of the document.\n\nAnswer: joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM)\n\nExample 3:\n\nQuestion: What extraction model did they use?\n\nExplanation: \n\n[Document 1]: We use a multi-encoder, constrained-decoder model for our information extraction system. The encoder part of the model is a recurrent neural network (LSTM) that reads the input sequence and encodes it into a fixed-size vector. The decoder part of the model is also a recurrent neural network (LSTM) that reads the encoded vector and generates the output sequence.\n\n[Document 2]: The encoder part of the model is a recurrent neural network (LSTM) that reads the input sequence and encodes it into a fixed-size vector. The decoder part of the model is also a recurrent neural network (LSTM) that reads the encoded vector and generates the output sequence.\n\nAnswer: Multi-Encoder, Constrained-Decoder model\n\nExample 4:\n\n[Document 1]: Nearest neighbors (NN) Estimator: We used the pre-trained GloVe embeddings BIBREF11 of concepts to implement a nearest-neighbor estimator as follows. Given a concept $c$, we extract all labeled concepts within a given radius $r$ (cosine similarity $0.3$). In one variant, $c$'s controversiality score is taken to be the fraction of controversial concepts among them. In another variant, labeled concepts are weighted by their cosine similarity to $c$.\n\n[Document 2]: Naive Bayes (NB) Estimator: A Naive Bayes model was learned, with a bag-of-words feature set, using the word counts in the sentences of our training data \u2013 the contexts of the controversial and non-controversial concepts. The controversiality score of a concept $c$ for its occurrence in a sentence $s$, is taken as the posterior probability (according to the NB model) of $s$ to contain a controversial concept, given the words of $s$ excluding $c$, and taking a prior of $0.5$ for controversiality (as is the case in the datasets). The controversiality score of $c$ is then defined as the average score over all sentences referencing $c$.\n\n[Document 3]: Recurrent neural network (RNN): A bidirectional RNN using the architecture suggested in BIBREF10 was similarly trained. The network receives as input a concept and a referring sentence, and outputs a score. The controversiality score of a concept is defined, as above, to be the average of these scores.\n\nQuestion: what tools did they use?\n\nExplanation:"}
{"question_id": "2803709fba74e6098aae145abcbf0e9a3f4c35e5", "prompt": "For each example, use the documents to create an \"Answer\" and an \"Explanation\" to the \"Question\". Answer \"Unanswerable\" when you are not sure about the answer. Pay attention to answer only \"yes\" or \"no\" in boolean questions.\n\nExample 1:\n\n[Document 1]: DrQA is a CRC baseline coming with the CoQA dataset. Note that this implementation of DrQA is different from DrQA for SQuAD BIBREF8 in that it is modified to support answering no answer questions by having a special token unknown at the end of the document. So having a span with unknown indicates NO ANSWER. This baseline answers the research question RQ1.\n\n[Document 2]: DrQA+CoQA is the above baseline pre-tuned on CoQA dataset and then fine-tuned on $(\\text{RC})_2$ . We use this baseline to show that even DrQA pre-trained on CoQA is sub-optimal for RCRC. This baseline is used to answer RQ1 and RQ3.\n\n[Document 3]: BERT is the vanilla BERT model directly fine-tuned on $(\\text{RC})_2$ . We use this baseline for ablation study on the effectiveness of pre-tuning. All these BERT's variants are used to answer RQ2.\n\n[Document 4]: BERT+review first tunes BERT on domain reviews using the same objectives as BERT pre-training and then fine-tunes on $(\\text{RC})_2$ . We use this baseline to show that a simple domain-adaptation of BERT is not good.\n\n[Document 5]: BERT+CoQA first fine-tunes BERT on the supervised CoQA data and then fine-tunes on $(\\text{RC})_2$ . We use this baseline to show that pre-tuning is very competitive even compared with models trained from large-scale supervised data. This also answers RQ3.\n\nQuestion: What is the baseline model used?\n\nExplanation: \n\nAccording to [Document 1], the baseline model used is DrQA modified to support answering no answer questions. This is further supported by [Document 2], which states that DrQA+CoQA is the above baseline pre-tuned on CoQA dataset. [Document 3] states that BERT is the vanilla BERT model directly fine-tuned on $(\\text{RC})_2$ . [Document 4] states that BERT+review first tunes BERT on domain reviews. [Document 5] states that BERT+CoQA first fine-tunes BERT on the supervised CoQA data.\n\nAnswer: The baseline models used are DrQA modified to support answering no answer questions, DrQA+CoQA which is pre-tuned on CoQA dataset, vanilla BERT, BERT+review tuned on domain reviews, BERT+CoQA tuned on the supervised CoQA data\n\nExample 2:\n\nQuestion: What baseline model is used?\n\nExplanation: \n\n[Document 1]: In order to evaluate the performance of our models, we first need a baseline. The simplest baseline is a human evaluator, who can label a set of data with the desired output. However, this is not always practical, so we often use a machine learning model as a baseline.\n\n[Document 2]: In this paper, we use a Support Vector Machine (SVM) as our baseline model.\n\nThe baseline model in this case is a human evaluator, as stated in [Document 1]. This is further supported by [Document 2], which states that an SVM is used as the baseline model.\n\nAnswer: Human evaluators\n\nExample 3:\n\nQuestion: What baseline models are used?\n\nExplanation:  There is not enough information provided to answer the question.\n\nAnswer: Unanswerable\n\nExample 4:\n\n[Document 1]: The experimental results for NER are summarized in Table TABREF20. The top part of the table shows recently published results. BIBREF14's work is using gazetteers with HSCRF and BIBREF4's work is using the Flair language model which is much larger than ELMo. BIBREF27 is the current state-of-the-art language model that uses cloze-driven pretraining. The bottom part of the table is shows our baseline models and results with included gazetteers. We experiment with the Neural CRF model with and without ELMo embeddings. Including ELMo embeddings the CoNLL-03 and Ontonotes 5, F$_1$ score improves from 92.34 to 92.86 and 89.11 to 89.32 respectively. Without ELMo embeddings the F$_1$ score improves from 90.42 to 91.12 and 86.63 to 87 respectively. We observe that GazSelfAttn relative improvements are similar with and without ELMo embeddings. We obtain slightly better CoNLL-03 F$_1$ score compared to BIBREF14 work that uses the HSCRF model, and we match the Ononotes 5 F$_1$ scores of BIBREF4 that uses a much bigger model. BIBREF14 Ononotes 5 results use subset of the dataset labels and are not comparable. Note that because of computation constrains, we did not perform extensive hyperparameter tuning except for the gazetteer dropout rate.\n\nQuestion: What is used as a baseline model?\n\nExplanation:"}
