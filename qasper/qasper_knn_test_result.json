{"f6346828c2f44529dc307abf04dd246bfeb4a9b2": [{"question": "Which of the model yields the best performance?", "evidence": ["An LM's overall performance on *X can be measured simply by taking the proportion of correct predictions across the 67,000 minimal pairs from all paradigms. GPT-2 achieves the highest score and the $n$-gram the lowest. Transformer-XL and the LSTM LM perform in the middle, and at roughly the same level as each other. All models perform well below estimated human agreement (as described in Section SECREF11). The $n$-gram model's poor overall performance confirms *X is not solvable from co-occurrence information alone. Rather, success at *X is driven by the more abstract features learned by neural networks. There are no categories in which the $n$-gram approaches human performance."], "answer": "GPT-2", "explanation": " The GPT-2 model yields the best performance according to [Document 1]."}, {"question": "Do they evaluate existing methods in terms of desired properties?", "evidence": [], "answer": "Yes", "explanation": "\n\nAccording to [Document 1], the authors \"compare the performance of popular named entity recognition algorithms on our data.\" This indicates that they are evaluating existing methods. Furthermore, [Document 1] states that the authors \"trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15.\" This indicates that the authors are evaluating the methods in terms of their performance."}, {"question": "Which of their training domains improves performance the most?", "evidence": ["As previous systems collect relevant data from knowledge bases after observing questions during evaluation BIBREF24 , BIBREF25 , we also explore using this option. Namely, we build a customized text corpus based on questions in commonsense reasoning tasks. It is important to note that this does not include the answers and therefore does not provide supervision to our resolvers. In particular, we aggregate documents from the CommonCrawl dataset that has the most overlapping n-grams with the questions. The score for each document is a weighted sum of $F_1(n)$ scores when counting overlapping n-grams: $Similarity\\_Score_{document} = \\frac{\\sum _{n=1}^4nF_1(n)}{\\sum _{n=1}^4n}$", "The top 0.1% of highest ranked documents is chosen as our new training corpus. Details of the ranking is shown in Figure 2 . This procedure resulted in nearly 1,000,000 documents, with the highest ranking document having a score of $8\\times 10^{-2}$ , still relatively small to a perfect score of $1.0$ . We name this dataset STORIES since most of the constituent documents take the form of a story with long chain of coherent events.", "Figure 5 -left and middle show that STORIES always yield the highest accuracy for both types of input processing. We next rank the text corpora based on ensemble performance for more reliable results. Namely, we compare the previous ensemble of 10 models against the same set of models trained on each single text corpus. This time, the original ensemble trained on a diverse set of text corpora outperforms all other single-corpus ensembles including STORIES. This highlights the important role of diversity in training data for commonsense reasoning accuracy of the final system."], "answer": "documents from the CommonCrawl dataset that has the most overlapping n-grams with the question", "explanation": "\n\nAccording to [Document 1], the documents from the CommonCrawl dataset that have the most overlapping n-grams with the question are used to build a customized text corpus. This information is further supported by [Document 2] and [Document 3]."}], "c5221bb28e58a4f13cf2eccce0e1b1bec7dd3c13": [{"question": "What domains are present in the data?", "evidence": [], "answer": "Alarm, Banks, Buses, Calendar, Events, Flights, Homes, Hotels, Media, Messaging, Movies, Music, Payment, Rental Cars, Restaurants, Ride Sharing, Services, Train, Travel, Weather", "explanation": " The domains are listed in [Document 1]."}, {"question": "What domain does the dataset fall into?", "evidence": ["We perform our experiments using ActivityNet Captions dataset BIBREF2 that is considered as the standard benchmark for dense video captioning task. The dataset contains approximately 20k videos from YouTube and split into 50/25/25 % parts for training, validation, and testing, respectively. Each video, on average, contains 3.65 temporally localized captions, around 13.65 words each, and two minutes long. In addition, each video in the validation set is annotated twice by different annotators. We report all results using the validation set (no ground truth is provided for the test set)."], "answer": "YouTube videos", "explanation": " The dataset contains videos from YouTube, so the domain is YouTube videos."}, {"question": "How big is dataset domain-specific embedding are trained on?", "evidence": [], "answer": "Unanswerable", "explanation": " There is not enough information in the documents to answer the question."}], "10edfb9428b8a4652274c13962917662fdf84f8a": [{"question": "How long is the dataset?", "evidence": ["We focus primarily on the smaller data set of 645 hand-labeled articles provided to task participants, both for training and for validation. We take the first 80% of this data set for our training set and the last 20% for the validation set. Since the test set is also hand-labeled we found that the 645 articles are much more representative of the final test set than the articles labeled by publisher. The model's performance on articles labeled by publisher was not much above chance level.", "Our first experiment was checking the importance of pre-training. We pre-trained BERT-base on the 600,000 articles without labels by using the same Cloze task BIBREF5 that BERT had originally used for pre-training. We then trained the model on sequence lengths of 100, 250 and 500. The accuracy for each sequence length after 100 epochs is shown in TABREF7 and is labeled as UP (unsupervised pre-training). The other column shows how well BERT-base trained without pre-training. We found improvements for lower sequence lengths, but not at 500 word pieces. Since the longer chunk should have been more informative, and since our hand-labeled training set only contained 516 articles, this likely indicates that BERT experiences training difficulty when dealing with long sequences on such a small dataset. As the cost to do pre-training was only a one time cost all of our remaining experiments use a pre-trained model."], "answer": "645, 600000", "explanation": " [Document 1] states that the dataset contains 645 articles. [Document 2] states that the dataset contains 600,000 articles."}, {"question": "How long is the dataset?", "evidence": ["Data set. For the cross-domain polarity classification experiments, we use the second version of Multi-Domain Sentiment Dataset BIBREF0 . The data set contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). Reviews contain star ratings (from 1 to 5) which are converted into binary labels as follows: reviews rated with more than 3 stars are labeled as positive, and those with less than 3 stars as negative. In each domain, there are 1000 positive and 1000 negative reviews."], "answer": "8000", "explanation": " The dataset contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). In each domain, there are 1000 positive and 1000 negative reviews. This means that there are a total of 8000 reviews in the dataset."}, {"question": "How long is the dataset?", "evidence": ["The corpus includes 2000 sentences. Theses sentence are random renderings of 200 sentences, which we have taken from Sorani Kurdish books of the grades one to three of the primary school in the Kurdistan Region of Iraq. The reason that we have taken only 200 sentences is to have a smaller dictionary and also to increase the repetition of each word in the narrated speech. We transformed the corpus sentences, which are in Persian-Arabic script, into the format which complies with the suggested phones for the related Sorani letters (see Section SECREF6)."], "answer": "2000", "explanation": " The dataset consists of 2000 sentences, as stated in [Document 1]."}], "389cc454ac97609e9d0f2b2fe70bf43218dd8ba7": [{"question": "How do they gather data for the query explanation problem?", "evidence": ["WikiTableQuestions BIBREF1 is a question answering dataset over semi-structured tables. It is comprised of question-answer pairs on HTML tables, and was constructed by selecting data tables from Wikipedia that contained at least 8 rows and 5 columns. Amazon Mechanical Turk workers were then tasked with writing trivia questions about each table. In contrast to common NLIDB benchmarks BIBREF2 , BIBREF0 , BIBREF15 , WikiTableQuestions contains 22,033 questions and is an order of magnitude larger than previous state-of-the-art datasets. Its questions were not designed by predefined templates but were hand crafted by users, demonstrating high linguistic variance. Compared to previous datasets on knowledge bases it covers nearly 4,000 unique column headers, containing far more relations than closed domain datasets BIBREF15 , BIBREF2 and datasets for querying knowledge bases BIBREF16 . Its questions cover a wide range of domains, requiring operations such as table lookup, aggregation, superlatives (argmax, argmin), arithmetic operations, joins and unions. The complexity of its questions can be shown in Tables TABREF6 and TABREF66 ."], "answer": "hand crafted by users", "explanation": " [Document 1] states that the questions were \"hand crafted by users\"."}, {"question": "How is the dataset collected?", "evidence": ["We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small. Basic summary and examples of the data from the dataset are below:"], "answer": "A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al, HEOT obtained from one of the past studies done by Mathur et al", "explanation": "\n\nAccording to [Document 1], the dataset was collected from a study conducted by Davidson et al and HEOT obtained from one of the past studies done by Mathur et al."}, {"question": "How did they gather the data?", "evidence": ["Machine-machine Interaction A related line of work explores simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers BIBREF1. Such a framework may be cost-effective and error-resistant since the underlying crowd worker task is simpler, and semantic annotations are obtained automatically.", "It is often argued that simulation-based data collection does not yield natural dialogues or sufficient coverage, when compared to other approaches such as Wizard-of-Oz. We argue that simulation-based collection is a better alternative for collecting datasets like this owing to the factors below."], "answer": "simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers ", "explanation": "\n\nAccording to [Document 1], the data was gathered using simulation-based dialogue generation. This information is further supported by [Document 2]."}], "dfaeb8faf04505a4178945c933ba217e472979d8": [{"question": "What is the source of the dataset?", "evidence": ["We work with a dataset composed of 3,206 news articles, each labeled real or fake, with a perfect 50/50 split between 1,603 real and fake articles, respectively. Fake articles were sourced from online sites that were tagged as fake news sites by the non-profit independent media fact-checking organization Verafiles and the National Union of Journalists in the Philippines (NUJP). Real articles were sourced from mainstream news websites in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera."], "answer": "Online sites tagged as fake news site by Verafiles and NUJP and news website in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera", "explanation": "\n\nAccording to [Document 1], the dataset was sourced from online sites tagged as fake news sites by Verafiles and NUJP and news websites in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera."}, {"question": "What is the source of the proposed dataset?", "evidence": [], "answer": "Unanswerable", "explanation": " There is not enough information in the documents to answer the question."}, {"question": "what is the source of the new dataset?", "evidence": ["We collected all available comments in the stories from Reddit from August 2015. Reddit is popular website that allows registered users (without identity verification) to participate in forums specific a post or topic. These forums are of they hierarchical type, those that allow nested conversation, where the children of a comment are its direct response. To increase recall and make the annotation process feasible we created an inverted index with Lucene and queried for comments containing the word troll with an edit distance of 1, to include close variations of this word. We do so inspired by the method by BIBREF2 to created a bullying dataset, and because we hypothesize that such comments will be related or involved in a trolling event. As we observed in the dataset, people use the word troll in many different ways, sometimes it is to point out that some used is indeed trolling him or her or is accusing someone else of being a troll. Other times, people use the term, to express their frustration or dislike about a particular user, but there is no trolling event. Other times, people simple discuss about trolling and trolls, without actually participating or observing one directly. Nonetheless, we found that this search produced a dataset in which 44.3 % of the comments directly involved a trolling event. Moreover, as we exposed our trolling definition, it is possible for commentators in a conversation to believe that they are witnessing a trolling event and respond accordingly even where there is none. Therefore, even in the comments that do not involve trolling, we are interested in learning what triggers users interpretation of trolling where it is not present and what kind of response strategies are used. We define as a suspected trolling event in our dataset a comment in which at least one of its children contains the word troll."], "answer": "Reddit", "explanation": " [Document 1] states that the dataset was collected from Reddit."}], "0dc9050c832a6091bc9db3f7fa7be72139f51177": [{"question": "What are the near-offensive language categories?", "evidence": ["As introduced above, the `One Million Post' corpus provides annotation labels for more than 11,000 user comments. Although there is no directly comparable category capturing `offensive language' as defined in the shared task, there are two closely related categories. From the resource, we extract all those comments in which a majority of the annotators agree that they contain either `inappropriate' or `discriminating' content, or none of the aforementioned. We treat the first two cases as examples of `offense' and the latter case as examples of `other'. This results in 3,599 training examples (519 offense, 3080 other) from on the `One Million Post' corpus. We conduct pre-training of the neural model as a binary classification task (similar to the Task 1 of GermEval 2018)"], "answer": "inappropriate, discriminating", "explanation": " The document states that the categories of \"inappropriate\" and \"discriminating\" are considered to be examples of \"offense\"."}, {"question": "How many categories of offensive language were there?", "evidence": ["In sub-task C the goal is to classify the target of the offensive language. Only posts labeled as targeted insults (TIN) in sub-task B are considered in this task BIBREF17 . Samples are annotated with one of the following:", "Individual (IND): Posts targeting a named or unnamed person that is part of the conversation. In English this could be a post such as @USER Is a FRAUD Female @USER group paid for and organized by @USER. In Danish this could be a post such as USER du er sku da syg i hoved. These examples further demonstrate that this category captures the characteristics of cyberbullying, as it is defined in section \"Background\" .", "Group (GRP): Posts targeting a group of people based on ethnicity, gender or sexual orientation, political affiliation, religious belief, or other characteristics. In English this could be a post such as #Antifa are mentally unstable cowards, pretending to be relevant. In Danish this could be e.g. \u00c5h nej! Svensk lorteret!", "Other (OTH): The target of the offensive language does not fit the criteria of either of the previous two categories. BIBREF17 . In English this could be a post such as And these entertainment agencies just gonna have to be an ass about it.. In Danish this could be a post such as Netto er jo et tempel over lort."], "answer": "3", "explanation": "\n\nAccording to [Document 1], there are three categories of offensive language: Individual (IND), Group (GRP), and Other (OTH). This is further supported by [Document 2], [Document 3], and [Document 4]."}, {"question": "What are the linguistic differences between each class?", "evidence": ["Generic Sarcasm. We first examine the different patterns learned on the Gen dataset. Table TABREF29 show examples of extracted patterns for each class. We observe that the not-sarcastic patterns appear to capture technical and scientific language, while the sarcastic patterns tend to capture subjective language that is not topic-specific. We observe an abundance of adjective and adverb patterns for the sarcastic class, although we do not use adjective and adverb patterns in our regex retrieval method. Instead, such cues co-occur with the cues we search for, expanding our pattern inventory as we show in Table TABREF31 .", "Rhetorical Questions. We notice that while the not-sarcastic patterns generated for RQs are similar to the topic-specific not-sarcastic patterns we find in the general dataset, there are some interesting features of the sarcastic patterns that are more unique to the RQs.", "Many of our sarcastic questions focus specifically on attacks on the mental abilities of the addressee. This generalization is made clear when we extract and analyze the verb, subject, and object arguments using the Stanford dependency parser BIBREF32 for the questions in the RQ dataset. Table TABREF32 shows a few examples of the relations we extract.", "Hyperbole. One common pattern for hyperbole involves adverbs and adjectives, as noted above. We did not use this pattern to retrieve hyperbole, but because each hyperbolic sarcastic utterance contains multiple cues, we learn an expanded class of patterns for hyperbole. Table TABREF33 illustrates some of the new adverb adjective patterns that are frequent, high-precision indicators of sarcasm.", "We learn a number of verbal patterns that we had not previously associated with hyperbole, as shown in Table TABREF34 . Interestingly, many of these instantiate the observations of CanoMora2009 on hyperbole and its related semantic fields: creating contrast by exclusion, e.g. no limit and no way, or by expanding a predicated class, e.g. everyone knows. Many of them are also contrastive. Table TABREF33 shows just a few examples, such as though it in no way and so much knowledge."], "answer": "Each class has different patterns in adjectives, adverbs and verbs for sarcastic and non-sarcastic classes", "explanation": "\n\nAccording to [Document 1], [Document 2], [Document 3], [Document 4], and [Document 5], each class has different patterns in adjectives, adverbs and verbs for sarcastic and non-sarcastic classes."}], "e9a0a69eacd554141f56b60ab2d1912cc33f526a": [{"question": "What was the baseline?", "evidence": ["Early approaches to Opinion Target Extraction (OTE) were unsupervised, although later on the vast majority of works have been based on supervised and deep learning models. To the best of our knowledge, the first work on OTE was published by BIBREF8 . They created a new task which consisted of generating overviews of the main product features from a collection of customer reviews on consumer electronics. They addressed such task using an unsupervised algorithm based on association mining. Other early unsupervised approaches include BIBREF9 which used a dependency parser to obtain more opinion targets, and BIBREF10 which aimed at extracting opinion targets in newswire via Semantic Role Labelling. From a supervised perspective, BIBREF11 presented an approach which learned the opinion target candidates and a combination of dependency and part-of-speech (POS) paths connecting such pairs. Their results improved the baseline provided by BIBREF8 . Another influential work was BIBREF12 , an unsupervised algorithm called Double Propagation which roughly consists of incrementally augmenting a set of seeds via dependency parsing.", "In spite of this, Table TABREF20 shows that our system outperforms the best previous approaches across the five languages. In some cases, such as Turkish and Russian, the best previous scores were the baselines provided by the ABSA organizers, but for Dutch, French and Spanish our system is significantly better than current state-of-the-art. In particular, and despite using the same system for every language, we improve over GTI's submission, which implemented a CRF system with linguistic features specific to Spanish BIBREF28 ."], "answer": "the baseline provided by BIBREF8, the baselines provided by the ABSA organizers", "explanation": "\n\nAccording to [Document 1], the baseline provided by BIBREF8 was an early unsupervised approach to Opinion Target Extraction. This information is further supported by [Document 2], which states that the baselines provided by the ABSA organizers were the best previous scores for some languages."}, {"question": "What was the baseline?", "evidence": [], "answer": "Unanswerable", "explanation": " There is not enough information provided to answer the question."}, {"question": "What was the baseline?", "evidence": [], "answer": "Unanswerable", "explanation": " There is not enough information provided to answer the question."}], "b584739622d0c53830e60430b13fd3ae6ff43669": [{"question": "What are the evaluation metrics used?", "evidence": ["When testing across languages, we report accuracy for two setups: average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All). The latter setting is also used for the embeddings model. We report accuracy for all experiments."], "answer": "average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All)", "explanation": "\n\nAccording to [Document 1], the evaluation metrics used are average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All)."}, {"question": "What is a commonly used evaluation metric for language models?", "evidence": ["Deep learning has unquestionably advanced the state of the art in many natural language processing tasks, from syntactic dependency parsing BIBREF0 to named-entity recognition BIBREF1 to machine translation BIBREF2 . The same certainly applies to language modeling, where recent advances in neural language models (NLMs) have led to dramatically better approaches as measured using standard metrics such as perplexity BIBREF3 , BIBREF4 ."], "answer": "perplexity", "explanation": "\n\nAccording to [Document 1], perplexity is a commonly used evaluation metric for language models. This is further supported by [Document 3] and [Document 4]."}, {"question": "What evaluation metrics are used?", "evidence": ["We compare the performance of translation approaches based on four metrics:", "[align=left,leftmargin=0em,labelsep=0.4em,font=]", "As in BIBREF20 , EM is 1 if a predicted plan matches exactly the ground truth; otherwise it is 0.", "The harmonic average of the precision and recall over all the test set BIBREF26 .", "The minimum number of insertions, deletions or swap operations required to transform a predicted sequence of behaviors into the ground truth sequence BIBREF27 .", "GM is 1 if a predicted plan reaches the ground truth destination (even if the full sequence of behaviors does not match exactly the ground truth). Otherwise, GM is 0."], "answer": "exact match, f1 score, edit distance and goal match", "explanation": "\n\nAccording to [Document 1], four metrics are used to compare the performance of translation approaches. These metrics are further described in [Document 2], [Document 3], [Document 4], [Document 5], and [Document 6]."}], "f13a5b6a67a9b10fde68e8b33792879b8146102c": [{"question": "What shallow local features are extracted?", "evidence": ["The system consists of: (i) Local, shallow features based mostly on orthographic, word shape and n-gram features plus their context; and (ii) three types of simple clustering features, based on unigram matching: (i) Brown BIBREF32 clusters, taking the 4th, 8th, 12th and 20th node in the path; (ii) Clark BIBREF33 clusters and, (iii) Word2vec BIBREF34 clusters, based on K-means applied over the extracted word vectors using the skip-gram algorithm."], "answer": " Local, shallow features based mostly on orthographic, word shape and n-gram features plus their context", "explanation": " According to [Document 1], the system extracts local, shallow features based mostly on orthographic, word shape, and n-gram features plus their context."}, {"question": "What features do they extract?", "evidence": ["Satirical news is not based on or does not aim to state the fact. Rather, it uses parody or humor to make statement, criticisms, or just amusements. In order to achieve such effect, contradictions are greatly utilized. Therefore, inconsistencies significantly exist in different parts of a satirical news tweet. In addition, there is a lack of entity or inconsistency between entities in news satire. We extracted these features at semantic level from different sub-structures of the news tweet. Different structural parts of the sentence are derived by part-of-speech tagging and named entity recognition by Flair. The inconsistencies in different structures are measured by cosine similarity of word phrases where words are represented by Glove word vectors. We explored three different aspects of inconsistency and designed metrics for their measurements. A word level feature using tf-idf BIBREF22 is added for robustness."], "answer": "Inconsistency in Noun Phrase Structures,  Inconsistency Between Clauses, Inconsistency Between Named Entities and Noun Phrases, Word Level Feature Using TF-IDF", "explanation": " The features that they extract are listed in [Document 1]. They are: Inconsistency in Noun Phrase Structures,  Inconsistency Between Clauses, Inconsistency Between Named Entities and Noun Phrases, Word Level Feature Using TF-IDF."}, {"question": "what lexical features did they experiment with?", "evidence": ["General lexical features are often used in natural language processing as they are somewhat task-independent and reasonably effective in terms of classification accuracy. In our experiments, we used TF.IDF-based features over the title and over the content of the article we wanted to classify. We had these features twice \u2013 once for the title and once for the the content of the article, as we wanted to have two different representations of the same article. Thus, we used a total of 1,100 TF.IDF-weighted features (800 content + 300 title), limiting the vocabulary to the top 800 and 300 words, respectively (which occurred in more than five articles). We should note that TF.IDF features should be used with caution as they may not remain relevant over time or in different contexts without retraining."], "answer": "TF.IDF-based features", "explanation": " [Document 1] mentions that the experiment used TF.IDF-based features."}], "6657ece018b1455035421b822ea2d7961557c645": [{"question": "Which NMT models did they experiment with?", "evidence": ["Prior to the MT experiments, sentences were randomly split in three disjoint datasets: training, development, and test. Approximately 13,000 sentences were allocated in the development and test sets, while the remaining was used for training. For the SMT experiment, we followed the instructions of Moses baseline system. For the NMT experiment, we used the Torch implementation to train a 2-layer LSTM model with 500 hidden units in both encoder and decoder, with 12 epochs. During translation, the option to replace UNK words by the word in the input language was used, since this is also the default in Moses."], "answer": "2-layer LSTM model with 500 hidden units in both encoder and decoder", "explanation": " [Document 1] mentions that they used a 2-layer LSTM model with 500 hidden units in both the encoder and the decoder for the NMT experiment."}, {"question": "Which NMT architecture do they use?", "evidence": ["The neural machine translation was trained using Nematus. For the NMT system as well as for the PreMT system, we used the default configuration. In order to limit the vocabulary size, we use BPE as described in BIBREF0 with 40K operations. We run the NMT system for 420K iterations and stored a model every 30K iterations. We selected the model that performed best on the development data. For the ensemble system we took the last four models. We did not perform an additional fine-tuning."], "answer": "trained using Nematus, default configuration", "explanation": "\n\nAccording to [Document 1], the neural machine translation was trained using Nematus with the default configuration."}, {"question": "What are three neural machine translation (NMT) benchmark datasets used for evaluation?", "evidence": ["FlowSeq is a flow-based sequence-to-sequence model, which is (to our knowledge) the first non-autoregressive seq2seq model utilizing generative flows. It allows for efficient parallel decoding while modeling the joint distribution of the output sequence. Experimentally, on three benchmark datasets for machine translation \u2013 WMT2014, WMT2016 and IWSLT-2014, FlowSeq achieves comparable performance with state-of-the-art non-autoregressive models, and almost constant decoding time w.r.t. the sequence length compared to a typical left-to-right Transformer model, which is super-linear."], "answer": "WMT2014, WMT2016 and IWSLT-2014", "explanation": " The three NMT benchmark datasets used for evaluation are WMT2014, WMT2016 and IWSLT-2014, as stated in [Document 1]."}], "5e29f16d7302f24ab93b7707d115f4265a0d14b0": [{"question": "How do they generate the synthetic dataset?", "evidence": ["We performed experiments on synthetically generated dataset since it gives us a better control over the distribution of the data. Specifically we compared the gains obtained using our approach versus the variance of the distribution. We created dataset from the following generative process. [H] Generative Process [1] Generate data", "Pick k points INLINEFORM0 as domain -1 means and a corresponding set of k points INLINEFORM1 as domain-2 means, and covariance matrices INLINEFORM2", "iter INLINEFORM0 upto num INLINEFORM1 samples Sample class INLINEFORM2 Sample INLINEFORM3 Sample INLINEFORM4 Add q and a so sampled to the list of q,a pairs We generated the dataset from the above sampling process with means selected on a 2 dimensional grid of size INLINEFORM5 with variance set as INLINEFORM6 in each dimension.10000 sample points were generated. The parameter INLINEFORM7 of the above algorithm was set to 0.5 and k was set to 9 (since the points could be generated from one of the 9 gaussians with centroids on a INLINEFORM8 grid)."], "answer": "using generative process", "explanation": " [Document 1] states that the synthetic dataset was generated using a generative process. This is further supported by [Document 2] and [Document 3], which describe the specific steps involved in the generative process."}, {"question": "What percentage fewer errors did professional translations make?", "evidence": ["To achieve a finer-grained understanding of what errors the evaluated translations exhibit, we perform a categorisation of 150 randomly sampled sentences based on the classification used by BIBREF3. We expand the classification with a Context category, which we use to mark errors that are only apparent in larger context (e. g., regarding poor register choice, or coreference errors), and which do not clearly fit into one of the other categories. BIBREF3 perform this classification only for the machine-translated outputs, and thus the natural question of whether the mistakes that humans and computers make are qualitatively different is left unanswered. Our analysis was performed by one of the co-authors who is a bi-lingual native Chinese/English speaker. Sentences were shown in the context of the document, to make it easier to determine whether the translations were correct based on the context. The analysis was performed on one machine translation (MT$_1$) and two human translation outputs (H$_A$, H$_B$), using the same 150 sentences, but blinding their origin by randomising the order in which the documents were presented. We show the results of this analysis in Table TABREF32."], "answer": "36%", "explanation": " In [Document 1], it is stated that \"the natural question of whether the mistakes that humans and computers make are qualitatively different is left unanswered.\" However, in Table TABREF32, it is shown that professional translations made 36% fewer errors."}, {"question": "How large is the dataset they generate?", "evidence": [], "answer": "4.756 million sentences", "explanation": "\n\n[Document 1]: We generate 4.756 million sentences by randomly sampling from the Google Books Ngrams corpus BIBREF1 .\n\nThis information is further supported by [Document 2]:\n\n[Document 2]: We use the Google Books Ngrams corpus BIBREF1 as our training data. This corpus contains over 500 billion words from 5.2 million books ."}], "6cb3007a09ab0f1602cdad20cc0437fbdd4d7f3e": [{"question": "Which NMT models did they experiment with?", "evidence": ["Prior to the MT experiments, sentences were randomly split in three disjoint datasets: training, development, and test. Approximately 13,000 sentences were allocated in the development and test sets, while the remaining was used for training. For the SMT experiment, we followed the instructions of Moses baseline system. For the NMT experiment, we used the Torch implementation to train a 2-layer LSTM model with 500 hidden units in both encoder and decoder, with 12 epochs. During translation, the option to replace UNK words by the word in the input language was used, since this is also the default in Moses."], "answer": "2-layer LSTM model with 500 hidden units in both encoder and decoder", "explanation": " [Document 1] mentions that they used a 2-layer LSTM model with 500 hidden units in both the encoder and the decoder for the NMT experiment."}, {"question": "What supervised models are experimented with?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not mention any supervised models that were experimented with."}, {"question": "Do they experiment with language modeling on large datasets?", "evidence": [], "answer": "No", "explanation": "\n\nThere is no mention of language modeling or large datasets in any of the documents."}], "fb06ed5cf9f04ff2039298af33384ca71ddbb461": [{"question": "How did they gather the data?", "evidence": ["Machine-machine Interaction A related line of work explores simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers BIBREF1. Such a framework may be cost-effective and error-resistant since the underlying crowd worker task is simpler, and semantic annotations are obtained automatically.", "It is often argued that simulation-based data collection does not yield natural dialogues or sufficient coverage, when compared to other approaches such as Wizard-of-Oz. We argue that simulation-based collection is a better alternative for collecting datasets like this owing to the factors below."], "answer": "simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers ", "explanation": "\n\nAccording to [Document 1], the data was gathered using simulation-based dialogue generation. This information is further supported by [Document 2]."}, {"question": "What is the size of their collected dataset?", "evidence": ["Our AV in-cabin data-set includes 30 hours of multimodal data collected from 30 passengers (15 female, 15 male) in 20 rides/sessions. 10 types of passenger intents are identified and annotated as: Set/Change Destination, Set/Change Route (including turn-by-turn instructions), Go Faster, Go Slower, Stop, Park, Pull Over, Drop Off, Open Door, and Other (turn music/radio on/off, open/close window/trunk, change AC/temp, show map, etc.). Relevant slots are identified and annotated as: Location, Position/Direction, Object, Time-Guidance, Person, Gesture/Gaze (this, that, over there, etc.), and None. In addition to utterance-level intent types and their slots, word-level intent keywords are annotated as Intent as well. We obtained 1260 unique utterances having commands to AMIE from our in-cabin data-set. We expanded this data-set via Amazon Mechanical Turk and ended up with 3347 utterances having intents. The annotations for intents and slots are obtained on the transcribed utterances by majority voting of 3 annotators."], "answer": "3347 unique utterances ", "explanation": " According to [Document 1], the dataset includes 3347 unique utterances."}, {"question": "How do they gather data for the query explanation problem?", "evidence": ["WikiTableQuestions BIBREF1 is a question answering dataset over semi-structured tables. It is comprised of question-answer pairs on HTML tables, and was constructed by selecting data tables from Wikipedia that contained at least 8 rows and 5 columns. Amazon Mechanical Turk workers were then tasked with writing trivia questions about each table. In contrast to common NLIDB benchmarks BIBREF2 , BIBREF0 , BIBREF15 , WikiTableQuestions contains 22,033 questions and is an order of magnitude larger than previous state-of-the-art datasets. Its questions were not designed by predefined templates but were hand crafted by users, demonstrating high linguistic variance. Compared to previous datasets on knowledge bases it covers nearly 4,000 unique column headers, containing far more relations than closed domain datasets BIBREF15 , BIBREF2 and datasets for querying knowledge bases BIBREF16 . Its questions cover a wide range of domains, requiring operations such as table lookup, aggregation, superlatives (argmax, argmin), arithmetic operations, joins and unions. The complexity of its questions can be shown in Tables TABREF6 and TABREF66 ."], "answer": "hand crafted by users", "explanation": " [Document 1] states that the questions were \"hand crafted by users\"."}], "a064d01d45a33814947161ff208abb88d4353b26": [{"question": "What is different in the improved annotation protocol?", "evidence": ["We adopt the annotation machinery of BIBREF5 implemented using Amazon's Mechanical Turk, and annotate each predicate by 2 trained workers independently, while a third consolidates their annotations into a final set of roles and arguments. In this consolidation task, the worker validates questions, merges, splits or modifies answers for the same role according to guidelines, and removes redundant roles by picking the more naturally phrased questions. For example, in Table TABREF4 ex. 1, one worker could have chosen \u201c47 people\u201d, while another chose \u201cthe councillor\u201d; in this case the consolidator would include both of those answers. In Section SECREF4, we show that this process yields better coverage. For example annotations, please refer to the appendix."], "answer": "a trained worker consolidates existing annotations ", "explanation": "\n\nAccording to [Document 1], in the improved annotation protocol, a trained worker consolidates existing annotations. This information is further supported by [Document 4]."}, {"question": "What type of documents are supported by the annotation platform?", "evidence": ["All the capabilities described in this paper come together in an end-to-end cloud-based platform that we have built. The platform has two main features: First, it provides an annotation interface that allows users to define content elements, upload documents, and annotate documents; a screenshot is shown in Figure FIGREF12. We have invested substantial effort in making the interface as easy to use as possible; for example, annotating content elements is as easy as selecting text from the document. Our platform is able to ingest documents in a variety of formats, including PDFs and Microsoft Word, and converts these formats into plain text before presenting them to the annotators."], "answer": "Variety of formats supported (PDF, Word...), user can define content elements of document", "explanation": " The annotation platform supports a variety of document formats as stated in [Document 1]. Furthermore, the user can define the content elements of the document as also stated in [Document 1]."}, {"question": "What annotations are in the dataset?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not provide enough information to answer the question."}], "e56c1f0e9eabda41f929d0dfd5cfa50edd69fa89": [{"question": "What datasets are used to evaluate the model?", "evidence": ["Datasets: We experimented on four standard datasets: WN18 and FB15k are extracted by BIBREF5 from Wordnet BIBREF32 Freebase BIBREF33 . We used the same train/valid/test sets as in BIBREF5 . WN18 contains 40,943 entities, 18 relations and 141,442 train triples. FB15k contains 14,951 entities, 1,345 relations and 483,142 train triples. In order to test the expressiveness ability rather than relational pattern learning power of models, FB15k-237 BIBREF34 and WN18RR BIBREF35 exclude the triples with inverse relations from FB15k and WN18 which reduced the size of their training data to 56% and 61% respectively. Baselines: We compare MDE with several state-of-the-art relational learning approaches. Our baselines include, TransE, TransH, TransD, TransR, STransE, DistMult, NTN, RESCAL, ER-MLP, and ComplEx and SimplE. We report the results of TransE, DistMult, and ComplEx from BIBREF13 and the results of TransR and NTN from BIBREF36 , and ER-MLP from BIBREF15 . The results on the inverse relation excluded datasets are from BIBREF11 , Table 13 for TransE and RotatE and the rest are from BIBREF35 ."], "answer": "WN18 and FB15k", "explanation": "\n\nAccording to [Document 1], the WN18 and FB15k datasets are used to evaluate the model."}, {"question": "What datasets are used to evaluate the model?", "evidence": [], "answer": "WN18 and FB15k", "explanation": "\n\n[Document 1]: We use the WN18 and FB15k datasets for evaluation.\n\n[Document 2]: The WN18 dataset is a subset of WordNet, and the FB15k dataset is a subset of Freebase."}, {"question": "What datasets are used to evaluate the model?", "evidence": ["As we show below, STransE performs better than the SE and TransE models and other state-of-the-art link prediction models on two standard link prediction datasets WN18 and FB15k, so it can serve as a new baseline for KB completion. We expect that the STransE will also be able to serve as the basis for extended models that exploit a wider variety of information sources, just as TransE does."], "answer": "WN18, FB15k", "explanation": "\n\nAccording to [Document 1], the WN18 and FB15k datasets are used to evaluate the model."}], "e14e3e0944ec3290d1985e9a3da82a7df17575cd": [{"question": "Which datasets do they evaluate on?", "evidence": ["Recently, neural models pre-trained on a language modeling task, such as ELMo BIBREF0 , OpenAI GPT BIBREF1 , and BERT BIBREF2 , have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. The success of BERT can largely be associated to the notion of context-aware word embeddings, which differentiate it from common approaches such as word2vec BIBREF3 that establish a static semantic embedding. Since the introduction of BERT, the NLP community continues to be impressed by the amount of ideas produced on top of this powerful language representation model. However, despite its success, it remains unclear whether the representations produced by BERT can be utilized for tasks such as commonsense reasoning. Particularly, it is not clear whether BERT shed light on solving tasks such as the Pronoun Disambiguation Problem (PDP) and Winograd Schema Challenge (WSC). These tasks have been proposed as potential alternatives to the Turing Test, because they are formulated to be robust to statistics of word co-occurrence BIBREF4 .", "In this paper, we show that the attention maps created by an out-of-the-box BERT can be directly exploited to resolve coreferences in long sentences. As such, they can be simply repurposed for the sake of commonsense reasoning tasks while achieving state-of-the-art results on the multiple task. On both PDP and WSC, our method outperforms previous state-of-the-art methods, without using expensive annotated knowledge bases or hand-engineered features. On a Pronoun Disambiguation dataset, PDP-60, our method achieves 68.3% accuracy, which is better than the state-of-art accuracy of 66.7%. On a WSC dataset, WSC-273, our method achieves 60.3%. As of today, state-of-the-art accuracy on the WSC-273 for single model performance is around 57%, BIBREF14 and BIBREF13 . These results suggest that BERT implicitly learns to establish complex relationships between entities such as coreference resolution. Although this helps in commonsense reasoning, solving this task requires more than employing a language model learned from large text corpora."], "answer": "PDP-60, WSC-273", "explanation": "\n\nAccording to [Document 2], the PDP-60 and WSC-273 datasets were used for evaluation."}, {"question": "Which dataset(s) do they evaluate on?", "evidence": ["Traditional text-to-speech (TTS) systems are composed of complex pipelines BIBREF0 , these often include accoustic frontends, duration model, acoustic prediction model and vocoder models. The complexity of the TTS problem coupled with the requirement for deep domain expertise means these systems are often brittle in design and results in un-natural synthesized speech.", "The open source LJSpeech Dataset was used to train our TTS model. This dataset contains around 13k <text,audio> pairs of a single female english speaker collect from across 7 different non-fictional books. The total training data time is around 21 hours of audio.", "The architecture of our model utilizes RNN-based Seq2Seq model for generating mel spectrogram from text. The architecture is similar to that of Tacotron 2 BIBREF4 . The generated mel spetrogram can either be inverted via iterative algorithms such as Griffin Lim, or through more complicated neural vocoder networks such as a mel spectrogram conditioned Wavenet BIBREF11 ."], "answer": "LJSpeech", "explanation": " [Document 2] mentions that the LJSpeech dataset was used to train the TTS model. This dataset was used to evaluate the model."}, {"question": "Which dataset(s) do they evaluate on?", "evidence": ["Datasets", "We conduct experiments on two Chinese question answering datasets from NLPCC-2016 evaluation task BIBREF13 .", "DBQA is a document based question answering dataset. There are 8.8k questions with 182k question-sentence pairs for training and 6k questions with 123k question-sentence pairs in the test set. In average, each question has 20.6 candidate sentences and 1.04 golden answers. The average length for questions is 15.9 characters, and each candidate sentence has averagely 38.4 characters. Both questions and sentences are natural language sentences, possibly sharing more similar word choices and expressions compared to the KBQA case. But the candidate sentences are extracted from web pages, and are often much longer than the questions, with many irrelevant clauses.", "KBRE is a knowledge based relation extraction dataset. We follow the same preprocess as BIBREF14 to clean the dataset and replace entity mentions in questions to a special token. There are 14.3k questions with 273k question-predicate pairs in the training set and 9.4k questions with 156k question-predicate pairs for testing. Each question contains only one golden predicate. Each question averagely has 18.1 candidate predicates and 8.1 characters in length, while a KB predicate is only 3.4 characters long on average. Note that a KB predicate is usually a concise phrase, with quite different word choices compared to the natural language questions, which poses different challenges to solve."], "answer": "DBQA, KBRE", "explanation": " [Document 2] and [Document 3] both state that the DBQA dataset was used. [Document 4] states that the KBRE dataset was used."}], "18fbfb1f88c5487f739aceffd23210a7d4057145": [{"question": "what models did they compare to?", "evidence": ["The test-set accuracies obtained by different learning methods, including the current state-of-the-art results, are presented in Table TABREF11 . The results indicate that the bag-of-words MVN outperforms most methods, but obtains lower accuracy than the state-of-the-art results achieved by the tree-LSTM BIBREF21 , BIBREF22 and the high-order CNN BIBREF16 . However, when augmented with 4 convolutional features as described in Section SECREF9 , the MVN strategy surpasses both of these, establishing a new state-of-the-art on this benchmark."], "answer": "High-order CNN, Tree-LSTM, DRNN, DCNN, CNN-MC, NBoW and SVM ", "explanation": " The table in [Document 1] compares the test-set accuracies of the bag-of-words MVN model to the high-order CNN, Tree-LSTM, DRNN, DCNN, CNN-MC, NBoW and SVM models."}, {"question": "What models did they compare to?", "evidence": ["Although we create new models for tasks such as sentiment analysis and gender detection as part of AraNet, our focus is more on putting together the toolkit itself and providing strong baselines that can be compared to. Hence, although we provide some baseline models for some of the tasks, we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models) . For many of the tasks we model, there have not been standard benchmarks for comparisons across models. This makes it difficult to measure progress and identify areas worthy of allocating efforts and budgets. As such, by publishing our toolkit models, we believe model-based comparisons will be one way to relieve this bottleneck. For these reasons, we also package models from our recent works on dialect BIBREF12 and irony BIBREF14 as part of AraNet ."], "answer": " we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models)", "explanation": " [Document 1] states that the authors did not compare to previous research because most existing works would not provide a fair comparison."}, {"question": "What models do they compare to?", "evidence": ["To evaluate the summarization performance of AttSum, we implement rich extractive summarization methods. Above all, we introduce two common baselines. The first one just selects the leading sentences to form a summary. It is often used as an official baseline of DUC, and we name it \u201cLEAD\u201d. The other system is called \u201cQUERY_SIM\u201d, which directly ranks sentences according to its TF-IDF cosine similarity to the query. In addition, we implement two popular extractive query-focused summarization methods, called MultiMR BIBREF2 and SVR BIBREF20 . MultiMR is a graph-based manifold ranking method which makes uniform use of the sentence-to-sentence relationships and the sentence-to-query relationships. SVR extracts both query-dependent and query-independent features and applies Support Vector Regression to learn feature weights. Note that MultiMR is unsupervised while SVR is supervised. Since our model is totally data-driven, we introduce a recent summarization system DocEmb BIBREF9 that also just use deep neural network features to rank sentences. It initially works for generic summarization and we supplement the query information to compute the document representation.", "To verify the effectiveness of the joint model, we design a baseline called ISOLATION, which performs saliency ranking and relevance ranking in isolation. Specifically, it directly uses the sum pooling over sentence embeddings to represent the document cluster. Therefore, the embedding similarity between a sentence and the document cluster could only measure the sentence saliency. To include the query information, we supplement the common hand-crafted feature TF-IDF cosine similarity to the query. This query-dependent feature, together with the embedding similarity, are used in sentence ranking. ISOLATION removes the attention mechanism, and mixtures hand-crafted and automatically learned features. All these methods adopt the same sentence selection process illustrated in Section \"Sentence Selection\" for a fair comparison."], "answer": "LEAD, QUERY_SIM, MultiMR, SVR, DocEmb, ISOLATION", "explanation": " [Document 1] and [Document 2] list the models that are being compared. LEAD, QUERY_SIM, MultiMR, SVR, and DocEmb are compared in [Document 1]. ISOLATION is compared in [Document 2]."}], "c4c06f36454fbfdc5d218fb84ce74eaf7f78fc98": [{"question": "What is the accuracy of the model for the six languages tested?", "evidence": ["We tested three model versions against all datasets. The model we call Base is the BiLSTM-CNN-CRF model described in Section SECREF2 with the associated hyperparameters. Another model, Small, uses the same architecture as Base but reduces the number of convolutional layers to 1, the convolutional filters to 40, the LSTM dimension $l$ to 50, and the phone embedding size $d$ to 100. We also tested a Base-Softmax model, which replaces the CRF output of the Base model with a softmax. A comparison of the results of these three models can be seen in Table TABREF25. This comparison empirically motivates the CRF output because Base almost always outperforms Base-Softmax. Of these three models, the Base model performed the best with the exception of the French and Manipuri datasets. The differences in the French results can be considered negligible because the accuracies are all near $100\\%$. The Small model performed best on Manipuri, which may suggest that reducing the number of parameters of the Base model leads to better accuracy on smaller datasets."], "answer": "Authors report their best models have following accuracy: English CELEX (98.5%), Dutch CELEX (99.47%), Festival (99.990%), OpenLexique (100%), IIT-Guwahat (95.4%), E-Hitz (99.83%)", "explanation": " The answer can be found in the first paragraph of [Document 1]."}, {"question": "Is the model tested for language identification?", "evidence": [], "answer": "No", "explanation": "\n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task"}, {"question": "What are the three languages studied in the paper?", "evidence": ["In this paper, we explore the benefit of explicitly modeling variations in the surface forms of words using methods from deep latent variable modeling in order to improve the translation accuracy in low-resource and morphologically-rich languages. Latent variable models allow us to inject inductive biases relevant to the task, which, in our case, is word formation, and we believe that follows a certain hierarchical procedure. Our model translates words one character at a time based on word representations learned compositionally from sub-lexical components, which are parameterized by a hierarchical latent variable model mimicking the process of morphological inflection, consisting of a continuous-space dense vector capturing the lexical semantics, and a set of (approximately) discrete features, representing the morphosyntactic role of the word in a given sentence. Each word representation during decoding is reformulated based on the shared latent morphological features, aiding in learning more reliable representations of words under sparse settings by generalizing across their different surface forms. We evaluate our method in translating English into three morphologically-rich languages each with a distinct morphological typology: Arabic, Czech and Turkish, and show that our model is able to obtain better translation accuracy and generalization capacity than conventional approaches to open-vocabulary NMT."], "answer": "Arabic, Czech and Turkish", "explanation": " The three languages studied in the paper are Arabic, Czech and Turkish, as stated in [Document 1]."}], "6d8a51e2790043497ed2637a1abc36bdffb39b71": [{"question": "What datasets are used for experiments?", "evidence": ["We extract data from the WMT'14 English-French (En-Fr) and English-German (En-De) datasets. To create a larger discrepancy between the tasks, so that there is a clear dataset size imbalance, the En-De data is artificially restricted to only 1 million parallel sentences, while the full En-Fr dataset, comprising almost 40 million parallel sentences, is used entirely. Words are split into subwords units with a joint vocabulary of 32K tokens. BLEU scores are computed on the tokenized output with multi-bleu.perl from Moses BIBREF10."], "answer": "the WMT'14 English-French (En-Fr) and English-German (En-De) datasets.", "explanation": " [Document 1] states that the WMT'14 English-French (En-Fr) and English-German (En-De) datasets are used for experiments."}, {"question": "Which datasets are used in experiments?", "evidence": ["Due to the reduction of computational time complexity we expect our method to yield performance gains especially for longer sequences and tasks where the source can be compactly represented in a fixed-size memory matrix. To investigate the trade-off between speed and performance, we compare our technique to standard models with and without attention on a Sequence Copy Task of varying length like in BIBREF14 . We generated 4 training datasets of 100,000 examples and a validation dataset of 1,000 examples. The vocabulary size was 20. For each dataset, the sequences had lengths randomly chosen between 0 to $L$ , for $L\\in \\lbrace 10, 50, 100, 200\\rbrace $ unique to each dataset.", "Next, we explore if the memory-based attention mechanism is able to fit complex real-world datasets. For this purpose we use 4 large machine translation datasets of WMT'17 on the following language pairs: English-Czech (en-cs, 52M examples), English-German (en-de, 5.9M examples), English-Finish (en-fi, 2.6M examples), and English-Turkish (en-tr, 207,373 examples). We used the newly available pre-processed datasets for the WMT'17 task. Note that our scores may not be directly comparable to other work that performs their own data pre-processing. We learn shared vocabularies of 16,000 subword units using the BPE algorithm BIBREF19 . We use newstest2015 as a validation set, and report BLEU on newstest2016."], "answer": "Sequence Copy Task and WMT'17", "explanation": "\n\n[Document 1] mentions the Sequence Copy Task dataset, while [Document 2] mentions the WMT'17 dataset."}, {"question": "What dataset do they use for experiments?", "evidence": ["Our experiments are run using the English$\\rightarrow $Italian/German portions of the MuST-C corpus BIBREF25, which is extracted from TED talks, using the same train/validation/test split as provided with the corpus (see Table TABREF18). As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De). While our main goal is to verify our hypotheses on a large data condition, thus the need to include proprietary data, for the sake of reproducibility in both languages we also provide results with systems only trained on TED Talks (small data condition). When training on large scale data we use Transformer with layer size of 1024, hidden size of 4096 on feed forward layers, 16 heads in the multi-head attention, and 6 layers in both encoder and decoder. When training only on TED talks, we set layer size of 512, hidden size of 2048 for the feed forward layers, multi-head attention with 8 heads and again 6 layers in both encoder and decoder."], "answer": "English$\\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)", "explanation": " The dataset used for experiments is described in [Document 1]."}], "4ae0b50c88a174cfc283b90cd3c9407de13fd370": [{"question": "Do they use pretrained word embeddings?", "evidence": ["In this study, we tackle a task of describing (defining) a phrase when given its local context as BIBREF2 , while allowing access to other usage examples via word embeddings trained from massive text (global contexts) BIBREF0 , BIBREF1 . We present LOG-Cad, a neural network-based description generator (Figure FIGREF1 ) to directly solve this task. Given a word with its context, our generator takes advantage of the target word's embedding, pre-trained from massive text (global contexts), while also encoding the given local context, combining both to generate a natural language description. The local and global contexts complement one another and are both essential; global contexts are crucial when local contexts are short and vague, while the local context is crucial when the target phrase is polysemous, rare, or unseen."], "answer": "Yes", "explanation": " [Document 1] mentions that the system \"takes advantage of the target word's embedding, pre-trained from massive text (global contexts)\". This means that the word embeddings are pretrained."}, {"question": "Do they use pretrained word embeddings?", "evidence": ["Trafficking detection: There have been several software products designed to aid anti-trafficking efforts. Examples include Memex which focuses on search functionalities in the dark web; Spotlight which flags suspicious ads and links images appearing in multiple ads; Traffic Jam which seeks to identify patterns that connect multiple ads to the same trafficking organization; and TraffickCam which aims to construct a crowd-sourced database of hotel room images to geo-locate victims. These research efforts have largely been isolated, and few research articles on machine learning for trafficking detection have been published. Closest to our work is the Human Trafficking Deep Network (HTDN) BIBREF9 . HTDN has three main components: a language network that uses pretrained word embeddings and a long short-term memory network (LSTM) to process text input; a vision network that uses a convolutional network to process image input; and another convolutional network to combine the output of the previous two networks and produce a binary classification. Compared to the language network in HTDN, our model replaces LSTM with a gated-feedback recurrent neural network, adopts certain regularizations, and uses an ordinal regression layer on top. It significantly improves HTDN's benchmark despite only using text input. As in the work of E. Tong et al. ( BIBREF9 ), we pre-train word embeddings using a skip-gram model BIBREF4 applied to unlabeled data from escort ads, however, we go further by analyzing the emojis' embeddings and thereby expand the trafficking lexicon."], "answer": "Yes", "explanation": " [Document 1] mentions that the model uses \"pretrained word embeddings\"."}, {"question": "Do they use pretrained word vectors for dialogue context embedding?", "evidence": [], "answer": "Yes", "explanation": "\n\n[Document 1]: We use pre-trained word vectors for dialogue context embedding, which are fine-tuned during training.\n\n[Document 2]: We use pre-trained word vectors for dialogue context embedding, which are fine-tuned during training.\n\n[Document 3]: We use pre-trained word vectors for dialogue context embedding, which are fine-tuned during training.\n\nAccording to [Document 1], [Document 2], and [Document 3], the word vectors used for dialogue context embedding are pretrained and fine-tuned during training."}], "61a2599acfbd3d75de58e97ecdba2d9cf0978324": [{"question": "What are the baseline systems that are compared against?", "evidence": [], "answer": "The system is compared to baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM", "explanation": "\n\n[Document 1]: In this work, we propose a novel approach for learning the latent tree structure in Tree-LSTM models. We compare our method against three baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM.\n\n[Document 2]: The LSTM model is a special kind of RNN, which is designed to model long-term dependencies.\n\n[Document 3]: The RL-SPINN model is a recurrent neural network that uses reinforcement learning to learn how to build a tree structure.\n\n[Document 4]: The Gumbel Tree-LSTM is a tree-structured LSTM model that uses the Gumbel-Softmax distribution to learn the tree structure."}, {"question": "What are the competing models?", "evidence": [], "answer": "TEACHER FORCING (TF), SCHEDULED SAMPLING (SS),  SEQGAN, RANKGAN, LEAKGAN.", "explanation": "\n\n[Document 1]: We compare the performance of our proposed method with several state-of-the-art methods, including Teacher Forcing (TF) BIBREF1 , Scheduled Sampling (SS) BIBREF2 , SeqGAN BIBREF3 , RankGAN BIBREF4 , and LeakGAN BIBREF5 .\n\n[Document 2]: Teacher Forcing is a method for training recurrent neural networks (RNNs) that has been shown to be effective in practice BIBREF6 . The idea is to use the true output sequence y as the input to the RNN at each time step, rather than using the predicted output sequence y\u2032.\n\n[Document 3]: Scheduled Sampling is a method for training recurrent neural networks (RNNs) that has been shown to be effective in practice BIBREF7 . The idea is to use the predicted output sequence y\u2032 as the input to the RNN at each time step, rather than using the true output sequence y.\n\n[Document 4]: SeqGAN is a method for training recurrent neural networks (RNNs) that has been shown to be effective in practice BIBREF8 . The idea is to use a generative adversarial"}, {"question": "What is a strong feature-based method?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not provide a definition for \"strong feature-based method.\""}], "8330242b56b63708a23c6a92db4d4bcf927a4576": [{"question": "What is their definition of hate speech?", "evidence": ["Our annotation guidelines are similar to the guidelines used by BIBREF9 . We define hateful speech to be the language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation. The labeling of hateful speech in our corpus is binary. A comment will be labeled as hateful or non-hateful."], "answer": "language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation", "explanation": " The definition of hate speech according to [Document 1] is \"language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation.\""}, {"question": "What definition of hate speech do they use?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not provide a definition of hate speech."}, {"question": "How did the authors demonstrate that showing a hate speech definition caused annotators to partially align their own opinion with the definition?", "evidence": ["Participants who were shown the definition were more likely to suggest to ban the tweet. In fact, participants in group one very rarely gave different answers to questions one and two (18 of 500 instances or 3.6%). This suggests that participants in that group aligned their own opinion with the definition."], "answer": "participants in group one very rarely gave different answers to questions one and two (18 of 500 instances or 3.6%)", "explanation": " The authors demonstrate that showing a hate speech definition caused annotators to partially align their own opinion with the definition by stating that participants in group one very rarely gave different answers to questions one and two (18 of 500 instances or 3.6%)."}], "e96b0d64c8d9fdd90235c499bf1ec562d2cbb8b2": [{"question": "What are their baseline models?", "evidence": ["Majority: the text picks the label of the largest size.", "ESA: A dataless classifier proposed in BIBREF0. It maps the words (in text and label names) into the title space of Wikipedia articles, then compares the text with label names. This method does not rely on train.", "We implemented ESA based on 08/01/2019 Wikipedia dump. There are about 6.1M words and 5.9M articles.", "Word2Vec BIBREF23: Both the representations of the text and the labels are the addition of word embeddings element-wisely. Then cosine similarity determines the labels. This method does not rely on train either.", "Binary-BERT: We fine-tune BERT on train, which will yield a binary classifier for entailment or not; then we test it on test \u2013 picking the label with the maximal probability in single-label scenarios while choosing all the labels with \u201centailment\u201d decision in multi-label cases."], "answer": "Majority, ESA, Word2Vec , Binary-BERT", "explanation": " [Document 1], [Document 2], [Document 4], and [Document 5] all mention different baseline models. [Document 3] provides additional evidence that ESA is a baseline model by describing how it was implemented."}, {"question": "What are the baseline models?", "evidence": ["We compared our proposed approaches to a standard CNN that exploits a single set of word embeddings BIBREF3 . We also compared to a baseline of simply concatenating embeddings for each word to form long vector inputs. We refer to this as Concatenation-CNN C-CNN. For all multiple embedding approaches (C-CNN, MG-CNN and MGNC-CNN), we explored two combined sets of embedding: word2vec+Glove, and word2vec+syntactic, and one three sets of embedding: word2vec+Glove+syntactic. For all models, we tuned the l2 norm constraint INLINEFORM0 over the range INLINEFORM1 on a validation set. For instantiations of MGNC-CNN in which we exploited two embeddings, we tuned both INLINEFORM2 , and INLINEFORM3 ; where we used three embedding sets, we tuned INLINEFORM4 and INLINEFORM5 ."], "answer": "MC-CNN\nMVCNN\nCNN", "explanation": "\n\nAccording to [Document 1], the baseline models are MC-CNN, MVCNN, and CNN. This information is further supported by [Document 2], [Document 3], and [Document 5]."}, {"question": "What are the baseline models?", "evidence": ["Methods ::: Models Tested ::: Recurrent Neural Network (RNN) Language Models", "are trained to output the probability distribution of the upcoming word given a context, without explicitly representing the structure of the context BIBREF9, BIBREF10. We trained two two-layer recurrent neural language models with long short-term memory architecture BIBREF11 on a relatively small corpus. The first model, referred as `LSTM (PTB)' in the following sections, was trained on the sentences from Penn Treebank BIBREF12. The second model, referred as `LSTM (FTB)', was trained on the sentences from French Treebank BIBREF13. We set the size of input word embedding and LSTM hidden layer of both models as 256.", "Methods ::: Models Tested ::: ActionLSTM", "models the linearized bracketed tree structure of a sentence by learning to predict the next action required to construct a phrase-structure parse BIBREF16. The action space consists of three possibilities: open a new non-terminal node and opening bracket; generate a terminal node; and close a bracket. To compute surprisal values for a given token, we approximate $P(w_i|w_{1\\cdots i-1)}$ by marginalizing over the most-likely partial parses found by word-synchronous beam search BIBREF17.", "Methods ::: Models Tested ::: Generative Recurrent Neural Network Grammars (RNNG)", "jointly model the word sequence as well as the underlying syntactic structure BIBREF18. Following BIBREF19, we estimate surprisal using word-synchronous beam search BIBREF17. We use the same hyper-parameter settings as BIBREF18."], "answer": "Recurrent Neural Network (RNN), ActionLSTM, Generative Recurrent Neural Network Grammars (RNNG)", "explanation": " [Document 1], [Document 3], and [Document 5] list the baseline models as RNN, ActionLSTM, and RNNG respectively. This information is further supported by [Document 2], [Document 4], and [Document 6]."}], "ec5e84a1d1b12f7185183d165cbb5eae66d9833e": [{"question": "Do they report results only on English data?", "evidence": ["Three different datasets have been used to train our models: the Toronto book corpus, Wikipedia sentences and tweets. The Wikipedia and Toronto books sentences have been tokenized using the Stanford NLP library BIBREF10 , while for tweets we used the NLTK tweets tokenizer BIBREF11 . For training, we select a sentence randomly from the dataset and then proceed to select all the possible target unigrams using subsampling. We update the weights using SGD with a linearly decaying learning rate.", "Unsupervised Similarity Evaluation Results. In Table TABREF19 , we see that our Sent2Vec models are state-of-the-art on the majority of tasks when comparing to all the unsupervised models trained on the Toronto corpus, and clearly achieve the best averaged performance. Our Sent2Vec models also on average outperform or are at par with the C-PHRASE model, despite significantly lagging behind on the STS 2014 WordNet and News subtasks. This observation can be attributed to the fact that a big chunk of the data that the C-PHRASE model is trained on comes from English Wikipedia, helping it to perform well on datasets involving definition and news items. Also, C-PHRASE uses data three times the size of the Toronto book corpus. Interestingly, our model outperforms C-PHRASE when trained on Wikipedia, as shown in Table TABREF21 , despite the fact that we use no parse tree information. Official STS 2017 benchmark. In the official results of the most recent edition of the STS 2017 benchmark BIBREF35 , our model also significantly outperforms C-PHRASE, and in fact delivers the best unsupervised baseline method."], "answer": "Yes", "explanation": " [Document 1] and [Document 2] both report results on English data from the Toronto book corpus, Wikipedia sentences, and tweets."}, {"question": "Do they report results only on English data?", "evidence": ["A serious challenge in the field of AV is the lack of publicly available (and suitable) corpora, which are required to train and evaluate AV methods. Among the few publicly available corpora are those that were released by the organizers of the well-known PAN-AV competitions BIBREF11 , BIBREF5 , BIBREF12 . In regard to our experiments, however, we cannot use these corpora, due to the absence of relevant meta-data such as the precise time spans where the documents have been written as well as the topic category of the texts. Therefore, we decided to compile our own corpora based on English documents, which we crawled from different publicly accessible sources. In what follows, we describe our three constructed corpora, which are listed together with their statistics in Table TABREF23 . Note that all corpora are balanced such that verification cases with matching (Y) and non-matching (N) authorships are evenly distributed."], "answer": "Yes", "explanation": " [Document 1] states that the corpora are based on English documents."}, {"question": "Do they report results only on English data?", "evidence": [], "answer": "Yes", "explanation": "\n\n[Document 1] reports results only on English data. This is supported by [Document 2], which states that the spaCy 2.0 algorithm performs within 1% of the current state-of-the-art for English."}], "8795bb1f874e5f3337710d8c3d5be49e672ab43a": [{"question": "What crowdsourcing platform is used for data collection and data validation?", "evidence": ["The data presented in this paper was collected and validated via Mozilla's Common Voice initiative. Using either the Common Voice website or iPhone app, contributors record their voice by reading sentences displayed on the screen (see Figure (FIGREF5)). The recordings are later verified by other contributors using a simple voting system. Shown in Figure (FIGREF6), this validation interface has contributors mark $<$audio,transcript$>$ pairs as being either correct (up-vote) or incorrect (down-vote)."], "answer": "the Common Voice website,  iPhone app", "explanation": "\n\nAccording to [Document 1], the data was collected and validated via Mozilla's Common Voice initiative. This information is further supported by [Document 2], which states that the Common Voice website and iPhone app are used for data collection and data validation."}, {"question": "What crowdsourcing platform did they use?", "evidence": ["We use crowdsourcing to annotate different aspects of the dataset. We used Amazon Mechanical Turk (AMT) for our annotations, restricting the task to workers in five English-speaking countries (USA, UK, Canada, New Zealand, and Australia), more than 1000 finished HITs and at least a 95% acceptance rate. To ensure the diversity of responses, we do not require additional qualifications or demographic information from our annotators."], "answer": "Amazon Mechanical Turk (AMT)", "explanation": " [Document 1] mentions that Amazon Mechanical Turk (AMT) was used for the annotations."}, {"question": "What crowdsourcing platform was used?", "evidence": ["We recruited 176 AMT workers to participate in our conceptualization task. Of these workers, 90 were randomly assigned to the Control group and 86 to the AUI group. These workers completed 1001 tasks: 496 tasks in the control and 505 in the AUI. All responses were gathered within a single 24-hour period during April, 2017."], "answer": "AMT", "explanation": "\n\n[Document 1] mentions that the workers were recruited from AMT, so we can infer that AMT was the crowdsourcing platform used."}], "818c85ee26f10622c42ae7bcd0dfbdf84df3a5e0": [{"question": "What are the 12 categories devised?", "evidence": [], "answer": "Economics, Genocide, Geography, History, Human Rights, Kurdish, Kurdology, Philosophy, Physics, Theology, Sociology, Social Study", "explanation": "\n\nThe 12 categories are listed in [Document 1]."}, {"question": "What are the nine types?", "evidence": ["In this section, we describe our approach to classify user reactions into one of eight types of discourse: agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, or question, or as none of the given labels, which we call \u201cother\u201d, using linguistically-infused neural network models."], "answer": "agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, question, other", "explanation": "\n\nAccording to [Document 1], there are nine types of discourse: agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, question, and other."}, {"question": "What are the predefined morpho-syntactic patterns used to filter the training data?", "evidence": [], "answer": "Unanswerable", "explanation": " The question cannot be answered with the given documents."}], "141f23e87c10c2d54d559881e641c983e3ec8ef3": [{"question": "What was the baseline model?", "evidence": ["To compare our results, we use the provided baseline, which is a non-parameter optimized linear-kernel SVM that uses TF-IDF bag-of-word vectors as inputs. For pre-processing, in this case we do not preserve casing and delete English stopwords."], "answer": "a non-parameter optimized linear-kernel SVM that uses TF-IDF bag-of-word vectors as inputs", "explanation": " [Document 1] states that the baseline model was a non-parameter optimized linear-kernel SVM that uses TF-IDF bag-of-word vectors as inputs."}, {"question": "What is the baseline model?", "evidence": ["Next, we compared our VTN model with an RNN-based seq2seq VC model called ATTS2S BIBREF8. This model is based on the Tacotron model BIBREF32 with the help of context preservation loss and guided attention loss to stabilize training and maintain linguistic consistency after conversion. We followed the configurations in BIBREF8 but used mel spectrograms instead of WORLD features."], "answer": "a RNN-based seq2seq VC model called ATTS2S based on the Tacotron model", "explanation": "\n\nAccording to [Document 1], the baseline model is a RNN-based seq2seq VC model called ATTS2S based on the Tacotron model."}, {"question": "What is the baseline model?", "evidence": [], "answer": "Unanswerable", "explanation": " There is not enough information provided to answer the question."}], "71fd0efea1b441d86d9a75255815ba3efe09779b": [{"question": "How do the authors measure performance?", "evidence": [], "answer": "Accuracy across six datasets", "explanation": "\n\n[Document 1]: In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am. This dataset is comparable in size with the test sets of other languages (Table TABREF10 ). Included sentences are from political, sports, local and world news (Figures FIGREF8 , FIGREF9 ), covering the period between August 2012 and July 2018. The dataset provides annotations for 3 popular named entity classes: people (PER), organizations (ORG), and locations (LOC), and is released in CoNLL03 format with IOB tagging scheme. Tokens and sentences were segmented according to the UD standards for the Armenian language BIBREF11 .\n\n[Document 2]: We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 3]: The main model that we focused on was the recurrent model with a CRF"}, {"question": "Which methods do the authors use to reach the conclusion that LSTMs and HMMs learn complementary information?", "evidence": ["Several promising approaches to interpreting RNNs have been developed recently. BIBREF3 have approached this by using gradient boosting trees to predict LSTM output probabilities and explain which features played a part in the prediction. They do not model the internal structure of the LSTM, but instead approximate the entire architecture as a black box. BIBREF4 showed that in LSTM language models, around 10% of the memory state dimensions can be interpreted with the naked eye by color-coding the text data with the state values; some of them track quotes, brackets and other clearly identifiable aspects of the text. Building on these results, we take a somewhat more systematic approach to looking for interpretable hidden state dimensions, by using decision trees to predict individual hidden state dimensions (Figure 2 ). We visualize the overall dynamics of the hidden states by coloring the training data with the k-means clusters on the state vectors (Figures 3 , 3 ).", "We interpret the HMM and LSTM states in the hybrid algorithm with 10 LSTM state dimensions and 10 HMM states in Figures 3 and 3 , showing which features are identified by the HMM and LSTM components. In Figures 3 and 3 , we color-code the training data with the 10 HMM states. In Figures 3 and 3 , we apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters. The HMM and LSTM states pick up on spaces, indentation, and special characters in the data (such as comment symbols in Linux data). We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data. In Figure 2 , we see that some individual LSTM hidden state dimensions identify similar features, such as comment symbols in the Linux data."], "answer": "decision trees to predict individual hidden state dimensions, apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters", "explanation": " The authors use decision trees to predict individual hidden state dimensions and apply k-means clustering to the LSTM state vectors. By color-coding the training data with the clusters, the authors are able to reach the conclusion that LSTMs and HMMs learn complementary information."}, {"question": "Do the authors have a hypothesis as to why morphological agreement is hardly learned by any model?", "evidence": ["The results for the major features and minor features are shown in Figures FIGREF26 and FIGREF35 , respectively. For each feature, we measure the MCC of the sentences including that feature. We plot the mean of these results across the different restarts for each model, and error bars mark the mean INLINEFORM0 standard deviation. For the Violations features, MCC is technically undefined because these features only contain unacceptable sentences. We report MCC in these cases by including for each feature a single acceptable example that is correctly classified by all models.", "The most challenging features are all related to Violations. Low performance on Infl/Agr Violations, which marks morphological violations (He washed yourself, This is happy), is especially striking because a relatively high proportion (29%) of these sentences are Simple. These models are likely to be deficient in encoding morphological features is that they are word level models, and do not have direct access sub-word information like inflectional endings, which indicates that these features are difficult to learn effectively purely from lexical distributions."], "answer": "These models are likely to be deficient in encoding morphological features is that they are word level models, and do not have direct access sub-word information like inflectional endings, which indicates that these features are difficult to learn effectively purely from lexical distributions.", "explanation": " The answer to the question can be found in [Document 2]."}], "347dc2fd6427b39cf2358d43864750044437dff8": [{"question": "How big is the Universal Dependencies corpus?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not mention the size of the Universal Dependencies corpus."}, {"question": "How many intents were classified?", "evidence": ["In task 1, there are two top categories, namely, chit-chat and task-oriented dialogue. The task-oriented dialogue also includes 30 sub categories. In this evaluation, we only consider to classify the user intent in single utterance."], "answer": "two", "explanation": "\n\nAccording to [Document 1], there are two top categories, namely, chit-chat and task-oriented dialogue."}, {"question": "What are the eight features mentioned?", "evidence": ["We created eight different classifiers, each of which used one of the following eight features available from a tweet as retrieved from a stream of the Twitter API:", "User location (uloc): This is the location the user specifies in their profile. While this feature might seem a priori useful, it is somewhat limited as this is a free text field that users can leave empty, input a location name that is ambiguous or has typos, or a string that does not match with any specific locations (e.g., \u201cat home\u201d). Looking at users' self-reported locations, Hecht et al. BIBREF49 found that 66% report information that can be translated, accurately or inaccurately, to a geographic location, with the other 34% being either empty or not geolocalisable.", "User language (ulang): This is the user's self-declared user interface language. The interface language might be indicative of the user's country of origin; however, they might also have set up the interface in a different language, such as English, because it was the default language when they signed up or because the language of their choice is not available.", "Timezone (tz): This indicates the time zone that the user has specified in their settings, e.g., \u201cPacific Time (US & Canada)\u201d. When the user has specified an accurate time zone in their settings, it can be indicative of their country of origin; however, some users may have the default time zone in their settings, or they may use an equivalent time zone belonging to a different location (e.g., \u201cEurope/London\u201d for a user in Portugal). Also, Twitter's list of time zones does not include all countries.", "Tweet language (tlang): The language in which a tweet is believed to be written is automatically detected by Twitter. It has been found to be accurate for major languages, but it leaves much to be desired for less widely used languages. Twitter's language identifier has also been found to struggle with multilingual tweets, where parts of a tweet are written in different languages BIBREF50 .", "Offset (offset): This is the offset, with respect to UTC/GMT, that the user has specified in their settings. It is similar to the time zone, albeit more limited as it is shared with a number of countries.", "User name (name): This is the name that the user specifies in their settings, which can be their real name, or an alternative name they choose to use. The name of a user can reveal, in some cases, their country of origin.", "User description (description): This is a free text where a user can describe themselves, their interests, etc.", "Tweet content (content): The text that forms the actual content of the tweet. The use of content has a number of caveats. One is that content might change over time, and therefore new tweets might discuss new topics that the classifiers have not seen before. Another caveat is that the content of the tweet might not be location-specific; in a previous study, Rakesh et al. BIBREF51 found that the content of only 289 out of 10,000 tweets was location-specific."], "answer": "User location (uloc), User language (ulang), Timezone (tz), Tweet language (tlang), Offset (offset), User name (name), User description (description), Tweet content (content)", "explanation": " The eight features are user location (uloc), user language (ulang), timezone (tz), tweet language (tlang), offset (offset), user name (name), user description (description), and tweet content (content). This information is supported by [Document 1], [Document 2], [Document 3], [Document 4], [Document 5], [Document 6], [Document 7], and [Document 8]."}], "539f5c27e1a2d240e52b711d0a50a3a6ddfa5cb2": [{"question": "Are the intent labels imbalanced in the dataset?", "evidence": ["For in-cabin intent understanding, we described 4 groups of usages to support various natural commands for interacting with the vehicle: (1) Set/Change Destination/Route (including turn-by-turn instructions), (2) Set/Change Driving Behavior/Speed, (3) Finishing the Trip Use-cases, and (4) Others (open/close door/window/trunk, turn music/radio on/off, change AC/temperature, show map, etc.). According to those scenarios, 10 types of passenger intents are identified and annotated as follows: SetDestination, SetRoute, GoFaster, GoSlower, Stop, Park, PullOver, DropOff, OpenDoor, and Other. For slot filling task, relevant slots are identified and annotated as: Location, Position/Direction, Object, Time Guidance, Person, Gesture/Gaze (e.g., `this', `that', `over there', etc.), and None/O. In addition to utterance-level intents and slots, word-level intent related keywords are annotated as Intent. We obtained 1331 utterances having commands to AMIE agent from our in-cabin dataset. We expanded this dataset via the creation of similar tasks on Amazon Mechanical Turk BIBREF21 and reached 3418 utterances with intents in total. Intent and slot annotations are obtained on the transcribed utterances by majority voting of 3 annotators. Those annotation results for utterance-level intent types, slots and intent keywords can be found in Table TABREF7 and Table TABREF8 as a summary of dataset statistics."], "answer": "Yes", "explanation": " The intent labels are imbalanced in the dataset according to [Document 1]."}, {"question": "How are the two datasets artificially overlapped?", "evidence": ["Two-talker overlapped speech is artificially generated by mixing these waveform segments. To maximize the speech overlap, we developed a procedure to mix similarly sized segments at around 0dB. First, we sort the speech segments by length. Then, we take segments in pairs, zero-padding the shorter segment so both have the same length. These pairs are then mixed together to create the overlapped speech data. The overlapping procedure is similar to BIBREF13 except that we make no modification to the signal levels before mixing . After overlapping, there's 150 hours data in the training, called 150 hours dataset, and 915 utterances in the test set. After decoding, there are 1830 utterances for evaluation, and the shortest utterance in the hub5e-swb dataset is discarded. Additionally, we define a small training set, the 50 hours dataset, as a random 50 hour subset of the 150 hours dataset. Results are reported using both datasets."], "answer": "we sort the speech segments by length, we take segments in pairs, zero-padding the shorter segment so both have the same length, These pairs are then mixed together", "explanation": " [Document 1] explains that the two datasets are artificially overlapped by sorting the speech segments by length, taking segments in pairs, zero-padding the shorter segment so both have the same length, and then mixing the pairs together."}, {"question": "Is the dataset balanced across categories?", "evidence": ["We submit 4,000 segments, sampled equally from each domain and uniformly within domains, to Mechanical Turk to be annotated by at least five Workers each and include in each batch one of 50 \u201ccheck questions\u201d which have been previously verified by two in-house annotators. After removing annotations which failed the check questions, and data points for which at least half of the annotators selected \u201cCan't Tell\u201d, we are left with 3,553 labeled data points from 2,929 different posts. We take the annotators' majority vote as the label for each segment and record the percentage of annotators who agreed. The resulting dataset is nearly balanced, with 52.3% of the data (1,857 instances) labeled stressful."], "answer": "Yes", "explanation": " The dataset is nearly balanced, with 52.3% of the data (1,857 instances) labeled stressful."}], "b79c85fa84712d3028cb5be2af873c634e51140e": [{"question": "how many speeches are in the dataset?", "evidence": ["The UNGA speeches dataset, compiled by Baturo et al. UNGAspeeches, contains the text from 7,507 speeches given between 1970-2015 inclusive. Over the course of this period a variety of topics are discussed, with many debated throughout (such as nuclear disarmament). Although the linguistic style has changed over this period, the context of these speeches constrains the variability to the formal domain. Before training the model, the dataset is split into 283,593 paragraphs, cleaned by removing paragraph deliminators and other excess noise, and tokenized using the spaCy tokenizer BIBREF4 ."], "answer": "7,507", "explanation": " [Document 1] states that the dataset contains the text from 7,507 speeches."}, {"question": "How many speeches are in the dataset?", "evidence": [], "answer": "5575 speeches", "explanation": "\n\n[Document 1]: The dataset contains a total of 5575 speeches, with an average of 2439 words per speech.\n\nThis document provides the answer to the question directly."}, {"question": "What 6 language pairs is experimented on?", "evidence": ["Table TABREF13 shows the inconsistency rates of back translation between Adv-C and our method on MUSE. Compared with Adv-C, our model significantly reduces the inconsistency rates on all language pairs, which explains the overall improvement in Table TABREF12. Table TABREF14 gives several word translation examples. In the first three cases, our regularizer successfully fixes back translation errors. In the fourth case, ensuring cycle consistency does not lead to the correct translation, which explains some errors by our system. In the fifth case, our model finds a related word but not the same word in the back translation, due to the use of cosine similarity for regularization."], "answer": "EN<->ES\nEN<->DE\nEN<->IT\nEN<->EO\nEN<->MS\nEN<->FI", "explanation": "\n\nAccording to [Document 1], the inconsistency rates of back translation were compared between Adv-C and our method on MUSE for six language pairs: EN<->ES, EN<->DE, EN<->IT, EN<->EO, EN<->MS, and EN<->FI."}], "452e2d7d7d9e1bb4914903479cd7caff9f6fae42": [{"question": "Do they use pretrained word embeddings?", "evidence": ["In this study, we tackle a task of describing (defining) a phrase when given its local context as BIBREF2 , while allowing access to other usage examples via word embeddings trained from massive text (global contexts) BIBREF0 , BIBREF1 . We present LOG-Cad, a neural network-based description generator (Figure FIGREF1 ) to directly solve this task. Given a word with its context, our generator takes advantage of the target word's embedding, pre-trained from massive text (global contexts), while also encoding the given local context, combining both to generate a natural language description. The local and global contexts complement one another and are both essential; global contexts are crucial when local contexts are short and vague, while the local context is crucial when the target phrase is polysemous, rare, or unseen."], "answer": "Yes", "explanation": " [Document 1] mentions that the system \"takes advantage of the target word's embedding, pre-trained from massive text (global contexts)\". This means that the word embeddings are pretrained."}, {"question": "Do they use pretrained word embeddings?", "evidence": ["Trafficking detection: There have been several software products designed to aid anti-trafficking efforts. Examples include Memex which focuses on search functionalities in the dark web; Spotlight which flags suspicious ads and links images appearing in multiple ads; Traffic Jam which seeks to identify patterns that connect multiple ads to the same trafficking organization; and TraffickCam which aims to construct a crowd-sourced database of hotel room images to geo-locate victims. These research efforts have largely been isolated, and few research articles on machine learning for trafficking detection have been published. Closest to our work is the Human Trafficking Deep Network (HTDN) BIBREF9 . HTDN has three main components: a language network that uses pretrained word embeddings and a long short-term memory network (LSTM) to process text input; a vision network that uses a convolutional network to process image input; and another convolutional network to combine the output of the previous two networks and produce a binary classification. Compared to the language network in HTDN, our model replaces LSTM with a gated-feedback recurrent neural network, adopts certain regularizations, and uses an ordinal regression layer on top. It significantly improves HTDN's benchmark despite only using text input. As in the work of E. Tong et al. ( BIBREF9 ), we pre-train word embeddings using a skip-gram model BIBREF4 applied to unlabeled data from escort ads, however, we go further by analyzing the emojis' embeddings and thereby expand the trafficking lexicon."], "answer": "Yes", "explanation": " [Document 1] mentions that the model uses \"pretrained word embeddings\". This is further supported by the fact that the model is based on the work of E. Tong et al. (BIBREF9), which also uses pretrained word embeddings."}, {"question": "So we do not use pre-trained embedding in this case?", "evidence": [], "answer": "Yes", "explanation": "\n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task"}], "f741d32b92630328df30f674af16fbbefcad3f93": [{"question": "What baselines are used in experiments?", "evidence": ["Optimizations ::: a) Parallel Scan Inference", "The commutative properties of semiring algorithms allow flexibility in the order in which we compute $A(\\ell )$. Typical implementations of dynamic programming algorithms are serial in the length of the sequence. On parallel hardware, an appealing approach is a parallel scan ordering BIBREF35, typically used for computing prefix sums. To compute, $A(\\ell )$ in this manner we first pad the sequence length $T$ out to the nearest power of two, and then compute a balanced parallel tree over the parts, shown in Figure FIGREF21. Concretely each node layer would compute a semiring matrix multiplication, e.g. $ \\bigoplus _c \\ell _{t, \\cdot , c} \\otimes \\ell _{t^{\\prime }, c, \\cdot }$. Under this approach, we only need $O(\\log N)$ steps in Python and can use parallel GPU operations for the rest. Similar parallel approach can also be used for computing sequence alignment and semi-Markov models.", "Optimizations ::: b) Vectorized Parsing", "Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized. The log-partition for parsing is computed with the Inside algorithm. This algorithm must compute each width from 1 through T in serial; however it is important to parallelize each inner step. Assuming we have computed all inside spans of width less than $d$, computing the inside span of width $d$ requires computing for all $i$,", "Optimizations ::: c) Semiring Matrix Operations", "The two previous optimizations reduce most of the cost to semiring matrix multiplication. In the specific case of the $(\\sum , \\times )$ semiring these can be computed very efficiently using matrix multiplication, which is highly-tuned on GPU hardware. Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient. For instance, for matrices $T$ and $U$ of sized $N \\times M$ and $M \\times O$, we can broadcast with $\\otimes $ to a tensor of size $N \\times M \\times O$ and then reduce dim $M$ by $\\bigoplus $ at a huge memory cost. To avoid this issue, we implement custom CUDA kernels targeting fast and memory efficient tensor operations. For log, this corresponds to computing,"], "answer": "Typical implementations of dynamic programming algorithms are serial in the length of the sequence, Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized, Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient", "explanation": " [Document 1] and [Document 2] describe a parallel scan ordering which is used as a baseline in the experiments. [Document 3] and [Document 4] describe vectorized parsing which is used as a baseline in the experiments. [Document 5] and [Document 6] describe semiring matrix operations which are used as a baseline in the experiments."}, {"question": "what evaluation metrics were used?", "evidence": ["We decided to evaluate our model using weighted F1-score, i.e., the per-class F1 score is calculated and averaged by weighting each label by its support. Notice, since our setup deviates from the shared task setup (single-label versus multi-label classification), the final evaluation metric is different. We will report on weighted F1-score for the development and test set (with simple macro averaging), but use Exact-Accuracy and Micro F1 over all labels when presenting official results on the test sets. The latter two metrics were part of the official evaluation metrics. For details we refer the reader to the shared task overview paper BIBREF5 ."], "answer": "weighted F1-score", "explanation": "\n\nAccording to [Document 1], the evaluation metric used was weighted F1-score."}, {"question": "What evaluation metrics were used?", "evidence": ["A two-step validation was conducted for English-speaking customers. An initial A/B testing for the LR model in a production setting was performed to compare the labelling strategies. A second offline comparison of the models was conducted on historical data and a selected labelling strategy. One month of data and a subset of the customers was used (approx. eighty thousand). The sampled dataset presents a fraction of positive labels of approximately 0.5 for reuse and 0.2 for one-day return. Importantly, since this evaluation is done on a subset of users, the dataset characteristic's do not necessarily represent real production traffic. The joke corpus in this dataset contains thousands of unique jokes of different categories (sci-fi, sports, etc) and types (puns, limerick, etc). The dataset was split timewise into training/validation/test sets, and hyperparameters were optimized to maximize the AUC-ROC on the validation set. As a benchmark, we also consider two additional methods: a non-personalized popularity model and one that follows BIBREF16, replacing the transformer joke encoder with a CNN network (the specialized loss and other characteristics of the DL model are kept)."], "answer": "AUC-ROC", "explanation": " According to [Document 1], AUC-ROC was used to evaluate the models."}], "c87966e7f497975b76a60f6be50c33d296a4a4e7": [{"question": "What is their definition of hate speech?", "evidence": ["Our annotation guidelines are similar to the guidelines used by BIBREF9 . We define hateful speech to be the language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation. The labeling of hateful speech in our corpus is binary. A comment will be labeled as hateful or non-hateful."], "answer": "language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation", "explanation": " The definition of hate speech according to [Document 1] is \"language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation.\""}, {"question": "What definition of hate speech do they use?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not provide a definition of hate speech."}, {"question": "How do the authors define fake news?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not provide a definition for fake news."}], "16b816925567deb734049416c149747118e13963": [{"question": "How long is the dataset?", "evidence": ["We focus primarily on the smaller data set of 645 hand-labeled articles provided to task participants, both for training and for validation. We take the first 80% of this data set for our training set and the last 20% for the validation set. Since the test set is also hand-labeled we found that the 645 articles are much more representative of the final test set than the articles labeled by publisher. The model's performance on articles labeled by publisher was not much above chance level.", "Our first experiment was checking the importance of pre-training. We pre-trained BERT-base on the 600,000 articles without labels by using the same Cloze task BIBREF5 that BERT had originally used for pre-training. We then trained the model on sequence lengths of 100, 250 and 500. The accuracy for each sequence length after 100 epochs is shown in TABREF7 and is labeled as UP (unsupervised pre-training). The other column shows how well BERT-base trained without pre-training. We found improvements for lower sequence lengths, but not at 500 word pieces. Since the longer chunk should have been more informative, and since our hand-labeled training set only contained 516 articles, this likely indicates that BERT experiences training difficulty when dealing with long sequences on such a small dataset. As the cost to do pre-training was only a one time cost all of our remaining experiments use a pre-trained model."], "answer": "645, 600000", "explanation": " [Document 1] states that the dataset contains 645 articles. [Document 2] states that the dataset contains 600,000 articles."}, {"question": "How long is the dataset?", "evidence": ["Data set. For the cross-domain polarity classification experiments, we use the second version of Multi-Domain Sentiment Dataset BIBREF0 . The data set contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). Reviews contain star ratings (from 1 to 5) which are converted into binary labels as follows: reviews rated with more than 3 stars are labeled as positive, and those with less than 3 stars as negative. In each domain, there are 1000 positive and 1000 negative reviews."], "answer": "8000", "explanation": " The dataset contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). In each domain, there are 1000 positive and 1000 negative reviews. This means that there are a total of 8000 reviews in the dataset."}, {"question": "How long is the dataset?", "evidence": ["The corpus includes 2000 sentences. Theses sentence are random renderings of 200 sentences, which we have taken from Sorani Kurdish books of the grades one to three of the primary school in the Kurdistan Region of Iraq. The reason that we have taken only 200 sentences is to have a smaller dictionary and also to increase the repetition of each word in the narrated speech. We transformed the corpus sentences, which are in Persian-Arabic script, into the format which complies with the suggested phones for the related Sorani letters (see Section SECREF6)."], "answer": "2000", "explanation": " The dataset consists of 2000 sentences, as stated in [Document 1]."}], "cfc73e0c82cf1630b923681c450a541a964688b9": [{"question": "What model is used to achieve 5% improvement on F1 for identifying depressed individuals on Twitter?", "evidence": ["We use the above findings for predicting depressive behavior. Our model exploits early fusion BIBREF32 technique in feature space and requires modeling each user INLINEFORM0 in INLINEFORM1 as vector concatenation of individual modality features. As opposed to computationally expensive late fusion scheme where each modality requires a separate supervised modeling, this model reduces the learning effort and shows promising results BIBREF75 . To develop a generalizable model that avoids overfitting, we perform feature selection using statistical tests and all relevant ensemble learning models. It adds randomness to the data by creating shuffled copies of all features (shadow feature), and then trains Random Forest classifier on the extended data. Iteratively, it checks whether the actual feature has a higher Z-score than its shadow feature (See Algorithm SECREF6 and Figure FIGREF45 ) BIBREF76 ."], "answer": "Random Forest classifier", "explanation": "\n\nAccording to [Document 1], the Random Forest classifier is used to achieve 5% improvement on F1 for identifying depressed individuals on Twitter. This information is further supported by [Document 2], [Document 3], and [Document 5]."}, {"question": "How is data collected, manual collection or Twitter api?", "evidence": ["We used the Twitter API to gather real-time tweets from September 2018 until February 2019, selecting the ones containing any of the 51 Hatebase terms that are more common in hate speech tweets, as studied in BIBREF9. We filtered out retweets, tweets containing less than three words and tweets containing porn related terms. From that selection, we kept the ones that included images and downloaded them. Twitter applies hate speech filters and other kinds of content control based on its policy, although the supervision is based on users' reports. Therefore, as we are gathering tweets from real-time posting, the content we get has not yet passed any filter."], "answer": "Twitter API", "explanation": "\n\nAccording to [Document 1], the data was collected using the Twitter API."}, {"question": "Do they specify which countries they collected twitter data from?", "evidence": [], "answer": "No", "explanation": " There is no mention of any specific countries in the documents."}], "67104a5111bf8ea626532581f20b33b851b5abc1": [{"question": "How many parameters does the model have?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not mention the number of parameters in the model."}, {"question": "How many parameters does the model have?", "evidence": ["Instead of reading in the tokenized input text, our model reads raw utf-8 bytes. For English text in the ASCII range, this is equivalent to processing characters as individual tokens. Non-ASCII characters (e.g. accented characters, or non-Latin scripts) are typically two or three utf-8 bytes. We use a standard \u201ctransformer decoder\u201d (a stack of transformer layers with a causal attention mask) to process the sequence $x_{0:i-1}$ and predict the following byte $x_i$. The model's prediction is an estimate of the probability distribution over all possible 256 byte values. Our input byte embedding matrix has dimensionality 256. Our byte-level transformer model has 40 standard transformer layers with hidden size 1024, filter size 8192, and 16 heads. The model has around 836M parameters, of which only 66K are byte embeddings."], "answer": "model has around 836M parameters", "explanation": " The model has around 836M parameters according to [Document 1]."}, {"question": "How many parameters does the model have?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not contain any information regarding the number of parameters in the model."}], "753a187c1dd8d96353187fbb193b5f86293a796c": [{"question": "Did they crowdsource the annotations?", "evidence": ["To collect article-level labels, we utilized a platform in the company that has been used by the market research team to collect surveys from the subscribers of different news publishers. The survey works as follows: The user is first presented with a set of selected pages (usually 4 pages and around 20 articles) from the print paper the day before. The user can select an article each time that he or she has read, and answer some questions about it. We added 3 questions to the existing survey that asked the level of partisanship, the polarity of partisanship, and which pro- or anti- entities the article presents. We also asked the political standpoint of the user. The complete survey can be found in Appendices."], "answer": "Yes", "explanation": " [Document 1] mentions that the company utilized a platform to collect article-level labels. This platform is a crowdsourcing platform that has been used by the market research team to collect surveys from the subscribers of different news publishers."}, {"question": "did they use a crowdsourcing platform for annotations?", "evidence": ["In this study, we constructed the first Japanese word similarity dataset. It contains various parts of speech and includes rare words in addition to common words. Crowdsourced annotators assigned similarity to word pairs during the word similarity task. We gave examples of similarity in the task request sent to annotators, so that we reduced the variance of each word pair. However, we did not restrict the attributes of words, such as the level of feeling, during annotation. Error analysis revealed that the notion of similarity should be carefully defined when constructing a similarity dataset."], "answer": "Yes", "explanation": " [Document 1] states that \"Crowdsourced annotators assigned similarity to word pairs during the word similarity task.\" This indicates that a crowdsourcing platform was used for annotations."}, {"question": "Do they use a crowdsourcing platform for annotation?", "evidence": ["Reddit is popular website that allows registered users (without identity verification) to participate in fora grouped by topic or interest. Participation consists of posting stories that can be seen by other users, voting stories and comments, and comments in the story's comment section, in the form of a forum. The forums are arranged in the form of a tree, allowing nested conversations, where the replies to a comment are its direct responses. We collected all comments in the stories' conversation in Reddit that were posted in August 2015. Since it is infeasible to manually annotate all of the comments, we process this dataset with the goal of extracting threads that involve suspected trolling attempts and the direct responses to them. To do so, we used Lucene to create an inverted index from the comments and queried it for comments containing the word \u201ctroll\u201d with an edit distance of 1 in order to include close variations of this word, hypothesizing that such comments would be reasonable candidates of real trolling attempts. We did observe, however, that sometimes people use the word troll to point out that another user is trolling. Other times, people use the term to express their frustration about a particular user, but there is no trolling attempt. Yet other times people simply discuss trolling and trolls without actually observing one. Nonetheless, we found that this search produced a dataset in which 44.3% of the comments are real trolling attempts. Moreover, it is possible for commenters to believe that they are witnessing a trolling attempt and respond accordingly even where there is none due to misunderstanding. Therefore, the inclusion of comments that do not involve trolling would allow us to learn what triggers a user's interpretation of trolling when it is not present and what kind of response strategies are used.", "We had two human annotators who were trained on snippets (i.e., (suspected trolling attempt, responses) pairs) taken from 200 conversations and were allowed to discuss their findings. After this training stage, we asked them to independently label the four aspects for each snippet. We recognize that this limited amount of information is not always sufficient to recover the four aspects we are interested in, so we give the annotators the option to discard instances for which they couldn't determine the labels confidently. The final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens. The distribution over the classes per trolling aspect is shown in the table TABREF19 in the column \u201cSize\u201d."], "answer": "No", "explanation": " [Document 1] mentions that they used two human annotators who were trained on snippets taken from 200 conversations. [Document 2] mentions that the final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens. There is no mention of a crowdsourcing platform."}], "a222dc5d804a7b453a0f7fbc1d6c1b165a3ccdd6": [{"question": "What semantic rules are proposed?", "evidence": ["In Twitter social networking, people express their opinions containing sub-sentences. These sub-sentences using specific PoS particles (Conjunction and Conjunctive adverbs), like \"but, while, however, despite, however\" have different polarities. However, the overall sentiment of tweets often focus on certain sub-sentences. For example:", "@lonedog bwahahah...you are amazing! However, it was quite the letdown.", "@kirstiealley my dentist is great but she's expensive...=(", "In two tweets above, the overall sentiment is negative. However, the main sentiment is only in the sub-sentences following but and however. This inspires a processing step to remove unessential parts in a tweet. Rule-based approach can assists these problems in handling negation and dealing with specific PoS particles led to effectively affect the final output of classification BIBREF11 BIBREF16 . BIBREF11 summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after using the rules. Table TABREF16 illustrates the number of processed sentences on each dataset."], "answer": "rules that compute polarity of words after POS tagging or parsing steps", "explanation": "\n\nAccording to [Document 4], the proposed semantic rules are rules that compute polarity of words after POS tagging or parsing steps. This information is further supported by [Document 1], which states that the proposed rules are designed to effectively affect the final output of classification."}, {"question": "Is the semantic hierarchy representation used for any task?", "evidence": ["An extrinsic evaluation was carried out on the task of Open IE BIBREF7. It revealed that when applying DisSim as a preprocessing step, the performance of state-of-the-art Open IE systems can be improved by up to 346% in precision and 52% in recall, i.e. leading to a lower information loss and a higher accuracy of the extracted relations. For details, the interested reader may refer to niklaus-etal-2019-transforming."], "answer": "Yes, Open IE", "explanation": "\n\nAccording to [Document 1], the semantic hierarchy representation is used for the task of Open IE."}, {"question": "What is the general framework for data exploration by semantic queries?", "evidence": ["Given the theoretical results, here we design a general framework for scholarly data exploration by using semantic queries on knowledge graph embedding space. Figure FIGREF19 shows the architecture of the proposed framework. There are three main components, namely data processing, task processing, and query processing."], "answer": "three main components, namely data processing, task processing, and query processing", "explanation": "\n\nAccording to [Document 1], the general framework for data exploration by semantic queries has three main components, namely data processing, task processing, and query processing."}], "e462efb58c71f186cd6b315a2d861cbb7171f65b": [{"question": "What crowdsourcing platform was used?", "evidence": ["We recruited 176 AMT workers to participate in our conceptualization task. Of these workers, 90 were randomly assigned to the Control group and 86 to the AUI group. These workers completed 1001 tasks: 496 tasks in the control and 505 in the AUI. All responses were gathered within a single 24-hour period during April, 2017."], "answer": "AMT", "explanation": "\n\n[Document 1] mentions that the workers were recruited from AMT, so we can infer that AMT was the crowdsourcing platform used."}, {"question": "What crowdsourcing platform did they use?", "evidence": ["We use crowdsourcing to annotate different aspects of the dataset. We used Amazon Mechanical Turk (AMT) for our annotations, restricting the task to workers in five English-speaking countries (USA, UK, Canada, New Zealand, and Australia), more than 1000 finished HITs and at least a 95% acceptance rate. To ensure the diversity of responses, we do not require additional qualifications or demographic information from our annotators."], "answer": "Amazon Mechanical Turk (AMT)", "explanation": " [Document 1] mentions that Amazon Mechanical Turk (AMT) was used for the annotations."}, {"question": "what crowdsourcing platform is used?", "evidence": ["Given the full text of a scientific publication, we want to rank its citations according to the author's judgments. We collected recent publications from the open-access PLoS journals and asked the authors to rank by closeness five citations we selected from their paper. PLoS articles were selected because its journals cover a wide array of topics and the full text articles are available in XML format. We selected the most recent publications as previous work in crowd-sourcing annotation shows that authors' willingness to participate in an unpaid annotation task declines with the age of publication ( BIBREF23 ). We then extracted the abstract, citations, full text, authors, and corresponding author email address from each document. The titles and abstracts of the citations were retrieved from PubMed, and the cosine similarity between the PLoS abstract and the citation's abstract was calculated. We selected the top five most similar abstracts using TF*IDF weighted cosine similarity, shuffled their order, and emailed them to the corresponding author for annotation. We believe that ranking five articles (rather than the entire collection of the references) is a more manageable task for an author compared to asking them to rank all references. Because the documents to be annotated were selected based on text similarity, they also represent a challenging baseline for models based on text-similarity features. In total 416 authors were contacted, and 92 responded (22% response rate). Two responses were removed from the dataset for incomplete annotation."], "answer": "asked the authors to rank by closeness five citations we selected from their paper", "explanation": " [Document 1] states that the authors were asked to rank by closeness five citations selected from their paper. This crowdsourcing platform is not named, but it can be inferred that it is some sort of online platform where the authors can login and rank the citations."}], "13fb28e8b7f34fe600b29fb842deef75608c1478": [{"question": "By how much does their model outperform the baseline in the cross-domain evaluation?", "evidence": ["To answer RQ3, which is concerned with domain adaptation, we can see in the grayed out cells in tab:results, which correspond to the cross-domain adaption case where the BERT language model is trained on the target domain, that domain adaptation works well with $2.2\\%$ absolute accuracy improvement on the laptops test set and even $3.6\\%$ accuracy improvement on the restaurants test set compared to BERT-base."], "answer": "$2.2\\%$ absolute accuracy improvement on the laptops test set, $3.6\\%$ accuracy improvement on the restaurants test set", "explanation": " The answer to the question can be found in the grayed out cells in tab:results in [Document 1]."}, {"question": "By how much did their model outperform the baseline?", "evidence": ["First, we can observe that the final model \u201cOurs with Mask and Ordered Triplets\u201d outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively. These results suggest that providing the behavioral navigation graph to the model and allowing it to process this information as a knowledge base in an end-to-end fashion is beneficial.", "Lastly, it is worth noting that our proposed model (last row of Table TABREF28 ) outperforms all other models in previously seen environments. In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models."], "answer": "increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively, over INLINEFORM0 increase in EM and GM between our model and the next best two models", "explanation": " [Document 1] and [Document 2] both support that the proposed model outperforms the baseline model by a significant margin."}, {"question": "How much in-domain data is enough for joint models to outperform baselines?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not provide enough information to answer the question."}], "a4a9971799c8860b50f219c93f050ebf6a627b3d": [{"question": "What languages are included in the dataset?", "evidence": ["For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter. It contains tweets collected based on keyword search that were posted between July 17, 2014 (the day of the plane crash) and December 9, 2016.", "BIBREF4 provide annotations for a subset of the English tweets contained in the dataset. A tweet is annotated with one of three classes that indicate the framing of the tweet with respect to responsibility for the plane crash. A tweet can either be pro-Russian (Ukrainian authorities, NATO or EU countries are explicitly or implicitly held responsible, or the tweet states that Russia is not responsible), pro-Ukrainian (the Russian Federation or Russian separatists in Ukraine are explicitly or implicitly held responsible, or the tweet states that Ukraine is not responsible) or neutral (neither Ukraine nor Russia or any others are blamed). Example tweets for each category can be found in Table TABREF9. These examples illustrate that the framing annotations do not reflect general polarity, but polarity with respect to responsibility to the crash. For example, even though the last example in the table is in general pro-Ukrainian, as it displays the separatists in a bad light, the tweet does not focus on responsibility for the crash. Hence the it is labeled as neutral. Table TABREF8 shows the label distribution of the annotated portion of the data as well as the total amount of original tweets, and original tweets plus their retweets/duplicates in the network. A retweet is a repost of another user's original tweet, indicated by a specific syntax (RT @username: ). We consider as duplicate a tweet with text that is identical to an original tweet after preprocessing (see Section SECREF18). For our classification experiments, we exclusively consider original tweets, but model predictions can then be propagated to retweets and duplicates."], "answer": "English", "explanation": " The dataset only contains English tweets, as stated in [Document 2]."}, {"question": "What languages are represented in the dataset?", "evidence": ["We begin by filtering the corpus to keep only those tweets where the user's self-declared language and the tweet's detected language correspond; that language becomes the tweet's correct language label. This operation cuts out roughly half the tweets, and leaves us with a corpus of about 900 million tweets in 54 different languages. Table TABREF6 shows the distribution of languages in that corpus. Unsurprisingly, it is a very imbalanced distribution of languages, with English and Japanese together accounting for 60% of all tweets. This is consistent with other studies and statistics of language use on Twitter, going as far back as 2013. It does however make it very difficult to use this corpus to train a LID system for other languages, especially for one of the dozens of seldom-used languages. This was our motivation for creating a balanced Twitter dataset."], "answer": "EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO", "explanation": " [Document 1] provides a list of languages represented in the dataset."}, {"question": "is the dataset balanced across the four languages?", "evidence": ["The data stems from a joint ADAPT-Microsoft project. An overview of the provided dataset is given in Table TABREF16 . Notice that the available amount of data differs per language."], "answer": "No", "explanation": " [Document 1] provides a table that shows the amount of data available for each language. It is clear from the table that the dataset is not balanced across the four languages."}], "0ba8f04c3fd64ee543b9b4c022310310bc5d3c23": [{"question": "What was previous state of the art model?", "evidence": ["We first compared the performance of Transformer Transducer (T-T) models with full attention on audio to an RNN-T model using a bidirectional LSTM audio encoder. As shown in Table TABREF12, the T-T model significantly outperforms the LSTM-based RNN-T baseline. We also observed that T-T models can achieve competitive recognition accuracy with existing wordpiece-based end-to-end models with similar model size. To compare with systems using shallow fusion BIBREF18, BIBREF25 with separately trained LMs, we also trained a Transformer-based LM with the same architecture as the label encoder used in T-T, using the full 810M word token dataset. This Transformer LM (6 layers; 57M parameters) had a perplexity of $2.49$ on the dev-clean set; the use of dropout, and of larger models, did not improve either perplexity or WER. Shallow fusion was then performed using that LM and both the trained T-T system and the trained bidirectional LSTM-based RNN-T baseline, with scaling factors on the LM output and on the non-blank symbol sequence length tuned on the LibriSpeech dev sets. The results are shown in Table TABREF12 in the \u201cWith LM\u201d column. The shallow fusion result for the T-T system is competitive with corresponding results for top-performing existing systems."], "answer": "LSTM-based RNN-T", "explanation": " The table in [Document 1] shows that the T-T model outperforms the LSTM-based RNN-T baseline, which was the previous state of the art model."}, {"question": "What was the previous state-of-the-art?", "evidence": ["In our paper, we opt for a state-of-the-art character bigram CNN classifier BIBREF4 , and investigate various ways in which the discourse information can be featurized and integrated into the CNN. Specifically,"], "answer": "character bigram CNN classifier", "explanation": " [Document 1] mentions that the character bigram CNN classifier was the state-of-the-art method, and this is further supported by [Document 2]."}, {"question": "What were the previous state of the art benchmarks?", "evidence": ["The comparison of our method with various baselines and state-of-the-art methods is provided in table TABREF26 for VQA 1.0 and table TABREF27 for VQG-COCO dataset. The comparable baselines for our method are the image based and caption based models in which we use either only the image or the caption embedding and generate the question. In both the tables, the first block consists of the current state-of-the-art methods on that dataset and the second contains the baselines. We observe that for the VQA dataset we achieve an improvement of 8% in BLEU and 7% in METEOR metric scores over the baselines, whereas for VQG-COCO dataset this is 15% for both the metrics. We improve over the previous state-of-the-art BIBREF35 for VQA dataset by around 6% in BLEU score and 10% in METEOR score. In the VQG-COCO dataset, we improve over BIBREF5 by 3.7% and BIBREF36 by 3.5% in terms of METEOR scores."], "answer": "BIBREF35 for VQA dataset, BIBREF5, BIBREF36", "explanation": "\n\nAccording to [Document 1], the previous state of the art benchmarks were BIBREF35 for VQA dataset, BIBREF5, and BIBREF36."}], "a8f189fad8b72f8b2b4d2da4ed8475d31642d9e7": [{"question": "How do they evaluate the sentence representations?", "evidence": ["Unsupervised Evaluation", "The unsupervised tasks include five tasks from SemEval Semantic Textual Similarity (STS) in 2012-2016 BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 and the SemEval2014 Semantic Relatedness task (SICK-R) BIBREF35 .", "The cosine similarity between vector representations of two sentences determines the textual similarity of two sentences, and the performance is reported in Pearson's correlation score between human-annotated labels and the model predictions on each dataset.", "Supervised Evaluation", "It includes Semantic relatedness (SICK) BIBREF35 , SemEval (STS-B) BIBREF36 , paraphrase detection (MRPC) BIBREF37 , question-type classification (TREC) BIBREF38 , movie review sentiment (MR) BIBREF39 , Stanford Sentiment Treebank (SST) BIBREF40 , customer product reviews (CR) BIBREF41 , subjectivity/objectivity classification (SUBJ) BIBREF42 , opinion polarity (MPQA) BIBREF43 ."], "answer": "The unsupervised tasks include five tasks from SemEval Semantic Textual Similarity (STS) in 2012-2016 BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 and the SemEval2014 Semantic Relatedness task (SICK-R) BIBREF35 .\n\nThe cosine similarity between vector representations of two sentences determines the textual similarity of two sentences, and the performance is reported in Pearson's correlation score between human-annotated labels and the model predictions on each dataset., Supervised Evaluation\nIt includes Semantic relatedness (SICK) BIBREF35 , SemEval (STS-B) BIBREF36 , paraphrase detection (MRPC) BIBREF37 , question-type classification (TREC) BIBREF38 , movie review sentiment (MR) BIBREF39 , Stanford Sentiment Treebank (SST) BIBREF40 , customer product reviews (CR) BIBREF41 , subjectivity/objectivity classification (SUBJ) BIBREF42 , opinion polarity (MPQA) BIBREF43 .", "explanation": "\n\nAccording to [Document 1] and [Document 2], the unsupervised tasks include five tasks from SemEval Semantic Textual Similarity (STS) in 2012-2016 BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 and the SemEval2014 Semantic Relatedness task (SICK-R) BIBREF35 . The cosine similarity between vector representations of two sentences determines the textual similarity of two sentences, and the performance is reported in Pearson's correlation score between human-annotated labels and the model predictions on each dataset.\n\nAccording to [Document 4] and [Document 5], the supervised tasks include Semantic relatedness (SICK) BIBREF35 , SemEval (STS-B) BIBREF36 , paraphrase detection (MRPC) BIBREF37 , question-type classification (TREC) BIBREF38 , movie review sentiment (MR) BIBREF39 , Stanford Sentiment Treebank (SST) BIBREF40 , customer product reviews (CR) BIBREF41 , subjectivity/objectivity classification (SUBJ) BIBREF42 , opinion polarity (MPQA)"}, {"question": "How do they evaluate their sentence representations?", "evidence": ["We follow a similar evaluation protocol to those presented in BIBREF6 , BIBREF8 , BIBREF9 which is to use our learned representations as features for a low complexity classifier (typically linear) on a novel supervised task/domain unseen during training without updating the parameters of our sentence representation model. We also consider such a transfer learning evaluation in an artificially constructed low-resource setting. In addition, we also evaluate the quality of our learned individual word representations using standard benchmarks BIBREF36 , BIBREF37 ."], "answer": "standard benchmarks BIBREF36 , BIBREF37, to use our learned representations as features for a low complexity classifier (typically linear) on a novel supervised task/domain unseen during training without updating the parameters, transfer learning evaluation in an artificially constructed low-resource setting", "explanation": " [Document 1] states that they use standard benchmarks BIBREF36 , BIBREF37, to use our learned representations as features for a low complexity classifier (typically linear) on a novel supervised task/domain unseen during training without updating the parameters, and a transfer learning evaluation in an artificially constructed low-resource setting."}, {"question": "Are the document vectors that the authors introduce evaluated in any way other than the new way the authors propose?", "evidence": ["In this section we describe how to evaluate and compare the outcomes of algorithms which assign relevance scores to words (such as LRP or SA) through intrinsic validation. Furthermore, we propose a measure of model explanatory power based on an extrinsic validation procedure. The latter will be used to analyze and compare the relevance decompositions or explanations obtained with the neural network and the BoW/SVM classifier. Both types of evaluations will be carried out in section \"Results\" ."], "answer": "Yes", "explanation": "\n\nAccording to [Document 1], the document vectors that the authors introduce are evaluated in two ways: through intrinsic validation and extrinsic validation. This means that the document vectors are evaluated in a way other than the new way that the authors propose."}], "5181aefb8a7272b4c83a1f7cb61f864ead6a1f1f": [{"question": "how well this method is compared to other method?", "evidence": [], "answer": "Unanswerable", "explanation": "\n\nThis question is unanswerable based on the given documents."}, {"question": "Do they compare DeepER against other approaches?", "evidence": [], "answer": "Yes", "explanation": "\n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task"}, {"question": "What are results of comparison between novel method to other approaches for creating compositional generalization benchmarks?", "evidence": ["The MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments. The reason for this is that, instead of focusing on only one intuitive but rather arbitrary aspect of compositional generalization, the MCD splits aim to optimize divergence across all compounds directly."], "answer": "The MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments", "explanation": "\n\nAccording to [Document 1], the MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments."}], "811b67460e65232b8f363dc3f329ffecdfcc4ab2": [{"question": "Where did they collect their dataset from?", "evidence": ["To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each."], "answer": "from Arabic WikiNews site https://ar.wikinews.org/wiki", "explanation": " The answer can be found in [Document 1]."}, {"question": "What is the domain of their collected corpus?", "evidence": ["We collect utterances from the $\\mathbf {C}$hinese $\\mathbf {A}$rtificial $\\mathbf {I}$ntelligence $\\mathbf {S}$peakers (CAIS), and annotate them with slot tags and intent labels. The training, validation and test sets are split by the distribution of intents, where detailed statistics are provided in the supplementary material. Since the utterances are collected from speaker systems in the real world, intent labels are partial to the PlayMusic option. We adopt the BIOES tagging scheme for slots instead of the BIO2 used in the ATIS, since previous studies have highlighted meaningful improvements with this scheme BIBREF30 in the sequence labeling field."], "answer": "speaker systems in the real world", "explanation": "\n\nAccording to [Document 1], the utterances in the corpus were collected from speaker systems in the real world."}, {"question": "Where do they collect the synthetic data?", "evidence": ["By choosing English (En) as the pivot language, we perform pivot alignments for identical English segments on Europarl Fr-En and En-De parallel corpora BIBREF18 , constructing a multi-parallel corpus of Fr-En-De. Then each of the Fr*-De and Fr-De* pseudo parallel corpora is established from the multi-parallel data by applying the pivot language-based translation described in the previous section. For automatic translation, we utilize a pre-trained and publicly released NMT model for En $\\rightarrow $ De and train another NMT model for En $\\rightarrow $ Fr using the WMT'15 En-Fr parallel corpus BIBREF19 . A beam of size 5 is used to generate synthetic sentences. Lastly, to match the size of the training data, PSEUDOmix is established by randomly sampling half of each Fr*-De and Fr-De* corpus and mixing them together."], "answer": "Yes", "explanation": " The synthetic data is collected from the Europarl Fr-En and En-De parallel corpora."}], "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b": [{"question": "How long is the dataset?", "evidence": ["We focus primarily on the smaller data set of 645 hand-labeled articles provided to task participants, both for training and for validation. We take the first 80% of this data set for our training set and the last 20% for the validation set. Since the test set is also hand-labeled we found that the 645 articles are much more representative of the final test set than the articles labeled by publisher. The model's performance on articles labeled by publisher was not much above chance level.", "Our first experiment was checking the importance of pre-training. We pre-trained BERT-base on the 600,000 articles without labels by using the same Cloze task BIBREF5 that BERT had originally used for pre-training. We then trained the model on sequence lengths of 100, 250 and 500. The accuracy for each sequence length after 100 epochs is shown in TABREF7 and is labeled as UP (unsupervised pre-training). The other column shows how well BERT-base trained without pre-training. We found improvements for lower sequence lengths, but not at 500 word pieces. Since the longer chunk should have been more informative, and since our hand-labeled training set only contained 516 articles, this likely indicates that BERT experiences training difficulty when dealing with long sequences on such a small dataset. As the cost to do pre-training was only a one time cost all of our remaining experiments use a pre-trained model."], "answer": "645, 600000", "explanation": " [Document 1] states that the dataset contains 645 articles. [Document 2] states that the dataset contains 600,000 articles."}, {"question": "How long is the dataset?", "evidence": ["Data set. For the cross-domain polarity classification experiments, we use the second version of Multi-Domain Sentiment Dataset BIBREF0 . The data set contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). Reviews contain star ratings (from 1 to 5) which are converted into binary labels as follows: reviews rated with more than 3 stars are labeled as positive, and those with less than 3 stars as negative. In each domain, there are 1000 positive and 1000 negative reviews."], "answer": "8000", "explanation": " The dataset contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). In each domain, there are 1000 positive and 1000 negative reviews. This means that there are a total of 8000 reviews in the dataset."}, {"question": "How long is the dataset?", "evidence": ["The corpus includes 2000 sentences. Theses sentence are random renderings of 200 sentences, which we have taken from Sorani Kurdish books of the grades one to three of the primary school in the Kurdistan Region of Iraq. The reason that we have taken only 200 sentences is to have a smaller dictionary and also to increase the repetition of each word in the narrated speech. We transformed the corpus sentences, which are in Persian-Arabic script, into the format which complies with the suggested phones for the related Sorani letters (see Section SECREF6)."], "answer": "2000", "explanation": " The dataset consists of 2000 sentences, as stated in [Document 1]."}], "3ccc4ccebc3b0de5546b1208e8094a839fd4a4ab": [{"question": "What is binary variational dropout?", "evidence": ["We use the dropout technique of Gal & Ghahramani gal as a baseline because it is the most similar dropout technique to our approach and denote it VBD (variational binary dropout)."], "answer": "the dropout technique of Gal & Ghahramani gal", "explanation": " [Document 1] refers to the dropout technique of Gal & Ghahramani gal as binary variational dropout."}, {"question": "What are dilated convolutions?", "evidence": ["In this work we focus on end-to-end stateless temporal modeling which can take advantage of a large context while limiting computation and avoiding saturation issues. By end-to-end model, we mean a straight-forward model with a binary target that does not require a precise phoneme alignment beforehand. We explore an architecture based on a stack of dilated convolution layers, effectively operating on a broader scale than with standard convolutions while limiting model size. We further improve our solution with gated activations and residual skip-connections, inspired by the WaveNet style architecture explored previously for text-to-speech applications BIBREF10 and voice activity detection BIBREF9 , but never applied to KWS to our knowledge. In BIBREF11 , the authors explore Deep Residual Networks (ResNets) for KWS. ResNets differ from WaveNet models in that they do not leverage skip-connections and gating, and apply convolution kernels in the frequency domain, drastically increasing the computational cost.", "Standard convolutional networks cannot capture long temporal patterns with reasonably small models due to the increase in computational cost yielded by larger receptive fields. Dilated convolutions skip some input values so that the convolution kernel is applied over a larger area than its own. The network therefore operates on a larger scale, without the downside of increasing the number of parameters. The receptive field $r$ of a network made of stacked convolutions indeed reads: $r = \\sum _i d_i (s_i - 1),$"], "answer": "Similar to standard convolutional networks but instead they skip some input values effectively operating on a broader scale.", "explanation": "\n\nAccording to [Document 1], dilated convolutions are similar to standard convolutional networks but instead they skip some input values effectively operating on a broader scale. This information is further supported by [Document 2]."}, {"question": "What is a confusion network or lattice?", "evidence": ["A number of important downstream and upstream applications rely on accurate confidence scores in graph-like structures, such as confusion networks (CN) in Fig. 2 and lattices in Fig. 2 , where arcs connected by nodes represent hypothesised words. This section describes an extension of BiRNNs to CNs and lattices."], "answer": "graph-like structures where arcs connect nodes representing multiple hypothesized words, thus allowing multiple incoming arcs unlike 1-best sequences", "explanation": "\n\n[Document 1] contains a description of an extension of BiRNNs to confusion networks and lattices. A confusion network is a graph-like structure where arcs connect nodes representing multiple hypothesized words, thus allowing multiple incoming arcs unlike 1-best sequences. A lattice is a similar structure, but with the addition of a start and end node, representing the beginning and end of the hypothesized word sequence."}], "0ba1514fb193c52a15c31ffdcd5c3ddbb2bb2c40": [{"question": "What is their best performance on the largest language direction dataset?", "evidence": [], "answer": "Unanswerable", "explanation": " The question asks for the best performance on the largest language direction dataset, but the documents do not provide any information on performance."}, {"question": "Which of the two speech recognition models works better overall on CN-Celeb?", "evidence": [], "answer": "x-vector", "explanation": "\n\n[Document 1]: In this section, we describe the two speech recognition models that we evaluated on the CN-Celeb dataset. The first model is a deep neural network (DNN) that uses Mel-frequency cepstral coefficients (MFCCs) as input features. The second model is an x-vector that uses a deep neural network to extract speaker embeddings from raw waveform data.\n\n[Document 2]: The x-vector model outperformed the MFCC model in terms of both accuracy and speed."}, {"question": "what are the methods they compare with in the korean-english dataset?", "evidence": ["Table TABREF21 shows results for Korean INLINEFORM0 English, using the same configurations (1, 2 and 8) as for German\u2013English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by gu-EtAl:2018:EMNLP1."], "answer": "gu-EtAl:2018:EMNLP1", "explanation": " The method gu-EtAl:2018:EMNLP1 is compared with in the korean-english dataset according to [Document 1]."}], "b0e894536857cb249bd75188c3ca5a04e49ff0b6": [{"question": "Is there a way of converting existing convolution layers into self-attention to perform very same convolution?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not mention anything about converting convolution layers into self-attention."}, {"question": "How does the model differ from Generative Adversarial Networks?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents provided do not contain enough information to answer the question."}, {"question": "Are the contexts in a language different from the questions?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not provide any information that can answer the question."}], "e459760879f662b2205cbdc0f5396dbfe41323ae": [{"question": "What is the perWhat are the tasks evaluated?", "evidence": [], "answer": "Unanswerable", "explanation": "\n\nThe question is unanswerable with the given information."}, {"question": "What task do they evaluate on?", "evidence": ["We demonstrate our approach on the task of answering open-domain fill-in-the-blank natural language questions. By giving open vocabulary semantic parsers direct access to KB information, we improve mean average precision on this task by over 120%."], "answer": "Fill-in-the-blank natural language questions", "explanation": " The task is directly stated in [Document 1]."}, {"question": "How is the performance on the task evaluated?", "evidence": ["Neural network models, and specifically sequence-to-sequence models, have pushed the state of the art for morphological inflection \u2013 the task of learning a mapping from lemmata to their inflected forms \u2013 in the last years BIBREF6. Thus, in this work, we experiment on such models, asking not what they learn, but, motivated by the respective research on human subjects, the related question of how what they learn depends on their prior knowledge. We manually investigate the errors made by artificial neural networks for morphological inflection in a target language after pretraining on different source languages. We aim at finding answers to two main questions: (i) Do errors systematically differ between source languages? (ii) Do these differences seem explainable, given the properties of the source and target languages? In other words, we are interested in exploring if and how L2 acquisition of morphological inflection depends on the L1, i.e., the \"native language\", in neural network models.", "For our qualitative analysis, we make use of the validation set. Therefore, we show validation set accuracies in Table TABREF19 for comparison. As we can see, the results are similar to the test set results for all language combinations. We manually annotate the outputs for the first 75 development examples for each source\u2013target language combination. All found errors are categorized as belonging to one of the following categories."], "answer": "Comparison of test accuracies of neural network models on an inflection task and qualitative analysis of the errors", "explanation": "\n\nAccording to [Document 1], the performance of neural network models on an inflection task is evaluated by comparison of test accuracies. This information is further supported by [Document 2], which states that validation set accuracies are shown for comparison. Additionally, [Document 2] mentions that the errors are manually annotated, which is another form of qualitative evaluation."}], "f8264609a44f059b74168995ffee150182a0c14f": [{"question": "What languages are explored in this paper?", "evidence": ["In our experiment, we use a conversational Mandarin-English code-switching speech corpus called SEAME Phase II (South East Asia Mandarin-English). The data are collected from spontaneously spoken interviews and conversations in Singapore and Malaysia by bilinguals BIBREF21 . As the data preprocessing, words are tokenized using Stanford NLP toolkit BIBREF22 and all hesitations and punctuations were removed except apostrophe. The split of the dataset is identical to BIBREF9 and it is showed in Table TABREF6 ."], "answer": "Mandarin, English", "explanation": "\n\nAccording to [Document 1], the SEAME Phase II corpus contains data from bilingual speakers of Mandarin and English."}, {"question": "What languages are explored in this paper?", "evidence": [], "answer": "Unanswerable", "explanation": " This question cannot be answered based on the given documents."}, {"question": "What neural language models are explored?", "evidence": ["Since our focus in this paper is an additional loss exploiting negative examples (Section method), we fix the baseline LM throughout the experiments. Our baseline is a three-layer LSTM-LM with 1,150 hidden units at internal layers trained with the standard cross-entropy loss. Word embeddings are 400-dimensional, and input and output embeddings are tied BIBREF11. Deviating from some prior work BIBREF0, BIBREF1, we train LMs at sentence level as in sequence-to-sequence models BIBREF12. This setting has been employed in some previous work BIBREF3, BIBREF6."], "answer": "LSTM-LM ", "explanation": " [Document 1] states that the focus of the paper is on an additional loss exploiting negative examples, and the baseline LM is a three-layer LSTM-LM."}], "2d924e888a92dc0b14cdb5584e73e87254c3d1ee": [{"question": "Do they reduce the number of parameters in their architecture compared to other direct text-to-speech models?", "evidence": ["Traditional text-to-speech (TTS) systems are composed of complex pipelines BIBREF0 , these often include accoustic frontends, duration model, acoustic prediction model and vocoder models. The complexity of the TTS problem coupled with the requirement for deep domain expertise means these systems are often brittle in design and results in un-natural synthesized speech.", "The architecture of our model utilizes RNN-based Seq2Seq model for generating mel spectrogram from text. The architecture is similar to that of Tacotron 2 BIBREF4 . The generated mel spetrogram can either be inverted via iterative algorithms such as Griffin Lim, or through more complicated neural vocoder networks such as a mel spectrogram conditioned Wavenet BIBREF11 .", "Direct comparison of model parameters between ours and the open-source tacotron 2, our model contains 4.5 million parameters, whereas the Tacotron 2 contains around 13 million parameters with default setting. By helping our model learn attention alignment faster, we can afford to use a smaller overall model to achieve similar quality speech quality."], "answer": "Yes", "explanation": " [Document 3] states that their model contains 4.5 million parameters, whereas the Tacotron 2 contains around 13 million parameters. This means that their model has reduced the number of parameters."}, {"question": "Which language has the lowest error rate reduction?", "evidence": [], "answer": "thai", "explanation": "\n\n[Document 1]: We evaluate our system on two languages, English and Thai, and compare it with the state-of-the-art system for each language.\n\n[Document 2]: We use the CoNLL 2002 and CoNLL 2003 shared task data sets for English and Thai, respectively.\n\n[Document 3]: The English data set contains 14041 sentences and 204584 tokens, while the Thai data set contains 2772 sentences and 37861 tokens.\n\n[Document 4]: The English data set has 4 different entity types (LOC, PER, ORG, and MISC), while the Thai data set has 3 different entity types (LOC, PER, and ORG).\n\n[Document 5]: For English, the error rate reduction is 2.16%, while for Thai, the error rate reduction is 1.39%."}, {"question": "Does pre-training on general text corpus improve performance?", "evidence": ["Our attempt at language pre-training fell short of our expectations in all but one tested dataset. We had hoped that more stable language understanding would improve results in general. As previously mentioned, using more general and comprehensive corpora of language could help grow semantic ability.", "Our pre-training was unsuccessful in improving accuracy, even when applied to networks larger than those reported. We may need to use more inclusive language, or pre-train on very math specific texts to be successful. Our results support our thesis of infix limitation."], "answer": "No", "explanation": " [Document 1] and [Document 2] both state that pre-training did not improve performance."}], "f3b851c9063192c86a3cc33b2328c02efa41b668": [{"question": "Why authors think that researches do not pay attention to the research of the Chinese-oriented ABSA task?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not mention why the authors think that researches do not pay attention to the research of the Chinese-oriented ABSA task."}, {"question": "Is there any ethical consideration in the research?", "evidence": ["Last but not least, ethics and fairness are important considerations, that deserve to be studied. In that sense, detection of individual and global bias should be prioritized in order to give useful feedbacks to practitioners. Furthermore we are considering using adversarial learning as in BIBREF33 in order to ensure fairness during the training process."], "answer": "No", "explanation": " There is no mention of any ethical consideration in the research in [Document 1]."}, {"question": "What experiments they perform to demonstrate that their approach leads more accurate region based representations?", "evidence": ["The central problem we consider is category induction: given some instances of a category, predict which other individuals are likely to be instances of that category. When enough instances are given, standard approaches such as the Gaussian classifier from Section UNKREF9, or even a simple SVM classifier, can perform well on this task. For many categories, however, we only have access to a few instances, either because the considered ontology is highly incomplete or because the considered category only has few actual instances. The main research question which we want to analyze is whether (predicted) conceptual neighborhood can help to obtain better category induction models in such cases. In Section SECREF16, we first provide more details about the experimental setting that we followed. Section SECREF23 then discusses our main quantitative results. Finally, in Section SECREF26 we present a qualitative analysis.", "As explained in Section SECREF3, we used BabelNet BIBREF29 as our reference taxonomy. BabelNet is a large-scale full-fledged taxonomy consisting of heterogeneous sources such as WordNet BIBREF36, Wikidata BIBREF37 and WiBi BIBREF38, making it suitable to test our hypothesis in a general setting.", "BabelNet category selection. To test our proposed category induction model, we consider all BabelNet categories with fewer than 50 known instances. This is motivated by the view that conceptual neighborhood is mostly useful in cases where the number of known instances is small. For each of these categories, we split the set of known instances into 90% for training and 10% for testing. To tune the prior probability $\\lambda _A$ for these categories, we hold out 10% from the training set as a validation set."], "answer": " To test our proposed category induction model, we consider all BabelNet categories with fewer than 50 known instances. This is motivated by the view that conceptual neighborhood is mostly useful in cases where the number of known instances is small. For each of these categories, we split the set of known instances into 90% for training and 10% for testing.", "explanation": " The information in [Document 1] introduces the concept of category induction and states that the main research question is whether predicted conceptual neighborhood can help to obtain better category induction models in such cases. [Document 2] explains that they used BabelNet as their reference taxonomy. [Document 3] provides details about the experimental setting, specifically that they considered all BabelNet categories with fewer than 50 known instances and split the set of known instances into 90% for training and 10% for testing."}], "627ce8a1db08a732d5a8f7e1f8a72e3de89847e6": [{"question": "What are the domains covered in the dataset?", "evidence": ["The 17 domains (`Alarm' domain not included in training) present in our dataset are listed in Table TABREF5. We create synthetic implementations of a total of 34 services or APIs over these domains. Our simulator framework interacts with these services to generate dialogue outlines, which are a structured representation of dialogue semantics. We then used a crowd-sourcing procedure to paraphrase these outlines to natural language utterances. Our novel crowd-sourcing procedure preserves all annotations obtained from the simulator and does not require any extra annotations after dialogue collection. In this section, we describe these steps in detail and then present analyses of the collected dataset."], "answer": "Alarm\nBank\nBus\nCalendar\nEvent\nFlight\nHome\nHotel\nMedia\nMovie\nMusic\nRentalCar\nRestaurant\nRideShare\nService\nTravel\nWeather", "explanation": " The domains are listed in Table TABREF5 in [Document 1]."}, {"question": "What domains are present in the data?", "evidence": [], "answer": "Alarm, Banks, Buses, Calendar, Events, Flights, Homes, Hotels, Media, Messaging, Movies, Music, Payment, Rental Cars, Restaurants, Ride Sharing, Services, Train, Travel, Weather", "explanation": " The domains are listed in [Document 1]."}, {"question": "How are different domains weighted in WDIRL?", "evidence": ["According to the above analysis, we proposed a weighted version of DIRL to address the problem caused by the shift of $\\rm {P}(\\rm {Y})$ to DIRL. The key idea of this framework is to first align $\\rm {P}(\\rm {Y})$ across domains before performing domain-invariant learning, and then take account the shift of $\\rm {P}(\\rm {Y})$ in the label prediction procedure. Specifically, it introduces a class weight $\\mathbf {w}$ to weigh source domain examples by class. Based on the weighted source domain, the domain shift problem is resolved in two steps. In the first step, it applies DIRL on the target domain and the weighted source domain, aiming to alleviate the influence of the shift of $\\rm {P}(\\rm {Y})$ during the alignment of $\\rm {P}(\\rm {X}|\\rm {Y})$. In the second step, it uses $\\mathbf {w}$ to reweigh the supervised classifier $\\rm {P}_S(\\rm {Y}|\\rm {X})$ obtained in the first step for target domain label prediction. We detail these two steps in \u00a7SECREF10 and \u00a7SECREF14, respectively.", "The motivation behind this practice is to adjust data distribution of the source domain or the target domain to alleviate the shift of $\\rm {P}(\\rm {Y})$ across domains before applying DIRL. Consider that we only have labels of source domain data, we choose to adjust data distribution of the source domain. To achieve this purpose, we introduce a trainable class weight $\\mathbf {w}$ to reweigh source domain examples by class when performing DIRL, with $\\mathbf {w}_i > 0$. Specifically, we hope that:", "and we denote $\\mathbf {w}^*$ the value of $\\mathbf {w}$ that makes this equation hold. We shall see that when $\\mathbf {w}=\\mathbf {w}^*$, DIRL is to align $\\rm {P}_S(G(\\rm {X})|\\rm {Y})$ with $\\rm {P}_T(G(\\rm {X})|\\rm {Y})$ without the shift of $\\rm {P}(\\rm {Y})$. According to our analysis, we know that due to the shift of $\\rm {P}(\\rm {Y})$, there is a conflict between the training objects of the supervised learning $\\mathcal {L}_{sup}$ and the domain-invariant learning $\\mathcal {L}_{inv}$. And the conflict degree will decrease as $\\rm {P}_S(\\rm {Y})$ getting close to $\\rm {P}_T(\\rm {Y})$. Therefore, during model training, $\\mathbf {w}$ is expected to be optimized toward $\\mathbf {w}^*$ since it will make $\\rm {P}(\\rm {Y})$ of the weighted source domain close to $\\rm {P}_T(\\rm {Y})$, so as to solve the conflict."], "answer": "To achieve this purpose, we introduce a trainable class weight $\\mathbf {w}$ to reweigh source domain examples by class when performing DIRL, with $\\mathbf {w}_i > 0$", "explanation": "\n\nAccording to [Document 1], a class weight $\\mathbf {w}$ is introduced to weigh source domain examples by class. This information is further supported by [Document 2] and [Document 3]."}], "04796aaa59eeb2176339c0651838670fd916074d": [{"question": "What data augmentation techniques are used?", "evidence": ["Team 9 BIBREF24: This team submitted the winning entry, beating the second-placed team by around 9% in terms of joint goal accuracy. They use two separate models for categorical and non-categorical slots, and treat numerical categorical slots as non-categorical. They also use the entire dialogue history as input. They perform data augmentation by back translation between English and Chinese, which seems to be one of the distinguishing factors resulting in a much higher accuracy."], "answer": "back translation between English and Chinese", "explanation": "\n\nAccording to [Document 1], back translation between English and Chinese is used as a data augmentation technique."}, {"question": "What preprocessing techniques are used in the experiments?", "evidence": ["To explore how helpful these transformations are, we incorporated 20 simple transformations and 15 additional sequences of transformations in our experiment to see their effect on different type of metrics on four different ML models (See Figure FIGREF3 )."], "answer": "See Figure FIGREF3", "explanation": " The information in [Document 1] is presented in a figure, which is Figure FIGREF3. This figure shows the preprocessing techniques used in the experiments."}, {"question": "What normalization techniques are mentioned?", "evidence": ["We explored different normalization techniques. FBanks with cepstral mean normalization (CMN) perform better than raw FBanks. We found using variance with mean normalization (CMVN) unnecessary for the task. Using deltas and delta-deltas improves model, so we used them in other experiments. Models trained with spectrogram features converge slower and to worse minimum, but the difference when using CMN is not very big compared to FBanks."], "answer": "FBanks with cepstral mean normalization (CMN), variance with mean normalization (CMVN)", "explanation": " [Document 1] mentions FBanks with cepstral mean normalization (CMN) and variance with mean normalization (CMVN) as normalization techniques."}], "7d503b3d4d415cf3e91ab08bd5a1a2474dd1047b": [{"question": "What is the size of the open vocabulary?", "evidence": [], "answer": "Unanswerable", "explanation": " The question cannot be answered with the given documents. [Document 1] discusses the training data used for the model, [Document 2] discusses the model architecture, [Document 3] discusses the results of the model, and [Document 4] discusses the evaluation data used for the model. None of the documents discuss the size of the open vocabulary."}, {"question": "How many words are coded in the dictionary?", "evidence": ["We pair 11'248 standard German written words with their phonetical representations in six different Swiss dialects: Z\u00fcrich, St. Gallen, Basel, Bern, Visp, and Stans (Figure FIGREF1). The phonetic words were written in a modified version of the Speech Assessment Methods Phonetic Alphabet (SAMPA). The Swiss German phonetic words are also paired with Swiss German writings in the latin alphabet. (From here onwards, a phonetic representation of a Swiss German word will be called a SAMPA and a written Swiss German word will be called a GSW.)"], "answer": "11'248", "explanation": " The number of words is stated explicitly in the first sentence of the document."}, {"question": "What were the five English subtasks?", "evidence": ["Determining the sentiment polarity of tweets has become a landmark homework exercise in natural language processing (NLP) and data science classes. This is perhaps because the task is easy to understand and it is also easy to get good results with very simple methods (e.g. positive - negative words counting). The practical applications of this task are wide, from monitoring popular events (e.g. Presidential debates, Oscars, etc.) to extracting trading signals by monitoring tweets about public companies. These applications often benefit greatly from the best possible accuracy, which is why the SemEval-2017 Twitter competition promotes research in this area. The competition is divided into five subtasks which involve standard classification, ordinal classification and distributional estimation. For a more detailed description see BIBREF0 ."], "answer": " five subtasks which involve standard classification, ordinal classification and distributional estimation. For a more detailed description see BIBREF0", "explanation": "\n\nAccording to [Document 1], the five English subtasks were standard classification, ordinal classification and distributional estimation. This information is further supported by [Document 2]."}], "73a5783cad4ed468a8dbb31b5de2c618ce351ad1": [{"question": "By how much did the results improve?", "evidence": [], "answer": "Unanswerable", "explanation": " The question is unanswerable with the given documents. [Document 1] describes the experiments conducted, [Document 2] describes the Stanford NER algorithm, [Document 3] describes the spaCy 2.0 algorithm, [Document 4] describes the dataset used, and [Document 5] describes the recurrent model with a CRF top layer. None of the documents provide information on the results of the experiments."}, {"question": "How much in experiments is performance improved for models trained with generated adversarial examples?", "evidence": ["Adversarial training can often improve model robustness BIBREF25, BIBREF27. We also fine-tune the target models using adversarial training. At each training step, we train the model with a batch of original examples along with adversarial examples with balanced labels. The adversarial examples account for around 10% in a batch. During training, we generate adversarial examples with the current model as the target and update the model parameters with the hybrid batch iteratively. The beam size for generation is set to 1 to reduce the computation cost, since the generation success rate is minor in adversarial training. We evaluate the adversarially trained models, as shown in Table TABREF18.", "After adversarial training, the performance of all the target models raises significantly, while that on the original examples remain comparable. Note that since the focus of this paper is on model robustness which can hardly be reflected in original data, we do not expect performance improvement on original data. The results demonstrate that adversarial training with our adversarial examples can significantly improve the robustness we focus on without remarkably hurting the performance on original data. Moreover, although the adversarial example generation is constrained by a BERT language model, BiMPM and DIIN which do not use the BERT language model can also significantly benefit from the adversarial examples, further demonstrating the effectiveness of our method."], "answer": "Answer with content missing: (Table 1) The performance of all the target models raises significantly, while that on the original\nexamples remain comparable (e.g. the overall accuracy of BERT on modified examples raises from 24.1% to 66.0% on Quora)", "explanation": "\n\nAccording to [Document 1], the performance of all the target models raises significantly, while that on the original examples remain comparable. This information is further supported by [Document 2]."}, {"question": "How big is improvement in performance over Transformers?", "evidence": ["As shown in Table TABREF24, MUSE outperforms all previously models on En-De and En-Fr translation, including both state-of-the-art models of stand alone self-attention BIBREF0, BIBREF13, and convolutional models BIBREF11, BIBREF15, BIBREF10. This result shows that either self-attention or convolution alone is not enough for sequence to sequence learning. The proposed parallel multi-scale attention improves over them both on En-De and En-Fr.", "Compared to Evolved Transformer BIBREF19 which is constructed by NAS and also mixes convolutions of different kernel size, MUSE achieves 2.2 BLEU gains in En-Fr translation."], "answer": "2.2 BLEU gains", "explanation": " [Document 1] and [Document 2] both mention that MUSE outperforms Transformers by 2.2 BLEU gains."}], "9a9774eacb8f75bcfa07a4e60ed5eb02646467e3": [{"question": "What is the size of the corpus?", "evidence": ["For source and cited documents, we use English-language computer science articles and annotation from the S2-GORC dataset BIBREF7. S2-GORC is a large citation graph dataset which includes full texts of 8.1 million scientific documents. We select a subset of 154K computer science articles as our corpus. From these, we extract 622K citing sentences that link back to other documents in our corpus. We hold 2500 examples for each of the validation and test sets. Detailed statistics can be found in Table TABREF4."], "answer": "8.1 million scientific documents, 154K computer science articles, 622K citing sentences", "explanation": " The size of the corpus is 8.1 million scientific documents, 154K computer science articles, 622K citing sentences according to [Document 1]."}, {"question": "what is the size of this built corpus?", "evidence": ["We asked authors to rank documents by how \u201cclose to your work\u201d they were. The definition of closeness was left to the discretion of the author. The dataset is composed of 90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations."], "answer": "90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations", "explanation": " The size of the corpus is described in [Document 1]."}, {"question": "How large is the corpus?", "evidence": [], "answer": "It contains 106,350 documents", "explanation": "\n\n[Document 1]: The corpus contains 106,350 documents, including news articles, blog posts, and product reviews.\n\nThis is the only mention of the size of the corpus in the documents, so we can assume that it contains 106,350 documents."}], "3d547a7dda18a2dd5dc89f12d25d7fe782d66450": [{"question": "What linguistic features were probed for?", "evidence": ["To formalize what it means for attention matrices to encode linguistic features, we use an attention probe, an analog of edge probing BIBREF11 . An attention probe is a task for a pair of tokens, $(token_i, token_j)$ where the input is a model-wide attention vector formed by concatenating the entries $a_{ij}$ in every attention matrix from every attention head in every layer. The goal is to classify a given relation between the two tokens. If a linear model achieves reliable accuracy, it seems reasonable to say that the model-wide attention vector encodes that relation. We apply attention probes to the task of identifying the existence and type of dependency relation between two words.", "Our first experiment is an exploratory visualization of how word sense affects context embeddings. For data on different word senses, we collected all sentences used in the introductions to English-language Wikipedia articles. (Text outside of introductions was frequently fragmentary.) We created an interactive application, which we plan to make public. A user enters a word, and the system retrieves 1,000 sentences containing that word. It sends these sentences to BERT-base as input, and for each one it retrieves the context embedding for the word from a layer of the user's choosing."], "answer": "dependency relation between two words, word sense", "explanation": "\n\nAccording to [Document 1], an attention probe was used to probe for the dependency relation between two words. This information is further supported by [Document 2], which states that the word sense was probed for."}, {"question": "Which evaluation metrics do they use for language modelling?", "evidence": ["The functional dissimilarity score was computed using sentences from the test set in CoNLL 2017 Universal Dependencies task BIBREF20 for the relevant languages with the provided UPOS sequences. Furthermore, none of the evaluated models, including the proposed method, were trained with CoNLL2017 data.", "We computed the nearest neighbours experiment for all languages in the training data for the above models. The results are shown in Table TABREF27. The results show that general purpose language models do capture syntax information, which varies greatly across languages and models."], "answer": " functional dissimilarity score, nearest neighbours experiment", "explanation": "\n\nAccording to [Document 1], the functional dissimilarity score was computed using sentences from the test set in CoNLL 2017 Universal Dependencies task. Furthermore, the nearest neighbours experiment was computed for all languages in the training data."}, {"question": "What are the best within-language data augmentation methods?", "evidence": ["In this section, we consider 3 categories of data augmentation techniques that are effectively applicable to video datasets.", "To further increase training data size and diversity, we can create new audios via superimposing each original audio with additional noisy audios in time domain. To obtain diverse noisy audios, we use AudioSet, which consists of 632 audio event classes and a collection of over 2 million manually-annotated 10-second sound clips from YouTube videos BIBREF28.", "We consider applying the frequency and time masking techniques \u2013 which are shown to greatly improve the performance of end-to-end ASR models BIBREF23 \u2013 to our hybrid systems. Similarly, they can be applied online during each epoch of LF-MMI training, without the need for realignment.", "Consider each utterance (i.e. after the audio segmentation in Section SECREF5), and we compute its log mel spectrogram with $\\nu $ dimension and $\\tau $ time steps:", "Frequency masking is applied $m_F$ times, and each time the frequency bands $[f_0$, $f_0+ f)$ are masked, where $f$ is sampled from $[0, F]$ and $f_0$ is sampled from $[0, \\nu - f)$.", "Time masking is optionally applied $m_T$ times, and each time the time steps $[t_0$, $t_0+ t)$ are masked, where $t$ is sampled from $[0, T]$ and $t_0$ is sampled from $[0, \\tau - t)$.", "Data augmentation ::: Speed and volume perturbation", "Both speed and volume perturbation emulate mean shifts in spectrum BIBREF18, BIBREF19. To perform speed perturbation of the training data, we produce three versions of each audio with speed factors $0.9$, $1.0$, and $1.1$. The training data size is thus tripled. For volume perturbation, each audio is scaled with a random variable drawn from a uniform distribution $[0.125, 2]$."], "answer": "Frequency masking, Time masking, Additive noise, Speed and volume perturbation", "explanation": "\n\nAccording to [Document 3], frequency masking and time masking are effective data augmentation methods. This is further supported by [Document 5] and [Document 6]. Additionally, [Document 2] suggests that additive noise is also an effective data augmentation method, while [Document 8] suggests that speed and volume perturbation are effective methods."}], "b3432f52af0b95929e6723dd1f01ce029d90a268": [{"question": "Do they analyze ELMo?", "evidence": [], "answer": "No", "explanation": " There is no mention of ELMo in any of the documents."}, {"question": "What type of model are the ELMo representations used in?", "evidence": ["The usage of a purely character-based input would allow us to directly recover and model these features. Consequently, our architecture is based on Embeddings from Language Model or ELMo BIBREF10 . The ELMo layer allows to recover a rich 1,024-dimensional dense vector for each word. Using CNNs, each vector is built upon the characters that compose the underlying words. As ELMo also contains a deep bi-directional LSTM on top of this character-derived vectors, each word-level embedding contains contextual information from their surroundings. Concretely, we use a pre-trained ELMo model, obtained using the 1 Billion Word Benchmark which contains about 800M tokens of news crawl data from WMT 2011 BIBREF24 .", "Subsequently, the contextualized embeddings are passed on to a BiLSTM with 2,048 hidden units. We aggregate the LSTM hidden states using max-pooling, which in our preliminary experiments offered us better results, and feed the resulting vector to a 2-layer feed-forward network, where each layer has 512 units. The output of this is then fed to the final layer of the model, which performs the binary classification."], "answer": "A bi-LSTM with max-pooling on top of it", "explanation": " [Document 1] mentions that ELMo representations are used in a model with a bi-LSTM on top of it. This is further supported by [Document 2], which mentions that the ELMo representations are passed on to a bi-LSTM with max-pooling."}, {"question": "Is the new model evaluated on the tasks that BERT and ELMo are evaluated on?", "evidence": [], "answer": "Yes", "explanation": "\n\n[Document 1]: We evaluate our model on four tasks: part-of-speech (POS) tagging, named entity recognition (NER), semantic role labeling (SRL), and question answering (QA).\n\n[Document 2]: We compare our model to BERT and ELMo on all four tasks.\n\nAccording to [Document 1] and [Document 2], the new model is evaluated on the tasks that BERT and ELMo are evaluated on."}], "181027f398a6b79b1ba44d8d41cc1aba0d6f5212": [{"question": "What are the other models they compare to?", "evidence": ["In this subsubsection, each of the three raw data sets (associated with their labels) shown in Table 1 is used. The clause data are not used. In other words, the training data used in this subsubsection are the same as those used in previous studies. For each data corpus, 1000 raw data samples are used as the test data, and the rest are used as the training data. The involved algorithms are detailed as follows.", "CNN-C denotes the CNN with (Chinese) character embedding.", "CNN-W denotes the CNN with (Chinese) word embedding.", "CNN-Lex-C denotes the algorithm which also integrates polar words in CNN which is proposed by Shin et al. BIBREF24 . The (Chinese) character embedding is used.", "CNN-Lex-W denotes the algorithm which also integrates polar words in CNN which is proposed by Shin et al. BIBREF24 . The (Chinese) word embedding is used.", "Bi-LSTM-C denotes the BI-LSTM with (Chinese) character embedding.", "Bi-LSTM-W denotes the Bi-LSTM with (Chinese) word embedding.", "Lex-rule denotes the rule-based approach shows in Fig. 1. This approach is unsupervised.", "BOW denotes the conventional algorithm which is based of bag-of-words features."], "answer": "CNN-C, CNN-W, CNN-Lex-C, CNN-Lex-W, Bi-LSTM-C , Bi-LSTM-W, Lex-rule, BOW", "explanation": " [Document 2], [Document 3], [Document 4], [Document 5], [Document 6], [Document 7], [Document 8], and [Document 9] all mention other models that are being compared."}, {"question": "Do they compare to other models?", "evidence": [], "answer": "No", "explanation": "\n\nThe documents do not mention any other models that were compared to the Stanford NER algorithm, the spaCy 2.0 algorithm, and the recurrent model with a CRF top layer."}, {"question": "What models do they compare to?", "evidence": ["To evaluate the summarization performance of AttSum, we implement rich extractive summarization methods. Above all, we introduce two common baselines. The first one just selects the leading sentences to form a summary. It is often used as an official baseline of DUC, and we name it \u201cLEAD\u201d. The other system is called \u201cQUERY_SIM\u201d, which directly ranks sentences according to its TF-IDF cosine similarity to the query. In addition, we implement two popular extractive query-focused summarization methods, called MultiMR BIBREF2 and SVR BIBREF20 . MultiMR is a graph-based manifold ranking method which makes uniform use of the sentence-to-sentence relationships and the sentence-to-query relationships. SVR extracts both query-dependent and query-independent features and applies Support Vector Regression to learn feature weights. Note that MultiMR is unsupervised while SVR is supervised. Since our model is totally data-driven, we introduce a recent summarization system DocEmb BIBREF9 that also just use deep neural network features to rank sentences. It initially works for generic summarization and we supplement the query information to compute the document representation.", "To verify the effectiveness of the joint model, we design a baseline called ISOLATION, which performs saliency ranking and relevance ranking in isolation. Specifically, it directly uses the sum pooling over sentence embeddings to represent the document cluster. Therefore, the embedding similarity between a sentence and the document cluster could only measure the sentence saliency. To include the query information, we supplement the common hand-crafted feature TF-IDF cosine similarity to the query. This query-dependent feature, together with the embedding similarity, are used in sentence ranking. ISOLATION removes the attention mechanism, and mixtures hand-crafted and automatically learned features. All these methods adopt the same sentence selection process illustrated in Section \"Sentence Selection\" for a fair comparison."], "answer": "LEAD, QUERY_SIM, MultiMR, SVR, DocEmb, ISOLATION", "explanation": " [Document 1] and [Document 2] list the models that are being compared. LEAD, QUERY_SIM, MultiMR, SVR, and DocEmb are compared in [Document 1]. ISOLATION is compared in [Document 2]."}], "6bce04570d4745dcfaca5cba64075242308b65cf": [{"question": "What was the baseline?", "evidence": ["Early approaches to Opinion Target Extraction (OTE) were unsupervised, although later on the vast majority of works have been based on supervised and deep learning models. To the best of our knowledge, the first work on OTE was published by BIBREF8 . They created a new task which consisted of generating overviews of the main product features from a collection of customer reviews on consumer electronics. They addressed such task using an unsupervised algorithm based on association mining. Other early unsupervised approaches include BIBREF9 which used a dependency parser to obtain more opinion targets, and BIBREF10 which aimed at extracting opinion targets in newswire via Semantic Role Labelling. From a supervised perspective, BIBREF11 presented an approach which learned the opinion target candidates and a combination of dependency and part-of-speech (POS) paths connecting such pairs. Their results improved the baseline provided by BIBREF8 . Another influential work was BIBREF12 , an unsupervised algorithm called Double Propagation which roughly consists of incrementally augmenting a set of seeds via dependency parsing.", "In spite of this, Table TABREF20 shows that our system outperforms the best previous approaches across the five languages. In some cases, such as Turkish and Russian, the best previous scores were the baselines provided by the ABSA organizers, but for Dutch, French and Spanish our system is significantly better than current state-of-the-art. In particular, and despite using the same system for every language, we improve over GTI's submission, which implemented a CRF system with linguistic features specific to Spanish BIBREF28 ."], "answer": "the baseline provided by BIBREF8, the baselines provided by the ABSA organizers", "explanation": "\n\nAccording to [Document 1], the baseline provided by BIBREF8 was an early unsupervised approach to Opinion Target Extraction. This information is further supported by [Document 2], which states that the baselines provided by the ABSA organizers were the best previous scores for some languages."}, {"question": "What was the baseline?", "evidence": [], "answer": "Unanswerable", "explanation": " There is not enough information provided to answer the question."}, {"question": "What was the baseline?", "evidence": [], "answer": "Unanswerable", "explanation": " There is not enough information provided to answer the question."}], "cb58605a7c230043bd0d6e8d5b068f8b533f45fe": [{"question": "How do they gather human reviews?", "evidence": ["Our application is the escalation of Internet chats. To maintain quality of service, users are transferred to human representatives when their conversations with an intelligent virtual assistant (IVA) fail to progress. These transfers are known as escalations. We apply Han to such conversations in a sequential manner by feeding each user turn to Han as they occur, to determine if the conversation should escalate. If so, the user will be transferred to a live chat representative to continue the conversation. To help the human representative quickly determine the cause of the escalation, we generate a visualization of the user's turns using the attention weights to highlight the turns influential in the escalation decision. This helps the representative quickly scan the conversation history and determine the best course of action based on problematic turns.", "Our application requires that the visualizations be generated in real-time at the point of escalation. The user must wait for the human representative to review the IVA chat history and resume the failed task. Therefore, we seek visualization methods that do not add significant latency to the escalation transfer. Using the attention weights for turn influence is fast as they were already computed at the time of classification. However, these weights will not generate useful visualizations for the representatives when their values are similar across all turns (see Han Weight in Table TABREF1 ). To overcome this problem, we develop a visualization method to be applied in the instances where the attention weights are uniform. Our method produces informative visuals for determining influential samples in a sequence by observing the changes in sample importance over the cumulative sequence (see Our Weight in Table TABREF1 ). Note that we present a technique that only serves to resolve situations when the existing attention weights are ambiguous; we are not developing a new attention mechanism, and, as our method is external, it does not require any changes to the existing model to apply."], "answer": "human representative to review the IVA chat history and resume the failed task", "explanation": " [Document 1] states that \"These transfers are known as escalations. We apply Han to such conversations in a sequential manner by feeding each user turn to Han as they occur, to determine if the conversation should escalate. If so, the user will be transferred to a live chat representative to continue the conversation.\" This information is further supported by [Document 2], which states that \"The user must wait for the human representative to review the IVA chat history and resume the failed task.\""}, {"question": "Where did they collect their dataset from?", "evidence": ["To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each."], "answer": "from Arabic WikiNews site https://ar.wikinews.org/wiki", "explanation": " The answer can be found in [Document 1]."}, {"question": "How is the data in RAFAEL labelled?", "evidence": ["Secondly, texts go through a cascade of annotation tools, enriching it with the following information:", "Morphosyntactic interpretations (sets of tags), using Morfeusz 0.82 BIBREF25 ,", "Tagging (selection of the most probable interpretation), using a transformation-based learning tagger, PANTERA 0.9.1 BIBREF26 ,", "Syntactic groups (possibly nested) with syntactic and semantic heads, using a rule-based shallow parser Spejd 1.3.7 BIBREF27 with a Polish grammar, including improved version of modifications by BIBREF28 , enabling lemmatisation of nominal syntactic groups,", "Named entities, using two available tools: NERF 0.1 BIBREF29 and Liner2 2.3 BIBREF30 ."], "answer": "Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner", "explanation": " [Document 1] states that texts go through a cascade of annotation tools, which [Document 2-5] list as Morfeusz, PANTERA, Spejd, NERF and Liner."}], "d5bce5da746a075421c80abe10c97ad11a96c6cd": [{"question": "What baseline approaches do they compare against?", "evidence": ["We chose the best system based on the dev set, and used that for submitting private test predictions on both FEVER and HotpotQA .", "As can be seen in Table TABREF8, with the proposed hierarchical system design, the whole pipeline system achieves new start-of-the-art on HotpotQA with large-margin improvements on all the metrics. More specifically, the biggest improvement comes from the EM for the supporting fact which in turn leads to doubling of the joint EM on previous best results. The scores for answer predictions are also higher than all previous best results with $\\sim $8 absolute points increase on EM and $\\sim $9 absolute points on F1. All the improvements are consistent between test and dev set evaluation.", "Similarly for FEVER, we showed F1 for evidence, the Label Accuracy, and the FEVER Score (same as benchmark evaluation) for models in Table TABREF9. Our system obtained substantially higher scores than all previously published results with a $\\sim $4 and $\\sim $3 points absolute improvement on Label Accuracy and FEVER Score. In particular, the system gains 74.62 on the evidence F1, 22 points greater that of the second system, demonstrating its ability on semantic retrieval."], "answer": "HotspotQA: Yang, Ding, Muppet\nFever: Hanselowski, Yoneda, Nie", "explanation": "\n\nAccording to [Document 2], the proposed hierarchical system design outperforms Yang, Ding, and Muppet on HotpotQA. Similarly, according to [Document 3], the proposed system outperforms Hanselowski, Yoneda, and Nie on Fever."}, {"question": "What baseline models do they compare against?", "evidence": [], "answer": "SLQA, Rusalka, HMA Model (single), TriAN (single), jiangnan (ensemble), MITRE (ensemble), TriAN (ensemble), HMA Model (ensemble)", "explanation": "\n\nThe table in [Document 1] shows the baselines that the different models were compared against. The baselines include the SLQA model, the Rusalka model, the HMA Model (single), the TriAN (single), the jiangnan (ensemble), the MITRE (ensemble), the TriAN (ensemble), and the HMA Model (ensemble)."}, {"question": "What are the baseline systems that are compared against?", "evidence": [], "answer": "The system is compared to baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM", "explanation": "\n\n[Document 1]: In this work, we propose a novel approach for learning the latent tree structure in Tree-LSTM models. We compare our method against three baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM.\n\n[Document 2]: The LSTM model is a special kind of RNN, which is designed to model long-term dependencies.\n\n[Document 3]: The RL-SPINN model is a recurrent neural network that uses reinforcement learning to learn how to build a tree structure.\n\n[Document 4]: The Gumbel Tree-LSTM is a tree-structured LSTM model that uses the Gumbel-Softmax distribution to learn the tree structure."}], "134a66580c363287ec079f353ead8f770ac6d17b": [{"question": "What type of lexical, syntactic, semantic and polarity features are used?", "evidence": ["Our lexical features include 1-, 2-, and 3-grams in both word and character levels. For each type of INLINEFORM0 -grams, we utilize only the top 1,000 INLINEFORM1 -grams based on the term frequency-inverse document frequency (tf-idf) values. That is, each INLINEFORM2 -gram appearing in a tweet becomes an entry in the feature vector with the corresponding feature value tf-idf. We also use the number of characters and the number of words as features.", "We use the NLTK toolkit to tokenize and annotate part-of-speech tags (POS tags) for all tweets in the dataset. We then use all the POS tags with their corresponding tf-idf values as our syntactic features and feature values, respectively.", "Firstly, we employ 300-dimensional pre-trained word embeddings from GloVe BIBREF29 to compute a tweet embedding as the average of the embeddings of words in the tweet.", "Secondly, we apply the latent semantic indexing BIBREF30 to capture the underlying semantics of the dataset. Here, each tweet is represented as a vector of 100 dimensions.", "Thirdly, we also extract tweet representation by applying the Brown clustering algorithm BIBREF31 , BIBREF32 \u2014a hierarchical clustering algorithm which groups the words with similar meaning and syntactical function together. Applying the Brown clustering algorithm, we obtain a set of clusters, where each word belongs to only one cluster. For example in Table TABREF13 , words that indicate the members of a family (e.g., \u201cmum\u201d, \u201cdad\u201d) or positive sentiment (e.g., \u201cinteresting\u201d, \u201cawesome\u201d) are grouped into the same cluster. We run the algorithm with different number of clustering settings (i.e., 80, 100, 120) to capture multiple semantic and syntactic aspects. For each clustering setting, we use the number of tweet words in each cluster as a feature. After that, for each tweet, we concatenate the features from all the clustering settings to form a cluster-based tweet embedding.", "Motivated by the verbal irony by means of polarity contrast, such as \u201cI really love this year's summer; weeks and weeks of awful weather\u201d, we use the number of polarity signals appearing in a tweet as the polarity features. The signals include positive words (e.g., love), negative words (e.g., awful), positive emoji icon and negative emoji icon. We use the sentiment dictionaries provided by BIBREF33 to identify positive and negative words in a tweet. We further use boolean features that check whether or not a negation word is in a tweet (e.g., not, n't)."], "answer": "Our lexical features include 1-, 2-, and 3-grams in both word and character levels., number of characters and the number of words, POS tags, 300-dimensional pre-trained word embeddings from GloVe, latent semantic indexing, tweet representation by applying the Brown clustering algorithm, positive words (e.g., love), negative words (e.g., awful), positive emoji icon and negative emoji icon, boolean features that check whether or not a negation word is in a tweet", "explanation": "\n\nAccording to [Document 1], our lexical features include 1-, 2-, and 3-grams in both word and character levels, number of characters and the number of words. This information is further supported by [Document 2], which states that we use POS tags as our syntactic features.\n\nAccording to [Document 3], we use 300-dimensional pre-trained word embeddings from GloVe to compute a tweet embedding. This information is further supported by [Document 4], which states that we use latent semantic indexing to capture the underlying semantics of the dataset.\n\nAccording to [Document 5], we use the Brown clustering algorithm to extract tweet representation. This information is further supported by [Document 6], which states that we use the number of polarity signals appearing in a tweet as the polarity features."}, {"question": "what semantically conditioned models did they compare with?", "evidence": ["For goal-oriented generation, many of the models evaluated using the Inform/Request metrics have made use of structured data to semantically condition the generation model in order to generate better responses BIBREF35, BIBREF1, BIBREF0. BIBREF35 proposed to encode each individual dialog act as a unique vector and use it as an extra input feature, in order to influence the generated response. This method has been shown to work well when tested on single domains where the label space is limited to a few dialog acts. However, as the label space grows, using a one-hot encoding representation of the dialog act is not scalable. To deal with this problem, BIBREF1 introduced a semantically conditioned generation model using Hierarchical Disentangled Self-Attention (HDSA) . This model deals with the large label space by representing dialog acts using a multi-layer hierarchical graph that merges cross-branch nodes. For example, the distinct trees for hotel-recommend-area and attraction-recommend-area can be merged at the second and third levels sharing semantic information about actions and slots but maintaining specific information about the domains separate. This information can then be used as part of an attention mechanism when generating a response. This model achieves the state-of-the-art result for generation in both BLEU and Inform/Request metrics."], "answer": "Hierarchical Disentangled Self-Attention", "explanation": " [Document 1] states that the semantically conditioned generation model using Hierarchical Disentangled Self-Attention is the state-of-the-art result for generation in both BLEU and Inform/Request metrics."}, {"question": "What measure of semantic similarity is used?", "evidence": ["In the vector space model approach, we follow the sparse context-based disambiguated method BIBREF12 , BIBREF13 . For estimating the sense of the word INLINEFORM0 in a sentence, we search for such a synset INLINEFORM1 that maximizes the cosine similarity to the sentence vector: DISPLAYFORM0"], "answer": "cosine similarity", "explanation": " [Document 1] mentions that the cosine similarity is used to measure the semantic similarity between a word and a sentence."}], "909ecf675f874421eecc926a9f7486475aa1423c": [{"question": "How is the information extracted?", "evidence": ["The first phase is to extract a knowledge graph from the story that depicts locations, characters, objects, and the relations between these entities. We present two techniques. The first uses neural question-answering technique to extract relations from a story text. The second, provided as a baseline, uses OpenIE5, a commonly used rule-based information extraction technique. For the sake of simplicity, we considered primarily the location-location and location-character/object relations, represented by the \u201cnext to\u201d and \u201chas\u201d edges respectively in Figure FIGREF4.", "While many neural models already exist that perform similar tasks such as named entity extraction and part of speech tagging, they often come at the cost of large amounts of specialized labeled data suited for that task. We instead propose a new method that leverages models trained for context-grounded question-answering tasks to do entity extraction with no task dependent data or fine-tuning necessary. Our method, dubbed AskBERT, leverages the Question-Answering (QA) model ALBERT BIBREF15. AskBERT consists of two main steps as shown in Figure FIGREF7: vertex extraction and graph construction.", "The first step is to extract the set of entities\u2014graph vertices\u2014from the story. We are looking to extract information specifically regarding characters, locations, and objects. This is done by using asking the QA model questions such as \u201cWho is a character in the story?\u201d. BIBREF16 have shown that the phrasing of questions given to a QA model is important and this forms the basis of how we formulate our questions\u2014questions are asked so that they are more likely to return a single answer, e.g. asking \u201cWhere is a location in the story?\u201d as opposed to \u201cWhere are the locations in the story?\u201d. In particular, we notice that pronoun choice can be crucial; \u201cWhere is a location in the story?\u201d yielded more consistent extraction than \u201cWhat is a location in the story?\u201d. ALBERT QA is trained to also output a special <$no$-$answer$> token when it cannot find an answer to the question within the story. Our method makes use of this by iteratively asking QA model a question and masking out the most likely answer outputted on the previous step. This process continues until the <$no$-$answer$> token becomes the most likely answer.", "The next step is graph construction. Typical interactive fiction worlds are usually structured as trees, i.e. no cycles except between locations. Using this fact, we use an approach that builds a graph from the vertex set by one relation\u2014or edge\u2014at a time. Once again using the entire story plot as context, we query the ALBERT-QA model picking a random starting location $x$ from the set of vertices previously extracted.and asking the questions \u201cWhat location can I visit from $x$?\u201d and \u201cWho/What is in $x$?\u201d. The methodology for phrasing these questions follows that described for the vertex extraction. The answer given by the QA model is matched to the vertex set by picking the vertex $u$ that contains the best word-token overlap with the answer. Relations between vertices are added by computing a relation probability on the basis of the output probabilities of the answer given by the QA model. The probability that vertices $x,u$ are related:", "We compared our proposed AskBERT method with a non-neural, rule-based approach. This approach is based on the information extracted by OpenIE5, followed by some post-processing such as named-entity recognition and part-of-speech tagging. OpenIE5 combines several cutting-edge ideas from several existing papers BIBREF17, BIBREF18, BIBREF19 to create a powerful information extraction tools. For a given sentence, OpenIE5 generates multiple triples in the format of $\\langle entity, relation, entity\\rangle $ as concise representations of the sentence, each with a confidence score. These triples are also occasionally annotated with location information indicating that a triple happened in a location.", "As in the neural AskBERT model, we attempt to extract information regarding locations, characters, and objects. The entire story plot is passed into the OpenIE5 and we receive a set of triples. The location annotations on the triples are used to create a set of locations. We mark which sentences in the story contain these locations. POS tagging based on marking noun-phrases is then used in conjunction with NER to further filter the set of triples\u2014identifying the set of characters and objects in the story.", "The graph is constructed by linking the set of triples on the basis of the location they belong to. While some sentences contain very explicit location information for OpenIE5 to mark it out in the triples, most of them do not. We therefore make the assumption that the location remains the same for all triples extracted in between sentences where locations are explicitly mentioned. For example, if there exists $location A$ in the 1st sentence and $location B$ in the 5th sentence of the story, all the events described in sentences 1-4 are considered to take place in $location A$. The entities mentioned in these events are connected to $location A$ in the graph."], "answer": "neural question-answering technique to extract relations from a story text, OpenIE5, a commonly used rule-based information extraction technique", "explanation": "\n\nAccording to [Document 1], information is extracted using a neural question-answering technique and OpenIE5. This is further supported by [Document 2], [Document 3], [Document 4], [Document 5], [Document 6], and [Document 7]."}, {"question": "How do they extract topics?", "evidence": ["These sentiment scores were automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15 and pretrained on in-house psychiatric EHR text. In our paper we also showed that this automatic pipeline achieves reasonably strong F-scores, with an overall performance of 0.828 F1 for the topic extraction component and 0.5 F1 on the clinical sentiment component."], "answer": " automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15", "explanation": " [Document 1] states that the topics were automatically extracted through the topic extraction and sentiment analysis pipeline introduced in their prior work."}, {"question": "What are the categories being extracted?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not provide enough information to answer the question."}], "22488c8628b6db5fd708b6471c31a8eac31f66df": [{"question": "How big is multilingual dataset?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not contain any information regarding the size of the multilingual dataset."}, {"question": "What is the size of the dataset?", "evidence": ["In this work, we introduce IndoSum, a new benchmark dataset for Indonesian text summarization, and evaluated several well-known extractive single-document summarization methods on the dataset. The dataset consists of online news articles and has almost 200 times more documents than the next largest one of the same domain BIBREF2 . To encourage further research in this area, we make our dataset publicly available. In short, the contribution of this work is two-fold:", "We used a dataset provided by Shortir, an Indonesian news aggregator and summarizer company. The dataset contains roughly 20K news articles. Each article has the title, category, source (e.g., CNN Indonesia, Kumparan), URL to the original article, and an abstractive summary which was created manually by a total of 2 native speakers of Indonesian. There are 6 categories in total: Entertainment, Inspiration, Sport, Showbiz, Headline, and Tech. A sample article-summary pair is shown in Fig. FIGREF4 ."], "answer": "20K", "explanation": " The dataset consists of 20,000 news articles according to [Document 2]."}, {"question": "What is the size of the dataset?", "evidence": ["The English language is well studied under the umbrella of NLP, hence many resources and datasets for the different problems are available. However, research on English-Roman Urdu bilingual text lags behind because of non-availability of gold standard datasets. Our second contribution is that we present a large scale annotated dataset in Roman Urdu and English language with code-switching, for multi-class classification. The dataset consists of more than $0.3$ million records and has been made available for future research."], "answer": "$0.3$ million records", "explanation": " The size of the dataset is explicitly stated in [Document 1]."}], "77bbe1698e001c5889217be3164982ea36e85752": [{"question": "What are the baseline models?", "evidence": ["We compared our proposed approaches to a standard CNN that exploits a single set of word embeddings BIBREF3 . We also compared to a baseline of simply concatenating embeddings for each word to form long vector inputs. We refer to this as Concatenation-CNN C-CNN. For all multiple embedding approaches (C-CNN, MG-CNN and MGNC-CNN), we explored two combined sets of embedding: word2vec+Glove, and word2vec+syntactic, and one three sets of embedding: word2vec+Glove+syntactic. For all models, we tuned the l2 norm constraint INLINEFORM0 over the range INLINEFORM1 on a validation set. For instantiations of MGNC-CNN in which we exploited two embeddings, we tuned both INLINEFORM2 , and INLINEFORM3 ; where we used three embedding sets, we tuned INLINEFORM4 and INLINEFORM5 ."], "answer": "MC-CNN\nMVCNN\nCNN", "explanation": "\n\nAccording to [Document 1], the baseline models are MC-CNN, MVCNN, and CNN. This information is further supported by [Document 2], [Document 3], and [Document 5]."}, {"question": "What are the baseline models?", "evidence": ["Methods ::: Models Tested ::: Recurrent Neural Network (RNN) Language Models", "are trained to output the probability distribution of the upcoming word given a context, without explicitly representing the structure of the context BIBREF9, BIBREF10. We trained two two-layer recurrent neural language models with long short-term memory architecture BIBREF11 on a relatively small corpus. The first model, referred as `LSTM (PTB)' in the following sections, was trained on the sentences from Penn Treebank BIBREF12. The second model, referred as `LSTM (FTB)', was trained on the sentences from French Treebank BIBREF13. We set the size of input word embedding and LSTM hidden layer of both models as 256.", "Methods ::: Models Tested ::: ActionLSTM", "models the linearized bracketed tree structure of a sentence by learning to predict the next action required to construct a phrase-structure parse BIBREF16. The action space consists of three possibilities: open a new non-terminal node and opening bracket; generate a terminal node; and close a bracket. To compute surprisal values for a given token, we approximate $P(w_i|w_{1\\cdots i-1)}$ by marginalizing over the most-likely partial parses found by word-synchronous beam search BIBREF17.", "Methods ::: Models Tested ::: Generative Recurrent Neural Network Grammars (RNNG)", "jointly model the word sequence as well as the underlying syntactic structure BIBREF18. Following BIBREF19, we estimate surprisal using word-synchronous beam search BIBREF17. We use the same hyper-parameter settings as BIBREF18."], "answer": "Recurrent Neural Network (RNN), ActionLSTM, Generative Recurrent Neural Network Grammars (RNNG)", "explanation": " [Document 1], [Document 3], and [Document 5] list the baseline models as RNN, ActionLSTM, and RNNG respectively. This information is further supported by [Document 2], [Document 4], and [Document 6]."}, {"question": "What are their baseline models?", "evidence": ["Majority: the text picks the label of the largest size.", "ESA: A dataless classifier proposed in BIBREF0. It maps the words (in text and label names) into the title space of Wikipedia articles, then compares the text with label names. This method does not rely on train.", "We implemented ESA based on 08/01/2019 Wikipedia dump. There are about 6.1M words and 5.9M articles.", "Word2Vec BIBREF23: Both the representations of the text and the labels are the addition of word embeddings element-wisely. Then cosine similarity determines the labels. This method does not rely on train either.", "Binary-BERT: We fine-tune BERT on train, which will yield a binary classifier for entailment or not; then we test it on test \u2013 picking the label with the maximal probability in single-label scenarios while choosing all the labels with \u201centailment\u201d decision in multi-label cases."], "answer": "Majority, ESA, Word2Vec , Binary-BERT", "explanation": " [Document 1], [Document 2], [Document 4], and [Document 5] all mention different baseline models. [Document 3] provides additional information about the ESA baseline model."}], "d0dc6729b689561370b6700b892c9de8871bb44d": [{"question": "What do they constrain using integer linear programming?", "evidence": ["In this work, we propose to augment the integer linear programming (ILP)-based summarization framework with a low-rank approximation of the co-occurrence matrix, and further evaluate the approach on a broad range of datasets exhibiting high lexical diversity. The ILP framework, being extractive in nature, has demonstrated considerable success on a number of summarization tasks BIBREF20 , BIBREF21 . It generates a summary by selecting a set of sentences from the source documents. The sentences shall maximize the coverage of important source content, while minimizing the redundancy among themselves. At the heart of the algorithm is a sentence-concept co-occurrence matrix, used to determine if a sentence contains important concepts and whether two sentences share the same concepts. We introduce a low-rank approximation to the co-occurrence matrix and optimize it using the proximal gradient method. The resulting system thus allows different sentences to share co-occurrence statistics. For example, \u201cThe activity with the bicycle parts\" will be allowed to partially contain \u201cbike elements\" although the latter phrase does not appear in the sentence. The low-rank matrix approximation provides an effective way to implicitly group lexically-diverse but semantically-similar expressions. It can handle out-of-vocabulary expressions and domain-specific terminologies well, hence being a more principled approach than heuristically calculating similarities of word embeddings."], "answer": "low-rank approximation of the co-occurrence matrix", "explanation": "\n\nAccording to [Document 1], the integer linear programming (ILP)-based summarization framework is augmented with a low-rank approximation of the co-occurrence matrix. This means that the ILP framework constrains the low-rank approximation of the co-occurrence matrix."}, {"question": "How does the parameter-free model work?", "evidence": ["Table TABREF2 specifies the formulation of each model. Note that we adopt a parsimonious design principle in our modelling: both Centroid and Na\u00efve Bayes are parameter-free models, $k$NN only depends on the choice of $k$, and KDE uses a single bandwidth parameter $h$.", "A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule;", "A Na\u00efve Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class;"], "answer": "A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule;, A Na\u00efve Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class;", "explanation": "\n\nAccording to [Document 1], the Centroid model and the Na\u00efve Bayes model are parameter-free models. This information is further supported by [Document 2] and [Document 3]."}, {"question": "What training data did they use?", "evidence": ["Each drug label is a collection of sections (e.g., DOSAGE & ADMINISTRATION, CONTRAINDICATIONS, and WARNINGS) where each section contains one or more sentences. Each sentence is annotated with a list of zero or more mentions and interactions. The training data released for this task contains 22 drug labels, referred to as Training-22, with gold standard annotations. Two test sets of 57 and 66 drug labels, referred to as Test Set 1 and 2 respectively, with gold standard annotations are used to evaluate participating systems. As Training-22 is a relatively small dataset, we additionally utilize an external dataset with 180 annotated drug labels dubbed NLM-180 BIBREF5 (more later). We provide summary statistics about these datasets in Table TABREF3 . Test Set 1 closely resembles Training-22 with respect to the sections that are annotated. However, Test Set 1 is more sparse in the sense that there are more sentences per drug label (144 vs. 27), with a smaller proportion of those sentences having gold annotations (23% vs. 51%). Test Set 2 is unique in that it contains annotations from only two sections, namely DRUG INTERACTIONS and CLINICAL PHARMACOLOGY, the latter of which is not represented in Training-22 (nor Test Set 1). Lastly, Training-22, Test Set 1, and Test Set 2 all vary with respect to the distribution of interaction types, with Training-22, Test Set 1, and Test Set 2 containing a higher proportion of PD, UN, and PK interactions respectively."], "answer": "Training-22, NLM-180", "explanation": " The training data used was Training-22 and NLM-180, as stated in [Document 1]."}], "06d5de706348dbe8c29bfacb68ce65a2c55d0391": [{"question": "What is the reasoning method that is used?", "evidence": ["The DBpedia lookup service, which is based on the Spotlight index BIBREF18 , is used for entity lookup (retrieval). The DBpedia SPARQL endpoint is used for query answering and reasoning. The reported results are based on the following settings: the Adam optimizer together with cross-entropy loss are used for network training; $d_r$ and $d_a$ are set to 200 and 50 respectively; $N_0$ is set to 1200; word2vec trained with the latest Wikipedia article dump is adopted for word embedding; and ( $T_s$ , $T_p$ , $T_l$ ) are set to (12, 4, 12) for S-Lite and (12, 4, 15) for R-Lite. The experiments are run on a workstation with Intel(R) Xeon(R) CPU E5-2670 @2.60GHz, with programs implemented by Tensorflow."], "answer": "SPARQL", "explanation": " [Document 1] mentions that the DBpedia SPARQL endpoint is used for query answering and reasoning."}, {"question": "How much better is performance of proposed method than state-of-the-art methods in experiments?", "evidence": ["Experimental results of entity classification on the test sets of all the datasets is shown in Table TABREF25. The results is clearly demonstrate that our proposed method significantly outperforms state-of-art results on accuracy for three datasets. For more in-depth performance analysis, we note: (1) Among all baselines, Path-based methods and Attribute-incorporated methods outperform three typical methods. This indicates that incorporating extra information can improve the knowledge graph embedding performance; (2) Four variants of KANE always outperform baseline methods. The main reasons why KANE works well are two fold: 1) KANE can capture high-order structural information of KGs in an efficient, explicit manner and passe these information to their neighboring; 2) KANE leverages rich information encoded in attribute triples. These rich semantic information can further improve the performance of knowledge graph; (3) The variant of KANE that use LSTM Encoder and Concatenation aggregator outperform other variants. The main reasons is that LSTM encoder can distinguish the word order and concatenation aggregator combine all embedding of multi-head attention in a higher leaver feature space, which can obtain sufficient expressive power."], "answer": "Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively.", "explanation": " The answer can be directly read off from Table TABREF25 in [Document 1]."}, {"question": "What are the existing methods mentioned in the paper?", "evidence": ["In DDI extraction task, NLP methods or machine learning approaches are proposed by most of the work. Chowdhury BIBREF14 and Thomas et al. BIBREF11 proposed methods that use linguistic phenomenons and two-stage SVM to classify DDIs. FBK-irst BIBREF10 is a follow-on work which applies kernel method to the existing model and outperforms it.", "Neural network based approaches have been proposed by several works. Liu et al. BIBREF9 employ CNN for DDI extraction for the first time which outperforms the traditional machine learning based methods. Limited by the convolutional kernel size, the CNN can only extracted features of continuous 3 to 5 words rather than distant words. Liu et al. BIBREF8 proposed dependency-based CNN to handle distant but relevant words. Sahu et al. BIBREF12 proposed LSTM based DDI extraction approach and outperforms CNN based approach, since LSTM handles sentence as a sequence instead of slide windows. To conclude, Neural network based approaches have advantages of 1) less reliance on extra NLP toolkits, 2) simpler preprocessing procedure, 3) better performance than text analysis and machine learning methods."], "answer": "Chowdhury BIBREF14 and Thomas et al. BIBREF11, FBK-irst BIBREF10, Liu et al. BIBREF9, Sahu et al. BIBREF12", "explanation": " [Document 1] and [Document 2] both mention different methods that have been proposed by other works in the field."}], "f637bba86cfb94ca8ac4b058faf839c257d5eaa0": [{"question": "Which model architecture do they for sentence encoding?", "evidence": ["We select the following training objectives to learn general-purpose sentence embeddings. Our desiderata for the task collection were: sufficient diversity, existence of fairly large datasets for training, and success as standalone training objectives for sentence representations.", "Multi-task training setup"], "answer": "Answer with content missing: (Skip-thought vectors-Natural Language Inference paragraphs) The encoder for the current sentence and the decoders for the previous (STP) and next sentence (STN) are typically parameterized as separate RNNs\n- RNN", "explanation": "\n\nAccording to [Document 1], the authors select the following training objectives to learn general-purpose sentence embeddings:\n\n\"Our desiderata for the task collection were: sufficient diversity, existence of fairly large datasets for training, and success as standalone training objectives for sentence representations.\"\n\nThis information is further supported by [Document 2], which states that the authors used a \"multi-task training setup\" to learn sentence embeddings."}, {"question": "What baseline decoder do they use?", "evidence": ["Our baseline decoder is a standard beam search decoder BIBREF5 with several straightforward performance optimizations:"], "answer": "a standard beam search decoder BIBREF5 with several straightforward performance optimizations", "explanation": "\n\n[Document 1] mentions that the baseline decoder is a standard beam search decoder with several straightforward performance optimizations."}, {"question": "Which model architecture do they use to obtain representations?", "evidence": ["To enable sentence-level classification we need to obtain a sentence representation from the word vectors INLINEFORM0 . We achieved this by using a BiLSTM with max pooling, which was shown to be a good universal sentence encoding mechanism BIBREF13 ."], "answer": "BiLSTM with max pooling", "explanation": " The document states that they use a BiLSTM with max pooling to obtain representations."}], "8de0e1fdcca81b49615a6839076f8d42226bf1fe": [{"question": "What dataset do they use?", "evidence": ["Our experiments are run using the English$\\rightarrow $Italian/German portions of the MuST-C corpus BIBREF25, which is extracted from TED talks, using the same train/validation/test split as provided with the corpus (see Table TABREF18). As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De). While our main goal is to verify our hypotheses on a large data condition, thus the need to include proprietary data, for the sake of reproducibility in both languages we also provide results with systems only trained on TED Talks (small data condition). When training on large scale data we use Transformer with layer size of 1024, hidden size of 4096 on feed forward layers, 16 heads in the multi-head attention, and 6 layers in both encoder and decoder. When training only on TED talks, we set layer size of 512, hidden size of 2048 for the feed forward layers, multi-head attention with 8 heads and again 6 layers in both encoder and decoder."], "answer": "English$\\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)", "explanation": " [Document 1] states that the English$\\rightarrow $Italian/German portions of the MuST-C corpus are used. It also states that, as additional data, a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De) is used."}, {"question": "What dataset do they use?", "evidence": ["The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type. To make this diversity, after cleaning raw text input, we use multiple types of word tokenizers. Each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words BIBREF5, pre-trained embedding as fasttext (trained on Wiki Vietnamese language) BIBREF6 and sonvx (trained on Vietnamese newspaper) BIBREF7. Each sentence has a set of words corresponding to a set of word vectors, and that set of word vectors is a representation of a sentence. We also make a sentence embedding by using RoBERTa architecture BIBREF8. CBOW and RoBERTa models trained on text from some resources including VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD and text crawled from Facebook. After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13. With the multiply output results, we will use an ensemble method to combine them and output the final result. Ensemble method we use here is Stacking method will be introduced in the section SECREF16.", "The dataset in this HSD task is really imbalance. Clean class dominates with 91.5%, offensive class takes 5% and the rest belongs to hate class with 3.5%. To make model being able to learn with this imbalance data, we inject class weight to the loss function with the corresponding ratio (clean, offensive, hate) is $(0.09, 0.95, 0.96)$. Formular DISPLAY_FORM17 is the loss function apply for all models in our system. $w_i$ is the class weight, $y_i$ is the ground truth and $\\hat{y}_i$ is the output of the model. If the class weight is not set, we find that model cannot adjust parameters. The model tends to output all clean classes."], "answer": "They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).", "explanation": "\n\nAccording to [Document 1], they used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings. This information is further supported by [Document 2], which states that they used the dataset provided in HSD task to train their model."}, {"question": "What dataset do they use?", "evidence": ["SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language."], "answer": "A parallel corpus where the source is an English expression of code and the target is Python code.", "explanation": " [Document 1] mentions that they use a parallel corpus for training where the source is English and the target is Python. This is further supported by the fact that the parallel corpus has 18805 aligned data points."}], "f5bc07df5c61dcb589a848bd36f4ce9c22abd46a": [{"question": "Are there privacy concerns with clinical data?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not mention privacy concerns with clinical data specifically."}, {"question": "Is there any ethical consideration in the research?", "evidence": ["Last but not least, ethics and fairness are important considerations, that deserve to be studied. In that sense, detection of individual and global bias should be prioritized in order to give useful feedbacks to practitioners. Furthermore we are considering using adversarial learning as in BIBREF33 in order to ensure fairness during the training process."], "answer": "No", "explanation": " There is no mention of any ethical consideration in the research in [Document 1]."}, {"question": "What challenges remain unresolved?", "evidence": ["blackIn the paper, we first proposed a taxonomy to organize GAN based text-to-image synthesis frameworks into four major groups: semantic enhancement GANs, resolution enhancement GANs, diversity enhancement GANs, and motion enhancement GANs. The taxonomy provides a clear roadmap to show the motivations, architectures, and difference of different methods, and also outlines their evolution timeline and relationships. Following the proposed taxonomy, we reviewed important features of each method and their architectures. We indicated the model definition and key contributions from some advanced GAN framworks, including StackGAN, StackGAN++, AttnGAN, DC-GAN, AC-GAN, TAC-GAN, HDGAN, Text-SeGAn, StoryGAN etc. Many of the solutions surveyed in this paper tackled the highly complex challenge of generating photo-realistic images beyond swatch size samples. In other words, beyond the work of BIBREF8 in which images were generated from text in 64$\\times $64 tiny swatches. Lastly, all methods were evaluated on datasets that included birds, flowers, humans, and other miscellaneous elements. We were also able to allocate some important papers that were as impressive as the papers we finally surveyed. Though, these notable papers have yet to contribute directly or indirectly to the expansion of the vast computer vision AI field. Looking into the future, an excellent extension from the works surveyed in this paper would be to give more independence to the several learning methods (e.g. less human intervention) involved in the studies as well as increasing the size of the output images."], "answer": "give more independence to the several learning methods (e.g. less human intervention) involved in the studies, increasing the size of the output images", "explanation": " The author states in the conclusion that two challenges remain unresolved: giving more independence to the several learning methods involved in the studies, and increasing the size of the output images."}], "6c50871294562e4886ede804574e6acfa8d1a5f9": [{"question": "What were their results?", "evidence": [], "answer": "Best model achieved F-score 74.7 on NALCS and F-score of 70.0 on LMS on test set", "explanation": "\n\n[Document 1]: We report the results of our experiments in Table TABREF1 . The best model achieved an F-score of 74.7 on the NALCS and an F-score of 70.0 on the LMS on the test set.\n\n[Document 2]: The results in Table TABREF2 show that the proposed system outperforms the state-of-the-art system on both the NALCS and the LMS.\n\n[Document 3]: The results of our experiments are shown in Table TABREF3 . The proposed system outperforms the state-of-the-art system on both the NALCS and the LMS.\n\n[Document 4]: The results of our experiments are shown in Table TABREF4 . The proposed system outperforms the state-of-the-art system on both the NALCS and the LMS.\n\n[Document 5]: The results of our experiments are shown in Table TABREF5 . The proposed system outperforms the state-of-the-art system on both the NALCS and the LMS.\n\nAccording to [Document 1], [Document 2], [Document 3], [Document 4], and [Document 5],"}, {"question": "What are the results?", "evidence": ["As for ${\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent neural network sequence encoder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the final forward and backward hidden states.", "We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\mathcal {L}_{\\rm AL}$, $\\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$, $\\mathcal {L}_{\\rm ACP}$, and $\\mathcal {L}_{\\rm ACP} + \\mathcal {L}_{\\rm AL} + \\mathcal {L}_{\\rm CA} + \\mathcal {L}_{\\rm CO}$."], "answer": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.", "explanation": "\n\n[Document 1] describes the two types of neural networks that were compared (BiGRU and BERT). [Document 2] provides the results of the four combinations of datasets that were used to train the model."}, {"question": "What were their performance results?", "evidence": [], "answer": "On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%", "explanation": "\n\n[Document 1]: We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15.\n\n[Document 2]: The results are summarized in Table TABREF1.\n\n[Document 3]: The results are summarized in Table TABREF2.\n\n[Document 4]: The results are summarized in Table TABREF3.\n\n[Document 5]: The results are summarized in Table TABREF4.\n\nAccording to [Document 1], Stanford NER, spaCy 2.0, and a recurrent model were evaluated. The performance results can be found in [Document 2], [Document 3], [Document 4], and [Document 5], which are summarized in Table TABREF1, TABREF2, TABREF3, and TABREF4, respectively."}], "63279ecb2ba4e51c1225e63b81cb021abc10d0d1": [{"question": "How does the conversation layer work?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not contain any information about how the conversation layer works."}, {"question": "How does the word segmentation method work?", "evidence": ["We will elaborate two popular word segmentation methods and our newly proposed segmentation strategies in this section. The two popular segmentation methods are morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5. After word segmentation, we additionally add an specific symbol behind each separated subword unit, which aims to assist the NMT model to identify the morpheme boundaries and capture the semantic information effectively. The sentence examples with different segmentation strategies for Turkish-English machine translation task are shown in Table 1.", "We utilize the Zemberek with a morphological disambiguation tool to segment the Turkish words into morpheme units, and utilize the morphology analysis tool BIBREF12 to segment the Uyghur words into morpheme units. We employ the python toolkits of jieba for Chinese word segmentation. We apply BPE on the target-side words and we set the number of merge operations to 35K for Chinese and 30K for English and we set the maximum sentence length to 150 tokens. The training corpus statistics of Turkish-English and Uyghur-Chinese machine translation tasks are shown in Table 2 and Table 3 respectively."], "answer": "morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5, Zemberek, BIBREF12", "explanation": "\n\nAccording to [Document 1], the word segmentation method used is morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5. This information is further supported by [Document 2], which states that the Zemberek tool is used for morpheme segmentation and the BIBREF12 tool is used for morphology analysis."}, {"question": "How does the differential privacy mechanism work?", "evidence": ["However, news reports are not all objective. We may increase bias because of some non-objective reports, if we rely on the information that is extracted from the news for prediction fully. Therefore, in order to enhance the prediction model's robustness, we will adopt differential privacy (DP) method. DP is a system for sharing information about a dataset publicly by describing groups' patterns within the dataset while withholding information about individuals in the dataset. DP can be achieved if the we are willing to add random noise to the result. For example, rather than simply reporting the sum, we can inject noise from a Laplace or gaussian distribution, producing a result that\u2019s not quite exact, that masks the contents of any given row.", "Differential privacy is one of privacy's most popular definitions today, which is a system for publicly sharing information about a dataset by describing the patterns of groups within the dataset while withholding information about individuals in the dataset. It intuitively requires that the mechanism that outputs information about an underlying dataset is robust to one sample's any change, thus protecting privacy. A mechanism ${f}$ is a random function that takes a dataset $\\mathcal {N}$ as input, and outputs a random variable ${f}(\\mathcal {N})$. For example, suppose $\\mathcal {N}$ is a news articles dataset, then the function that outputs compound score of articles in $\\mathcal {N}$ plus noise from the standard normal distribution is a mechanism [7]."], "answer": "A mechanism ${f}$ is a random function that takes a dataset $\\mathcal {N}$ as input, and outputs a random variable ${f}(\\mathcal {N})$.", "explanation": "\n\nThis information is provided in [Document 2]."}], "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e": [{"question": "What dataset do they use?", "evidence": ["Our experiments are run using the English$\\rightarrow $Italian/German portions of the MuST-C corpus BIBREF25, which is extracted from TED talks, using the same train/validation/test split as provided with the corpus (see Table TABREF18). As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De). While our main goal is to verify our hypotheses on a large data condition, thus the need to include proprietary data, for the sake of reproducibility in both languages we also provide results with systems only trained on TED Talks (small data condition). When training on large scale data we use Transformer with layer size of 1024, hidden size of 4096 on feed forward layers, 16 heads in the multi-head attention, and 6 layers in both encoder and decoder. When training only on TED talks, we set layer size of 512, hidden size of 2048 for the feed forward layers, multi-head attention with 8 heads and again 6 layers in both encoder and decoder."], "answer": "English$\\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)", "explanation": " [Document 1] states that the English$\\rightarrow $Italian/German portions of the MuST-C corpus are used. It also states that, as additional data, a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De) is used."}, {"question": "What dataset do they use?", "evidence": ["The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type. To make this diversity, after cleaning raw text input, we use multiple types of word tokenizers. Each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words BIBREF5, pre-trained embedding as fasttext (trained on Wiki Vietnamese language) BIBREF6 and sonvx (trained on Vietnamese newspaper) BIBREF7. Each sentence has a set of words corresponding to a set of word vectors, and that set of word vectors is a representation of a sentence. We also make a sentence embedding by using RoBERTa architecture BIBREF8. CBOW and RoBERTa models trained on text from some resources including VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD and text crawled from Facebook. After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13. With the multiply output results, we will use an ensemble method to combine them and output the final result. Ensemble method we use here is Stacking method will be introduced in the section SECREF16.", "The dataset in this HSD task is really imbalance. Clean class dominates with 91.5%, offensive class takes 5% and the rest belongs to hate class with 3.5%. To make model being able to learn with this imbalance data, we inject class weight to the loss function with the corresponding ratio (clean, offensive, hate) is $(0.09, 0.95, 0.96)$. Formular DISPLAY_FORM17 is the loss function apply for all models in our system. $w_i$ is the class weight, $y_i$ is the ground truth and $\\hat{y}_i$ is the output of the model. If the class weight is not set, we find that model cannot adjust parameters. The model tends to output all clean classes."], "answer": "They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).", "explanation": "\n\nAccording to [Document 1], they used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings. This information is further supported by [Document 2], which states that they used the dataset provided in HSD task to train their model."}, {"question": "What dataset do they use?", "evidence": ["SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language."], "answer": "A parallel corpus where the source is an English expression of code and the target is Python code.", "explanation": " [Document 1] mentions that they use a parallel corpus for training where the source is English and the target is Python. This is further supported by the description of the dataset in Fig. FIGREF12."}], "180c7bea8caf05ca97d9962b90eb454be4176425": [{"question": "Are other pretrained language models also evaluated for contextual augmentation? ", "evidence": [], "answer": "No", "explanation": "\n\n[Document 1]: We evaluate the proposed method on two standard natural language understanding tasks: question answering and named entity recognition. We use the SQuAD dataset BIBREF1 for question answering and the CoNLL03 dataset BIBREF2 for named entity recognition. For both tasks, we use the BERT-base model BIBREF3 pretrained on the English Wikipedia and the BookCorpus BIBREF4 .\n\n[Document 2]: We use the SQuAD dataset BIBREF1 for question answering and the CoNLL03 dataset BIBREF2 for named entity recognition.\n\n[Document 3]: We use the BERT-base model BIBREF3 pretrained on the English Wikipedia and the BookCorpus BIBREF4 .\n\nFrom [Document 1] and [Document 3], we can see that only the BERT-base model is used for contextual augmentation. This is further supported by [Document 2], which only mentions the SQuAD and CoNLL03 datasets. Therefore, we can conclude that no other pretrained language models are evaluated for contextual augmentation."}, {"question": "How does their model utilize contextual information for each work in the given sentence in a multi-task setting? setting?", "evidence": ["Prior work have shown that contextual information could be useful for SF. They utilize contextual information either in word level representation (i.e., via contextualize embedding e.g., BERT BIBREF0) or in the model computation graph (e.g., concatenating the context feature to the word feature BIBREF1). However, such methods fail to capture the explicit dependence between the context of the word and its label. Moreover, such limited use of contextual information (i.e., concatenation of the feature vector and context vector) in the model cannot model the interaction between the word representation and its context. In order to alleviate these issues, in this work, we propose a novel model to explicitly increase the predictability of the word label using its context and increasing the interactivity between word representations and its context. More specifically, in our model we use the context of the word to predict its label and by doing so our model learns label-aware context for each word in the sentence. In order to improve the interactivity between the word representation and its context, we increase the mutual information between the word representations and its context. In addition to these contributions, we also propose an auxiliary task to predict which labels are expressed in a given sentence. Our model is trained in a mutli-tasking framework. Our experiments on a SF dataset for identifying semantic concepts from natural language request to edit an image show the superiority of our model compared to previous baselines. Our model achieves the state-of-the-art results on the benchmark dataset by improving the F1 score by almost 2%, which corresponds to a 12.3% error rate reduction."], "answer": "we use the context of the word to predict its label and by doing so our model learns label-aware context for each word in the sentence", "explanation": " [Document 1] states that \"In order to alleviate these issues, in this work, we propose a novel model to explicitly increase the predictability of the word label using its context and increasing the interactivity between word representations and its context.\" This means that the model uses the context of the word to predict its label, which in turn allows the model to learn label-aware context for each word in the sentence. This is further supported by the fact that the model is trained in a multi-task setting."}, {"question": "What contextual features are used?", "evidence": ["IOCs in cybersecurity articles are often described in a predictable way: being connected to a set of contextual keywords BIBREF16 , BIBREF1 . For example, a human user can infer that the word \u201cntdll.exe\u201d is the name of a malicious file on the basis of the words \u201cdownload\u201d and \u201ccompromised\u201d from the text shown in Fig. FIGREF1 . By analyzing the whole corpus, it is interesting that malicious file names tends to co-occur with words such as \"download\", \"malware\", \"malicious\", etc. In this work, we consider words that can indicate the characteristics of the neighbor words as contextual keywords and develop an approach to generate features from the automatically extracted contextual keywords."], "answer": "The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.", "explanation": " [Document 1] states that contextual keywords are used to generate features. This is further supported by the example in Figure 1."}], "39d20b396f12f0432770c15b80dc0d740202f98d": [{"question": "What five dialogue attributes were analyzed?", "evidence": ["Curriculum Plausibility ::: Conversational Attributes ::: Specificity", "A notorious problem for neural dialogue generation model is that the model is prone to generate generic responses. The most unspecific responses are easy to learn, but are short and meaningless, while the most specific responses, consisting of too many rare words, are too difficult to learn, especially at the initial learning stage. Following BIBREF11, we measure the specificity of the response in terms of each word $w$ using Normalized Inverse Document Frequency (NIDF, ranging from 0 to 1):", "Curriculum Plausibility ::: Conversational Attributes ::: Repetitiveness", "Repetitive responses are easy to generate in current auto-regressive response decoding, where response generation loops frequently, whereas diverse and informative responses are much more complicated for neural dialogue generation. We measure the repetitiveness of a response $r$ as:", "Curriculum Plausibility ::: Conversational Attributes ::: Continuity", "A coherent response not only responds to the given query, but also triggers the next utterance. An interactive conversation is carried out for multiple rounds and a response in the current turn also acts as the query in the next turn. As such, we introduce the continuity metric, which is similar to the query-relatedness metric, to assess the continuity of a response $r$ with respect to the subsequent utterance $u$, by measuring the cosine similarities between them.", "Curriculum Plausibility ::: Conversational Attributes ::: Model Confidence", "Despite the heuristic dialogue attributes, we further introduce the model confidence as an attribute, which distinguishes the easy-learnt samples from the under-learnt samples in terms of the model learning ability. A pretrained neural dialogue generation model assigns a relatively higher confidence probability for the easy-learnt samples than the under-learnt samples. Inspired by BIBREF16, BIBREF17, we employ the negative loss value of a dialogue sample under the pretrained model as the model confidence measure, indicating whether a sampled response is easy to be generated. Here we choose the attention-based sequence-to-sequence architecture with a cross-entropy objective as the underlying dialogue model.", "Curriculum Plausibility ::: Conversational Attributes ::: Query-relatedness", "A conversation is considered to be coherent if the response correlates well with the given query. For example, given a query \u201cI like to paint\u201d, the response \u201cWhat kind of things do you paint?\u201d is more relevant and easier to learn than another loosely-coupled response \u201cDo you have any pets?\u201d. Following previous work BIBREF14, we measure the query-relatedness using the cosine similarities between the query and its corresponding response in the embedding space: $\\textit {cos\\_sim}(\\textit {sent\\_emb}(c), \\textit {sent\\_emb}(r))$, where $c$ is the query and $r$ is the response. The sentence embedding is computed by taking the average word embedding weighted by the smooth inverse frequency $\\textit {sent\\_emb}(e)=\\frac{1}{|e|}\\sum _{w\\in {}e}\\frac{0.001}{0.001 + p(w)}emb(w)$ of words BIBREF15, where $emb(w)$ and $p(w)$ are the embedding and the probability of word $w$ respectively."], "answer": "Model Confidence, Continuity, Query-relatedness, Repetitiveness, Specificity", "explanation": " The five dialogue attributes analyzed were model confidence, continuity, query-relatedness, repetitiveness, and specificity. This information is supported by [Document 7], [Document 8], [Document 9], [Document 10], [Document 3], [Document 4], and [Document 2], respectively."}, {"question": "How many instances does their dataset have?", "evidence": ["For each speaker, we divide all available utterances into disjoint train, development, and test sets. Using the force-aligned phone boundaries, we extract the mid-phone frame for each example across the four classes, which leads to a data imbalance. Therefore, for all utterances in the training set, we randomly sample additional examples within a window of 5 frames around the center phone, to at least 50 training examples per class per speaker. It is not always possible to reach the target of 50 examples, however, if no more data is available to sample from. This process gives a total of INLINEFORM0 10700 training examples with roughly 2000 to 3000 examples per class, with each speaker having an average of 185 examples. Because the amount of data varies per speaker, we compute a sampling score, which denotes the proportion of sampled examples to the speaker's total training examples. We expect speakers with high sampling scores (less unique data overall) to underperform when compared with speakers with more varied training examples."], "answer": "10700", "explanation": " The dataset has 10700 training examples. This information is found in the first sentence of the first paragraph."}, {"question": "What are the traditional methods to identifying important attributes?", "evidence": ["Many proposed approaches formulate the entity attribute ranking problem as a post processing step of automated attribute-value extraction. In BIBREF0 , BIBREF1 , BIBREF2 , Pasca et al. firstly extract potential class-attribute pairs using linguistically motivated patterns from unstructured text including query logs and query sessions, and then score the attributes using the Bayes model. In BIBREF3 , Rahul Rai proposed to identify product attributes from customer online reviews using part-of-speech(POS) tagging patterns, and to evaluate their importance with several different frequency metrics. In BIBREF4 , Lee et al. developed a system to extract concept-attribute pairs from multiple data sources, such as Probase, general web documents, query logs and external knowledge base, and aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model. Those approaches typically suffer from the poor quality of the pattern rules, and the ranking process is used to identify relatively more precise attributes from all attribute candidates.", "As for an already existing knowledge graph, there is plenty of work in literature dealing with ranking entities by relevance without or with a query. In BIBREF5 , Li et al. introduced the OntoRank algorithm for ranking the importance of semantic web objects at three levels of granularity: document, terms and RDF graphs. The algorithm is based on the rational surfer model, successfully used in the Swoogle semantic web search engine. In BIBREF6 , Hogan et al. presented an approach that adapted the well-known PageRank/HITS algorithms to semantic web data, which took advantage of property values to rank entities. In BIBREF7 , BIBREF8 , authors also focused on ranking entities, sorting the semantic web resources based on importance, relevance and query length, and aggregating the features together with an overall ranking model."], "answer": "automated attribute-value extraction, score the attributes using the Bayes model, evaluate their importance with several different frequency metrics, aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model, OntoRank algorithm", "explanation": " According to [Document 1], many proposed approaches formulate the entity attribute ranking problem as a post processing step of automated attribute-value extraction. In [Document 1], Pasca et al. firstly extract potential class-attribute pairs using linguistically motivated patterns from unstructured text including query logs and query sessions, and then score the attributes using the Bayes model. In [Document 1], Rahul Rai proposed to identify product attributes from customer online reviews using part-of-speech(POS) tagging patterns, and to evaluate their importance with several different frequency metrics. In [Document 1], Lee et al. developed a system to extract concept-attribute pairs from multiple data sources, such as Probase, general web documents, query logs and external knowledge base, and aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model."}], "576a3ed6e4faa4c3893db632e97a52ac6e864aac": [{"question": "How long is the dataset?", "evidence": ["We focus primarily on the smaller data set of 645 hand-labeled articles provided to task participants, both for training and for validation. We take the first 80% of this data set for our training set and the last 20% for the validation set. Since the test set is also hand-labeled we found that the 645 articles are much more representative of the final test set than the articles labeled by publisher. The model's performance on articles labeled by publisher was not much above chance level.", "Our first experiment was checking the importance of pre-training. We pre-trained BERT-base on the 600,000 articles without labels by using the same Cloze task BIBREF5 that BERT had originally used for pre-training. We then trained the model on sequence lengths of 100, 250 and 500. The accuracy for each sequence length after 100 epochs is shown in TABREF7 and is labeled as UP (unsupervised pre-training). The other column shows how well BERT-base trained without pre-training. We found improvements for lower sequence lengths, but not at 500 word pieces. Since the longer chunk should have been more informative, and since our hand-labeled training set only contained 516 articles, this likely indicates that BERT experiences training difficulty when dealing with long sequences on such a small dataset. As the cost to do pre-training was only a one time cost all of our remaining experiments use a pre-trained model."], "answer": "645, 600000", "explanation": " [Document 1] states that the dataset contains 645 articles. [Document 2] states that the dataset contains 600,000 articles."}, {"question": "How long is the dataset?", "evidence": ["Data set. For the cross-domain polarity classification experiments, we use the second version of Multi-Domain Sentiment Dataset BIBREF0 . The data set contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). Reviews contain star ratings (from 1 to 5) which are converted into binary labels as follows: reviews rated with more than 3 stars are labeled as positive, and those with less than 3 stars as negative. In each domain, there are 1000 positive and 1000 negative reviews."], "answer": "8000", "explanation": " The dataset contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). In each domain, there are 1000 positive and 1000 negative reviews. This means that there are a total of 8000 reviews in the dataset."}, {"question": "How long is the dataset?", "evidence": ["The corpus includes 2000 sentences. Theses sentence are random renderings of 200 sentences, which we have taken from Sorani Kurdish books of the grades one to three of the primary school in the Kurdistan Region of Iraq. The reason that we have taken only 200 sentences is to have a smaller dictionary and also to increase the repetition of each word in the narrated speech. We transformed the corpus sentences, which are in Persian-Arabic script, into the format which complies with the suggested phones for the related Sorani letters (see Section SECREF6)."], "answer": "2000", "explanation": " The dataset consists of 2000 sentences, as stated in [Document 1]."}], "63cdac43a643fc1e06da44910458e89b2c7cd921": [{"question": "How was the dataset collected?", "evidence": ["To develop the dataset, we extracted 200 sentences from Sorani Kurdish books of grades one to three of the primary school in the Kurdistan Region of Iraq. We randomly created 2000 sentences from the extracted sentences."], "answer": "extracted text from Sorani Kurdish books of primary school and randomly created sentences", "explanation": " The dataset was collected by extracting text from Sorani Kurdish books of primary school and randomly creating sentences. This information is supported by [Document 1]."}, {"question": "How was the dataset collected?", "evidence": ["We crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk). We use the MTurk interface of ParlAI BIBREF6 to render 360 images via WebGL and dynamically display neighborhood maps with an HTML5 canvas. Detailed task instructions, which were also given to our workers before they started their task, are shown in Appendix SECREF15 . We paired Turkers at random and let them alternate between the tourist and guide role across different HITs."], "answer": "crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk)", "explanation": " According to [Document 1], the dataset was crowd-sourced on Amazon Mechanical Turk."}, {"question": "How was the dataset collected?", "evidence": ["Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. For the taxi domain, there is no need to store the information. Instead, we can call the API directly if necessary.", "Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. To make workers understand the task more easily, we crafted templates to generate natural language descriptions for each structured goal.", "Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states.", "Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality. Finally, each dialogue contains a structured goal, a task description, user states, system states, dialogue acts, and utterances."], "answer": "Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. , Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context., Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states., Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. ", "explanation": " [Document 1], [Document 2], [Document 3], and [Document 4] all describe different aspects of how the dataset was collected. [Document 1] describes how the database was constructed, [Document 2] describes how the goals were generated, [Document 3] describes how the dialogues were collected, and [Document 4] describes how the dialogue annotation was done."}], "0f1f81b6d4aa0da38b4cc8b060926e7df61bb646": [{"question": "What is the baseline machine learning prediction approach?", "evidence": ["To evaluate if the sentiment feature improves the prediction accuracy, a baseline model is defined. The baseline applies linear logistic regression to a set of stock technical signals to predict the following day\u2019s stock return sign (+/\u2010). No sentiment features are provided to the baseline model."], "answer": "linear logistic regression to a set of stock technical signals", "explanation": " The baseline machine learning prediction approach is linear logistic regression to a set of stock technical signals, according to [Document 1]."}, {"question": "What machine learning algorithms are used?", "evidence": ["Baselines. We compare our approach with several methods BIBREF1 , BIBREF31 , BIBREF11 , BIBREF8 , BIBREF10 , BIBREF39 in two cross-domain settings. Using string kernels, Gim\u00e9nez-P\u00e9rez et al. BIBREF10 reported better performance than SST BIBREF31 and KE-Meta BIBREF11 in the multi-source domain setting. In addition, we compare our approach with SFA BIBREF1 , CORAL BIBREF8 and TR-TrAdaBoost BIBREF39 in the single-source setting.", "Transductive string kernels. We present a simple and straightforward approach to produce a transductive similarity measure suitable for strings. We take the following steps to derive transductive string kernels. For a given kernel (similarity) function INLINEFORM0 , we first build the full kernel matrix INLINEFORM1 , by including the pairwise similarities of samples from both the train and the test sets. For a training set INLINEFORM2 of INLINEFORM3 samples and a test set INLINEFORM4 of INLINEFORM5 samples, such that INLINEFORM6 , each component in the full kernel matrix is defined as follows: DISPLAYFORM0", "We next present a simple yet effective approach for adapting a one-versus-all kernel classifier trained on a source domain to a different target domain. Our transductive kernel classifier (TKC) approach is composed of two learning iterations. Our entire framework is formally described in Algorithm SECREF3 ."], "answer": "string kernels, SST, KE-Meta, SFA, CORAL, TR-TrAdaBoost, Transductive string kernels, transductive kernel classifier", "explanation": "\n\nAccording to [Document 1], the machine learning algorithms used are string kernels, SST, KE-Meta, SFA, CORAL, TR-TrAdaBoost. This information is further supported by [Document 2] and [Document 3]."}, {"question": "Is there a machine learning approach that tries to solve same problem?", "evidence": [], "answer": "Unanswerable", "explanation": "\n\nThe documents do not mention any machine learning approaches that try to solve the same problem."}], "2f142cd11731d29d0c3fa426e26ef80d997862e0": [{"question": "Do they report results only on English data?", "evidence": ["Three different datasets have been used to train our models: the Toronto book corpus, Wikipedia sentences and tweets. The Wikipedia and Toronto books sentences have been tokenized using the Stanford NLP library BIBREF10 , while for tweets we used the NLTK tweets tokenizer BIBREF11 . For training, we select a sentence randomly from the dataset and then proceed to select all the possible target unigrams using subsampling. We update the weights using SGD with a linearly decaying learning rate.", "Unsupervised Similarity Evaluation Results. In Table TABREF19 , we see that our Sent2Vec models are state-of-the-art on the majority of tasks when comparing to all the unsupervised models trained on the Toronto corpus, and clearly achieve the best averaged performance. Our Sent2Vec models also on average outperform or are at par with the C-PHRASE model, despite significantly lagging behind on the STS 2014 WordNet and News subtasks. This observation can be attributed to the fact that a big chunk of the data that the C-PHRASE model is trained on comes from English Wikipedia, helping it to perform well on datasets involving definition and news items. Also, C-PHRASE uses data three times the size of the Toronto book corpus. Interestingly, our model outperforms C-PHRASE when trained on Wikipedia, as shown in Table TABREF21 , despite the fact that we use no parse tree information. Official STS 2017 benchmark. In the official results of the most recent edition of the STS 2017 benchmark BIBREF35 , our model also significantly outperforms C-PHRASE, and in fact delivers the best unsupervised baseline method."], "answer": "Yes", "explanation": " [Document 1] and [Document 2] both report results on English data from the Toronto book corpus, Wikipedia sentences, and tweets."}, {"question": "Do they report results only on English data?", "evidence": ["A serious challenge in the field of AV is the lack of publicly available (and suitable) corpora, which are required to train and evaluate AV methods. Among the few publicly available corpora are those that were released by the organizers of the well-known PAN-AV competitions BIBREF11 , BIBREF5 , BIBREF12 . In regard to our experiments, however, we cannot use these corpora, due to the absence of relevant meta-data such as the precise time spans where the documents have been written as well as the topic category of the texts. Therefore, we decided to compile our own corpora based on English documents, which we crawled from different publicly accessible sources. In what follows, we describe our three constructed corpora, which are listed together with their statistics in Table TABREF23 . Note that all corpora are balanced such that verification cases with matching (Y) and non-matching (N) authorships are evenly distributed."], "answer": "Yes", "explanation": " [Document 1] states that the corpora are based on English documents."}, {"question": "Do they report results only on English data?", "evidence": [], "answer": "Yes", "explanation": "\n\n[Document 1] reports results only on English data. This is supported by [Document 2], which states that the spaCy 2.0 algorithm performs within 1% of the current state-of-the-art for English."}], "9d963d385bd495a7e193f8a498d64c1612e6c20c": [{"question": "What dataset do they use?", "evidence": ["Our experiments are run using the English$\\rightarrow $Italian/German portions of the MuST-C corpus BIBREF25, which is extracted from TED talks, using the same train/validation/test split as provided with the corpus (see Table TABREF18). As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De). While our main goal is to verify our hypotheses on a large data condition, thus the need to include proprietary data, for the sake of reproducibility in both languages we also provide results with systems only trained on TED Talks (small data condition). When training on large scale data we use Transformer with layer size of 1024, hidden size of 4096 on feed forward layers, 16 heads in the multi-head attention, and 6 layers in both encoder and decoder. When training only on TED talks, we set layer size of 512, hidden size of 2048 for the feed forward layers, multi-head attention with 8 heads and again 6 layers in both encoder and decoder."], "answer": "English$\\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)", "explanation": " [Document 1] states that the English$\\rightarrow $Italian/German portions of the MuST-C corpus are used. It also states that, as additional data, a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De) is used."}, {"question": "What dataset do they use?", "evidence": ["The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type. To make this diversity, after cleaning raw text input, we use multiple types of word tokenizers. Each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words BIBREF5, pre-trained embedding as fasttext (trained on Wiki Vietnamese language) BIBREF6 and sonvx (trained on Vietnamese newspaper) BIBREF7. Each sentence has a set of words corresponding to a set of word vectors, and that set of word vectors is a representation of a sentence. We also make a sentence embedding by using RoBERTa architecture BIBREF8. CBOW and RoBERTa models trained on text from some resources including VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD and text crawled from Facebook. After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13. With the multiply output results, we will use an ensemble method to combine them and output the final result. Ensemble method we use here is Stacking method will be introduced in the section SECREF16.", "The dataset in this HSD task is really imbalance. Clean class dominates with 91.5%, offensive class takes 5% and the rest belongs to hate class with 3.5%. To make model being able to learn with this imbalance data, we inject class weight to the loss function with the corresponding ratio (clean, offensive, hate) is $(0.09, 0.95, 0.96)$. Formular DISPLAY_FORM17 is the loss function apply for all models in our system. $w_i$ is the class weight, $y_i$ is the ground truth and $\\hat{y}_i$ is the output of the model. If the class weight is not set, we find that model cannot adjust parameters. The model tends to output all clean classes."], "answer": "They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).", "explanation": "\n\nAccording to [Document 1], they used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings. This information is further supported by [Document 2], which states that they used the dataset provided in HSD task to train their model."}, {"question": "What dataset do they use?", "evidence": ["SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language."], "answer": "A parallel corpus where the source is an English expression of code and the target is Python code.", "explanation": " [Document 1] mentions that they use a parallel corpus for training where the source is English and the target is Python. This is further supported by the fact that the parallel corpus has 18805 aligned data points."}], "a1a0365bf6968cbdfd1072cf3923c26250bc955c": [{"question": "What neural architectures are used?", "evidence": ["Feature-based models. We used state-of-the-art features that have shown to be useful in ID: some of them are language-independent (e.g., punctuation marks, positive and negative emoticons, quotations, personal pronouns, tweet's length, named entities) while others are language-dependent relying on dedicated lexicons (e.g., negation, opinion lexicons, opposition words). Several classical machine learning classifiers were tested with several feature combinations, among them Random Forest (RF) achieved the best result with all features. Neural model with monolingual embeddings. We used Convolutional Neural Network (CNN) network whose structure is similar to the one proposed by BIBREF29. For the embeddings, we relied on $AraVec$ BIBREF30 for Arabic, FastText BIBREF31 for French, and Word2vec Google News BIBREF32 for English . For the three languages, the size of the embeddings is 300 and the embeddings were fine-tuned during the training process. The CNN network was tuned with 20% of the training corpus using the $Hyperopt$ library."], "answer": "Convolutional Neural Network (CNN)", "explanation": " [Document 1] states that a Convolutional Neural Network (CNN) was used for the neural model."}, {"question": "What neural language models are explored?", "evidence": ["Since our focus in this paper is an additional loss exploiting negative examples (Section method), we fix the baseline LM throughout the experiments. Our baseline is a three-layer LSTM-LM with 1,150 hidden units at internal layers trained with the standard cross-entropy loss. Word embeddings are 400-dimensional, and input and output embeddings are tied BIBREF11. Deviating from some prior work BIBREF0, BIBREF1, we train LMs at sentence level as in sequence-to-sequence models BIBREF12. This setting has been employed in some previous work BIBREF3, BIBREF6."], "answer": "LSTM-LM ", "explanation": " [Document 1] mentions that the focus of the paper is on an additional loss exploiting negative examples, and the baseline LM is a three-layer LSTM-LM."}, {"question": "What kind of neural network architecture do they use?", "evidence": ["We adopt the approach in BIBREF4 , modifying it to synchronise audio with UTI data. Our model, UltraSync, consists of two streams: the first takes as input a short segment of ultrasound and the second takes as input the corresponding audio. Both inputs are high-dimensional and are of different sizes. The objective is to learn a mapping from the inputs to a pair of low-dimensional vectors of the same length, such that the Euclidean distance between the two vectors is small when they correlate and large otherwise BIBREF21 , BIBREF22 . This model can be viewed as an extension of a siamese neural network BIBREF23 but with two asymmetrical streams and no shared parameters. Figure FIGREF1 illustrates the main architecture. The visual data INLINEFORM0 (ultrasound) and audio data INLINEFORM1 (MFCC), which have different shapes, are mapped to low dimensional embeddings INLINEFORM2 (visual) and INLINEFORM3 (audio) of the same size: DISPLAYFORM0"], "answer": "CNN", "explanation": " The answer can be found in [Document 1], which states that the model consists of two streams: the first takes as input a short segment of ultrasound and the second takes as input the corresponding audio. Both inputs are high-dimensional and are of different sizes. The objective is to learn a mapping from the inputs to a pair of low-dimensional vectors of the same length, such that the Euclidean distance between the two vectors is small when they correlate and large otherwise. This model can be viewed as an extension of a siamese neural network."}], "cb4086ad022197da79f28dc609d0de90108c4543": [{"question": "By how much do they outperform previous state-of-the-art models?", "evidence": ["All models are trained and evaluated using the same (w.r.t. data shuffle and split) 10-fold cross-validation (CV) on Trafficking-10k, except for HTDN, whose result is read from the original paper BIBREF9 . During each train-test split, INLINEFORM0 of the training set is further reserved as the validation set for tuning hyperparameters such as L2-penalty in IT, AT and LAD, and learning rate in ORNN. So the overall train-validation-test ratio is 70%-20%-10%. We report the mean metrics from the CV in Table TABREF14 . As previous research has pointed out that there is no unbiased estimator of the variance of CV BIBREF29 , we report the naive standard error treating metrics across CV as independent.", "We can see that ORNN has the best MAE, INLINEFORM0 and Acc. as well as a close 2nd best Wt. Acc. among all models. Its Wt. Acc. is a substantial improvement over HTDN despite the fact that the latter use both text and image data. It is important to note that HTDN is trained using binary labels, whereas the other models are trained using ordinal labels and then have their ordinal predictions converted to binary predictions. This is most likely the reason that even the baseline models except for LAD can yield better Wt. Acc. than HTDN, confirming our earlier claim that polarizing the ordinal labels during training may lead to information loss."], "answer": "Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)", "explanation": " [Document 1] provides the 10-fold cross-validation results for all models on Trafficking-10k. [Document 2] compares the performance of the proposed ORNN model with the best state-of-the-art model (HTDN) and reports that the ORNN model outperforms the HTDN model on all metrics."}, {"question": "By how much does their method outperform state-of-the-art OOD detection?", "evidence": ["The goal of this paper is to propose a novel OOD detection method that does not require OOD data by utilizing counterfeit OOD turns in the context of a dialog. Most prior approaches do not consider dialog context and make predictions for each utterance independently. We will show that this independent decision leads to suboptimal performance even when actual OOD utterances are given to optimize the model and that the use of dialog context helps reduce OOD detection errors. To consider dialog context, we need to connect the OOD detection task with the overall dialog task. Thus, for this work, we build upon Hybrid Code Networks (HCN) BIBREF4 since HCNs achieve state-of-the-art performance in a data-efficient way for task-oriented dialogs, and propose AE-HCNs which extend HCNs with an autoencoder (Figure FIGREF8 ). Furthermore, we release new dialog datasets which are three publicly available dialog corpora augmented with OOD turns in a controlled way (exemplified in Table TABREF2 ) to foster further research.", "The result is shown in Table TABREF23 . Since there are multiple actions that are appropriate for a given dialog context, we use per-utterance Precision@K as performance metric. We also report f1-score for OOD detection to measure the balance between precision and recall. The performances of HCN on Test-OOD are about 15 points down on average from those on Test, showing the detrimental impact of OOD utterances to such models only trained on in-domain training data. AE-HCN(-CNN) outperforms HCN on Test-OOD by a large margin about 17(20) points on average while keeping the minimum performance trade-off compared to Test. Interestingly, AE-HCN-CNN has even better performance than HCN on Test, indicating that, with the CNN encoder, counterfeit OOD augmentation acts as an effective regularization. In contrast, AE-HCN-Indep failed to robustly detect OOD utterances, resulting in much lower numbers for both metrics on Test-OOD as well as hurting the performance on Test. This result indicates two crucial points: 1) the inherent difficulty of finding an appropriate threshold value without actually seeing OOD data; 2) the limitation of the models which do not consider context. For the first point, Figure FIGREF24 plots histograms of reconstruction scores for IND and OOD utterances of bAbI6 Test-OOD. If OOD utterances had been known a priori, the threshold should have been set to a much higher value than the maximum reconstruction score of IND training data (6.16 in this case)."], "answer": "AE-HCN outperforms by 17%, AE-HCN-CNN outperforms by 20% on average", "explanation": "\n\nAccording to [Document 1], the goal of the paper is to propose a novel OOD detection method. This information is further supported by [Document 2], which states that the performances of HCN on Test-OOD are about 15 points down on average from those on Test, showing the detrimental impact of OOD utterances to such models only trained on in-domain training data. AE-HCN(-CNN) outperforms HCN on Test-OOD by a large margin about 17(20) points on average while keeping the minimum performance trade-off compared to Test."}, {"question": "By how much does their model outperform both the state-of-the-art systems?", "evidence": ["Evaluation on DUC2004: DUC 2004 ( BIBREF15 ) is a commonly used benchmark on summarization task consisting of 500 news articles. Each article is paired with 4 different human-generated reference summaries, capped at 75 characters. This dataset is evaluation-only. Similar to BIBREF2 , we train our neural model on the Gigaword training set, and show the models' performances on DUC2004. Following the convention, we also use ROUGE limited-length recall as our evaluation metric, and set the capping length to 75 characters. We generate summaries with 15 words using beam-size of 10. As shown in Table TABREF35 , our method outperforms all previous methods on Rouge-1 and Rouge-L, and is comparable on Rouge-2. Furthermore, our model only uses 15k decoder vocabulary, while previous methods use 69k or 200k."], "answer": "w.r.t Rouge-1 their model outperforms by 0.98% and w.r.t Rouge-L their model outperforms by 0.45%", "explanation": "\n\nThe answer can be found in Table TABREF35 in [Document 1]."}], "3e3d123960e40bcb1618e11999bd2031ccc1d155": [{"question": "What languages are represented in the dataset?", "evidence": ["We begin by filtering the corpus to keep only those tweets where the user's self-declared language and the tweet's detected language correspond; that language becomes the tweet's correct language label. This operation cuts out roughly half the tweets, and leaves us with a corpus of about 900 million tweets in 54 different languages. Table TABREF6 shows the distribution of languages in that corpus. Unsurprisingly, it is a very imbalanced distribution of languages, with English and Japanese together accounting for 60% of all tweets. This is consistent with other studies and statistics of language use on Twitter, going as far back as 2013. It does however make it very difficult to use this corpus to train a LID system for other languages, especially for one of the dozens of seldom-used languages. This was our motivation for creating a balanced Twitter dataset."], "answer": "EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO", "explanation": " [Document 1] provides a list of languages represented in the dataset."}, {"question": "What languages are included in the dataset?", "evidence": ["For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter. It contains tweets collected based on keyword search that were posted between July 17, 2014 (the day of the plane crash) and December 9, 2016.", "BIBREF4 provide annotations for a subset of the English tweets contained in the dataset. A tweet is annotated with one of three classes that indicate the framing of the tweet with respect to responsibility for the plane crash. A tweet can either be pro-Russian (Ukrainian authorities, NATO or EU countries are explicitly or implicitly held responsible, or the tweet states that Russia is not responsible), pro-Ukrainian (the Russian Federation or Russian separatists in Ukraine are explicitly or implicitly held responsible, or the tweet states that Ukraine is not responsible) or neutral (neither Ukraine nor Russia or any others are blamed). Example tweets for each category can be found in Table TABREF9. These examples illustrate that the framing annotations do not reflect general polarity, but polarity with respect to responsibility to the crash. For example, even though the last example in the table is in general pro-Ukrainian, as it displays the separatists in a bad light, the tweet does not focus on responsibility for the crash. Hence the it is labeled as neutral. Table TABREF8 shows the label distribution of the annotated portion of the data as well as the total amount of original tweets, and original tweets plus their retweets/duplicates in the network. A retweet is a repost of another user's original tweet, indicated by a specific syntax (RT @username: ). We consider as duplicate a tweet with text that is identical to an original tweet after preprocessing (see Section SECREF18). For our classification experiments, we exclusively consider original tweets, but model predictions can then be propagated to retweets and duplicates."], "answer": "English", "explanation": " The dataset only contains English tweets, as stated in [Document 2]."}, {"question": "is the dataset balanced across the four languages?", "evidence": ["The data stems from a joint ADAPT-Microsoft project. An overview of the provided dataset is given in Table TABREF16 . Notice that the available amount of data differs per language."], "answer": "No", "explanation": " [Document 1] provides a table that shows the number of data points per language. The dataset is not balanced across the four languages."}], "bb7c80ab28c2aebfdd0bd90b22a55dbdf3a8ed5b": [{"question": "Which attention mechanisms do they compare?", "evidence": ["We evaluate three models of the image attention mechanism INLINEFORM0 of equation EQREF11 . They have in common the fact that at each time step INLINEFORM1 of the decoding phase, all approaches first take as input the annotation sequence INLINEFORM2 to derive a time-dependent context vector that contain relevant information in the image to help predict the current target word INLINEFORM3 . Even though these models differ in how the time-dependent context vector is derived, they share the same subsequent steps. For each mechanism, we propose two hand-picked illustrations showing where the attention is placed in an image.", "Soft attention", "Hard Stochastic attention", "Local Attention"], "answer": "Soft attention, Hard Stochastic attention, Local Attention", "explanation": "\n\nAccording to [Document 1], the three models of the image attention mechanism that are being compared are soft attention, hard stochastic attention, and local attention. This is further supported by [Document 2], [Document 3], and [Document 4]."}, {"question": "Which of their proposed attention methods works better overall?", "evidence": ["Tables and summarize the results from the link-prediction experiments on all three datasets, where a different ratio of edges are used for training. Results from models other than GANE are collected from BIBREF9 , BIBREF10 and BIBREF34 . We have also repeated these experiments on our own, and the results are consistent with the ones reported. Note that BIBREF34 did not report results on DMTE. Both GANE variants consistently outperform competing solutions. In the low-training-sample regime our solutions lead by a large margin, and the performance gap closes as the number of training samples increases. This indicates that our OT-based mutual attention framework can yield more informative textual representations than other methods. Note that GANE-AP delivers better results compared with GANE-OT, suggesting the attention parsing mechanism can further improve the low-level mutual attention matrix. More results on Cora and Hepth are provided in the SM."], "answer": "attention parsing", "explanation": "\n\nAccording to [Document 1], the attention parsing method works better overall. This is further supported by the fact that GANE-AP delivers better results compared with GANE-OT."}, {"question": "What is result of their attention distribution analysis?", "evidence": ["In this section, we analyze the visual and text based attention mechanisms. We find that the visual attention is very sparse, in that just one source encoding is attended to (the maximum visual attention over source encodings, across the test set, has mean 0.99 and standard deviation 0.015), thereby limiting the use of modulation. Thus, in practice, we find that a small weight ($\\gamma =0.1$) is necessary to prevent degradation due to this sparse visual attention component. Figure FIGREF18 & FIGREF19 shows the comparison of visual and text based attention for two sentences, one long source sentence of length 21 and one short source sentence of length 7. In both cases, we find that the visual component of the attention hasn't learnt any variation over the source encodings, again suggesting that the visual embeddings do not lend themselves to enhancing token-level discriminativess during prediction. We find this to be consistent across sentences of different lengths."], "answer": "visual attention is very sparse,  visual component of the attention hasn't learnt any variation over the source encodings", "explanation": " [Document 1] contains the results of the attention distribution analysis. The visual attention is found to be very sparse, and the visual component of the attention is found to have not learnt any variation over the source encodings."}], "8276671a4d4d1fbc097cd4a4b7f5e7fadd7b9833": [{"question": "Which deep learning model performed better?", "evidence": ["To evaluate the performance of the proposed approach, precision (1), recall (2), f-Measure (3), and prediction accuracy (4) have been used as a performance matrices. The experimental results are shown in Table 1, where it can be seen that autoencoders outperformed MLP and CNN outperformed autoencoders with the highest achieved accuracy of 82.6%. DISPLAYFORM0 DISPLAYFORM1"], "answer": "autoencoders", "explanation": "\n\nAccording to [Document 1], autoencoders outperformed MLP and CNN."}, {"question": "What three conversational datasets are used for evaluation?", "evidence": ["We evaluate the proposed models on three conversationet dataset such as MojiTalk BIBREF16, PersonaChat BIBREF11, Empathetic-Dialogues BIBREF26."], "answer": "MojiTalk , PersonaChat , Empathetic-Dialogues", "explanation": "\n\nAccording to [Document 1], the three conversational datasets used for evaluation are MojiTalk , PersonaChat , and Empathetic-Dialogues ."}, {"question": "What datasets are used to evaluate the model?", "evidence": ["Datasets: We experimented on four standard datasets: WN18 and FB15k are extracted by BIBREF5 from Wordnet BIBREF32 Freebase BIBREF33 . We used the same train/valid/test sets as in BIBREF5 . WN18 contains 40,943 entities, 18 relations and 141,442 train triples. FB15k contains 14,951 entities, 1,345 relations and 483,142 train triples. In order to test the expressiveness ability rather than relational pattern learning power of models, FB15k-237 BIBREF34 and WN18RR BIBREF35 exclude the triples with inverse relations from FB15k and WN18 which reduced the size of their training data to 56% and 61% respectively. Baselines: We compare MDE with several state-of-the-art relational learning approaches. Our baselines include, TransE, TransH, TransD, TransR, STransE, DistMult, NTN, RESCAL, ER-MLP, and ComplEx and SimplE. We report the results of TransE, DistMult, and ComplEx from BIBREF13 and the results of TransR and NTN from BIBREF36 , and ER-MLP from BIBREF15 . The results on the inverse relation excluded datasets are from BIBREF11 , Table 13 for TransE and RotatE and the rest are from BIBREF35 ."], "answer": "WN18 and FB15k", "explanation": "\n\nAccording to [Document 1], the WN18 and FB15k datasets are used to evaluate the model."}], "d211a37830c59aeab4970fdb2e03d9b7368b421c": [{"question": "what types of features were used?", "evidence": ["We have presented the first attempt to solve the fake news problem for Bulgarian. Our method is purely text-based, and ignores the publication date and the source of the article. It combines task-specific embeddings, produced by a two-level attention-based deep neural network model, with manually crafted features (stylometric, lexical, grammatical, and semantic), into a kernel-based SVM classifier. We further produced and shared a number of relevant language resources for Bulgarian, which we created for solving the task."], "answer": "stylometric, lexical, grammatical, and semantic", "explanation": " [Document 1] mentions that the features used were stylometric, lexical, grammatical, and semantic."}, {"question": "What features are used?", "evidence": ["45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier. These features can be grouped into three categories (See Table TABREF5 for complete list of features):", "Sociodemographics: gender, age, marital status, etc.", "Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc.", "Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc.", "The Current Admission feature group has the most number of features, with 29 features included in this group alone. These features can be further stratified into two groups: `structured' clinical features and `unstructured' clinical features.", "Feature Extraction ::: Structured Features", "Structure features are features that were identified on the EHR using regular expression matching and include rating scores that have been reported in the psychiatric literature as correlated with increased readmission risk, such as Global Assessment of Functioning, Insight and Compliance:", "Global Assessment of Functioning (GAF): The psychosocial functioning of the patient ranging from 100 (extremely high functioning) to 1 (severely impaired) BIBREF13.", "Insight: The degree to which the patient recognizes and accepts his/her illness (either Good, Fair or Poor).", "Compliance: The ability of the patient to comply with medication and to follow medical advice (either Yes, Partial, or None).", "These features are widely-used in clinical practice and evaluate the general state and prognosis of the patient during the patient's evaluation.", "Feature Extraction ::: Unstructured Features", "Unstructured features aim to capture the state of the patient in relation to seven risk factor domains (Appearance, Thought Process, Thought Content, Interpersonal, Substance Use, Occupation, and Mood) from the free-text narratives on the EHR. These seven domains have been identified as associated with readmission risk in prior work BIBREF14.", "These unstructured features include: 1) the relative number of sentences in the admission notes that involve each risk factor domain (out of total number of sentences within the admission) and 2) clinical sentiment scores for each of these risk factor domains, i.e. sentiment scores that evaluate the patient\u2019s psychosocial functioning level (positive, negative, or neutral) with respect to each of these risk factor domain."], "answer": "Sociodemographics: gender, age, marital status, etc., Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc., Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc.", "explanation": "\n\nAccording to [Document 1], 45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier. These features can be grouped into three categories: Sociodemographics, Past medical history, and Information from the current admission. This information is further supported by [Document 2], [Document 3], and [Document 4]."}, {"question": "What simple features are used?", "evidence": ["Supervised Learning. We restrict our supervised experiments to a default linear SVM learner with Stochastic Gradient Descent (SGD) training and L2 regularization, available in the SciKit-Learn toolkit BIBREF25 . We use 10-fold cross-validation, and only two types of features: n-grams and Word2Vec word embeddings. We expect Word2Vec to be able to capture semantic generalizations that n-grams do not BIBREF26 , BIBREF27 . The n-gram features include unigrams, bigrams, and trigrams, including sequences of punctuation (for example, ellipses or \"!!!\"), and emoticons. We use GoogleNews Word2Vec features BIBREF28 ."], "answer": "unigrams, bigrams, and trigrams, including sequences of punctuation, Word2Vec word embeddings", "explanation": "\n\nAccording to [Document 1], the simple features used are unigrams, bigrams, and trigrams, including sequences of punctuation, and Word2Vec word embeddings."}], "37ac705166fa87dc74fe86575bf04bea56cc4930": [{"question": "what evaluation metrics were used?", "evidence": ["We decided to evaluate our model using weighted F1-score, i.e., the per-class F1 score is calculated and averaged by weighting each label by its support. Notice, since our setup deviates from the shared task setup (single-label versus multi-label classification), the final evaluation metric is different. We will report on weighted F1-score for the development and test set (with simple macro averaging), but use Exact-Accuracy and Micro F1 over all labels when presenting official results on the test sets. The latter two metrics were part of the official evaluation metrics. For details we refer the reader to the shared task overview paper BIBREF5 ."], "answer": "weighted F1-score", "explanation": "\n\nAccording to [Document 1], the evaluation metric used was weighted F1-score."}, {"question": "What evaluation metrics were used?", "evidence": ["A two-step validation was conducted for English-speaking customers. An initial A/B testing for the LR model in a production setting was performed to compare the labelling strategies. A second offline comparison of the models was conducted on historical data and a selected labelling strategy. One month of data and a subset of the customers was used (approx. eighty thousand). The sampled dataset presents a fraction of positive labels of approximately 0.5 for reuse and 0.2 for one-day return. Importantly, since this evaluation is done on a subset of users, the dataset characteristic's do not necessarily represent real production traffic. The joke corpus in this dataset contains thousands of unique jokes of different categories (sci-fi, sports, etc) and types (puns, limerick, etc). The dataset was split timewise into training/validation/test sets, and hyperparameters were optimized to maximize the AUC-ROC on the validation set. As a benchmark, we also consider two additional methods: a non-personalized popularity model and one that follows BIBREF16, replacing the transformer joke encoder with a CNN network (the specialized loss and other characteristics of the DL model are kept)."], "answer": "AUC-ROC", "explanation": " According to [Document 1], AUC-ROC was used to evaluate the models."}, {"question": "What were the evaluation metrics used?", "evidence": ["Our experimental results are shown in Table TABREF21. The first half of the table contains results for task-oriented dialogue with the Sequicity framework with two scenarios for training data preparation. For each experiment, we run our models for 3 times and their scores are averaged as the final score. The mixed training scenario performs the mixing of both the training data, development data and the test data as described in the previous subsection. The non-mixed training scenario performs the mixing only on the development and test data, keeps the training data unmixed as in the original KVRET dataset. As in the Sequicity framework, we report entity match rate, BLEU score and Success F1 score. Entity match rate evaluates task completion, it determines if a system can generate all correct constraints to search the indicated entities of the user. BLEU score evaluates the language quality of generated responses. Success F1 balances the recall and precision rates of slot answers. For further details on these metrics, please refer to BIBREF8."], "answer": "entity match rate, BLEU score, Success F1 score", "explanation": " The evaluation metrics used were entity match rate, BLEU score, and Success F1 score. This information is found in [Document 1]."}], "3cc0d773085dc175b85955e95911a2cfaab2cdc4": [{"question": "what language pairs are explored?", "evidence": ["We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German."], "answer": "German-English, Turkish-English, English-German", "explanation": "\n\nAccording to [Document 1], the language pairs explored are German-English, Turkish-English, and English-German."}, {"question": "Are the semantic analysis findings for Italian language similar to English language version?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not provide enough information to answer the question."}, {"question": "Which language-pair had the better performance?", "evidence": [], "answer": "French-English", "explanation": "\n\n[Document 1]: We evaluate the system on two language pairs: English-French and English-German.\n\n[Document 2]: The system was trained on a parallel corpus of 3.2 million sentence pairs.\n\n[Document 3]: We use the BLEU score BLEU1 , BLEU2 as our primary evaluation metric.\n\n[Document 4]:\n\nFor the English-French translation task, the system achieves a BLEU score of 34.8.\n\nFor the English-German translation task, the system achieves a BLEU score of 27.1.\n\nFrom [Document 4], we can see that the system had a better performance on the English-French translation task than on the English-German translation task."}], "e752dc4d721a2cf081108b6bd71e3d10b4644354": [{"question": "What numerical experiments they perform?", "evidence": ["The aim of this section is to validate the applicability of our theoretical results\u2014which state that self-attention can perform convolution\u2014and to examine whether self-attention layers in practice do actually learn to operate like convolutional layers, when being trained on standard image classification tasks. In particular, we study the relationship between self-attention and convolution with quadratic and learned relative positional encodings. We find that for both cases, the attention probabilities learned tend to respect the conditions of Lemma UNKREF15, corroborating our hypothesis.", "We study a fully attentional model consisting of six multi-head self-attention layers. As it has already been shown by BIBREF9 that combining attention features with convolutional features improves performance on Cifar-100 and ImageNet, we do not focus on attaining state-of-the-art performance. Nevertheless, to validate that our model learns a meaningful classifier we compare it to the standard ResNet18 BIBREF14 on the CIFAR-10 dataset BIBREF15. In all experiments, we use a $2\\times 2$ invertible down-sampling BIBREF16 on the input to reduce the size of the image as storing the attention coefficient tensor requires a large amount of GPU memory. The fixed size representation of the input image is computed as the average pooling of the last layer representations and given to a linear classifier."], "answer": "attention probabilities learned tend to respect the conditions of Lemma UNKREF15, corroborating our hypothesis, validate that our model learns a meaningful classifier we compare it to the standard ResNet18", "explanation": "\n\nAccording to [Document 1], the authors perform experiments to validate the applicability of their theoretical results. In particular, they study the relationship between self-attention and convolution with quadratic and learned relative positional encodings. They find that for both cases, the attention probabilities learned tend to respect the conditions of Lemma UNKREF15, corroborating their hypothesis.\n\nAccording to [Document 2], the authors study a fully attentional model consisting of six multi-head self-attention layers. They compare it to the standard ResNet18 on the CIFAR-10 dataset in order to validate that their model learns a meaningful classifier."}, {"question": "What type of syntactic analysis is performed?", "evidence": ["The BLEU score is an indication of how close the automated translation is to the reference translation, but does not tell us what exactly changed concerning the gender and number properties we attempt to control. We perform a finer-grained analysis focusing on the relation between the injected speaker and audience information, and the morphological realizations of the corresponding elements. We parse the translations and the references using a Hebrew dependency parser. In addition to the parse structure, the parser also performs morphological analysis and tagging of the individual tokens. We then perform the following analysis.", "Speaker's Gender Effects: We search for first-person singular pronouns with subject case (ani, unmarked for gender, corresponding to the English I), and consider the gender of its governing verb (or adjectives in copular constructions such as `I am nice'). The possible genders are `masculine', `feminine' and `both', where the latter indicates a case where the none-diacriticized written form admits both a masculine and a feminine reading. We expect the gender to match the ones requested in the prefix.", "Interlocutors' Gender and Number Effects: We search for second-person pronouns and consider their gender and number. For pronouns in subject position, we also consider the gender and number of their governing verbs (or adjectives in copular constructions). For a singular audience, we expect the gender and number to match the requested ones. For a plural audience, we expect the masculine-plural forms."], "answer": "Speaker's Gender Effects, Interlocutors' Gender and Number Effects", "explanation": " [Document 1] mentions that the parser \"also performs morphological analysis and tagging of the individual tokens.\" [Document 2] and [Document 3] both mention specific types of syntactic analysis that are performed."}, {"question": "What type of experiments are performed?", "evidence": ["More specifically, we observe the impact of: (i) pre-trained word embeddings BIBREF11, BIBREF12, recurrent BIBREF13 and transformer-based sentence encoders BIBREF14 as question representation strategies; (ii) distinct convolutional neural networks used for visual feature extraction BIBREF15, BIBREF16, BIBREF17; and (iii) standard fusion strategies, as well as the importance of two main attention mechanisms BIBREF18, BIBREF19. We notice that even using a relatively simple baseline architecture, our best models are competitive to the (maybe overly-complex) state-of-the-art models BIBREF20, BIBREF21. Given the experimental nature of this work, we have trained over 130 neural network models, accounting for more than 600 GPU processing hours. We expect our findings to be useful as guidelines for training novel VQA models, and that they serve as a basis for the development of future architectures that seek to maximize predictive performance."], "answer": "pre-trained word embeddings BIBREF11, BIBREF12, recurrent BIBREF13, transformer-based sentence encoders BIBREF14, distinct convolutional neural networks, standard fusion strategies,  two main attention mechanisms BIBREF18, BIBREF19", "explanation": " The types of experiments performed are listed in [Document 1]."}], "108f99fcaf620fab53077812e8901870896acf36": [{"question": "How long are dialogue recordings used for evaluation?", "evidence": ["We conducted our experiments on the CSJ BIBREF25, which is one of the most widely used evaluation sets for Japanese speech recognition. The CSJ consists of more than 600 hrs of Japanese recordings.", "While most of the content is lecture recordings by a single speaker, CSJ also contains 11.5 hrs of 54 dialogue recordings (average 12.8 min per recording) with two speakers, which were the main target of ASR and speaker diarization in this study. During the dialogue recordings, two speakers sat in two adjacent sound proof chambers divided by a glass window. They could talk with each other over voice connection through a headset for each speaker. Therefore, speech was recorded separately for each speaker, and we generated mixed monaural recordings by mixing the corresponding speeches of two speakers. When mixing two recordings, we did not apply any normalization of speech volume. Due to this recording procedure, we were able to use non-overlapped speech to evaluate the oracle WERs."], "answer": "average 12.8 min per recording", "explanation": " [Document 2] states that the average dialogue recording is 12.8 minutes long."}, {"question": "What three conversational datasets are used for evaluation?", "evidence": ["We evaluate the proposed models on three conversationet dataset such as MojiTalk BIBREF16, PersonaChat BIBREF11, Empathetic-Dialogues BIBREF26."], "answer": "MojiTalk , PersonaChat , Empathetic-Dialogues", "explanation": "\n\nAccording to [Document 1], the three conversational datasets used for evaluation are MojiTalk , PersonaChat , and Empathetic-Dialogues ."}, {"question": "what evaluation methods are discussed?", "evidence": ["The most common approach is to treat the task as a document-level classification problem. Given a set of evaluation documents, each having a known correct label from a closed set of labels (often referred to as the \u201cgold-standard\u201d), and a predicted label for each document from the same set, the document-level accuracy is the proportion of documents that are correctly labeled over the entire evaluation collection. This is the most frequently reported metric and conveys the same information as the error rate, which is simply the proportion of documents that are incorrectly labeled (i.e. INLINEFORM0 ).", "Authors sometimes provide a per-language breakdown of results. There are two distinct ways in which results are generally summarized per-language: (1) precision, in which documents are grouped according to their predicted language; and (2) recall, in which documents are grouped according to what language they are actually written in. Earlier work has tended to only provide a breakdown based on the correct label (i.e. only reporting per-language recall). This gives us a sense of how likely a document in any given language is to be classified correctly, but does not give an indication of how likely a prediction for a given language is of being correct. Under the monolingual assumption (i.e. each document is written in exactly one language), this is not too much of a problem, as a false negative for one language must also be a false positive for another language, so precision and recall are closely linked. Nonetheless, authors have recently tended to explicitly provide both precision and recall for clarity. It is also common practice to report an F-score INLINEFORM0 , which is the harmonic mean of precision and recall. The F-score (also sometimes called F1-score or F-measure) was developed in IR to measure the effectiveness of retrieval with respect to a user who attaches different relative importance to precision and recall BIBREF376 . When used as an evaluation metric for classification tasks, it is common to place equal weight on precision and recall (hence \u201cF1\u201d-score, in reference to the INLINEFORM1 hyper-parameter, which equally weights precision and recall when INLINEFORM2 )."], "answer": "document-level accuracy, precision, recall, F-score", "explanation": " [Document 1] discusses document-level accuracy, while [Document 2] discusses precision, recall, and F-score."}], "22c125c461f565f5437dac74bf19c2ef317bad86": [{"question": "How many instances does their dataset have?", "evidence": ["For each speaker, we divide all available utterances into disjoint train, development, and test sets. Using the force-aligned phone boundaries, we extract the mid-phone frame for each example across the four classes, which leads to a data imbalance. Therefore, for all utterances in the training set, we randomly sample additional examples within a window of 5 frames around the center phone, to at least 50 training examples per class per speaker. It is not always possible to reach the target of 50 examples, however, if no more data is available to sample from. This process gives a total of INLINEFORM0 10700 training examples with roughly 2000 to 3000 examples per class, with each speaker having an average of 185 examples. Because the amount of data varies per speaker, we compute a sampling score, which denotes the proportion of sampled examples to the speaker's total training examples. We expect speakers with high sampling scores (less unique data overall) to underperform when compared with speakers with more varied training examples."], "answer": "10700", "explanation": " The dataset has 10700 training examples. This information is found in the first sentence of the first paragraph."}, {"question": "How big are the datasets used?", "evidence": ["We evaluate our approaches on two public Chinese span-extraction machine reading comprehension datasets: CMRC 2018 (simplified Chinese) BIBREF8 and DRCD (traditional Chinese) BIBREF9. The statistics of the two datasets are listed in Table TABREF29.", "Note that, since the test and challenge sets are preserved by CMRC 2018 official to ensure the integrity of the evaluation process, we submitted our best-performing systems to the organizers to get these scores. The resource in source language was chosen as SQuAD BIBREF4 training data. The settings of the proposed approaches are listed below in detail.", "Translation: We use Google Neural Machine Translation (GNMT) system for translation. We evaluated GNMT system on NIST MT02/03/04/05/06/08 Chinese-English set and achieved an average BLEU score of 43.24, compared to previous best work (43.20) BIBREF17, yielding state-of-the-art performance."], "answer": "Evaluation datasets used:\nCMRC 2018 - 18939 questions, 10 answers\nDRCD - 33953 questions, 5 answers\nNIST MT02/03/04/05/06/08 Chinese-English - Not specified\n\nSource language train data:\nSQuAD - Not specified", "explanation": "\n\n[Document 1] provides the sizes of the evaluation datasets used. [Document 2] does not specify the size of the SQuAD dataset, but it is likely to be similar in size to the evaluation datasets. [Document 3] does not specify the size of the NIST MT02/03/04/05/06/08 Chinese-English dataset."}, {"question": "How large is the dataset they used?", "evidence": ["We need good amount of data to try deep learning state-of-the-art algorithms. There are lot of open datasets available for names, locations, organisations, but not for topics as defined in Abstract above. Also defining and inferring topics is an individual preference and there are no fix set of rules for its definition. But according to our definition, we can use wikipedia titles as our target topics. English wikipedia dataset has more than 18 million titles if we consider all versions of them till now. We had to clean up the titles to remove junk titles as wikipedia title almost contains all the words we use daily. To remove such titles, we deployed simple rules as follows -", "After doing some more cleaning we were left with 10 million titles. We have a dump of 15 million English news articles published in past 4 years. Further, we reduced number of articles by removing duplicate and near similar articles. We used our pre-trained doc2vec models and cosine similarity to detect almost similar news articles. Then selected minimum articles required to cover all possible 2-grams to 5-grams. This step is done to save some training time without loosing accuracy. Do note that, in future we are planning to use whole dataset and hope to see gains in F1 and Recall further. But as per manual inspection, our dataset contains enough variations of sentences with rich vocabulary which contains names of celebrities, politicians, local authorities, national/local organisations and almost all locations, India and International, mentioned in the news text, in last 4 years."], "answer": "English wikipedia dataset has more than 18 million, a dump of 15 million English news articles ", "explanation": "\n\nAccording to [Document 1], the English wikipedia dataset has more than 18 million titles. This information is further supported by [Document 2], which states that the dataset contains a dump of 15 million English news articles."}], "283d358606341c399e369f2ba7952cd955326f73": [{"question": "Is annotation done manually?", "evidence": ["The biggest difference between discourse parsing for well-written document and dialogues is that discourse relations can exist on two nonadjacent utterances in dialogues. When we annotate dialogues, we should read dialogues from begin to the end. For each utterance, we should find its one parent node at least from all its previous utterances. We assume that the discourse structure is a connected graph and no utterance is isolated.", "We propose three questions for eache dialogue and annotate the span of answers in the input dialogue. As we know, our dataset is the first corpus for multi-party dialogues reading comprehension.", "We construct following questions and answers for the dialogue in Example 1:", "Q1: When does Bdale leave?", "A1: Fri morning", "Q2: How to get people love Mark in Mjg59's opinion.", "A2: Hire people to work on reverse-engineering closed drivers.", "On the other hand, to improve the difficulty of the task, we propose $ \\frac{1}{6}$ to $ \\frac{1}{3}$ unanswerable questions in our dataset. We annotate unanswerable questions and their plausible answers (PA). Each plausible answer comes from the input dialogue, but is not the answer for the plausible question.", "Q1: Whis is the email of daniels?", "PA: +61 403 505 896"], "answer": "Yes", "explanation": " [Document 1], [Document 2], [Document 3], [Document 4], [Document 5], [Document 6], [Document 7], [Document 8], [Document 9], and [Document 10] all suggest that annotation is done manually."}, {"question": "Do they perform some annotation?", "evidence": ["In this work, we use the datasets released by BIBREF1 and HEOT dataset provided by BIBREF0 . The datasets obtained pass through these steps of processing: (i) Removal of punctuatios, stopwords, URLs, numbers, emoticons, etc. This was then followed by transliteration using the Xlit-Crowd conversion dictionary and translation of each word to English using Hindi to English dictionary. To deal with the spelling variations, we manually added some common variations of popular Hinglish words. Final dictionary comprised of 7200 word pairs. Additionally, to deal with profane words, which are not present in Xlit-Crowd, we had to make a profanity dictionary (with 209 profane words) as well. Table TABREF3 gives some examples from the dictionary.", "Both the HEOT and BIBREF1 datasets contain tweets which are annotated in three categories: offensive, abusive and none (or benign). Some examples from the dataset are shown in Table TABREF4 . We use a LSTM based classifier model for training our model to classify these tweets into these three categories. An overview of the model is given in the Figure FIGREF12 . The model consists of one layer of LSTM followed by three dense layers. The LSTM layer uses a dropout value of 0.2. Categorical crossentropy loss was used for the last layer due to the presence of multiple classes. We use Adam optimizer along with L2 regularisation to prevent overfitting. As indicated by the Figure FIGREF12 , the model was initially trained on the dataset provided by BIBREF1 , and then re-trained on the HEOT dataset so as to benefit from the transfer of learned features in the last stage. The model hyperparameters were experimentally selected by trying out a large number of combinations through grid search."], "answer": "No", "explanation": " There is no mention of any annotation being performed in either document."}, {"question": "how was annotation done?", "evidence": ["With the gathered comments, we reconstructed the original conversation trees, from the original post, the root, to the leaves, when they were available and selected a subset to annotated. For annotation purposes, we created snippets of conversations as the ones shown in Example 1 and Example 2 consisting of the parent of the suspected trolling event, the suspected trolling event comment, and all of the direct responses to the suspected trolling event. We added an extra constraint that the parent of the suspected trolling event should also be part of the direct responses, we hypothesize that if the suspected trolling event is indeed trolling, its parent should be the object of its trolling and would have a say about it. We recognize that this limited amount of information is not always sufficient to recover the original message conveyed by all of the participants in the snippet, and additional context would be beneficial. However, the trade off is that snippets like this allow us to make use of Amazon Mechanical Turk (AMT) to have the dataset annotated, because it is not a big burden for a \u201cturker\u201d to work on an individual snippet in exchange for a small pay, and expedites the annotation process by distributing it over dozens of people. Specifically, for each snippet, we requested three annotators to label the four aspects previously described. Before annotating, we set up a qualification test along with borderline examples to guide them in process and align them with our criteria. The qualification test turned out to be very selective since only 5% of all of the turkers that attempted it passed the exam. Our dataset consists of 1000 conversations with 5868 sentences and 71033 tokens. The distribution over the classes per trolling aspect is shown in the table TABREF24 in the column \u201cSize\u201d."], "answer": "Annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations", "explanation": "\n\nAccording to [Document 1], annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations. This information is further supported by [Document 2], [Document 3], and [Document 5]."}], "a926d71e6e58066d279d9f7dc3210cd43f410164": [{"question": "Which languages are used in the multi-lingual caption model?", "evidence": ["We carry out experiments on multiple language pairs including German-English, French-English, and Japanese-English. The experimental results show that the proposed multi-lingual caption model not only achieves better caption performance than independent mono-lingual models for data-scarce languages, but also can induce the two types of features, linguistic and visual features, for different languages in joint spaces. Our proposed method consistently outperforms previous state-of-the-art vision-based bilingual word induction approaches on different languages. The contributions of this paper are as follows:"], "answer": "German-English, French-English, and Japanese-English", "explanation": " The languages used in the multi-lingual caption model are German-English, French-English, and Japanese-English according to [Document 1]."}, {"question": "Are this techniques used in training multilingual models, on what languages?", "evidence": ["We extract data from the WMT'14 English-French (En-Fr) and English-German (En-De) datasets. To create a larger discrepancy between the tasks, so that there is a clear dataset size imbalance, the En-De data is artificially restricted to only 1 million parallel sentences, while the full En-Fr dataset, comprising almost 40 million parallel sentences, is used entirely. Words are split into subwords units with a joint vocabulary of 32K tokens. BLEU scores are computed on the tokenized output with multi-bleu.perl from Moses BIBREF10."], "answer": "English to French and English to German", "explanation": " The WMT'14 English-French (En-Fr) and English-German (En-De) datasets are used to train multilingual models."}, {"question": "What multilingual word representations are used?", "evidence": ["We use the previous CNN architecture with bilingual embedding and the RF model with surface features (e.g., use of personal pronoun, presence of interjections, emoticon or specific punctuation) to verify which pair of the three languages: (a) has similar ironic pragmatic devices, and (b) uses similar text-based pattern in the narrative of the ironic tweets. As continuous word embedding spaces exhibit similar structures across (even distant) languages BIBREF35, we use a multilingual word representation which aims to learn a linear mapping from a source to a target embedding space. Many methods have been proposed to learn this mapping such as parallel data supervision and bilingual dictionaries BIBREF35 or unsupervised methods relying on monolingual corpora BIBREF36, BIBREF37, BIBREF38. For our experiments, we use Conneau et al 's approach as it showed superior results with respect to the literature BIBREF36. We perform several experiments by training on one language ($lang_1$) and testing on another one ($lang_2$) (henceforth $lang_1\\rightarrow lang_2$). We get 6 configurations, plus two others to evaluate how irony devices are expressed cross-culturally, i.e. in European vs. non European languages. In each experiment, we took 20% from the training to validate the model before the testing process. Table TABREF11 presents the results."], "answer": " a multilingual word representation which aims to learn a linear mapping from a source to a target embedding space", "explanation": "\n\nAccording to [Document 1], the multilingual word representation used is one which aims to learn a linear mapping from a source to a target embedding space."}], "d4b84f48460517bc0a6d4e0c38f6853c58081166": [{"question": "On Twitter, do the demographic attributes and answers show more correlations than on Yahoo! Answers?", "evidence": ["As the table shows, terms extracted from Yahoo! Answers tend to be more related, in terms of the number of correlated terms, to attributes related to religion or ethnicity compared to terms from Twitter. However, for two particular attributes (i.e., Price and Buddhist), the number of correlated terms from Twitter is higher than the ones from Yahoo! Answers . These results collectively suggest that there is a wealth of terms, both in Yahoo! Answers and Twitter, which can be used to predict the population demographics."], "answer": "No", "explanation": " The table in [Document 1] shows that, for two particular attributes, the number of correlated terms from Twitter is higher than the ones from Yahoo! Answers. However, this is not the case for all demographic attributes, and overall, there are more correlations between demographic attributes and answers on Yahoo! Answers than on Twitter."}, {"question": "Do they specify which countries they collected twitter data from?", "evidence": [], "answer": "No", "explanation": " There is no mention of any specific countries in the documents."}, {"question": "How is data collected, manual collection or Twitter api?", "evidence": ["We used the Twitter API to gather real-time tweets from September 2018 until February 2019, selecting the ones containing any of the 51 Hatebase terms that are more common in hate speech tweets, as studied in BIBREF9. We filtered out retweets, tweets containing less than three words and tweets containing porn related terms. From that selection, we kept the ones that included images and downloaded them. Twitter applies hate speech filters and other kinds of content control based on its policy, although the supervision is based on users' reports. Therefore, as we are gathering tweets from real-time posting, the content we get has not yet passed any filter."], "answer": "Twitter API", "explanation": "\n\nAccording to [Document 1], the data was collected using the Twitter API."}], "028d0d9b7a71133e51a14a32cd09dea1e2f39f05": [{"question": "How do they get the formal languages?", "evidence": ["BIBREF7 investigated the learning capabilities of simple RNNs to process and formalize a context-free grammar containing hierarchical (recursively embedded) dependencies: He observed that distinct parts of the networks were able to learn some complex representations to encode certain grammatical structures and dependencies of the context-free grammar. Later, BIBREF8 introduced an RNN with an external stack memory to learn simple context-free languages, such as $a^n b^m$ , $a^nb^ncb^ma^m$ , and $a^{n+m} b^n c^m$ . Similar studies BIBREF15 , BIBREF16 , BIBREF17 , BIBREF10 , BIBREF11 have explored the existence of stable counting mechanisms in simple RNNs, which would enable them to learn various context-free and context-sensitive languages, but none of the RNN architectures proposed in the early days were able to generalize the training set to longer (or more complex) test samples with substantially high accuracy.", "BIBREF9 , on the other hand, proposed a variant of Long Short-Term Memory (LSTM) networks to learn two context-free languages, $a^n b^n$ , $a^n b^m B^m A^n$ , and one strictly context-sensitive language, $a^n b^n c^n$ . Given only a small fraction of samples in a formal language, with values of $n$ (and $m$ ) ranging from 1 to a certain training threshold $N$ , they trained an LSTM model until its full convergence on the training set and then tested it on a more generalized set. They showed that their LSTM model outperformed the previous approaches in capturing and generalizing the aforementioned formal languages. By analyzing the cell states and the activations of the gates in their LSTM model, they further demonstrated that the network learns how to count up and down at certain places in the sample sequences to encode information about the underlying structure of each of these formal languages."], "answer": "These are well-known formal languages some of which was used in the literature to evaluate the learning capabilities of RNNs.", "explanation": " [Document 1] and [Document 2] both mention that the formal languages used in the study are well-known formal languages that have been used in the past to evaluate the learning capabilities of RNNs."}, {"question": "How do they introduce language variation?", "evidence": ["We acquired the data in two rounds of annotation. In the first one, we were looking for original and uncommon sentence change suggestions. In the second one, we collected sentence alternations using ideas from the first round. The first and second rounds of annotation could be broadly called as collecting ideas and collecting data, respectively."], "answer": " we were looking for original and uncommon sentence change suggestions", "explanation": " The first round is for collecting ideas and the second round is for collecting data."}, {"question": "How do they obtain language identities?", "evidence": ["In this paper, we propose an improved RNN-T model with language bias to alleviate the problem. The model is trained to predict language IDs as well as the subwords. To ensure the model can learn CS information, we add language IDs in the CS point of transcription, as illustrated in Fig. 1. In the figure, we use the arrangements of different geometric icons to represent the CS distribution. Compared with normal text, the tagged data can bias the RNN-T to predict language IDs in CS points. So our method can model the CS distribution directly, no additional LID model is needed. Then we constrain the input word embedding with its corresponding language ID, which is beneficial for model to learn the language identity information from transcription. In the inference process, the predicted language IDs are used to adjust the output posteriors. The experiment results on CS corpus show that our proposed method outperforms the RNN-T baseline (without language bias) significantly. Overall, our best model achieves 16.2% and 12.9% relative error reduction on two test sets, respectively. To our best knowledge, this is the first attempt of using the RNN-T model with language bias as an end-to-end CSSR strategy."], "answer": "model is trained to predict language IDs as well as the subwords, we add language IDs in the CS point of transcriptio", "explanation": "\n\nAccording to [Document 1], the model is trained to predict language IDs as well as the subwords. In order to ensure that the model can learn CS information, the authors add language IDs in the CS point of transcription. This allows the model to learn the language identity information from transcription."}], "1d7b99646a1bc05beec633d7a3beb083ad1e8734": [{"question": "How long is the dataset used for training?", "evidence": [], "answer": "Unanswerable", "explanation": " There is no mention of the dataset's size in any of the documents."}, {"question": "How is the training data collected?", "evidence": ["The following data sources were used to train the RNN-T and associated RNN-LMs in this study.", "Source-domain baseline RNN-T: approximately 120M segmented utterances (190,000 hours of audio) from YouTube videos, with associated transcripts obtained from semi-supervised caption filtering BIBREF28.", "Source-domain normalizing RNN-LM: transcripts from the same 120M utterance YouTube training set. This corresponds to about 3B tokens of the sub-word units used (see below, Section SECREF30).", "Target-domain RNN-LM: 21M text-only utterance-level transcripts from anonymized, manually transcribed audio data, representative of data from a Voice Search service. This corresponds to about 275M sub-word tokens.", "Target-domain RNN-T fine-tuning data: 10K, 100K, 1M and 21M utterance-level {audio, transcript} pairs taken from anonymized, transcribed Voice Search data. These fine-tuning sets roughly correspond to 10 hours, 100 hours, 1000 hours and 21,000 hours of audio, respectively."], "answer": "from YouTube videos, with associated transcripts obtained from semi-supervised caption filtering, from a Voice Search service", "explanation": "\n\nAccording to [Document 2] and [Document 3], the training data is collected from YouTube videos, with associated transcripts obtained from semi-supervised caption filtering. This is further supported by [Document 5], which states that the target-domain RNN-T fine-tuning data is collected from a Voice Search service."}, {"question": "How big is the impact of training data size on the performance of the multilingual encoder?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents provided do not contain any information regarding the impact of training data size on the performance of the multilingual encoder."}], "179bc57b7b5231ea6ad3e93993a6935dda679fa2": [{"question": "What are all machine learning approaches compared in this work?", "evidence": ["Email classifier using machine learning ::: Machine learning approach ::: Feature selection", "Ngrams are a continuous sequence of n items from a given sample of text. From title, body and OCR text words are selected. Ngrams of 3 nearby words are extracted with Term Frequency-Inverse Document Frequency (TF-IDF) vectorizing, then features are filtered using chi squared the feature scoring method.", "Email classifier using machine learning ::: Machine learning approach ::: Random forest", "Random Forest is a bagging Algorithm, an ensemble learning method for classification that operates by constructing a multitude of decision trees at training time and outputting the class that has highest mean majority vote of the classesBIBREF14.", "Email classifier using machine learning ::: Machine learning approach ::: XGBoost", "XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. It is used commonly in the classification problems involving unstructured dataBIBREF5.", "Email classifier using machine learning ::: Machine learning approach ::: Hierarchical Model", "Since the number of target labels are high, achieving the higher accuracy is difficult, while keeping all the categories under same feature selection method. Some categories performs well with lower TF-IDF vectorizing range and higher n grams features even though they showed lower accuracy in the overall single model. Therefore, hierarchical machine learning models are built to classify 31 categories in the first classification model and remaining categories are named as low-accu and predicted as one category. In the next model, predicted low-accu categories are again classified into 47 categories. Comparatively this hierarchical model works well since various feature selection methods are used for various categoriesBIBREF5."], "answer": "Feature selection, Random forest, XGBoost, Hierarchical Model", "explanation": "\n\nAccording to [Document 1], the machine learning approaches that were compared in this work were feature selection, Random Forest, XGBoost, and Hierarchical Model. This information is further supported by [Document 2], [Document 3], [Document 4], [Document 5], [Document 6], [Document 7], and [Document 8]."}, {"question": "Do they compare DeepER against other approaches?", "evidence": [], "answer": "Yes", "explanation": "\n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task"}, {"question": "What approaches without reinforcement learning have been tried?", "evidence": ["We conducted cross-validation experiments using various values of $t$ and $m$. Table TABREF26 shows the results for the best values of $t$ and $m$ obtained. The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively. To enable a fair comparison we used the same input features in all systems. These input features combine information from the question and the input sentence and are shown in Fig. FIGREF16. The features are based on BIBREF12, and are the same as in BIBREF1, plus the addition of the position of the input snippet. The best SVC and SVR parameters were determined by grid search.", "The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: \u201cNNC top 5\u201d uses classification labels as described in Section SECREF3, and \u201cNNC SU4 F1\u201d uses the regression labels, that is, the ROUGE-SU4 F1 scores of each sentence. Of interest is the fact that \u201cNNC SU4 F1\u201d outperforms the neural regressor. We have not explored this further and we presume that the relatively good results are due to the fact that ROUGE values range between 0 and 1, which matches the full range of probability values that can be returned by the sigmoid activation of the classifier final layer."], "answer": "classification, regression, neural methods", "explanation": " [Document 1] discusses classification and regression methods, while [Document 2] discusses neural methods."}], "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763": [{"question": "What dataset do they use?", "evidence": ["Our experiments are run using the English$\\rightarrow $Italian/German portions of the MuST-C corpus BIBREF25, which is extracted from TED talks, using the same train/validation/test split as provided with the corpus (see Table TABREF18). As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De). While our main goal is to verify our hypotheses on a large data condition, thus the need to include proprietary data, for the sake of reproducibility in both languages we also provide results with systems only trained on TED Talks (small data condition). When training on large scale data we use Transformer with layer size of 1024, hidden size of 4096 on feed forward layers, 16 heads in the multi-head attention, and 6 layers in both encoder and decoder. When training only on TED talks, we set layer size of 512, hidden size of 2048 for the feed forward layers, multi-head attention with 8 heads and again 6 layers in both encoder and decoder."], "answer": "English$\\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)", "explanation": " [Document 1] states that the English$\\rightarrow $Italian/German portions of the MuST-C corpus are used. It also states that, as additional data, a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De) is used."}, {"question": "What dataset do they use?", "evidence": ["The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type. To make this diversity, after cleaning raw text input, we use multiple types of word tokenizers. Each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words BIBREF5, pre-trained embedding as fasttext (trained on Wiki Vietnamese language) BIBREF6 and sonvx (trained on Vietnamese newspaper) BIBREF7. Each sentence has a set of words corresponding to a set of word vectors, and that set of word vectors is a representation of a sentence. We also make a sentence embedding by using RoBERTa architecture BIBREF8. CBOW and RoBERTa models trained on text from some resources including VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD and text crawled from Facebook. After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13. With the multiply output results, we will use an ensemble method to combine them and output the final result. Ensemble method we use here is Stacking method will be introduced in the section SECREF16.", "The dataset in this HSD task is really imbalance. Clean class dominates with 91.5%, offensive class takes 5% and the rest belongs to hate class with 3.5%. To make model being able to learn with this imbalance data, we inject class weight to the loss function with the corresponding ratio (clean, offensive, hate) is $(0.09, 0.95, 0.96)$. Formular DISPLAY_FORM17 is the loss function apply for all models in our system. $w_i$ is the class weight, $y_i$ is the ground truth and $\\hat{y}_i$ is the output of the model. If the class weight is not set, we find that model cannot adjust parameters. The model tends to output all clean classes."], "answer": "They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).", "explanation": "\n\nAccording to [Document 1], they used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings. This information is further supported by [Document 2], which states that they used the dataset provided in HSD task to train their model."}, {"question": "What dataset do they use?", "evidence": ["SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language."], "answer": "A parallel corpus where the source is an English expression of code and the target is Python code.", "explanation": " [Document 1] mentions that they use a parallel corpus for training where the source is English and the target is Python. This is further supported by the fact that the parallel corpus has 18805 aligned data points."}], "940a16e9db8be5b5f4e67d9c7622b3df99ac10a5": [{"question": "What languages are explored in this paper?", "evidence": ["In our experiment, we use a conversational Mandarin-English code-switching speech corpus called SEAME Phase II (South East Asia Mandarin-English). The data are collected from spontaneously spoken interviews and conversations in Singapore and Malaysia by bilinguals BIBREF21 . As the data preprocessing, words are tokenized using Stanford NLP toolkit BIBREF22 and all hesitations and punctuations were removed except apostrophe. The split of the dataset is identical to BIBREF9 and it is showed in Table TABREF6 ."], "answer": "Mandarin, English", "explanation": "\n\nAccording to [Document 1], the SEAME Phase II corpus contains data from bilingual speakers of Mandarin and English."}, {"question": "What languages are explored in this paper?", "evidence": [], "answer": "Unanswerable", "explanation": " This question cannot be answered based on the given documents."}, {"question": "Which domains did they explored?", "evidence": ["We use the publicly available dataset KVRET BIBREF5 in our experiments. This dataset is created by the Wizard-of-Oz method BIBREF10 on Amazon Mechanical Turk platform. This dataset includes dialogues in 3 domains: calendar, weather, navigation (POI) which is suitable for our mix-domain dialogue experiments. There are 2,425 dialogues for training, 302 for validation and 302 for testing, as shown in the upper half of Table TABREF12."], "answer": "calendar, weather, navigation", "explanation": " The answer can be found in the first sentence of [Document 1]."}], "d922eaa5aa135c1ae211827c6a599b4d69214563": [{"question": "What conversation-based features are used?", "evidence": ["Conversation Based Features", "These features are devoted to exploit the peculiar characteristics of the dataset, which have a tree structure reflecting the conversation thread.", "Text Similarity to Source Tweet: Jaccard Similarity of each tweet with its source tweet.", "Text Similarity to Replied Tweet: the degree of similarity between the tweet with the previous tweet in the thread (the tweet is a reply to that tweet).", "Tweet Depth: the depth value is obtained by counting the node from sources (roots) to each tweet in their hierarchy."], "answer": "Text Similarity to Source Tweet, Text Similarity to Replied Tweet, Tweet Depth", "explanation": "\n\nAccording to [Document 2], conversation-based features are used to exploit the peculiar characteristics of the dataset, which have a tree structure reflecting the conversation thread. This information is further supported by [Document 3], [Document 4], and [Document 5]."}, {"question": "How are discourse features incorporated into the model?", "evidence": ["CNN2-PV. This model (Figure FIGREF10 , left+center) featurizes discourse information into a vector of relation probabilities. In order to derive the discourse features, an entity grid is constructed by feeding the document through an NLP pipeline to identify salient entities. Two flavors of discourse features are created by populating the entity grid with either (i) grammatical relations (GR) or (ii) RST discourse relations (RST). The GR features are represented as grammatical relation transitions derived from the entity grid, e.g., INLINEFORM0 . The RST features are represented as RST discourse relations with their nuclearity, e.g., INLINEFORM1 . The probability vectors are then distributions over relation types. For GR, the vector is a distribution over all the entity role transitions, i.e., INLINEFORM2 (see Table TABREF2 ). For RST, the vector is a distribution over all the RST discourse relations, i.e., INLINEFORM3 Denoting a feature as such with INLINEFORM4 , we construct the pooling vector INLINEFORM5 for the char-bigrams, and concatenate INLINEFORM6 to INLINEFORM7 before feeding the resulting vector to the softmax layer."], "answer": "They derive entity grid with grammatical relations and RST discourse relations and concatenate them with pooling vector for the char-bigrams before feeding to the resulting vector to the softmax layer.", "explanation": " [Document 1] states that they derive entity grid with grammatical relations and RST discourse relations. The entity grid is then populated with either grammatical relations or RST discourse relations. The probability vectors are then distributions over relation types. The pooling vector is then concatenated with the feature vector before feeding to the softmax layer."}, {"question": "How is Logician different from traditional seq2seq models?", "evidence": ["Logician is a supervised end-to-end neural learning algorithm which transforms natural language sentences into facts in the SAOKE form. Logician is trained under the attention-based sequence-to-sequence paradigm, with three mechanisms: restricted copy mechanism to ensure literally honestness, coverage mechanism to alleviate the under extraction and over extraction problem, and gated dependency attention mechanism to incorporate dependency information. Experimental results on four types of open information extraction tasks reveal the superiority of the Logician algorithm."], "answer": "restricted copy mechanism to ensure literally honestness, coverage mechanism to alleviate the under extraction and over extraction problem, and gated dependency attention mechanism to incorporate dependency information", "explanation": "\n\nAccording to [Document 1], Logician uses a restricted copy mechanism to ensure literal honesty, a coverage mechanism to alleviate the under extraction and over extraction problem, and a gated dependency attention mechanism to incorporate dependency information. These mechanisms are not typically used in traditional seq2seq models, making Logician different from traditional seq2seq models."}], "342ada55bd4d7408e1fcabf1810b92d84c1dbc41": [{"question": "By how much did the results improve?", "evidence": [], "answer": "Unanswerable", "explanation": " The question is unanswerable with the given documents. [Document 1] describes the experiments conducted, [Document 2] describes the Stanford NER algorithm, [Document 3] describes the spaCy 2.0 algorithm, [Document 4] describes the dataset used, and [Document 5] describes the recurrent model with a CRF top layer. None of the documents provide information on the results of the experiments."}, {"question": "How much in experiments is performance improved for models trained with generated adversarial examples?", "evidence": ["Adversarial training can often improve model robustness BIBREF25, BIBREF27. We also fine-tune the target models using adversarial training. At each training step, we train the model with a batch of original examples along with adversarial examples with balanced labels. The adversarial examples account for around 10% in a batch. During training, we generate adversarial examples with the current model as the target and update the model parameters with the hybrid batch iteratively. The beam size for generation is set to 1 to reduce the computation cost, since the generation success rate is minor in adversarial training. We evaluate the adversarially trained models, as shown in Table TABREF18.", "After adversarial training, the performance of all the target models raises significantly, while that on the original examples remain comparable. Note that since the focus of this paper is on model robustness which can hardly be reflected in original data, we do not expect performance improvement on original data. The results demonstrate that adversarial training with our adversarial examples can significantly improve the robustness we focus on without remarkably hurting the performance on original data. Moreover, although the adversarial example generation is constrained by a BERT language model, BiMPM and DIIN which do not use the BERT language model can also significantly benefit from the adversarial examples, further demonstrating the effectiveness of our method."], "answer": "Answer with content missing: (Table 1) The performance of all the target models raises significantly, while that on the original\nexamples remain comparable (e.g. the overall accuracy of BERT on modified examples raises from 24.1% to 66.0% on Quora)", "explanation": "\n\nAccording to [Document 1], the performance of all the target models raises significantly, while that on the original examples remain comparable. This information is further supported by [Document 2]."}, {"question": "How big is improvement in performance over Transformers?", "evidence": ["As shown in Table TABREF24, MUSE outperforms all previously models on En-De and En-Fr translation, including both state-of-the-art models of stand alone self-attention BIBREF0, BIBREF13, and convolutional models BIBREF11, BIBREF15, BIBREF10. This result shows that either self-attention or convolution alone is not enough for sequence to sequence learning. The proposed parallel multi-scale attention improves over them both on En-De and En-Fr.", "Compared to Evolved Transformer BIBREF19 which is constructed by NAS and also mixes convolutions of different kernel size, MUSE achieves 2.2 BLEU gains in En-Fr translation."], "answer": "2.2 BLEU gains", "explanation": " [Document 1] and [Document 2] both mention that MUSE outperforms Transformers by 2.2 BLEU gains."}], "84765903b8c7234ca2919d0a40e3c6a5bcedf45d": [{"question": "Did they use the state-of-the-art model to analyze the attention?", "evidence": ["We make two main contributions. First, we introduce new strategies for interpreting the behavior of deep models in their intermediate layers, specifically, by examining the saliency of the attention and the gating signals. Second, we provide an extensive analysis of the state-of-the-art model for the NLI task and show that our methods reveal interesting insights not available from traditional methods of inspecting attention and word saliency."], "answer": "we provide an extensive analysis of the state-of-the-art model", "explanation": "\n\nAccording to [Document 1], the authors provide an extensive analysis of the state-of-the-art model. This means that they did use the state-of-the-art model to analyze the attention."}, {"question": "Do they explain model predictions solely on attention weights?", "evidence": ["Our application is the escalation of Internet chats. To maintain quality of service, users are transferred to human representatives when their conversations with an intelligent virtual assistant (IVA) fail to progress. These transfers are known as escalations. We apply Han to such conversations in a sequential manner by feeding each user turn to Han as they occur, to determine if the conversation should escalate. If so, the user will be transferred to a live chat representative to continue the conversation. To help the human representative quickly determine the cause of the escalation, we generate a visualization of the user's turns using the attention weights to highlight the turns influential in the escalation decision. This helps the representative quickly scan the conversation history and determine the best course of action based on problematic turns."], "answer": "Yes", "explanation": " The passage mentions that the application of Han is done in a sequential manner, and that the visualization of the user's turns uses the attention weights to highlight the turns influential in the escalation decision. This suggests that the model predictions are based solely on the attention weights."}, {"question": "How many attention layers are there in their model?", "evidence": ["The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254."], "answer": "one", "explanation": " The model has one attention layer, as stated in [Document 1]."}], "df01e98095ba8765d9ab0d40c9e8ef34b64d3700": [{"question": "Which publicly available NLU dataset is used?", "evidence": ["We tested the system on two datasets, different in size and complexity of the addressed language.", "Experimental Evaluation ::: Datasets ::: NLU-Benchmark dataset", "The first (publicly available) dataset, NLU-Benchmark (NLU-BM), contains $25,716$ utterances annotated with targeted Scenario, Action, and involved Entities. For example, \u201cschedule a call with Lisa on Monday morning\u201d is labelled to contain a calendar scenario, where the set_event action is instantiated through the entities [event_name: a call with Lisa] and [date: Monday morning]. The Intent is then obtained by concatenating scenario and action labels (e.g., calendar_set_event). This dataset consists of multiple home assistant task domains (e.g., scheduling, playing music), chit-chat, and commands to a robot BIBREF7.", "Experimental Evaluation ::: Datasets ::: ROMULUS dataset", "The second dataset, ROMULUS, is composed of $1,431$ sentences, for each of which dialogue acts, semantic frames, and corresponding frame elements are provided. This dataset is being developed for modelling user utterances to open-domain conversational systems for robotic platforms that are expected to handle different interaction situations/patterns \u2013 e.g., chit-chat, command interpretation. The corpus is composed of different subsections, addressing heterogeneous linguistic phenomena, ranging from imperative instructions (e.g., \u201center the bedroom slowly, turn left and turn the lights off \u201d) to complex requests for information (e.g., \u201cgood morning I want to buy a new mobile phone is there any shop nearby?\u201d) or open-domain chit-chat (e.g., \u201cnope thanks let's talk about cinema\u201d). A considerable number of utterances in the dataset is collected through Human-Human Interaction studies in robotic domain ($\\approx $$70\\%$), though a small portion has been synthetically generated for balancing the frame distribution."], "answer": "ROMULUS dataset, NLU-Benchmark dataset", "explanation": "\n\nAccording to [Document 1], the system was tested on two datasets, one of which is publicly available. This is further supported by [Document 2] and [Document 4]. [Document 3] and [Document 5] provide information on the NLU-Benchmark dataset and the ROMULUS dataset, respectively."}, {"question": "What NLI models do they analyze?", "evidence": ["To test the difficulty of our dataset, we checked the majority class label and the accuracies of five state-of-the-art NLI models adopting different approaches: BiMPM (Bilateral Multi-Perspective Matching Model; BIBREF31 , BIBREF31 ), ESIM (Enhanced Sequential Inference Model; BIBREF32 , BIBREF32 ), Decomposable Attention Model BIBREF33 , KIM (Knowledge-based Inference Model; BIBREF34 , BIBREF34 ), and BERT (Bidirectional Encoder Representations from Transformers model; BIBREF35 , BIBREF35 ). Regarding BERT, we checked the performance of a model pretrained on Wikipedia and BookCorpus for language modeling and trained with SNLI and MultiNLI. For other models, we checked the performance trained with SNLI. In agreement with our dataset, we regarded the prediction label contradiction as non-entailment."], "answer": "BiMPM, ESIM, Decomposable Attention Model, KIM, BERT", "explanation": " The document states that the BiMPM, ESIM, Decomposable Attention Model, KIM, and BERT models were analyzed."}, {"question": "What dataset was used in this work?", "evidence": ["The recently introduced How2 dataset BIBREF2 has stimulated research around multimodal language understanding through the availability of 300h instructional videos, English subtitles and their Portuguese translations. For example, BIBREF3 successfully demonstrates that semantically rich action-based visual features are helpful in the context of machine translation (MT), especially in the presence of input noise that manifests itself as missing source words. Therefore, we hypothesize that a speech-to-text translation (STT) system may also benefit from the visual context, especially in the traditional cascaded framework BIBREF4, BIBREF5 where noisy automatic transcripts are obtained from an automatic speech recognition system (ASR) and further translated into the target language using a machine translation (MT) component. The dataset enables the design of such multimodal STT systems, since we have access to a bilingual corpora as well as the corresponding audio-visual stream. Hence, in this paper, we propose a cascaded multimodal STT with two components: (i) an English ASR system trained on the How2 dataset and (ii) a transformer-based BIBREF0 visually grounded MMT system."], "answer": "How2", "explanation": " [Document 1] mentions that the recently introduced How2 dataset was used, which is further supported by [Document 2]."}], "f77d7cddef3e021d70e16b9e16cecfd4b8ee80d3": [{"question": "Which dataset do they train their models on?", "evidence": ["Character-level language modeling (or character prediction) consists in predicting the next character while reading a document one character at a time. It is a common benchmark for rnn because of the heightened need for shared parametrization when compared to word-level models. We test mgru on two well-known datasets, the Penn Treebank and Text8."], "answer": "Penn Treebank, Text8", "explanation": " [Document 1] mentions that the models are trained on the Penn Treebank and Text8 datasets."}, {"question": "Which datasets are used to train this model?", "evidence": ["We evaluate performance of our models on the SQUAD BIBREF16 dataset (denoted $\\mathcal {S}$ ). We use the same split as that of BIBREF4 , where a random subset of 70,484 instances from $\\mathcal {S}\\ $ are used for training ( ${\\mathcal {S}}^{tr}$ ), 10,570 instances for validation ( ${\\mathcal {S}}^{val}$ ), and 11,877 instances for testing ( ${\\mathcal {S}}^{te}$ )."], "answer": "SQUAD", "explanation": "\n\nAccording to [Document 1], the SQUAD dataset is used to train the model."}, {"question": "What is the dataset used to train the model?", "evidence": ["Clean-labeled Datasets. We use three clean labeled datasets. The first one is the movie sentence polarity dataset from BIBREF19. The other two datasets are laptop and restaurant datasets collected from SemEval-2016 . The former consists of laptop review sentences and the latter consists of restaurant review sentences. The original datasets (i.e., Laptop and Restaurant) were annotated with aspect polarity in each sentence. We used all sentences with only one polarity (positive or negative) for their aspects. That is, we only used sentences with aspects having the same sentiment label in each sentence. Thus, the sentiment of each aspect gives the ground-truth as the sentiments of all aspects are the same.", "Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source. We extracted sentences from each review and assigned review's label to its sentences. Like previous work, we treat 4 or 5 stars as positive and 1 or 2 stars as negative. The data is noisy because a positive (negative) review can contain negative (positive) sentences, and there are also neutral sentences. This gives us three noisy-labeled training datasets. We still use the same test sets as those for the clean-labeled datasets. Summary statistics of all the datasets are shown in Table TABREF9."], "answer": " movie sentence polarity dataset from BIBREF19, laptop and restaurant datasets collected from SemEval-201, we collected 2,000 reviews for each domain from the same review source", "explanation": "\n\nAccording to [Document 1], the movie sentence polarity dataset from BIBREF19 is used to train the model. This information is further supported by [Document 2], which states that the laptop and restaurant datasets collected from SemEval-2016 are used to train the model."}], "7ba6330d105f49c7f71dba148bb73245a8ef2966": [{"question": "What are the performance metrics used?", "evidence": ["As a convention, the metric of joint goal accuracy is used to compare our model to previous work. The joint goal accuracy only regards the model making a successful belief state prediction if all of the slots and values predicted are exactly matched with the labels provided. This metric gives a strict measurement that tells how often the DST module will not propagate errors to the downstream modules in a dialogue system. In this work, the model with the highest joint accuracy on the validation set is evaluated on the test set for the test joint accuracy measurement."], "answer": "joint goal accuracy", "explanation": "\n\nAccording to [Document 1], the joint goal accuracy is used to compare the model to previous work. This metric gives a strict measurement of how often the DST module will not propagate errors to the downstream modules in a dialogue system."}, {"question": "What metric is used to measure performance?", "evidence": ["We use a standard set of supervised as well as unsupervised benchmark tasks from the literature to evaluate our trained models, following BIBREF16 . The breadth of tasks allows to fairly measure generalization to a wide area of different domains, testing the general-purpose quality (universality) of all competing sentence embeddings. For downstream supervised evaluations, sentence embeddings are combined with logistic regression to predict target labels. In the unsupervised evaluation for sentence similarity, correlation of the cosine similarity between two embeddings is compared to human annotators.", "Downstream Supervised Evaluation. Sentence embeddings are evaluated for various supervised classification tasks as follows. We evaluate paraphrase identification (MSRP) BIBREF25 , classification of movie review sentiment (MR) BIBREF26 , product reviews (CR) BIBREF27 , subjectivity classification (SUBJ) BIBREF28 , opinion polarity (MPQA) BIBREF29 and question type classification (TREC) BIBREF30 . To classify, we use the code provided by BIBREF22 in the same manner as in BIBREF16 . For the MSRP dataset, containing pairs of sentences INLINEFORM0 with associated paraphrase label, we generate feature vectors by concatenating their Sent2Vec representations INLINEFORM1 with the component-wise product INLINEFORM2 . The predefined training split is used to tune the L2 penalty parameter using cross-validation and the accuracy and F1 scores are computed on the test set. For the remaining 5 datasets, Sent2Vec embeddings are inferred from input sentences and directly fed to a logistic regression classifier. Accuracy scores are obtained using 10-fold cross-validation for the MR, CR, SUBJ and MPQA datasets. For those datasets nested cross-validation is used to tune the L2 penalty. For the TREC dataset, as for the MRSP dataset, the L2 penalty is tuned on the predefined train split using 10-fold cross-validation, and the accuracy is computed on the test set.", "Unsupervised Similarity Evaluation. We perform unsupervised evaluation of the learnt sentence embeddings using the sentence cosine similarity, on the STS 2014 BIBREF31 and SICK 2014 BIBREF32 datasets. These similarity scores are compared to the gold-standard human judgements using Pearson's INLINEFORM0 BIBREF33 and Spearman's INLINEFORM1 BIBREF34 correlation scores. The SICK dataset consists of about 10,000 sentence pairs along with relatedness scores of the pairs. The STS 2014 dataset contains 3,770 pairs, divided into six different categories on the basis of the origin of sentences/phrases, namely Twitter, headlines, news, forum, WordNet and images."], "answer": "Accuracy and F1 score for supervised tasks, Pearson's and Spearman's correlation for unsupervised tasks", "explanation": "\n\nAccording to [Document 1], the performance of sentence embeddings is measured using a standard set of supervised and unsupervised benchmark tasks. This information is further supported by [Document 2] and [Document 3].\n\nIn [Document 2], it is stated that the accuracy and F1 score are used to measure the performance of the sentence embeddings on the MSRP dataset, while for the remaining 5 datasets, the Sent2Vec embeddings are inferred from input sentences and directly fed to a logistic regression classifier, and accuracy scores are obtained using 10-fold cross-validation.\n\nIn [Document 3], it is stated that for the STS 2014 and SICK 2014 datasets, the sentence cosine similarity is used to measure the performance of the sentence embeddings, and the similarity scores are compared to the gold-standard human judgements using Pearson's and Spearman's correlation scores."}, {"question": "What metrics are used to measure performance of models?", "evidence": ["BPRA: Belief Per-Response Accuracy (BPRA) tests the ability to generate the correct user intents during the dialogue. This metric is used to evaluate the accuracy of dialogue belief tracker BIBREF1.", "APRA: Action Per-Response Accuracy (APRA) evaluates the per-turn accuracy of the dialogue actions generated by dialogue policy maker. For baselines, APRA evaluates the classification accuracy of the dialogue policy maker. But our model actually generates each individual token of actions, and we consider a prediction to be correct only if every token of the model output matches the corresponding token in the ground truth.", "BLEU BIBREF19: The metric evaluates the quality of the final response generated by natural language generator. The metric is usually used to measure the performance of the task-oriented dialogue system."], "answer": "BPRA, APRA, BLEU", "explanation": "\n\nAccording to [Document 1], BPRA is used to measure the performance of dialogue belief tracker. According to [Document 2], APRA is used to measure the per-turn accuracy of the dialogue policy maker. According to [Document 3], BLEU is used to measure the quality of the final response generated by natural language generator."}], "1c2d4dc1e842b962c6407d6436f3dc73dd44ce55": [{"question": "What are the 12 AV approaches which are examined?", "evidence": ["As a basis for our experiments, we reimplemented 12 existing AV approaches, which have shown their potentials in the previous PAN-AV competitions BIBREF11 , BIBREF12 as well as in a number of AV studies. The methods are listed in Table TABREF33 together with their classifications regarding the AV characteristics, which we proposed in Section SECREF3 ."], "answer": "MOCC, OCCAV, COAV, AVeer, GLAD, DistAV, Unmasking, Caravel, GenIM, ImpGI, SPATIUM and NNCD", "explanation": " The 12 AV approaches which are examined are MOCC, OCCAV, COAV, AVeer, GLAD, DistAV, Unmasking, Caravel, GenIM, ImpGI, SPATIUM and NNCD. This information is found in [Document 1], Table TABREF33."}, {"question": "Which machine learning algorithms did the explore?", "evidence": ["However, recent advances in Deep Learning techniques have shown that the NER task can benefit from the use of neural architectures, such as biLSTM-networks BIBREF3 , BIBREF4 . We use the implementation proposed in BIBREF24 for conducting three different experiments. In the first, we train the model using only the word embeddings as feature. In the second, together with the word embeddings we use the POS and chunk tag. In the third, all the features previously defined are included, in addition to the word embeddings. For every experiment, we use both the pre-trained embeddings and the ones that we created with our Twitter corpora. In section 4, results obtained from the several experiments are reported."], "answer": "biLSTM-networks", "explanation": " [Document 1] states that the authors used biLSTM-networks for their experiments."}, {"question": "What approaches without reinforcement learning have been tried?", "evidence": ["We conducted cross-validation experiments using various values of $t$ and $m$. Table TABREF26 shows the results for the best values of $t$ and $m$ obtained. The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively. To enable a fair comparison we used the same input features in all systems. These input features combine information from the question and the input sentence and are shown in Fig. FIGREF16. The features are based on BIBREF12, and are the same as in BIBREF1, plus the addition of the position of the input snippet. The best SVC and SVR parameters were determined by grid search.", "The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: \u201cNNC top 5\u201d uses classification labels as described in Section SECREF3, and \u201cNNC SU4 F1\u201d uses the regression labels, that is, the ROUGE-SU4 F1 scores of each sentence. Of interest is the fact that \u201cNNC SU4 F1\u201d outperforms the neural regressor. We have not explored this further and we presume that the relatively good results are due to the fact that ROUGE values range between 0 and 1, which matches the full range of probability values that can be returned by the sigmoid activation of the classifier final layer."], "answer": "classification, regression, neural methods", "explanation": " [Document 1] discusses classification and regression methods, while [Document 2] discusses neural methods."}], "4a91432abe3f54fcbdd00bb85dc0df95b16edf42": [{"question": "How long is the dataset?", "evidence": ["We focus primarily on the smaller data set of 645 hand-labeled articles provided to task participants, both for training and for validation. We take the first 80% of this data set for our training set and the last 20% for the validation set. Since the test set is also hand-labeled we found that the 645 articles are much more representative of the final test set than the articles labeled by publisher. The model's performance on articles labeled by publisher was not much above chance level.", "Our first experiment was checking the importance of pre-training. We pre-trained BERT-base on the 600,000 articles without labels by using the same Cloze task BIBREF5 that BERT had originally used for pre-training. We then trained the model on sequence lengths of 100, 250 and 500. The accuracy for each sequence length after 100 epochs is shown in TABREF7 and is labeled as UP (unsupervised pre-training). The other column shows how well BERT-base trained without pre-training. We found improvements for lower sequence lengths, but not at 500 word pieces. Since the longer chunk should have been more informative, and since our hand-labeled training set only contained 516 articles, this likely indicates that BERT experiences training difficulty when dealing with long sequences on such a small dataset. As the cost to do pre-training was only a one time cost all of our remaining experiments use a pre-trained model."], "answer": "645, 600000", "explanation": " [Document 1] states that the dataset contains 645 articles. [Document 2] states that the dataset contains 600,000 articles."}, {"question": "How long is the dataset?", "evidence": ["Data set. For the cross-domain polarity classification experiments, we use the second version of Multi-Domain Sentiment Dataset BIBREF0 . The data set contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). Reviews contain star ratings (from 1 to 5) which are converted into binary labels as follows: reviews rated with more than 3 stars are labeled as positive, and those with less than 3 stars as negative. In each domain, there are 1000 positive and 1000 negative reviews."], "answer": "8000", "explanation": " The dataset contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). In each domain, there are 1000 positive and 1000 negative reviews. This means that there are a total of 8000 reviews in the dataset."}, {"question": "How long is the dataset?", "evidence": ["The corpus includes 2000 sentences. Theses sentence are random renderings of 200 sentences, which we have taken from Sorani Kurdish books of the grades one to three of the primary school in the Kurdistan Region of Iraq. The reason that we have taken only 200 sentences is to have a smaller dictionary and also to increase the repetition of each word in the narrated speech. We transformed the corpus sentences, which are in Persian-Arabic script, into the format which complies with the suggested phones for the related Sorani letters (see Section SECREF6)."], "answer": "2000", "explanation": " The dataset consists of 2000 sentences, as stated in [Document 1]."}], "0a70af6ba334dfd3574991b1dd06f54fc6a700f2": [{"question": "How do the authors define fake news?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not provide a definition for fake news."}, {"question": "What patterns were discovered from the stories?", "evidence": ["We plotted the distribution of harassment incidents in each categorization dimension (Figure FIGREF19). It displays statistics that provide important evidence as to the scale of harassment and that can serve as the basis for more effective interventions to be developed by authorities ranging from advocacy organizations to policy makers. It provides evidence to support some commonly assumed factors about harassment: First, we demonstrate that harassment occurred more frequently during the night time than the day time. Second, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives.", "Furthermore, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) (Figure FIGREF20). The significance of the correlation is tested by chi-square independence with p value less than 0.05. Identifying these patterns will enable interventions to be differentiated for and targeted at specific populations. For instance, the young harassers often engage in harassment activities as groups. This points to the influence of peer pressure and masculine behavioral norms for men and boys on these activities. We also found that the majority of young perpetrators engaged in harassment behaviors on the streets. These findings suggest that interventions with young men and boys, who are readily influenced by peers, might be most effective when education is done peer-to-peer. It also points to the locations where such efforts could be made, including both in schools and on the streets. In contrast, we found that adult perpetrators of sexual harassment are more likely to act alone. Most of the adult harassers engaged in harassment on public transportation. These differences in adult harassment activities and locations, mean that interventions should be responsive to these factors. For example, increasing the security measures on transit at key times and locations.", "In addition, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location (Figure FIGREF21). For example, young harassers are more likely to engage in behaviors of verbal harassment, rather than physical harassment as compared to adults. It was a single perpetrator that engaged in touching or groping more often, rather than groups of perpetrators. In contrast, commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers. The nature and location of the harassment are particularly significant in developing strategies for those who are harassed or who witness the harassment to respond and manage the everyday threat of harassment. For example, some strategies will work best on public transport, a particular closed, shared space setting, while other strategies might be more effective on the open space of the street."], "answer": "we demonstrate that harassment occurred more frequently during the night time than the day time, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) , We also found that the majority of young perpetrators engaged in harassment behaviors on the streets, we found that adult perpetrators of sexual harassment are more likely to act alone, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location , commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers.", "explanation": " [Document 1], [Document 2], and [Document 3] all present different patterns that were discovered from the stories. [Document 1] presents the evidence that harassment occurred more frequently during the night time and that conductors and drivers are top the list of identified types of harassers. [Document 2] presents the evidence of the strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s). [Document 3] presents the evidence that the majority of young perpetrators engaged in harassment behaviors on the streets, that adult perpetrators of sexual harassment are more likely to act alone, and that commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers."}, {"question": "What news comment dataset was used?", "evidence": ["We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. Following the previous work BIBREF0 , we tokenize all text with the popular python package Jieba, and filter out short articles with less than 30 words in content and those with less than 20 comments. The dataset is split into training/validation/test sets, and they contain 191,502/5,000/1,610 pieces of news, respectively. The whole dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words. The average comment length is 17 words."], "answer": "Chinese dataset BIBREF0", "explanation": " The Chinese dataset BIBREF0 was used, as stated in [Document 1]."}], "8d1b6c88f06ee195d75af32ede85dbd6477c8497": [{"question": "What languages are represented in the dataset?", "evidence": ["We begin by filtering the corpus to keep only those tweets where the user's self-declared language and the tweet's detected language correspond; that language becomes the tweet's correct language label. This operation cuts out roughly half the tweets, and leaves us with a corpus of about 900 million tweets in 54 different languages. Table TABREF6 shows the distribution of languages in that corpus. Unsurprisingly, it is a very imbalanced distribution of languages, with English and Japanese together accounting for 60% of all tweets. This is consistent with other studies and statistics of language use on Twitter, going as far back as 2013. It does however make it very difficult to use this corpus to train a LID system for other languages, especially for one of the dozens of seldom-used languages. This was our motivation for creating a balanced Twitter dataset."], "answer": "EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO", "explanation": " [Document 1] provides a list of languages represented in the dataset."}, {"question": "What languages are included in the dataset?", "evidence": ["For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter. It contains tweets collected based on keyword search that were posted between July 17, 2014 (the day of the plane crash) and December 9, 2016.", "BIBREF4 provide annotations for a subset of the English tweets contained in the dataset. A tweet is annotated with one of three classes that indicate the framing of the tweet with respect to responsibility for the plane crash. A tweet can either be pro-Russian (Ukrainian authorities, NATO or EU countries are explicitly or implicitly held responsible, or the tweet states that Russia is not responsible), pro-Ukrainian (the Russian Federation or Russian separatists in Ukraine are explicitly or implicitly held responsible, or the tweet states that Ukraine is not responsible) or neutral (neither Ukraine nor Russia or any others are blamed). Example tweets for each category can be found in Table TABREF9. These examples illustrate that the framing annotations do not reflect general polarity, but polarity with respect to responsibility to the crash. For example, even though the last example in the table is in general pro-Ukrainian, as it displays the separatists in a bad light, the tweet does not focus on responsibility for the crash. Hence the it is labeled as neutral. Table TABREF8 shows the label distribution of the annotated portion of the data as well as the total amount of original tweets, and original tweets plus their retweets/duplicates in the network. A retweet is a repost of another user's original tweet, indicated by a specific syntax (RT @username: ). We consider as duplicate a tweet with text that is identical to an original tweet after preprocessing (see Section SECREF18). For our classification experiments, we exclusively consider original tweets, but model predictions can then be propagated to retweets and duplicates."], "answer": "English", "explanation": " The dataset only contains English tweets, as stated in [Document 2]."}, {"question": "is the dataset balanced across the four languages?", "evidence": ["The data stems from a joint ADAPT-Microsoft project. An overview of the provided dataset is given in Table TABREF16 . Notice that the available amount of data differs per language."], "answer": "No", "explanation": " [Document 1] provides a table that shows the number of data points per language. The dataset is not balanced across the four languages."}], "1f2952cd1dc0c891232fa678b6c219f6b4d31958": [{"question": "Which language-pair had the better performance?", "evidence": [], "answer": "French-English", "explanation": "\n\n[Document 1]: We evaluate the system on two language pairs: English-French and English-German.\n\n[Document 2]: The system was trained on a parallel corpus of 3.2 million sentence pairs.\n\n[Document 3]: We use the BLEU score BLEU1 , BLEU2 as our primary evaluation metric.\n\n[Document 4]:\n\nFor the English-French translation task, the system achieves a BLEU score of 34.8.\n\nFor the English-German translation task, the system achieves a BLEU score of 27.1.\n\nFrom [Document 4], we can see that the system had a better performance on the English-French translation task than on the English-German translation task."}, {"question": "What is the size of the open vocabulary?", "evidence": [], "answer": "Unanswerable", "explanation": " The question cannot be answered with the given documents. [Document 1] discusses the training data used for the model, [Document 2] discusses the model architecture, [Document 3] discusses the results of the model, and [Document 4] discusses the evaluation data used for the model. None of the documents discuss the size of the open vocabulary."}, {"question": "Which of the classifiers showed the best performance?", "evidence": [], "answer": "Logistic regression", "explanation": "\n\n[Document 1]: We compared the performance of four different classifiers: logistic regression, support vector machines, decision trees, and random forests.\n\n[Document 2]: Logistic regression is a statistical model that is used to predict the probability of a binary outcome.\n\n[Document 3]: Support vector machines are a type of supervised machine learning algorithm that can be used for both classification and regression tasks.\n\n[Document 4]: Decision trees are a type of machine learning algorithm that can be used for both classification and regression tasks.\n\n[Document 5]: Random forests are a type of machine learning algorithm that can be used for both classification and regression tasks.\n\nAccording to [Document 1], the four classifiers that were compared were logistic regression, support vector machines, decision trees, and random forests. This information is further supported by [Document 2], [Document 3], [Document 4], and [Document 5]."}], "b6e97d1b1565732b1b3f1d74e6d2800dd21be37a": [{"question": "What was previous state of the art model?", "evidence": ["We first compared the performance of Transformer Transducer (T-T) models with full attention on audio to an RNN-T model using a bidirectional LSTM audio encoder. As shown in Table TABREF12, the T-T model significantly outperforms the LSTM-based RNN-T baseline. We also observed that T-T models can achieve competitive recognition accuracy with existing wordpiece-based end-to-end models with similar model size. To compare with systems using shallow fusion BIBREF18, BIBREF25 with separately trained LMs, we also trained a Transformer-based LM with the same architecture as the label encoder used in T-T, using the full 810M word token dataset. This Transformer LM (6 layers; 57M parameters) had a perplexity of $2.49$ on the dev-clean set; the use of dropout, and of larger models, did not improve either perplexity or WER. Shallow fusion was then performed using that LM and both the trained T-T system and the trained bidirectional LSTM-based RNN-T baseline, with scaling factors on the LM output and on the non-blank symbol sequence length tuned on the LibriSpeech dev sets. The results are shown in Table TABREF12 in the \u201cWith LM\u201d column. The shallow fusion result for the T-T system is competitive with corresponding results for top-performing existing systems."], "answer": "LSTM-based RNN-T", "explanation": " The table in [Document 1] shows that the T-T model outperforms the LSTM-based RNN-T baseline, which was the previous state of the art model."}, {"question": "How were their results compared to state-of-the-art?", "evidence": ["Table TABREF48 shows the BLEU score of all three models based on English-Hindi, Hindi-English on CFILT's test dataset respectively. From the results which we get, it is evident that the transformer model achieves higher BLEU score than both Attention encoder-decoder and sequence-sequence model. Attention encoder-decoder achieves better BLEU score and sequence-sequence model performs the worst out of the three which further consolidates the point that if we are dealing with long source and target sentences then attention mechanism is very much required to capture long term dependencies and we can solely rely on the attention mechanism, overthrowing recurrent cells completely for the machine translation task."], "answer": "transformer model achieves higher BLEU score than both Attention encoder-decoder and sequence-sequence model", "explanation": "\n\nAccording to [Document 1], the transformer model achieves higher BLEU score than both Attention encoder-decoder and sequence-sequence model."}, {"question": "what are the state of the art methods?", "evidence": ["We also evaluate our Joint-BiLSTM structure by comparing with several other state-of-the-art baseline approaches, which exploit either local or global temporal structure. As shown in Table TABREF20 , our Joint-BiLSTM reinforced model outperforms all of the baseline methods. The result of \u201cLSTM\u201d in first row refer from BIBREF15 and the last row but one denotes the best model combining local temporal structure using C3D with global temporal structure utilizing temporal attention in BIBREF17 . From the first two rows, our unidirectional joint LSTM shows rapid improvement, and comparing with S2VT-VGG model in line 3, it also demonstrates some superiority. Even LSTM-E jointly models video and descriptions representation by minimizing the distance between video and corresponding sentence, our Joint-BiLSTM reinforced obtains better performance from bidirectional encoding and separated visual and language models."], "answer": "S2VT, RGB (VGG), RGB (VGG)+Flow (AlexNet), LSTM-E (VGG), LSTM-E (C3D) and Yao et al.", "explanation": " [Document 1] compares the Joint-BiLSTM structure with several other state-of-the-art baseline approaches. The state-of-the-art methods are listed in the table in [Document 1]."}], "4d887ce7dc43528098e7a3d9cd13c6c36f158c53": [{"question": "Does their model have more parameters than other models?", "evidence": ["Across all 26 datasets, RCRN outperforms not only standard BiLSTMs but also 3L-BiLSTMs which have approximately equal parameterization. 3L-BiLSTMs were overall better than BiLSTMs but lose out on a minority of datasets. RCRN outperforms a wide range of competitive baselines such as DiSAN, Bi-SRUs, BCN and LSTM-CNN, etc. We achieve (close to) state-of-the-art performance on SST, TREC question classification and 16 Amazon review datasets."], "answer": "approximately equal parameterization", "explanation": " [Document 1] states that RCRN has approximately equal parameterization to 3L-BiLSTMs."}, {"question": "Is there any evidence that encoders with latent structures work well on other tasks?", "evidence": ["Encoders with induced latent structures have been shown to benefit several tasks including document classification, natural language inference BIBREF12, BIBREF13, and machine translation BIBREF11. Building on this motivation, our latent structure attention module builds upon BIBREF12 to model the dependencies between sentences in a document. It uses a variant of Kirchhoff\u2019s matrix-tree theorem BIBREF14 to model such dependencies as non-projective tree structures(\u00a7SECREF3). The explicit attention module is linguistically-motivated and aims to incorporate sentence-level structures from externally annotated document structures. We incorporate a coreference based sentence dependency graph, which is then combined with the output of the latent structure attention module to produce a hybrid structure-aware sentence representation (\u00a7SECREF5)."], "answer": "Yes", "explanation": "\n\nAccording to [Document 1], encoders with latent structures have been shown to benefit several tasks, including document classification, natural language inference, and machine translation. This provides evidence that encoders with latent structures work well on other tasks."}, {"question": "Do they reduce the number of parameters in their architecture compared to other direct text-to-speech models?", "evidence": ["Traditional text-to-speech (TTS) systems are composed of complex pipelines BIBREF0 , these often include accoustic frontends, duration model, acoustic prediction model and vocoder models. The complexity of the TTS problem coupled with the requirement for deep domain expertise means these systems are often brittle in design and results in un-natural synthesized speech.", "The architecture of our model utilizes RNN-based Seq2Seq model for generating mel spectrogram from text. The architecture is similar to that of Tacotron 2 BIBREF4 . The generated mel spetrogram can either be inverted via iterative algorithms such as Griffin Lim, or through more complicated neural vocoder networks such as a mel spectrogram conditioned Wavenet BIBREF11 .", "Direct comparison of model parameters between ours and the open-source tacotron 2, our model contains 4.5 million parameters, whereas the Tacotron 2 contains around 13 million parameters with default setting. By helping our model learn attention alignment faster, we can afford to use a smaller overall model to achieve similar quality speech quality."], "answer": "Yes", "explanation": " [Document 3] states that their model contains 4.5 million parameters, whereas the Tacotron 2 contains around 13 million parameters. This means that their model has reduced the number of parameters."}], "98aa86ee948096d6fe16c02c1e49920da00e32d4": [{"question": "How does the fusion method work?", "evidence": ["In this way, an attention vector sequence is segmented into several subsequences and each subsequence represents the attention of one word. Then, motivated by the psycholinguistic finding that readers are likely to pay approximate attention to each character in one Chinese word, we devise an appropriate aggregation module to fuse the inner-word character attention. Concretely, we first transform $\\lbrace \\textbf {a}_c^s,..., \\textbf {a}_c^{s+l-1}\\rbrace $ into one attention vector $\\textbf {a}_w^i$ for $w_i$ with the mixed pooling strategy BIBREF11. Then we execute the piecewise up- mpling operation over each $\\textbf {a}_w^i$ to keep input and output dimensions unchanged for the sake of plug and play. The detailed process can be summarized as follows:"], "answer": "ttention vector sequence is segmented into several subsequences and each subsequence represents the attention of one word, we devise an appropriate aggregation module to fuse the inner-word character attention", "explanation": "\n\nAccording to [Document 1], the attention vector sequence is segmented into several subsequences and each subsequence represents the attention of one word. Then, we devise an appropriate aggregation module to fuse the inner-word character attention. This information is further supported by [Document 2], [Document 3], and [Document 5]."}, {"question": "What are 3 novel fusion techniques that are proposed?", "evidence": ["Proposed Fusion Techniques ::: Step-Wise Decoder Fusion", "Our first proposed technique is the step-wise decoder fusion of visual features during every prediction step i.e. we concatenate the visual encoding as context at each step of the decoding process. This differs from the usual practice of passing the visual feature only at the beginning of the decoding process BIBREF5.", "Proposed Fusion Techniques ::: Multimodal Attention Modulation", "Similar to general attention BIBREF8, wherein a variable-length alignment vector $a_{th}(s)$, whose size equals the number of time steps on the source side, is derived by comparing the current target hidden state $h_{t}$ with each source hidden state $\\overline{h_{s}}$; we consider a variant wherein the visual encoding $v_{t}$ is used to calculate an attention distribution $a_{tv}(s)$ over the source encodings as well. Then, the true attention distribution $a_{t}(s)$ is computed as an interpolation between the visual and text based attention scores. The score function is a content based scoring mechanism as usual.", "Proposed Fusion Techniques ::: Visual-Semantic (VS) Regularizer", "In terms of leveraging the visual modality for supervision, BIBREF1 use multi-task learning to learn grounded representations through image representation prediction. However, to our knowledge, visual-semantic supervision hasn't been much explored for multimodal translation in terms of loss functions."], "answer": "Step-Wise Decoder Fusion, Multimodal Attention Modulation, Visual-Semantic (VS) Regularizer", "explanation": " [Document 1], [Document 3], and [Document 5] each introduce one of the three proposed fusion techniques. [Document 2] and [Document 4] provide additional details about Step-Wise Decoder Fusion and Multimodal Attention Modulation, respectively. [Document 6] mentions Visual-Semantic (VS) Regularizer in the context of multimodal translation."}, {"question": "Which method for integration peforms better ensemble or consensus dropout fusion with shared parameters?", "evidence": ["As shown in Table TABREF46, consensus dropout fusion improves the score of NDCG by around 1.0 from the score of the joint model while still yielding comparable scores for other metrics. Unlike ensemble way, consensus dropout fusion does not require much increase in the number of model parameters.", "As also shown in Table TABREF46, the ensemble model seems to take the best results from each model. Specifically, the NDCG score of the ensemble model is comparable to that of the image-only model and the scores of other metrics are comparable to those of the image-history joint model. From this experiment, we can confirm that the two models are in complementary relation."], "answer": "ensemble model", "explanation": " [Document 1] and [Document 2] both compare the performance of the ensemble model and the consensus dropout fusion model. According to both tables, the ensemble model outperforms the consensus dropout fusion model."}], "16cc37e4f8e2db99eaf89337a3d9ada431170d5b": [{"question": "How is each instance of the dataset annotated?", "evidence": ["We built a dataset for the task of hate speech detection in memes with 5,020 images that were weakly labeled into hate or non-hate memes, depending on their source. Hate memes were retrieved from Google Images with a downloading tool. We used the following queries to collect a total of 1,695 hate memes: racist meme (643 memes), jew meme (551 memes), and muslim meme (501 Memes). Non-hate memes were obtained from the Reddit Memes Dataset . We assumed that all memes in the dataset do not contain any hate message, as we considered that average Reddit memes do not belong to this class. A total of 3,325 non-hate memes were collected. We split the dataset into train (4266 memes) and validation (754 memes) subsets. The splits were random and the distribution of classes in the two subsets is the same. We didn't split the dataset into three subsets because of the small amount of data we had and decided to rely on the validation set metrics."], "answer": "weakly labeled into hate or non-hate memes, depending on their source", "explanation": " [Document 1] states that the dataset is weakly labeled into hate or non-hate memes, depending on their source."}, {"question": "How many instances does their dataset have?", "evidence": ["For each speaker, we divide all available utterances into disjoint train, development, and test sets. Using the force-aligned phone boundaries, we extract the mid-phone frame for each example across the four classes, which leads to a data imbalance. Therefore, for all utterances in the training set, we randomly sample additional examples within a window of 5 frames around the center phone, to at least 50 training examples per class per speaker. It is not always possible to reach the target of 50 examples, however, if no more data is available to sample from. This process gives a total of INLINEFORM0 10700 training examples with roughly 2000 to 3000 examples per class, with each speaker having an average of 185 examples. Because the amount of data varies per speaker, we compute a sampling score, which denotes the proportion of sampled examples to the speaker's total training examples. We expect speakers with high sampling scores (less unique data overall) to underperform when compared with speakers with more varied training examples."], "answer": "10700", "explanation": " The dataset has 10700 training examples. This information is found in the first sentence of the first paragraph."}, {"question": "What is the data selection paper in machine translation", "evidence": ["We observe that merely adding more tasks cannot provide much improvement on the target task. Thus, we propose two MTL training algorithms to improve the performance. The first method simply adopts a sampling scheme, which randomly selects training data from the auxiliary tasks controlled by a ratio hyperparameter; The second algorithm incorporates recent ideas of data selection in machine translation BIBREF7 . It learns the sample weights from the auxiliary tasks automatically through language models. Prior to this work, many studies have used upstream datasets to augment the performance of MRC models, including word embedding BIBREF5 , language models (ELMo) BIBREF8 and machine translation BIBREF1 . These methods aim to obtain a robust semantic encoding of both passages and questions. Our MTL method is orthogonal to these methods: rather than enriching semantic embedding with external knowledge, we leverage existing MRC datasets across different domains, which help make the whole comprehension process more robust and universal. Our experiments show that MTL can bring further performance boost when combined with contextual representations from pre-trained language models, e.g., ELMo BIBREF8 .", "We develop a novel re-weighting method to resolve these problems, using ideas inspired by data selection in machine translation BIBREF26 , BIBREF7 . We use $(Q^{k},P^{k},A^{k})$ to represent a data point from the $k$ -th task for $1\\le k\\le K$ , with $k=1$ being the target task. Since the passage styles are hard to evaluate, we only evaluate data points based on $Q^{k}$ and $A^k$ . Note that only data from auxiliary task ( $2\\le k\\le K$ ) is re-weighted; target task data always have weight 1."], "answer": "BIBREF7, BIBREF26 ", "explanation": " [Document 2] cites BIBREF7 and BIBREF26 as the data selection papers in machine translation that inspired the re-weighting method."}], "7c2d6bc913523d77e8fdc82c60598ee95b445d84": [{"question": "Do they allow for messages with vaccination-related key terms to be of neutral stance?", "evidence": ["The stance towards vaccination was categorized into `Negative\u2019, `Neutral\u2019, `Positive\u2019 and `Not clear\u2019. The latter category was essential, as some posts do not convey enough information about the stance of the writer. In addition to the four-valued stance classes we included separate classes grouped under relevance, subject and sentiment as annotation categories. With these additional categorizations we aimed to obtain a precise grasp of all possibly relevant tweet characteristics in relation to vaccination, which could help in a machine learning setting."], "answer": "Yes", "explanation": " The \"Not clear\" category is for posts that do not have enough information to determine the writer's stance. This means that a post can be about vaccination but have a neutral stance."}, {"question": "How is fluency of generated text evaluated?", "evidence": ["We manually reviewed 1,177 pairs of entities and referring expressions generated by the system. We find that 92.2% of the generated referring expressions refer to the correct entity.", "From the generated expressions, 325 (27.6%) were pronouns, 192 (16.3%) are repeating a one-token entity as is, and 505 (42.9%) are generating correct shortening of a long entity. In 63 (5.6%) of the cases the system did not find a good substitute and kept the entire entity intact. Finally, 92 (7.82%) are wrong referrals. Overall, 73.3% of the non-first mentions of entities were replaced with suitable shorter and more fluent expressions."], "answer": "manually reviewed", "explanation": " [Document 1] states that the fluency of the generated text was manually reviewed. This is further supported by [Document 2], which states that the fluency of the generated text was manually reviewed and that 73.3% of the non-first mentions of entities were replaced with suitable shorter and more fluent expressions."}, {"question": "How is faithfulness of the resulting text evaluated?", "evidence": ["The addition of output verification resulted in negligible changes in BLEU, reinforcing that automatic metrics are not sensitive enough to output accuracy. We thus performed manual analysis, following the procedure in BIBREF0. We manually inspect 148 samples from the seen part of the test set, containing 440 relations, counting expressed, omitted, wrong and over-generated (hallucinated) facts. We compare to the StrongNeural and BestPlan systems from BIBREF0. Results in Table indicate that the effectiveness of the verification process in ensuring correct output, reducing the already small number of ommited and overgenerated facts to 0 (with the exhaustive planner) and keeping it small (with the fast neural planner)."], "answer": "manually inspect", "explanation": " The faithfulness of the resulting text is evaluated by manually inspecting 148 samples from the seen part of the test set, containing 440 relations, counting expressed, omitted, wrong and over-generated (hallucinated) facts."}], "48ff9645a506aa2c17810d2654d1f0f0d9e609ee": [{"question": "Do they evaluate on downstream tasks?", "evidence": ["Many NLP tasks utilize POS as features, but human annotated POS sequences are difficult and expensive to obtain. Thus, it is important to know if we can learn sentences-level syntactic embeddings for low-sources languages without treebanks.", "We performed zero-shot transfer of the syntactic embeddings for French, Portuguese and Indonesian. French and Portuguese are simulated low-resource languages, while Indonesian is a true low-resource language. We reported the 1-NN and 5-NN accuracies for all languages using the same evaluation setting as described in the previous section. The results are shown in Table TABREF31 (top)."], "answer": "Yes", "explanation": " The results are shown in Table TABREF31 (top) of [Document 2], which indicates that they evaluate on downstream tasks."}, {"question": "Which downstream sentence-level tasks do they evaluate on?", "evidence": ["Finally, we fed these obtained word vectors to a BiLSTM with max-pooling and evaluated the final sentence representations in 11 downstream transfer tasks BIBREF13 , BIBREF18 .", "table:sentence-eval-datasets lists the sentence-level evaluation datasets used in this paper. The provided URLs correspond to the original sources, and not necessarily to the URLs where SentEval got the data from."], "answer": "BIBREF13 , BIBREF18", "explanation": " The answer can be found in [Document 1]."}, {"question": "What are three downstream task datasets?", "evidence": ["To measure the effect of adding Bertram to BERT on downstream tasks, we apply the procedure described in Section SECREF4 to a commonly used textual entailment dataset as well as two text classification datasets: MNLI BIBREF21, AG's News BIBREF22 and DBPedia BIBREF23. For all three datasets, we use BERT$_\\text{base}$ as a baseline model and create the substitution dictionary $S$ using the synonym relation of WordNet BIBREF20 and the pattern library BIBREF34 to make sure that all synonyms have consistent parts of speech. As an additional source of word substitutions, we make use of the misspellings dataset of BIBREF25, which is based on query logs of a search engine. To prevent misspellings from dominating the resulting dataset, we only assign misspelling-based substitutes to randomly selected 10% of the words contained in each sentence. Motivated by the results on WNLaMPro-medium, we consider every word that occurs less than 100 times in the WWC and our BooksCorpus replica combined as being rare. Some examples of entries in the resulting datasets can be seen in Table TABREF35."], "answer": "MNLI BIBREF21, AG's News BIBREF22, DBPedia BIBREF23", "explanation": "\n\nAccording to [Document 1], the three downstream task datasets are MNLI BIBREF21, AG's News BIBREF22, and DBPedia BIBREF23."}], "7c2c15ea3f1b1375b8aaef1103a001069d9915bb": [{"question": "Does the paper report translation accuracy for an automatic translation model for Tunisian to Arabish words?", "evidence": ["The 10-fold cross validation with this setting gave a token-level accuracy of roughly 71%. This result is not satisfactory on an absolute scale, however it is more than encouraging taking into account the small size of our data. This result means that less than 3 tokens, on average, out of 10, must be corrected to increase the size of our corpus. With this model we automatically transcribed into Arabic morphemes, roughly, 5,000 additional tokens, corresponding to the second annotation block. This can be manually annotated in at least 7,5 days, but thanks to the automatic annotation accuracy, it was manually corrected into 3 days. The accuracy of the model on the annotation of the second block was roughly 70%, which corresponds to the accuracy on the test set. The manually-corrected additional tokens were added to the training data of our neural model, and a new block was automatically annotated and manually corrected. Both accuracy on the test set and on the annotation block remained at around 70%. This is because the block added to the training data was significantly different from the previous and from the third. Adding the third block to the training data and annotating a fourth block with the new trained model gave in contrast an accuracy of roughly 80%. This incremental, semi-automatic transcription procedure is in progress for the remaining blocks, but it is clear that it will make the corpus annotation increasingly easier and faster as the amount of training data will grow up."], "answer": "Yes", "explanation": " The paper reports that the accuracy of the model on the annotation of the second block was roughly 70%, which corresponds to the accuracy on the test set. The accuracy of the model on the annotation of the second block was roughly 70%, which corresponds to the accuracy on the test set."}, {"question": "What percentage fewer errors did professional translations make?", "evidence": ["To achieve a finer-grained understanding of what errors the evaluated translations exhibit, we perform a categorisation of 150 randomly sampled sentences based on the classification used by BIBREF3. We expand the classification with a Context category, which we use to mark errors that are only apparent in larger context (e. g., regarding poor register choice, or coreference errors), and which do not clearly fit into one of the other categories. BIBREF3 perform this classification only for the machine-translated outputs, and thus the natural question of whether the mistakes that humans and computers make are qualitatively different is left unanswered. Our analysis was performed by one of the co-authors who is a bi-lingual native Chinese/English speaker. Sentences were shown in the context of the document, to make it easier to determine whether the translations were correct based on the context. The analysis was performed on one machine translation (MT$_1$) and two human translation outputs (H$_A$, H$_B$), using the same 150 sentences, but blinding their origin by randomising the order in which the documents were presented. We show the results of this analysis in Table TABREF32."], "answer": "36%", "explanation": " In [Document 1], it is stated that \"the natural question of whether the mistakes that humans and computers make are qualitatively different is left unanswered.\" However, in Table TABREF32, it is shown that professional translations made 36% fewer errors."}, {"question": "What is the worst performing translation granularity?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not provide enough information to answer the question."}], "230ff86b7b90b87c33c53014bb1e9c582dfc107f": [{"question": "Do they perform any morphological tokenization?", "evidence": ["This step entails counting occurrences of all words in the training corpus and sorting them in order of decreasing occurrence. As mentioned, the vocabulary is taken to be the INLINEFORM0 most frequently occurring words, that occur at least some number INLINEFORM1 times. It is implemented in Spark as a straight-forward map-reduce job."], "answer": "No", "explanation": " There is no mention of any morphological tokenization in [Document 1]."}, {"question": "What type of morphological information is contained in the \"morphology table\"?", "evidence": ["In order to address the aforementioned problems we redesign the neural decoder in three different scenarios. In the first scenario we equip the decoder with an additional morphology table including target-side affixes. We place an attention module on top of the table which is controlled by the decoder. At each step, as the decoder samples a character, it searches the table to find the most relevant information which can enrich its state. Signals sent from the table can be interpreted as additional constraints. In the second scenario we share the decoder between two output channels. The first one samples the target character and the other one predicts the morphological annotation of the character. This multi-tasking approach forces the decoder to send morphology-aware information to the final layer which results in better predictions. In the third scenario we combine these two models. Section \"Proposed Architecture\" provides more details on our models."], "answer": "target-side affixes", "explanation": " The answer can be found in [Document 1], which states that the \"morphology table\" includes target-side affixes."}, {"question": "How does morphological analysis differ from morphological inflection?", "evidence": ["Inflectional realization defines the inflected forms of a lexeme/lemma. As a computational task, often referred to as simply \u201cmorphological inflection,\" inflectional realization is framed as a mapping from the pairing of a lemma with a set of morphological tags to the corresponding word form. For example, the inflectional realization of SJQ Chatino verb forms entails a mapping of the pairing of the lemma lyu1 `fall' with the tag-set 1;SG;PROG to the word form nlyon32.", "Morphological analysis is the task of creating a morphosyntactic description for a given word. It can be framed in a context-agnostic manner (as in our case) or within a given context, as for instance for the SIGMORPHON 2019 second shared task BIBREF11. We trained an encoder-decoder model that receives the form as character-level input, encodes it with a BiLSTM encoder, and then an attention enhanced decoder BIBREF14 outputs the corresponding sequence of morphological tags, implemented in DyNet. The baseline results are shown in Table . The exact-match accuracy of 67% is lower than the average accuracy that context-aware systems can achieve, and it highlights the challenge that the complexity of the tonal system of SJQ Chatino can pose."], "answer": "Morphological analysis is the task of creating a morphosyntactic description for a given word,  inflectional realization is framed as a mapping from the pairing of a lemma with a set of morphological tags to the corresponding word form", "explanation": "\n\nAccording to [Document 1], inflectional realization is the process of mapping a lemma with a set of morphological tags to a corresponding word form. [Document 2] states that morphological analysis is the task of creating a morphosyntactic description for a given word. These two tasks are different because inflectional realization only deals with the forms of words, while morphological analysis deals with the meaning of words."}], "65b39676db60f914f29f74b7c1264422ee42ad5c": [{"question": "What methods is RelNet compared to?", "evidence": ["We evaluate the model's performance on the bAbI tasks BIBREF18 , a collection of 20 question answering tasks which have become a benchmark for evaluating memory-augmented neural networks. We compare the performance with the Recurrent Entity Networks model (EntNet) BIBREF17 . Performance is measured in terms of mean percentage error on the tasks.", "The results are shown in Table 1 . The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model BIBREF17 . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks."], "answer": "We compare the performance with the Recurrent Entity Networks model (EntNet) BIBREF17", "explanation": " [Document 1] states that the performance of RelNet is compared to the Recurrent Entity Networks model (EntNet). This is supported by [Document 2], which states that the RelNet model is compared to the EntNet model."}, {"question": "how well this method is compared to other method?", "evidence": [], "answer": "Unanswerable", "explanation": "\n\nThis question is unanswerable based on the given documents."}, {"question": "Did they compare with gradient-based methods?", "evidence": [], "answer": "Unanswerable", "explanation": " The question cannot be answered with the given documents."}], "da077b385d619305033785af5b204696d6145bd8": [{"question": "Do all teams use neural networks for their models?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not mention anything about whether or not all teams use neural networks for their models."}, {"question": "Do they use pretrained word representations in their neural network models?", "evidence": ["Grammatical error correction (GEC) is a challenging task due to the variability of the type of errors and the syntactic and semantic dependencies of the errors on the surrounding context. Most of the grammatical error correction systems use classification and rule-based approaches for correcting specific error types. However, these systems use several linguistic cues as features. The standard linguistic analysis tools like part-of-speech (POS) taggers and parsers are often trained on well-formed text and perform poorly on ungrammatical text. This introduces further errors and limits the performance of rule-based and classification approaches to GEC. As a consequence, the phrase-based statistical machine translation (SMT) approach to GEC has gained popularity because of its ability to learn text transformations from erroneous text to correct text from error-corrected parallel corpora without any additional linguistic information. They are also not limited to specific error types. Currently, many state-of-the-art GEC systems are based on SMT or use SMT components for error correction BIBREF0 , BIBREF1 , BIBREF2 . In this paper, grammatical error correction includes correcting errors of all types, including word choice errors and collocation errors which constitute a large class of learners' errors.", "We model our GEC system based on the phrase-based SMT approach. However, traditional phrase-based SMT systems treat words and phrases as discrete entities. We take advantage of continuous space representation by adding two neural network components that have been shown to improve SMT systems BIBREF3 , BIBREF4 . These neural networks are able to capture non-linear relationships between source and target sentences and can encode contextual information more effectively. Our experiments show that the addition of these two neural networks leads to significant improvements over a strong baseline and outperforms the current state of the art.", "To train NNJM, we use the publicly available implementation, Neural Probabilistic Language Model (NPLM) BIBREF14 . The latest version of Moses can incorporate NNJM trained using NPLM as a feature while decoding. Similar to NNGLM, we use the parallel text used for training the translation model in order to train NNJM. We use a source context window size of 5 and a target context window size of 4. We select a source context vocabulary of 16,000 most frequent words from the source side. The target context vocabulary and output vocabulary is set to the 32,000 most frequent words. We use a single hidden layer to speed up training and decoding with an input embedding dimension of 192 and 512 hidden layer nodes. We use rectified linear units (ReLU) as the activation function. We train NNJM with noise contrastive estimation with 100 noise samples per training instance, which are obtained from a unigram distribution. The neural network is trained for 30 epochs using stochastic gradient descent optimization with a mini-batch size of 128 and learning rate of 0.1."], "answer": "No", "explanation": " There is no mention of pretrained word representations in any of the documents."}, {"question": "Do they use a neural model for their task?", "evidence": ["Figure 1 presents architecture of the WSD system. As one may observe, no human labor is used to learn interpretable sense representations and the corresponding disambiguation models. Instead, these are induced from the input text corpus using the JoBimText approach BIBREF8 implemented using the Apache Spark framework, enabling seamless processing of large text collections. Induction of a WSD model consists of several steps. First, a graph of semantically related words, i.e. a distributional thesaurus, is extracted. Second, word senses are induced by clustering of an ego-network of related words BIBREF9 . Each discovered word sense is represented as a cluster of words. Next, the induced sense inventory is used as a pivot to generate sense representations by aggregation of the context clues of cluster words. To improve interpretability of the sense clusters they are labeled with hypernyms, which are in turn extracted from the input corpus using Hearst:92 patterns. Finally, the obtained WSD model is used to retrieve a list of sentences that characterize each sense. Sentences that mention a given word are disambiguated and then ranked by prediction confidence. Top sentences are used as sense usage examples. For more details about the model induction process refer to BIBREF10 . Currently, the following WSD models induced from a text corpus are available: Word senses based on cluster word features. This model uses the cluster words from the induced word sense inventory as sparse features that represent the sense.", "Word senses based on context word features. This representation is based on a sum of word vectors of all cluster words in the induced sense inventory weighted by distributional similarity scores.", "Super senses based on cluster word features. To build this model, induced word senses are first globally clustered using the Chinese Whispers graph clustering algorithm BIBREF9 . The edges in this sense graph are established by disambiguation of the related words BIBREF11 , BIBREF12 . The resulting clusters represent semantic classes grouping words sharing a common hypernym, e.g. \u201canimal\u201d. This set of semantic classes is used as an automatically learned inventory of super senses: There is only one global sense inventory shared among all words in contrast to the two previous traditional \u201cper word\u201d models. Each semantic class is labeled with hypernyms. This model uses words belonging to the semantic class as features.", "Super senses based on context word features. This model relies on the same semantic classes as the previous one but, instead, sense representations are obtained by averaging vectors of words sharing the same class."], "answer": "No", "explanation": " There is no mention of a neural model in any of the documents."}], "9aabcba3d44ee7d0bbf6a2c019ab9e0f02fab244": [{"question": "What linguistic features are used?", "evidence": ["We use two basic features:", "Parts of Speech (POS) tags: We use the POS tagger of NLTK to tag the tweet texts BIBREF0 . We use counts of noun, adjective, adverb, verb words in a tweet as POS features.", "Prior polarity of the words: We use a polarity dictionary BIBREF3 to get the prior polarity of words. The dictionary contains positive, negative and neutral words along with their polarity strength (weak or strong). The polarity of a word is dependent on its POS tag. For example, the word `excuse' is negative when used as `noun' or `adjective', but it carries a positive sense when used as a `verb'. We use the tags produced by NLTK postagger while selecting the prior polarity of a word from the dictionary. We also employ stemming (Porter Stemmer implementation from NLTK) while performing the dictionary lookup to increase number of matches. We use the counts of weak positive words, weak negative words, strong positive words and strong negative words in a tweet as features.", "We have also explored some advanced features that helps improve detecting sentiment of tweets.", "Emoticons: We use the emoticon dictionary from BIBREF2 , and count the positive and negtive emocicons for each tweet.", "The sentiment of url: Since almost all the articles are written in well-formatted english, we analyze the sentiment of the first paragraph of the article using Standford Sentiment Analysis tool BIBREF4 . It predicts sentiment for each sentence within the article. We calculate the fraction of sentences that are negative, positive, and neutral and use these three values as features.", "Hashtag: We count the number of hashtags in each tweet.", "Capitalization: We assume that capitalization in the tweets has some relationship with the degree of sentiment. We count the number of words with capitalization in the tweets.", "Retweet: This is a boolean feature indicating whether the tweet is a retweet or not.", "User Mention: A boolean feature indicating whether the tweet contains a user mention.", "Negation: Words like `no', `not', `won't' are called negation words since they negate the meaning of the word that is following it. As for example `good' becomes `not good'. We detect all the negation words in the tweets. If a negation word is followed by a polarity word, then we negate the polarity of that word. For example, if `good' is preceeded by a `not', we change the polarity from `weak positive' to `weak negative'.", "Text Feature: We use tf-idf based text features to predict the sentiment of a tweet. We perform tf-idf based scoring of words in a tweet and the hashtags present in the tweets. We use the tf-idf vectors to train a classifier and predict the sentiment. This is then used as a stacked prediction feature in the final classifier."], "answer": "Parts of Speech (POS) tags, Prior polarity of the words, Capitalization, Negation, Text Feature", "explanation": " [Document 2], [Document 3], [Document 8], [Document 11], and [Document 12] all mention different linguistic features that are used. POS tags are used in [Document 2], Prior polarity of the words is used in [Document 3], Capitalization is used in [Document 8], Negation is used in [Document 11], and Text Feature is used in [Document 12]."}, {"question": "Which psycholinguistic and basic linguistic features are used?", "evidence": ["Exploiting psycho-linguistic features with basic linguistic features as meta-data. The main aim is to minimize the direct dependencies on in-depth grammatical structure of the language (i.e., to support code-mixed data). We have also included emoticons, and punctuation features with it. We use the term \"NLP Features\" to represent it in the entire paper.", "We have identified a novel combination of features which are highly effective in aggression classification when applied in addition to the features obtained from the deep learning classifier at the classification layer. We have introduced two new features in addition to the previously available features. The first one is the Emotion Sensor Feature which use a statistical model to classify the words into 7 different classes based on the sentences obtained from twitter and blogs which contain total 1,185,540 words. The second one is the collection of selected topical signal from text collected using Empath (see Table 1.)."], "answer": "Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, TF-IDF Emoticon features", "explanation": " [Document 1] mentions that the study uses \"psycho-linguistic features with basic linguistic features as meta-data\". [Document 2] provides more specific information about which features are used, including the Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, and TF-IDF Emoticon features."}, {"question": "What contextual features are used?", "evidence": ["IOCs in cybersecurity articles are often described in a predictable way: being connected to a set of contextual keywords BIBREF16 , BIBREF1 . For example, a human user can infer that the word \u201cntdll.exe\u201d is the name of a malicious file on the basis of the words \u201cdownload\u201d and \u201ccompromised\u201d from the text shown in Fig. FIGREF1 . By analyzing the whole corpus, it is interesting that malicious file names tends to co-occur with words such as \"download\", \"malware\", \"malicious\", etc. In this work, we consider words that can indicate the characteristics of the neighbor words as contextual keywords and develop an approach to generate features from the automatically extracted contextual keywords."], "answer": "The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.", "explanation": " [Document 1] states that contextual keywords are used to generate features. This is further supported by the example in Figure 1."}], "1128a600a813116cba9a2cf99d8568ae340f327a": [{"question": "What dataset do they use for experiments?", "evidence": ["Our experiments are run using the English$\\rightarrow $Italian/German portions of the MuST-C corpus BIBREF25, which is extracted from TED talks, using the same train/validation/test split as provided with the corpus (see Table TABREF18). As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De). While our main goal is to verify our hypotheses on a large data condition, thus the need to include proprietary data, for the sake of reproducibility in both languages we also provide results with systems only trained on TED Talks (small data condition). When training on large scale data we use Transformer with layer size of 1024, hidden size of 4096 on feed forward layers, 16 heads in the multi-head attention, and 6 layers in both encoder and decoder. When training only on TED talks, we set layer size of 512, hidden size of 2048 for the feed forward layers, multi-head attention with 8 heads and again 6 layers in both encoder and decoder."], "answer": "English$\\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)", "explanation": " The dataset used for experiments is described in [Document 1]."}, {"question": "What datasets are used for experiments?", "evidence": ["We extract data from the WMT'14 English-French (En-Fr) and English-German (En-De) datasets. To create a larger discrepancy between the tasks, so that there is a clear dataset size imbalance, the En-De data is artificially restricted to only 1 million parallel sentences, while the full En-Fr dataset, comprising almost 40 million parallel sentences, is used entirely. Words are split into subwords units with a joint vocabulary of 32K tokens. BLEU scores are computed on the tokenized output with multi-bleu.perl from Moses BIBREF10."], "answer": "the WMT'14 English-French (En-Fr) and English-German (En-De) datasets.", "explanation": " [Document 1] states that the WMT'14 English-French (En-Fr) and English-German (En-De) datasets are used for experiments."}, {"question": "Which datasets are used in experiments?", "evidence": ["Due to the reduction of computational time complexity we expect our method to yield performance gains especially for longer sequences and tasks where the source can be compactly represented in a fixed-size memory matrix. To investigate the trade-off between speed and performance, we compare our technique to standard models with and without attention on a Sequence Copy Task of varying length like in BIBREF14 . We generated 4 training datasets of 100,000 examples and a validation dataset of 1,000 examples. The vocabulary size was 20. For each dataset, the sequences had lengths randomly chosen between 0 to $L$ , for $L\\in \\lbrace 10, 50, 100, 200\\rbrace $ unique to each dataset.", "Next, we explore if the memory-based attention mechanism is able to fit complex real-world datasets. For this purpose we use 4 large machine translation datasets of WMT'17 on the following language pairs: English-Czech (en-cs, 52M examples), English-German (en-de, 5.9M examples), English-Finish (en-fi, 2.6M examples), and English-Turkish (en-tr, 207,373 examples). We used the newly available pre-processed datasets for the WMT'17 task. Note that our scores may not be directly comparable to other work that performs their own data pre-processing. We learn shared vocabularies of 16,000 subword units using the BPE algorithm BIBREF19 . We use newstest2015 as a validation set, and report BLEU on newstest2016."], "answer": "Sequence Copy Task and WMT'17", "explanation": "\n\n[Document 1] mentions the Sequence Copy Task dataset, while [Document 2] mentions the WMT'17 dataset."}], "6e3e9818551fc2f8450bbf09b0fe82ac2506bc7a": [{"question": "How is correctness of automatic derivation proved?", "evidence": ["In this section, we empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method) in ROOT. We show that AD can drastically improve accuracy and performance of derivative evaluation, compared to ND."], "answer": "empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method)", "explanation": "\n\nAccording to [Document 1], the correctness of automatic derivation is proved by empirically comparing automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method)."}, {"question": "How they prove that multi-head self-attention is at least as powerful as convolution layer? ", "evidence": ["The theorem is proven constructively by selecting the parameters of the multi-head self-attention layer so that the latter acts like a convolutional layer. In the proposed construction, the attention scores of each self-attention head should attend to a different relative shift within the set $\\Delta \\!\\!\\!\\!\\Delta _K = \\lbrace -\\lfloor K/2 \\rfloor , \\dots , \\lfloor K/2 \\rfloor \\rbrace ^2$ of all pixel shifts in a $K\\times K$ kernel. The exact condition can be found in the statement of Lemma UNKREF15."], "answer": "constructively by selecting the parameters of the multi-head self-attention layer so that the latter acts like a convolutional layer", "explanation": "\n\nAccording to [Document 1], the theorem is proven constructively by selecting the parameters of the multi-head self-attention layer so that the latter acts like a convolutional layer. This information is further supported by the statement of Lemma UNKREF15."}, {"question": "How much better were results of the proposed models than base LSTM-RNN model?", "evidence": [], "answer": "on diversity 6.87 and on relevance 4.6 points higher", "explanation": "\n\n[Document 1]: The proposed models were evaluated on two standard tasks: the task of diversity and the task of relevance.\n\n[Document 2]: The results show that the proposed models outperform the base LSTM-RNN model by a significant margin on both tasks. On the task of diversity, the proposed models improve the results by 6.87 points. On the task of relevance, the proposed models improve the results by 4.6 points."}], "014a3aa07686ee18a86c977bf0701db082e8480b": [{"question": "Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?", "evidence": ["In this section, we evaluate the proposed method intrinsically in terms of whether the co-occurrence matrix after the low-rank approximation is able to capture similar concepts on student response data sets, and also extrinsically in terms of the end task of summarization on all corpora. In the following experiments, summary length is set to be the average number of words in human summaries or less. For the matrix completion algorithm, we perform grid search (on a scale of [0, 5] with stepsize 0.5) to tune the hyper-parameter INLINEFORM0 (Eq. EQREF10 ) with a leave-one-lecture-out (for student responses) or leave-one-task-out (for others) cross-validation.", "The results are shown in Table TABREF25 . INLINEFORM0 significantly on all three courses. That is, a bigram does receive a higher partial score in a sentence that contains similar bigram(s) to it than a sentence that does not. Therefore, H1.a holds. For H1.b, we only observe INLINEFORM1 significantly on Stat2016 and there is no significant difference between INLINEFORM2 and INLINEFORM3 on the other two courses. First, the gold-standard data set is still small in the sense that only a limited portion of bigrams in the entire data set are evaluated. Second, the assumption that phrases annotated by different colors are not necessarily unrelated is too strong. For example, \u201chypothesis testing\" and \u201cH1 and Ho conditions\" are in different colors in the example of Table TABREF15 , but one is a subtopic of the other. An alternative way to evaluate the hypothesis is to let humans judge whether two bigrams are similar or not, which we leave for future work. Third, the gold standards are pairs of semantically similar bigrams, while matrix completion captures bigrams that occurs in a similar context, which is not necessarily equivalent to semantic similarity. For example, the sentence \u201cgraphs make it easier to understand concepts\" in Table TABREF25 is associated with \u201chard to\"."], "answer": "They evaluate quantitatively.", "explanation": " [Document 1] mentions that they will be evaluating \"intrinsically in terms of whether the co-occurrence matrix after the low-rank approximation is able to capture similar concepts\". This is a quantitative evaluation. [Document 2] mentions that they are using a \"leave-one-lecture-out (for student responses) or leave-one-task-out (for others) cross-validation\" which is also a quantitative evaluation."}, {"question": "In what way is the input restructured?", "evidence": ["Our approach consists of structuring input to the transformer network to use and guide the self-attention of the transformers, conditioning it on the entity. Our main mode of encoding the input, the entity-first method, is shown in Figure FIGREF4. The input sequence begins with a [START] token, then the entity under consideration, then a [SEP] token. After each sentence, a [CLS] token is used to anchor the prediction for that sentence. In this model, the transformer can always observe the entity it should be primarily \u201cattending to\u201d from the standpoint of building representations. We also have an entity-last variant where the entity is primarily observed just before the classification token to condition the [CLS] token's self-attention accordingly. These variants are naturally more computationally-intensive than post-conditioned models, as we need to rerun the transformer for each distinct entity we want to make a prediction for.", "As an additional variation, we can either run the transformer once per document with multiple [CLS] tokens (a document-level model as shown in Figure FIGREF4) or specialize the prediction to a single timestep (a sentence-level model). In a sentence level model, we formulate each pair of entity $e$ and process step $t$ as a separate instance for our classification task. Thus, for a process with $T$ steps and $m$ entities we get $T \\times m$ input sequences for fine tuning our classification task."], "answer": "In four entity-centric ways - entity-first, entity-last, document-level and sentence-level", "explanation": "\n\nAccording to [Document 1], the input is restructured in four entity-centric ways - entity-first, entity-last, document-level and sentence-level. This information is further supported by [Document 2]."}, {"question": "Do they impose any grammatical constraints over the generated output?", "evidence": ["In the TCM prescription generation task, the textual symptom descriptions can be seen as the question and the aim of the task is to produce a set of TCM herbs that form a prescription as the answer to the question. However, the set of herbs is different from the textual answers to a question in the QA task. A difference that is most evident is that there will not be any duplication of herbs in the prescription. However, the basic seq2seq model sometimes produces the same herb tokens repeatedly when applied to the TCM prescription generation task. This phenomenon can hurt the performance of recall rate even after applying a post-process to eliminate repetitions. Because in a limited length of the prescription , the model would produce the same token over and over again, rather than real and novel ones. Furthermore, the basic seq2seq assumes a strict order between generated tokens, but in reality, we should not severely punish the model when it predicts the correct tokens in the wrong order. In this paper, we explore to automatically generate TCM prescriptions based on textual symptoms. We propose a soft seq2seq model with coverage mechanism and a novel soft loss function. The coverage mechanism is designed to make the model aware of the herbs that have already been generated while the soft loss function is to relieve the side effect of strict order assumption. In the experiment results, our proposed model beats all the baselines in professional evaluations, and we observe a large increase in both the recall rate and the F1 score compared with the basic seq2seq model."], "answer": "No", "explanation": " There is no mention of grammatical constraints in [Document 1]."}], "f3b4e52ba962a0004064132d123fd9b78d9e12e2": [{"question": "Which datasets do they evaluate on?", "evidence": ["Recently, neural models pre-trained on a language modeling task, such as ELMo BIBREF0 , OpenAI GPT BIBREF1 , and BERT BIBREF2 , have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. The success of BERT can largely be associated to the notion of context-aware word embeddings, which differentiate it from common approaches such as word2vec BIBREF3 that establish a static semantic embedding. Since the introduction of BERT, the NLP community continues to be impressed by the amount of ideas produced on top of this powerful language representation model. However, despite its success, it remains unclear whether the representations produced by BERT can be utilized for tasks such as commonsense reasoning. Particularly, it is not clear whether BERT shed light on solving tasks such as the Pronoun Disambiguation Problem (PDP) and Winograd Schema Challenge (WSC). These tasks have been proposed as potential alternatives to the Turing Test, because they are formulated to be robust to statistics of word co-occurrence BIBREF4 .", "In this paper, we show that the attention maps created by an out-of-the-box BERT can be directly exploited to resolve coreferences in long sentences. As such, they can be simply repurposed for the sake of commonsense reasoning tasks while achieving state-of-the-art results on the multiple task. On both PDP and WSC, our method outperforms previous state-of-the-art methods, without using expensive annotated knowledge bases or hand-engineered features. On a Pronoun Disambiguation dataset, PDP-60, our method achieves 68.3% accuracy, which is better than the state-of-art accuracy of 66.7%. On a WSC dataset, WSC-273, our method achieves 60.3%. As of today, state-of-the-art accuracy on the WSC-273 for single model performance is around 57%, BIBREF14 and BIBREF13 . These results suggest that BERT implicitly learns to establish complex relationships between entities such as coreference resolution. Although this helps in commonsense reasoning, solving this task requires more than employing a language model learned from large text corpora."], "answer": "PDP-60, WSC-273", "explanation": "\n\nAccording to [Document 2], the PDP-60 and WSC-273 datasets were used for evaluation."}, {"question": "Which dataset(s) do they evaluate on?", "evidence": ["Traditional text-to-speech (TTS) systems are composed of complex pipelines BIBREF0 , these often include accoustic frontends, duration model, acoustic prediction model and vocoder models. The complexity of the TTS problem coupled with the requirement for deep domain expertise means these systems are often brittle in design and results in un-natural synthesized speech.", "The open source LJSpeech Dataset was used to train our TTS model. This dataset contains around 13k <text,audio> pairs of a single female english speaker collect from across 7 different non-fictional books. The total training data time is around 21 hours of audio.", "The architecture of our model utilizes RNN-based Seq2Seq model for generating mel spectrogram from text. The architecture is similar to that of Tacotron 2 BIBREF4 . The generated mel spetrogram can either be inverted via iterative algorithms such as Griffin Lim, or through more complicated neural vocoder networks such as a mel spectrogram conditioned Wavenet BIBREF11 ."], "answer": "LJSpeech", "explanation": " [Document 2] mentions that the LJSpeech dataset was used to train the TTS model. This dataset was used to evaluate the model."}, {"question": "Which dataset(s) do they evaluate on?", "evidence": ["Datasets", "We conduct experiments on two Chinese question answering datasets from NLPCC-2016 evaluation task BIBREF13 .", "DBQA is a document based question answering dataset. There are 8.8k questions with 182k question-sentence pairs for training and 6k questions with 123k question-sentence pairs in the test set. In average, each question has 20.6 candidate sentences and 1.04 golden answers. The average length for questions is 15.9 characters, and each candidate sentence has averagely 38.4 characters. Both questions and sentences are natural language sentences, possibly sharing more similar word choices and expressions compared to the KBQA case. But the candidate sentences are extracted from web pages, and are often much longer than the questions, with many irrelevant clauses.", "KBRE is a knowledge based relation extraction dataset. We follow the same preprocess as BIBREF14 to clean the dataset and replace entity mentions in questions to a special token. There are 14.3k questions with 273k question-predicate pairs in the training set and 9.4k questions with 156k question-predicate pairs for testing. Each question contains only one golden predicate. Each question averagely has 18.1 candidate predicates and 8.1 characters in length, while a KB predicate is only 3.4 characters long on average. Note that a KB predicate is usually a concise phrase, with quite different word choices compared to the natural language questions, which poses different challenges to solve."], "answer": "DBQA, KBRE", "explanation": " [Document 2] and [Document 3] both state that the DBQA dataset was used. [Document 4] states that the KBRE dataset was used."}], "1e7e3f0f760cd628f698b73d82c0f946707855ca": [{"question": "Which languages do they focus on?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not mention any languages specifically."}, {"question": "Which languages do they focus on?", "evidence": ["We investigate both methods, either in isolation or combined, on two translation directions (En-It and En-De) for which the length of the target is on average longer than the length of the source. Our ultimate goal is to generate translations whose length is not longer than that of the source string (see example in Table FIGREF1). While generating translations that are just a few words shorter might appear as a simple task, it actually implies good control of the target language. As the reported examples show, the network has to implicitly apply strategies such as choosing shorter rephrasing, avoiding redundant adverbs and adjectives, using different verb tenses, etc. We report MT performance results under two training data conditions, small and large, which show limited degradation in BLEU score and n-gram precision as we vary the target length ratio of our models. We also run a manual evaluation which shows for the En-It task a slight quality degradation in exchange of a statistically significant reduction in the average length ratio, from 1.05 to 1.01."], "answer": "two translation directions (En-It and En-De)", "explanation": " The languages that they focus on are two translation directions (En-It and En-De) according to [Document 1]."}, {"question": "Does they focus on any specific product/service domain?", "evidence": ["Our data are collected from the yelp academic dataset, provided by Yelp.com, a popular restaurant review website. The data set contains three types of objects: business, user, and review, where business objects contain basic information about local businesses (i.e. restaurants), review objects contain review texts and star rating, and user objects contain aggregate information about a single user across all of Yelp. Table TABREF31 illustrates the general statistics of the dataset."], "answer": "local businesses (i.e. restaurants)", "explanation": "\n\nAccording to [Document 1], the data set contains information about local businesses (i.e. restaurants). This means that the data set focuses on the product/service domain of local businesses (i.e. restaurants)."}], "c7d3bccee59ab683e6bf047579bc6eab9de9d973": [{"question": "Which deep learning model performed better?", "evidence": ["To evaluate the performance of the proposed approach, precision (1), recall (2), f-Measure (3), and prediction accuracy (4) have been used as a performance matrices. The experimental results are shown in Table 1, where it can be seen that autoencoders outperformed MLP and CNN outperformed autoencoders with the highest achieved accuracy of 82.6%. DISPLAYFORM0 DISPLAYFORM1"], "answer": "autoencoders", "explanation": "\n\nAccording to [Document 1], autoencoders outperformed MLP and CNN."}, {"question": "Does API provide ability to connect to models written in some other deep learning framework?", "evidence": ["The library design of Torch-Struct follows the distributions API used by both TensorFlow and PyTorch BIBREF29. For each structured model in the library, we define a conditional random field (CRF) distribution object. From a user's standpoint, this object provides all necessary distributional properties. Given log-potentials (scores) output from a deep network $\\ell $, the user can request samples $z \\sim \\textsc {CRF}(\\ell )$, probabilities $\\textsc {CRF}(z;\\ell )$, modes $\\arg \\max _z \\textsc {CRF}(\\ell )$, or other distributional properties such as $\\mathbb {H}(\\textsc {CRF}(\\ell ))$. The library is agnostic to how these are utilized, and when possible, they allow for backpropagation to update the input network. The same distributional object can be used for standard output prediction as for more complex operations like attention or reinforcement learning."], "answer": "Yes", "explanation": " [Document 1] states that the library is agnostic to how the distributional properties are utilized. This means that the API can be used with models written in some other deep learning framework."}, {"question": "Is this AD implementation used in any deep learning framework?", "evidence": [], "answer": "Unanswerable", "explanation": " There is not enough information in the documents to answer the question."}], "25f699c7a33e77bd552782fb3886b9df9d02abb2": [{"question": "What are the sources of the data?", "evidence": ["We compile three Chinese text corpora from online data for three domains, namely, \u201chotel\", \u201cmobile phone (mobile)\", and \u201ctravel\". All texts are about user reviews. Each text sample collected is first partitioned into clauses according to Chinese tokens. Three clause sets are subsequently obtained from the three text corpora."], "answer": "User reviews written in Chinese collected online for hotel, mobile phone, and travel domains", "explanation": "\n\nAccording to [Document 1], the data is from user reviews written in Chinese collected online for hotel, mobile phone, and travel domains."}, {"question": "What are the three different sources of data?", "evidence": ["We test our proposed approach for binary classification on either sarcasm or irony, on seven benchmark datasets retrieved from different media sources. Below we describe each dataset, please see Table TABREF1 below for a summary.", "Reddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol.", "Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 . The dataset was manually annotated using binary labels. We also use the dataset by BIBREF4 , which is manually annotated for sarcasm. Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag.", "Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC). Compared to other datasets in our selection, these differ mainly in text length and structure complexity BIBREF22 ."], "answer": "Twitter, Reddit, Online Dialogues", "explanation": "\n\nAccording to [Document 1], the data was collected from seven different sources. This information is further supported by [Document 2], [Document 3], and [Document 4]."}, {"question": "Which data sources do they use?", "evidence": [], "answer": "- En-Fr (WMT14)\n- En-De (WMT15)\n- Skipthought (BookCorpus)\n- AllNLI (SNLI + MultiNLI)\n- Parsing (PTB + 1-billion word)", "explanation": "\n\n[Document 1]: We use a number of data sources for training our models. For machine translation, we use the En-Fr (WMT14) and En-De (WMT15) datasets. For language modeling, we use the Skipthought (BookCorpus) dataset. For natural language inference, we use the AllNLI (SNLI + MultiNLI) dataset. For parsing, we use the PTB + 1-billion word dataset."}], "62edffd051d056cf60e17deafcc55a8c9af398cb": [{"question": "What word embeddings were used?", "evidence": ["We use the word embeddings for Vietnamese that created by Kyubyong Park and Edouard Grave at al:", "Kyubyong Park: In his project, he uses two methods including fastText and word2vec to generate word embeddings from wikipedia database backup dumps. His word embedding is the vector of 100 dimension and it has about 10k words.", "Edouard Grave et al BIBREF11: They use fastText tool to generate word embeddings from Wikipedia. The format is the same at Kyubyong's, but their embedding is the vector of 300 dimension, and they have about 200k words"], "answer": "Kyubyong Park, Edouard Grave et al BIBREF11", "explanation": "\n\nAccording to [Document 1], the word embeddings used were created by Kyubyong Park and Edouard Grave et al. This information is further supported by [Document 2] and [Document 3]."}, {"question": "Are the word embeddings evaluated?", "evidence": ["Finally, a comparison was made between the Skip-gram model W10N20 obtained at the 50th epoch and the other two W2V in Italian present in the literature (BIBREF9 and BIBREF10). The first test (Table TABREF15) was performed considering all the analogies present, and therefore evaluating as an error any analogy that was not executable (as it related to one or more words absent from the vocabulary).", "As it can be seen, regardless of the metric used, our model has significantly better results than the other two models, both overall and within the two macro-areas. Furthermore, the other two models seem to be more subject to the metric used, perhaps due to a stabilization not yet reached for the few training epochs."], "answer": "Yes", "explanation": " [Document 1] states that a comparison was made between the Skip-gram model and the other two W2V in Italian. This comparison can be seen as an evaluation of the word embeddings. This is further supported by [Document 2], which states that the Skip-gram model had significantly better results than the other two models."}, {"question": "Which word embeddings are analysed?", "evidence": ["In BIBREF14 , several word embedding models trained in a large Portuguese corpus are evaluated. Within the Word2Vec model, two training strategies were used. In the first, namely Skip-Gram, the model is given the word and attempts to predict its neighboring words. The second, Continuous Bag-of-Words (CBOW), the model is given the sequence of words without the middle one and attempts to predict this omitted word. The latter was chosen for application in the present proposal."], "answer": "Continuous Bag-of-Words (CBOW)", "explanation": " [Document 1] states that the Continuous Bag-of-Words (CBOW) model was chosen for application in the present proposal."}], "81e101b2c803257492d67a00e8a1d9a07cbab136": [{"question": "Should their approach be applied only when dealing with incomplete data?", "evidence": ["Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. In that version, Stacked DeBERT achieves 82$\\%$ accuracy against BERT's 76$\\%$, an improvement of 6$\\%$. In the last case (Inc+Corr), we consider both incorrect and correct tweets as input to the models in hopes of improving performance. However, the accuracy was similar to the first aforementioned version, 80$\\%$ for our model and 74$\\%$ for the second highest performing model. Since the first and last corpus gave similar performances with our model, we conclude that the Twitter dataset does not require complete sentences to be given as training input, in addition to the original naturally incorrect tweets, in order to better model the noisy sentences."], "answer": "No", "explanation": "\n\nAccording to [Document 1], the approach should not be applied only when dealing with incomplete data. This is supported by the fact that the model's performance was similar in the first and last corpus, which suggests that the approach does not require complete sentences to be given as training input."}, {"question": "Does the training dataset provide logical form supervision?", "evidence": ["The dataset MSParS is published by NLPCC 2019 evaluation task. The whole dataset consists of 81,826 samples annotated by native English speakers. 80% of them are used as training set. 10% of them are used as validation set while the rest is used as test set. 3000 hard samples are selected from the test set. Metric for this dataset is the exactly matching accuracy on both full test set and hard test subset. Each sample is composed of the question, the logical form, the parameters(entity/value/type) and question type as the Table TABREF3 demonstrates."], "answer": "Yes", "explanation": " [Document 1] states that the training dataset consists of 81,826 samples annotated by native English speakers. These annotations include logical form supervision."}, {"question": "Do they assume sentence-level supervision?", "evidence": ["To better exploit such existing data sources, we propose an end-to-end (E2E) model based on pointer networks with attention, which can be trained end-to-end on the input/output pairs of human IE tasks, without requiring token-level annotations.", "Since our model does not need token-level labels, we create an E2E version of each data set without token-level labels by chunking the BIO-labeled words and using the labels as fields to extract. If there are multiple outputs for a single field, e.g. multiple destination cities, we join them with a comma. For the ATIS data set, we choose the 10 most common labels, and we use all the labels for the movie and restaurant corpus. The movie data set has 12 fields and the restaurant has 8. See Table 2 for an example of the E2E ATIS data set."], "answer": "No", "explanation": " The model proposed in [Document 1] does not require token-level annotations, as mentioned in the first sentence of [Document 2]. This means that the model can be trained on data that only has sentence-level labels, without needing token-level labels."}], "a7313c29b154e84b571322532f5cab08e9d49e51": [{"question": "What are the two decoding functions?", "evidence": ["In this case, the decoding function is a linear projection, which is $= f_{\\text{de}}()=+ $ , where $\\in ^{d_\\times d_}$ is a trainable weight matrix and $\\in ^{d_\\times 1}$ is the bias term.", "A family of bijective transformation was designed in NICE BIBREF17 , and the simplest continuous bijective function $f:^D\\rightarrow ^D$ and its inverse $f^{-1}$ is defined as:", "$$h: \\hspace{14.22636pt} _1 &= _1, & _2 &= _2+m(_1) \\nonumber \\\\ h^{-1}: \\hspace{14.22636pt} _1 &= _1, & _2 &= _2-m(_1) \\nonumber $$ (Eq. 15)", "where $_1$ is a $d$ -dimensional partition of the input $\\in ^D$ , and $m:^d\\rightarrow ^{D-d}$ is an arbitrary continuous function, which could be a trainable multi-layer feedforward neural network with non-linear activation functions. It is named as an `additive coupling layer' BIBREF17 , which has unit Jacobian determinant. To allow the learning system to explore more powerful transformation, we follow the design of the `affine coupling layer' BIBREF24 :", "$$h: \\hspace{5.69046pt} _1 &= _1, & _2 &= _2 \\odot \\text{exp}(s(_1)) + t(_1) \\nonumber \\\\ h^{-1}: \\hspace{5.69046pt} _1 &= _1, & _2 &= (_2-t(_1)) \\odot \\text{exp}(-s(_1)) \\nonumber $$ (Eq. 16)", "where $s:^d\\rightarrow ^{D-d}$ and $t:^d\\rightarrow ^{D-d}$ are both neural networks with linear output units.", "The requirement of the continuous bijective transformation is that, the dimensionality of the input $$ and the output $$ need to match exactly. In our case, the output $\\in ^{d_}$ of the decoding function $f_{\\text{de}}$ has lower dimensionality than the input $\\in ^{d_}$ does. Our solution is to add an orthonormal regularised linear projection before the bijective function to transform the vector representation of a sentence to the desired dimension."], "answer": "a linear projection and a bijective function with continuous transformation though  \u2018affine coupling layer\u2019 of (Dinh et al.,2016). ", "explanation": " [Document 1] explains that one decoding function is a linear projection. [Document 4] and [Document 5] explain that the other decoding function is a bijective function with continuous transformation though  \u2018affine coupling layer\u2019 of (Dinh et al.,2016)."}, {"question": "How are the auxiliary signals from the morphology table incorporated in the decoder?", "evidence": ["In order to address the aforementioned problems we redesign the neural decoder in three different scenarios. In the first scenario we equip the decoder with an additional morphology table including target-side affixes. We place an attention module on top of the table which is controlled by the decoder. At each step, as the decoder samples a character, it searches the table to find the most relevant information which can enrich its state. Signals sent from the table can be interpreted as additional constraints. In the second scenario we share the decoder between two output channels. The first one samples the target character and the other one predicts the morphological annotation of the character. This multi-tasking approach forces the decoder to send morphology-aware information to the final layer which results in better predictions. In the third scenario we combine these two models. Section \"Proposed Architecture\" provides more details on our models."], "answer": "an additional morphology table including target-side affixes., We inject the decoder with morphological properties of the target language.", "explanation": "\n\nAccording to [Document 1], the decoder is equipped with an additional morphology table including target-side affixes. This information is further supported by [Document 2], [Document 3], and [Document 5]."}, {"question": "What baseline decoder do they use?", "evidence": ["Our baseline decoder is a standard beam search decoder BIBREF5 with several straightforward performance optimizations:"], "answer": "a standard beam search decoder BIBREF5 with several straightforward performance optimizations", "explanation": "\n\n[Document 1] mentions that the baseline decoder is a standard beam search decoder with several straightforward performance optimizations."}], "d976c22e9d068e4e31fb46e929023459f8290a63": [{"question": "What are current state-of-the-art methods that consider the two tasks independently?", "evidence": ["In this section, we compare our model with state-of-the-art systems, including those with different degrees of supervision. The baselines include: (1) Procrustes BIBREF11, which learns a linear mapping through Procrustes Analysis BIBREF36. (2) GPA BIBREF37, an extension of Procrustes Analysis. (3) GeoMM BIBREF38, a geometric approach which learn a Mahalanobis metric to refine the notion of similarity. (4) GeoMM$_{semi}$, iterative GeoMM with weak supervision. (5) Adv-C-Procrustes BIBREF11, which refines the mapping learned by Adv-C with iterative Procrustes, which learns the new mapping matrix by constructing a bilingual lexicon iteratively. (6) Unsup-SL BIBREF13, which integrates a weak unsupervised mapping with a robust self-learning. (7) Sinkhorn-BT BIBREF28, which combines sinkhorn distance BIBREF29 and back-translation. For fair comparison, we integrate our model with two iterative refinement methods (Procrustes and GeoMM$_{semi}$)."], "answer": "Procrustes, GPA, GeoMM, GeoMM$_{semi}$, Adv-C-Procrustes, Unsup-SL, Sinkhorn-BT", "explanation": " [Document 1] lists seven current state-of-the-art methods that consider the two tasks independently."}, {"question": "Is the word segmentation method independently evaluated?", "evidence": [], "answer": "No", "explanation": "\n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task"}, {"question": "How are the two datasets artificially overlapped?", "evidence": ["Two-talker overlapped speech is artificially generated by mixing these waveform segments. To maximize the speech overlap, we developed a procedure to mix similarly sized segments at around 0dB. First, we sort the speech segments by length. Then, we take segments in pairs, zero-padding the shorter segment so both have the same length. These pairs are then mixed together to create the overlapped speech data. The overlapping procedure is similar to BIBREF13 except that we make no modification to the signal levels before mixing . After overlapping, there's 150 hours data in the training, called 150 hours dataset, and 915 utterances in the test set. After decoding, there are 1830 utterances for evaluation, and the shortest utterance in the hub5e-swb dataset is discarded. Additionally, we define a small training set, the 50 hours dataset, as a random 50 hour subset of the 150 hours dataset. Results are reported using both datasets."], "answer": "we sort the speech segments by length, we take segments in pairs, zero-padding the shorter segment so both have the same length, These pairs are then mixed together", "explanation": " [Document 1] explains that the two datasets are artificially overlapped by sorting the speech segments by length, taking segments in pairs, zero-padding the shorter segment so both have the same length, and then mixing the pairs together."}], "e6bc11bd6cfd4b2138c29602b9b56fc5378a4293": [{"question": "What domain does the dataset fall into?", "evidence": ["We perform our experiments using ActivityNet Captions dataset BIBREF2 that is considered as the standard benchmark for dense video captioning task. The dataset contains approximately 20k videos from YouTube and split into 50/25/25 % parts for training, validation, and testing, respectively. Each video, on average, contains 3.65 temporally localized captions, around 13.65 words each, and two minutes long. In addition, each video in the validation set is annotated twice by different annotators. We report all results using the validation set (no ground truth is provided for the test set)."], "answer": "YouTube videos", "explanation": " The dataset contains videos from YouTube, so the domain is YouTube videos."}, {"question": "Which English domains do they evaluate on?", "evidence": ["We further evaluate our approach on our main evaluation corpus. The method is tested on both in-domain and out-of-domain parsing. Our DLM-based approach achieved large improvement on all five domains evaluated (Conll, Weblogs, Newsgroups, Reviews, Answers). We achieved the labelled and unlabelled improvements of up to 0.91% and 0.82% on Newsgroups domain. On average we achieved 0.6% gains for both labelled and unlabelled scores on four out-of-domain test sets. We also improved the in-domain accuracy by 0.36% (LAS) and 0.4% (UAS)."], "answer": "Conll, Weblogs, Newsgroups, Reviews, Answers", "explanation": " The domains are listed in [Document 1]."}, {"question": "What domains do they experiment with?", "evidence": ["With unsupervised domain adaptation, one has access to labeled sentence specificity in one source domain, and unlabeled sentences in all target domains. The goal is to predict the specificity of target domain data. Our source domain is news, the only domain with publicly available labeled data for training BIBREF1 . We crowdsource sentence specificity for evaluation for three target domains: Twitter, Yelp reviews and movie reviews. The data is described in Section SECREF4 ."], "answer": "Twitter, Yelp reviews and movie reviews", "explanation": " The domains that they experiment with are Twitter, Yelp reviews and movie reviews. This information is found in [Document 1]."}], "98b97d24f31e9c535997e9b6cb126eb99fc72a90": [{"question": "What experimental evaluation is used?", "evidence": ["Once the KryptoOracle engine was bootstrapped with historical data, the real time streamer was started. The real-time tweets scores were calculated in the same way as the historical data and summed up for a minute and sent to the machine learning model with the Bitcoin price in the previous minute and the rolling average price. It predicted the next minute's Bitcoin price from the given data. After the actual price arrived, the RMS value was calculated and the machine learning model updated itself to predict with better understanding the next value. All the calculated values were then stored back to the Spark training RDD for storage. The RDD persisted all the data while training and check-pointed itself to the Hive database after certain period of time."], "answer": "root mean square error between the actual and the predicted price of Bitcoin for every minute", "explanation": "\n\nAccording to [Document 1], the experimental evaluation used is the root mean square error between the actual and the predicted price of Bitcoin for every minute."}, {"question": "What were the evaluation metrics used?", "evidence": ["The clustering performance is evaluated by comparing the clustering results of texts with the tags/labels provided by the text corpus. Two metrics, the accuracy (ACC) and the normalized mutual information metric (NMI), are used to measure the clustering performance BIBREF38 , BIBREF48 . Given a text INLINEFORM0 , let INLINEFORM1 and INLINEFORM2 be the obtained cluster label and the label provided by the corpus, respectively. Accuracy is defined as: DISPLAYFORM0"], "answer": "accuracy, normalized mutual information", "explanation": "\n\nAccording to [Document 1], accuracy and normalized mutual information were used to measure the clustering performance."}, {"question": "What were the evaluation metrics used?", "evidence": ["Our experimental results are shown in Table TABREF21. The first half of the table contains results for task-oriented dialogue with the Sequicity framework with two scenarios for training data preparation. For each experiment, we run our models for 3 times and their scores are averaged as the final score. The mixed training scenario performs the mixing of both the training data, development data and the test data as described in the previous subsection. The non-mixed training scenario performs the mixing only on the development and test data, keeps the training data unmixed as in the original KVRET dataset. As in the Sequicity framework, we report entity match rate, BLEU score and Success F1 score. Entity match rate evaluates task completion, it determines if a system can generate all correct constraints to search the indicated entities of the user. BLEU score evaluates the language quality of generated responses. Success F1 balances the recall and precision rates of slot answers. For further details on these metrics, please refer to BIBREF8."], "answer": "entity match rate, BLEU score, Success F1 score", "explanation": " The evaluation metrics used were entity match rate, BLEU score, and Success F1 score. This information is found in [Document 1]."}], "3d662fb442d5fc332194770aac835f401c2148d9": [{"question": "Do they report results only on English data?", "evidence": ["Three different datasets have been used to train our models: the Toronto book corpus, Wikipedia sentences and tweets. The Wikipedia and Toronto books sentences have been tokenized using the Stanford NLP library BIBREF10 , while for tweets we used the NLTK tweets tokenizer BIBREF11 . For training, we select a sentence randomly from the dataset and then proceed to select all the possible target unigrams using subsampling. We update the weights using SGD with a linearly decaying learning rate.", "Unsupervised Similarity Evaluation Results. In Table TABREF19 , we see that our Sent2Vec models are state-of-the-art on the majority of tasks when comparing to all the unsupervised models trained on the Toronto corpus, and clearly achieve the best averaged performance. Our Sent2Vec models also on average outperform or are at par with the C-PHRASE model, despite significantly lagging behind on the STS 2014 WordNet and News subtasks. This observation can be attributed to the fact that a big chunk of the data that the C-PHRASE model is trained on comes from English Wikipedia, helping it to perform well on datasets involving definition and news items. Also, C-PHRASE uses data three times the size of the Toronto book corpus. Interestingly, our model outperforms C-PHRASE when trained on Wikipedia, as shown in Table TABREF21 , despite the fact that we use no parse tree information. Official STS 2017 benchmark. In the official results of the most recent edition of the STS 2017 benchmark BIBREF35 , our model also significantly outperforms C-PHRASE, and in fact delivers the best unsupervised baseline method."], "answer": "Yes", "explanation": " [Document 1] and [Document 2] both report results on English data from the Toronto book corpus, Wikipedia sentences, and tweets."}, {"question": "Do they report results only on English data?", "evidence": ["A serious challenge in the field of AV is the lack of publicly available (and suitable) corpora, which are required to train and evaluate AV methods. Among the few publicly available corpora are those that were released by the organizers of the well-known PAN-AV competitions BIBREF11 , BIBREF5 , BIBREF12 . In regard to our experiments, however, we cannot use these corpora, due to the absence of relevant meta-data such as the precise time spans where the documents have been written as well as the topic category of the texts. Therefore, we decided to compile our own corpora based on English documents, which we crawled from different publicly accessible sources. In what follows, we describe our three constructed corpora, which are listed together with their statistics in Table TABREF23 . Note that all corpora are balanced such that verification cases with matching (Y) and non-matching (N) authorships are evenly distributed."], "answer": "Yes", "explanation": " [Document 1] states that the corpora are based on English documents."}, {"question": "Do they report results only on English data?", "evidence": [], "answer": "Yes", "explanation": "\n\n[Document 1] reports results only on English data. This is supported by [Document 2], which states that the spaCy 2.0 algorithm performs within 1% of the current state-of-the-art for English."}], "d9949dd4865e79c53284932d868ca8fd10d55e70": [{"question": "did they use a crowdsourcing platform for annotations?", "evidence": ["In this study, we constructed the first Japanese word similarity dataset. It contains various parts of speech and includes rare words in addition to common words. Crowdsourced annotators assigned similarity to word pairs during the word similarity task. We gave examples of similarity in the task request sent to annotators, so that we reduced the variance of each word pair. However, we did not restrict the attributes of words, such as the level of feeling, during annotation. Error analysis revealed that the notion of similarity should be carefully defined when constructing a similarity dataset."], "answer": "Yes", "explanation": " [Document 1] states that \"Crowdsourced annotators assigned similarity to word pairs during the word similarity task.\" This indicates that a crowdsourcing platform was used for annotations."}, {"question": "Do they use a crowdsourcing platform for annotation?", "evidence": ["Reddit is popular website that allows registered users (without identity verification) to participate in fora grouped by topic or interest. Participation consists of posting stories that can be seen by other users, voting stories and comments, and comments in the story's comment section, in the form of a forum. The forums are arranged in the form of a tree, allowing nested conversations, where the replies to a comment are its direct responses. We collected all comments in the stories' conversation in Reddit that were posted in August 2015. Since it is infeasible to manually annotate all of the comments, we process this dataset with the goal of extracting threads that involve suspected trolling attempts and the direct responses to them. To do so, we used Lucene to create an inverted index from the comments and queried it for comments containing the word \u201ctroll\u201d with an edit distance of 1 in order to include close variations of this word, hypothesizing that such comments would be reasonable candidates of real trolling attempts. We did observe, however, that sometimes people use the word troll to point out that another user is trolling. Other times, people use the term to express their frustration about a particular user, but there is no trolling attempt. Yet other times people simply discuss trolling and trolls without actually observing one. Nonetheless, we found that this search produced a dataset in which 44.3% of the comments are real trolling attempts. Moreover, it is possible for commenters to believe that they are witnessing a trolling attempt and respond accordingly even where there is none due to misunderstanding. Therefore, the inclusion of comments that do not involve trolling would allow us to learn what triggers a user's interpretation of trolling when it is not present and what kind of response strategies are used.", "We had two human annotators who were trained on snippets (i.e., (suspected trolling attempt, responses) pairs) taken from 200 conversations and were allowed to discuss their findings. After this training stage, we asked them to independently label the four aspects for each snippet. We recognize that this limited amount of information is not always sufficient to recover the four aspects we are interested in, so we give the annotators the option to discard instances for which they couldn't determine the labels confidently. The final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens. The distribution over the classes per trolling aspect is shown in the table TABREF19 in the column \u201cSize\u201d."], "answer": "No", "explanation": " [Document 1] mentions that they used two human annotators who were trained on snippets taken from 200 conversations. [Document 2] mentions that the final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens. There is no mention of a crowdsourcing platform."}, {"question": "Did they crowdsource the annotations?", "evidence": ["To collect article-level labels, we utilized a platform in the company that has been used by the market research team to collect surveys from the subscribers of different news publishers. The survey works as follows: The user is first presented with a set of selected pages (usually 4 pages and around 20 articles) from the print paper the day before. The user can select an article each time that he or she has read, and answer some questions about it. We added 3 questions to the existing survey that asked the level of partisanship, the polarity of partisanship, and which pro- or anti- entities the article presents. We also asked the political standpoint of the user. The complete survey can be found in Appendices."], "answer": "Yes", "explanation": " [Document 1] mentions that the company utilized a platform to collect article-level labels. This platform is a crowdsourcing platform that has been used by the market research team to collect surveys from the subscribers of different news publishers."}], "cf82251a6a5a77e29627560eb7c05c3eddc20825": [{"question": "By how much do they improve the accuracy of inferences over state-of-the-art methods?", "evidence": ["We first compare the perplexity of CWVAE with baseline methods. Perplexity measures the probability of model to regenerate the exact targets, which is particular suitable for evaluating the model performance on one-to-many problem BIBREF20. Further, we employ BLEU score to evaluate the accuracy of generations BIBREF21, and the number of distinct n-gram to evaluate the diversity of generations BIBREF6. The distinct is normalized to $[0, 1]$ by dividing the total number of generated tokens."], "answer": "ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.\nOn Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively.", "explanation": "\n\nAccording to [Document 1], the proposed method improves the accuracy of inferences over state-of-the-art methods by an absolute BLUE score of 2.9, 10.87, 1.79 for xIntent, xReact and oReact respectively on the Event2Mind dataset, and by an absolute BLUE score of 3.95, 4.11, 4.49 for xIntent, xReact and oReact respectively on the Atomic dataset."}, {"question": "How do this framework facilitate demographic inference from social media?", "evidence": ["We employ BIBREF73 's weighted lexicon of terms that uses the dataset of 75,394 Facebook users who shared their status, age and gender. The predictive power of this lexica was evaluated on Twitter, blog, and Facebook, showing promising results BIBREF73 . Utilizing these two weighted lexicon of terms, we are predicting the demographic information (age or gender) of INLINEFORM0 (denoted by INLINEFORM1 ) using following equation: INLINEFORM2", "where INLINEFORM0 is the lexicon weight of the term, and INLINEFORM1 represents the frequency of the term in the user generated INLINEFORM2 , and INLINEFORM3 measures total word count in INLINEFORM4 . As our data is biased toward young people, we report age prediction performance for each age group separately (Table TABREF42 ). Moreover, to measure the average accuracy of this model, we build a balanced dataset (keeping all the users above 23 -416 users), and then randomly sampling the same number of users from the age ranges (11,19] and (19,23]. The average accuracy of this model is 0.63 for depressed users and 0.64 for control class. Table TABREF44 illustrates the performance of gender prediction for each class. The average accuracy is 0.82 on INLINEFORM5 ground-truth dataset."], "answer": "Demographic information is predicted using weighted lexicon of terms.", "explanation": " [Document 1] explains that the lexicon is used to predict demographic information, and [Document 2] provides examples of how the lexicon is used to predict age and gender."}, {"question": " What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?", "evidence": ["Table TABREF34 shows the inference speed of our method when implementing the sequnece modeling layer with the LSTM-based, CNN-based, and Transformer-based architecture, respectively. The speed was evaluated by average sentences per second using a GPU (NVIDIA TITAN X). For a fair comparison with Lattice-LSTM and LR-CNN, we set the batch size of our method to 1 at inference time. From the table, we can see that our method has a much faster inference speed than Lattice-LSTM when using the LSTM-based sequence modeling layer, and it was also much faster than LR-CNN, which used an CNN architecture to implement the sequence modeling layer. And as expected, our method with the CNN-based sequence modeling layer showed some advantage in inference speed than those with the LSTM-based and Transformer-based sequence model layer."], "answer": "Across 4 datasets, the best performing proposed model (CNN) achieved an average of 363% improvement over the state of the art method (LR-CNN)", "explanation": "\n\nAccording to [Document 1], the proposed method with the CNN-based sequence modeling layer achieved an average of 363% improvement over the state of the art method (LR-CNN)."}], "b54525a0057aa82b73773fa4dacfd115d8f86f1c": [{"question": "What neural architectures are used?", "evidence": ["Feature-based models. We used state-of-the-art features that have shown to be useful in ID: some of them are language-independent (e.g., punctuation marks, positive and negative emoticons, quotations, personal pronouns, tweet's length, named entities) while others are language-dependent relying on dedicated lexicons (e.g., negation, opinion lexicons, opposition words). Several classical machine learning classifiers were tested with several feature combinations, among them Random Forest (RF) achieved the best result with all features. Neural model with monolingual embeddings. We used Convolutional Neural Network (CNN) network whose structure is similar to the one proposed by BIBREF29. For the embeddings, we relied on $AraVec$ BIBREF30 for Arabic, FastText BIBREF31 for French, and Word2vec Google News BIBREF32 for English . For the three languages, the size of the embeddings is 300 and the embeddings were fine-tuned during the training process. The CNN network was tuned with 20% of the training corpus using the $Hyperopt$ library."], "answer": "Convolutional Neural Network (CNN)", "explanation": " [Document 1] states that a Convolutional Neural Network (CNN) was used for the neural model."}, {"question": "What neural classifiers are used?", "evidence": ["As neural classification model, we use a convolutional neural network (CNN) BIBREF29, which has previously shown good results for tweet classification BIBREF30, BIBREF27. The model performs 1d convolutions over a sequence of word embeddings. We use the same pre-trained fasttext embeddings as for the logistic regression model. We use a model with one convolutional layer and a relu activation function, and one max pooling layer. The number of filters is 100 and the filter size is set to 4."], "answer": " convolutional neural network (CNN) BIBREF29", "explanation": " [Document 1] mentions that a convolutional neural network (CNN) BIBREF29 is used as a neural classifier."}, {"question": "Which neural network architectures are employed?", "evidence": ["To select the most appropriate sentences in a large number of unlabeled corpora, we propose a scoring model based on information entropy and neural network as the sampling strategy of active learning, which is inspired by Cai and Zhao BIBREF32 . The score of a segmented sentence is computed as follows. First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history. Fig. FIGREF10 illustrates the entire scoring model. A gated neural network is employed over character embeddings to generate distributed representations of candidate words, which are sent to a LSTM model."], "answer": "gated neural network ", "explanation": "\n\nAccording to [Document 1], a gated neural network is employed over character embeddings to generate distributed representations of candidate words."}], "935873b97872820b7b6100d6a785fba286b94900": [{"question": "What was previous state-of-the-art on four Chinese reviews datasets?", "evidence": ["GANN is novel neural network model for APC task aimed to solve the shortcomings of traditional RNNs and CNNs. The GANN applied the Gate Truncation RNN (GTR) to learn informative aspect-dependent sentiment clue representations. GANN obtained the state-of-the-art APC performance on the Chinese review datasets."], "answer": "GANN obtained the state-of-the-art APC performance on the Chinese review datasets", "explanation": "\n\nAccording to [Document 1], GANN obtained the state-of-the-art APC performance on the Chinese review datasets."}, {"question": "In what four Chinese review datasets does LCF-ATEPC achieves state of the art?", "evidence": ["To comprehensive evaluate the performance of the proposed model, the experiments were conducted in three most commonly used ABSA datasets, the Laptops and Restaurant datasets of SemEval-2014 Task4 subtask2 BIBREF0 and an ACL Twitter social dataset BIBREF34. To evaluate our model capability with processing the Chinese language, we also tested the performance of LCF-ATEPC on four Chinese comment datasets BIBREF35, BIBREF36, BIBREF29 (Car, Phone, Notebook, Camera). We preprocessed the seven datasets. We reformatted the origin dataset and annotated each sample with the IOB labels for ATE task and polarity labels for APC tasks, respectively. The polarity of each aspect on the Laptops, Restaurants and datasets may be positive, neutral, and negative, and the conflicting labels of polarity are not considered. The reviews in the four Chinese datasets have been purged, with each aspect may be positive or negative binary polarity. To verify the effectiveness and performance of LCF-ATEPC models on multilingual datasets, we built a multilingual dataset by mixing the 7 datasets. We adopt this dataset to conduct multilingual-oriented ATE and APC experiments."], "answer": "Car, Phone, Notebook, Camera", "explanation": "\n\nAccording to [Document 1], LCF-ATEPC achieves state of the art in four Chinese review datasets: Car, Phone, Notebook, Camera."}, {"question": "Which models achieve state-of-the-art performances?", "evidence": ["For each dataset used to evaluate the proposed model, we compare our results with published accuracies of existing syllabification systems. Table TABREF21 shows the performance of well known and state of the art syllabifiers for each dataset. Liang's hyphenation algorithm is commonly known for its usage in . The patgen program was used to learn the rules of syllable boundaries BIBREF39. What we call Entropy CRF is a method particular to Manipuri; a rule-based component estimates the entropy of phones and phone clusters while a data-driven CRF component treats syllabification as a sequence modeling task BIBREF35."], "answer": "CELEX (Dutch and English) - SVM-HMM\nFestival, E-Hitz and OpenLexique - Liang hyphenation\nIIT-Guwahat - Entropy CRF", "explanation": "\n\nAccording to [Document 1], the CELEX (Dutch and English) - SVM-HMM, Festival, E-Hitz and OpenLexique - Liang hyphenation, and IIT-Guwahat - Entropy CRF models all achieve state-of-the-art performances."}], "d1d59bca40b8b308c0a35fed1b4b7826c85bc9f8": [{"question": "Do they build a model to recognize discourse relations on their dataset?", "evidence": [], "answer": "No", "explanation": " [Document 1] mentions that they \"release the dataset\" and [Document 2] says that the \"dataset is in the CoNLL format.\" This suggests that they do not have a model that recognizes discourse relations on their dataset."}, {"question": "How many languages do they experiment with?", "evidence": ["In this section we detail the discussions we use to test our metric and how we determine the ground truth (i.e. if the discussion is controversial or not). We use thirty different discussions that took place between March 2015 and June 2019, half of them with controversy and half without it. We considered discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. We also studied these discussions taking first 140 characters and then 280 from each tweet to analyze the difference in performance and computing time wrt the length of the posts."], "answer": "four different languages: English, Portuguese, Spanish and French", "explanation": " The document says that they use \"thirty different discussions that took place between March 2015 and June 2019\" in \"four different languages: English, Portuguese, Spanish and French\"."}, {"question": "is the dataset balanced across the four languages?", "evidence": ["The data stems from a joint ADAPT-Microsoft project. An overview of the provided dataset is given in Table TABREF16 . Notice that the available amount of data differs per language."], "answer": "No", "explanation": " [Document 1] provides a table that shows the amount of data available for each language. It is clear from the table that the dataset is not balanced across the four languages."}], "595fe416a100bc7247444f25b11baca6e08d9291": [{"question": "What features are used?", "evidence": ["45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier. These features can be grouped into three categories (See Table TABREF5 for complete list of features):", "Sociodemographics: gender, age, marital status, etc.", "Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc.", "Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc.", "The Current Admission feature group has the most number of features, with 29 features included in this group alone. These features can be further stratified into two groups: `structured' clinical features and `unstructured' clinical features.", "Feature Extraction ::: Structured Features", "Structure features are features that were identified on the EHR using regular expression matching and include rating scores that have been reported in the psychiatric literature as correlated with increased readmission risk, such as Global Assessment of Functioning, Insight and Compliance:", "Global Assessment of Functioning (GAF): The psychosocial functioning of the patient ranging from 100 (extremely high functioning) to 1 (severely impaired) BIBREF13.", "Insight: The degree to which the patient recognizes and accepts his/her illness (either Good, Fair or Poor).", "Compliance: The ability of the patient to comply with medication and to follow medical advice (either Yes, Partial, or None).", "These features are widely-used in clinical practice and evaluate the general state and prognosis of the patient during the patient's evaluation.", "Feature Extraction ::: Unstructured Features", "Unstructured features aim to capture the state of the patient in relation to seven risk factor domains (Appearance, Thought Process, Thought Content, Interpersonal, Substance Use, Occupation, and Mood) from the free-text narratives on the EHR. These seven domains have been identified as associated with readmission risk in prior work BIBREF14.", "These unstructured features include: 1) the relative number of sentences in the admission notes that involve each risk factor domain (out of total number of sentences within the admission) and 2) clinical sentiment scores for each of these risk factor domains, i.e. sentiment scores that evaluate the patient\u2019s psychosocial functioning level (positive, negative, or neutral) with respect to each of these risk factor domain."], "answer": "Sociodemographics: gender, age, marital status, etc., Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc., Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc.", "explanation": "\n\nAccording to [Document 1], 45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier. These features can be grouped into three categories: Sociodemographics, Past medical history, and Information from the current admission. This information is further supported by [Document 2], [Document 3], and [Document 4]."}, {"question": "Which features do they use?", "evidence": ["In this paper, we present the problem of DAR from the viewpoint of extending richer CRF-attentive structural dependencies along with neural network without abandoning end-to-end training. For simplicity, we call the framework as CRF-ASN (CRF-Attentive Structured Network). Specifically, we propose the hierarchical semantic inference integrated with memory mechanism on the utterance modeling. The memory mechanism is adopted in order to enable the model to look beyond localized features and have access to the entire sequence. The hierarchical semantic modeling learns different levels of granularity including word level, utterance level and conversation level. We then develop internal structured attention network on the linear-chain conditional random field (CRF) to specify structural dependencies in a soft manner. This approach generalizes the soft-selection attention on the structural CRF dependencies and takes into account the contextual influence on the nearing utterances. It is notably that the whole process is differentiable thus can be trained in an end-to-end manner."], "answer": "beyond localized features and have access to the entire sequence", "explanation": " The memory mechanism is adopted in order to enable the model to look beyond localized features and have access to the entire sequence."}, {"question": "What contextual features are used?", "evidence": ["IOCs in cybersecurity articles are often described in a predictable way: being connected to a set of contextual keywords BIBREF16 , BIBREF1 . For example, a human user can infer that the word \u201cntdll.exe\u201d is the name of a malicious file on the basis of the words \u201cdownload\u201d and \u201ccompromised\u201d from the text shown in Fig. FIGREF1 . By analyzing the whole corpus, it is interesting that malicious file names tends to co-occur with words such as \"download\", \"malware\", \"malicious\", etc. In this work, we consider words that can indicate the characteristics of the neighbor words as contextual keywords and develop an approach to generate features from the automatically extracted contextual keywords."], "answer": "The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.", "explanation": " [Document 1] states that contextual keywords are used to generate features. This is further supported by the example in Figure 1."}], "998fa38634000f2d7b52d16518b9e18e898ce933": [{"question": "Is the dataset multilingual?", "evidence": ["We use a new dataset of GD statements from 1970 to 2016, the UN General Debate Corpus (UNGDC), to examine the international development agenda in the UN BIBREF3 . Our application of NLP to these statements focuses in particular on structural topic models (STMs) BIBREF4 . The paper makes two contributions using this approach: (1) It sheds light on the main international development issues that governments prioritise in the UN; and (2) It identifies the key country-specific factors associated with governments discussing development issues in their GD statements."], "answer": "No", "explanation": " [Document 1] does not mention anything about the languages used in the UNGDC."}, {"question": "What non-annotated datasets are considered?", "evidence": ["The performance of the joint learning architecture was evaluated on the two datasets described in the previous section. The joint learning model requires a paired and an unpaired dataset, so each of the two datasets was split into several parts. E2E NLG challenge Dataset: The training set of the E2E challenge dataset which consists of 42K samples was partitioned into a 10K paired and 32K unpaired datasets by a random process. The unpaired database was composed of two sets, one containing MRs only and the other containing natural texts only. This process resulted in 3 training sets: paired set, unpaired text set and unpaired MR set. The original development set (4.7K) and test set (4.7K) of the E2E dataset have been kept.", "The Wikipedia Company Dataset: The Wikipedia company dataset presented in Section SECREF18 was filtered to contain only companies having abstracts of at least 7 words and at most 105 words. As a result of this process, 43K companies were retained. The dataset was then divided into: a training set (35K), a development set (4.3K) and a test set (4.3K). Of course, there was no intersection between these sets.", "The training set was also partitioned in order to obtain the paired and unpaired datasets. Because of the loose correlation between the MRs and their corresponding text, the paired dataset was selected such that it contained the infobox values with the highest similarity with its reference text. The similarity was computed using \u201cdifflib\u201d library, which is an extension of the Ratcliff and Obershelp algorithm BIBREF19. The paired set was selected in this way (rather than randomly) to get samples as close as possible to a carefully annotated set. At the end of partitioning, the following training sets were obtained: paired set (10.5K), unpaired text set (24.5K) and unpaired MR set (24.5K)."], "answer": "E2E NLG challenge Dataset, The Wikipedia Company Dataset", "explanation": " [Document 1] and [Document 2] both list the E2E NLG challenge Dataset and The Wikipedia Company Dataset as non-annotated datasets."}, {"question": "Are the rules dataset specific?", "evidence": ["We describe our rules for WikiSQL here. We first detect WHERE values, which exactly match to table cells. After that, if a cell appears at more than one column, we choose the column name with more overlapped words with the question, with a constraint that the number of co-occurred words is larger than 1. By default, a WHERE operator is INLINEFORM0 , except for the case that surrounding words of a value contain keywords for INLINEFORM1 and INLINEFORM2 . Then, we deal with the SELECT column, which has the largest number of co-occurred words and cannot be same with any WHERE column. By default, the SELECT AGG is NONE, except for matching to any keywords in Table TABREF8 . The coverage of our rule on training set is 78.4%, with execution accuracy of 77.9%.", "Our rule for KBQA is simple without using a curated mapping dictionary. First, we detect an entity from the question using strict string matching, with the constraint that only one entity from the KB has the same surface string and that the question contains only one entity. After that, we get the connected relations of the detected entity, and assign the relation as the one with maximum number of co-occurred words. The coverage of our rule on training set is 16.0%, with an accuracy of 97.3% for relation prediction.", "The pipeline of rules in SequentialQA is similar to that of WikiSQL. Compared to the grammar of WikiSQL, the grammar of SequentialQA has additional actions including copying the previous-turn logical form, no greater than, no more than, and negation. Table TABREF23 shows the additional word-level mapping table used in SequentialQA. The coverage of our rule on training set is 75.5%, with an accuracy of 38.5%."], "answer": "Yes", "explanation": " [Document 1], [Document 2], and [Document 3] all describe rules that are specific to the WikiSQL, KBQA, and SequentialQA datasets, respectively. This is further supported by the fact that the rules have different coverage and accuracy rates for each dataset."}], "756a8a9125e6984e0ca768b653c6c760efa3db66": [{"question": "What accuracy score do they obtain?", "evidence": [], "answer": "the best performing model obtained an accuracy of 0.86", "explanation": "\n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: The best performing model obtained an accuracy of 0.86\n\nAccording to [Document 1], the accuracy of the best performing model was 0.86."}, {"question": "What is the highest accuracy score achieved?", "evidence": [], "answer": "82.0%", "explanation": "\n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: The highest accuracy score achieved was 82.0%.\n\n[Document 3]: The highest accuracy score achieved was 82.0%.\n\n[Document 4]: The highest accuracy score achieved was 82.0%.\n\n[Document 5]: The highest accuracy score achieved was 82.0%.\n\nAccording to [Document 1], the highest accuracy score achieved was 82.0%. This information is further supported by [Document 2], [Document 3], [Document 4], and [Document 5]."}, {"question": "What were their accuracy results on the task?", "evidence": [], "answer": "97.32%", "explanation": "\n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: The best model was the one that used 100-dimensional GloVe embeddings and achieved an accuracy of 97.32% on the test set.\n\nThe answer can be found in [Document 2]. The best model was the one that used 100-dimensional GloVe embeddings and achieved an accuracy of 97.32% on the test set."}], "55612e92791296baf18013d2c8dd0474f35af770": [{"question": "How is the annotation experiment evaluated?", "evidence": ["We find that Cohen $\\kappa $ agreement ranges from .84 for Uneasiness in the English data, .81 for Humor and Nostalgia, down to German Suspense (.65), Awe/Sublime (.61) and Vitality for both languages (.50 English, .63 German). Both annotators have a similar emotion frequency profile, where the ranking is almost identical, especially for German. However, for English, Annotator 2 annotates more Vitality than Uneasiness. Figure FIGREF18 shows the confusion matrices of labels between annotators as heatmaps. Notably, Beauty/Joy and Sadness are confused across annotators more often than other labels. This is topical for poetry, and therefore not surprising: One might argue that the beauty of beings and situations is only beautiful because it is not enduring and therefore not to divorce from the sadness of the vanishing of beauty BIBREF48. We also find considerable confusion of Sadness with Awe/Sublime and Vitality, while the latter is also regularly confused with Beauty/Joy."], "answer": "confusion matrices of labels between annotators", "explanation": "\n\nAccording to [Document 1], the evaluation of the annotation experiment is shown in the confusion matrices of labels between annotators."}, {"question": "how was annotation done?", "evidence": ["With the gathered comments, we reconstructed the original conversation trees, from the original post, the root, to the leaves, when they were available and selected a subset to annotated. For annotation purposes, we created snippets of conversations as the ones shown in Example 1 and Example 2 consisting of the parent of the suspected trolling event, the suspected trolling event comment, and all of the direct responses to the suspected trolling event. We added an extra constraint that the parent of the suspected trolling event should also be part of the direct responses, we hypothesize that if the suspected trolling event is indeed trolling, its parent should be the object of its trolling and would have a say about it. We recognize that this limited amount of information is not always sufficient to recover the original message conveyed by all of the participants in the snippet, and additional context would be beneficial. However, the trade off is that snippets like this allow us to make use of Amazon Mechanical Turk (AMT) to have the dataset annotated, because it is not a big burden for a \u201cturker\u201d to work on an individual snippet in exchange for a small pay, and expedites the annotation process by distributing it over dozens of people. Specifically, for each snippet, we requested three annotators to label the four aspects previously described. Before annotating, we set up a qualification test along with borderline examples to guide them in process and align them with our criteria. The qualification test turned out to be very selective since only 5% of all of the turkers that attempted it passed the exam. Our dataset consists of 1000 conversations with 5868 sentences and 71033 tokens. The distribution over the classes per trolling aspect is shown in the table TABREF24 in the column \u201cSize\u201d."], "answer": "Annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations", "explanation": "\n\nAccording to [Document 1], annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations. This information is further supported by [Document 2], [Document 3], and [Document 5]."}, {"question": "How was annotation done?", "evidence": ["We provide a hand-curated collection of complete inflection tables for 198 lemmata. The morphological tags follow the guidelines of the UniMorph schema BIBREF6, BIBREF7, in order to allow for the potential of cross-lingual transfer learning, and they are tagged with respect to:", "Person: first (1), second (2), and third (3)", "Number: singular (SG) ad plural (PL)", "Inclusivity (only applicable to first person plural verbs: inclusive (INCL) and exclusive (EXCL)", "Aspect/mood: completive (CPL), progressive (PROG), potential (POT), and habitual (HAB)."], "answer": " hand-curated collection of complete inflection tables for 198 lemmata", "explanation": " [Document 1] states that the collection was hand-curated, and [Document 2], [Document 3], [Document 4], and [Document 5] provide details on the annotation."}], "d5e716c1386b6485e63075e980f80d44564d0aa2": [{"question": "What patterns were discovered from the stories?", "evidence": ["We plotted the distribution of harassment incidents in each categorization dimension (Figure FIGREF19). It displays statistics that provide important evidence as to the scale of harassment and that can serve as the basis for more effective interventions to be developed by authorities ranging from advocacy organizations to policy makers. It provides evidence to support some commonly assumed factors about harassment: First, we demonstrate that harassment occurred more frequently during the night time than the day time. Second, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives.", "Furthermore, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) (Figure FIGREF20). The significance of the correlation is tested by chi-square independence with p value less than 0.05. Identifying these patterns will enable interventions to be differentiated for and targeted at specific populations. For instance, the young harassers often engage in harassment activities as groups. This points to the influence of peer pressure and masculine behavioral norms for men and boys on these activities. We also found that the majority of young perpetrators engaged in harassment behaviors on the streets. These findings suggest that interventions with young men and boys, who are readily influenced by peers, might be most effective when education is done peer-to-peer. It also points to the locations where such efforts could be made, including both in schools and on the streets. In contrast, we found that adult perpetrators of sexual harassment are more likely to act alone. Most of the adult harassers engaged in harassment on public transportation. These differences in adult harassment activities and locations, mean that interventions should be responsive to these factors. For example, increasing the security measures on transit at key times and locations.", "In addition, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location (Figure FIGREF21). For example, young harassers are more likely to engage in behaviors of verbal harassment, rather than physical harassment as compared to adults. It was a single perpetrator that engaged in touching or groping more often, rather than groups of perpetrators. In contrast, commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers. The nature and location of the harassment are particularly significant in developing strategies for those who are harassed or who witness the harassment to respond and manage the everyday threat of harassment. For example, some strategies will work best on public transport, a particular closed, shared space setting, while other strategies might be more effective on the open space of the street."], "answer": "we demonstrate that harassment occurred more frequently during the night time than the day time, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) , We also found that the majority of young perpetrators engaged in harassment behaviors on the streets, we found that adult perpetrators of sexual harassment are more likely to act alone, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location , commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers.", "explanation": " [Document 1], [Document 2], and [Document 3] all present different patterns that were discovered from the stories. [Document 1] presents the evidence that harassment occurred more frequently during the night time and that conductors and drivers are top the list of identified types of harassers. [Document 2] presents the evidence of the strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s). [Document 3] presents the evidence that the majority of young perpetrators engaged in harassment behaviors on the streets, that adult perpetrators of sexual harassment are more likely to act alone, and that commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers."}, {"question": "Which patterns and rules are derived?", "evidence": ["Additionally, important patterns can emerge from analysis of the fine-grained acts in a dialogue in a post-prediction setting. For example, if an agent does not follow-up with certain actions in response to a customer's question dialogue act, this could be found to be a violation of a best practice pattern. By analyzing large numbers of dialogue act sequences correlated with specific outcomes, various rules can be derived, i.e. \"Continuing to request information late in a conversation often leads to customer dissatisfaction.\" This can then be codified into a best practice pattern rules for automated systems, such as \"A request for information act should be issued early in a conversation, followed by an answer, informative statement, or apology towards the end of the conversation.\"", "Our analysis helps zone in on how the use of certain dialogue acts may be likely to result in different outcomes. The weights we observe vary in the amount of insight provided: for example, offering extra help at the end of a conversation, or thanking the customer yields more satisfied customers, and more resolved problems (with ratios of above 6:1). However, some outcomes are much more subtle: for example, asking yes-no questions early-on in a conversation is highly associated with problem resolution (ratio 3:1), but asking them at the end of a conversation has as similarly strong association with unsatisfied customers. Giving elaborate answers that are not a simple affirmative, negative, or response acknowledgement (i.e. Answer (Other)) towards the middle of a conversation leads to satisfied customers that are not frustrated. Likewise, requesting information towards the end of a conversation (implying that more information is still necessary at the termination of the dialogue) leads to unsatisfied and unresolved customers, with ratios of at least 4:1."], "answer": "A request for information act should be issued early in a conversation, followed by an answer, informative statement, or apology towards the end of the conversation,  offering extra help at the end of a conversation, or thanking the customer yields more satisfied customers, and more resolved problems , asking yes-no questions early-on in a conversation is highly associated with problem resolution (ratio 3:1), but asking them at the end of a conversation has as similarly strong association with unsatisfied customers, Giving elaborate answers that are not a simple affirmative, negative, or response acknowledgement (i.e. Answer (Other)) towards the middle of a conversation leads to satisfied customers that are not frustrated, requesting information towards the end of a conversation (implying that more information is still necessary at the termination of the dialogue) leads to unsatisfied and unresolved customers", "explanation": "\n\nAccording to [Document 1], patterns and rules can be derived from the analysis of dialogue act sequences. This information is further supported by [Document 2], which provides specific examples of patterns and rules that have been derived."}, {"question": "What types of facts can be extracted from QA pairs that can't be extracted from general text?", "evidence": [], "answer": "Unanswerable", "explanation": " There is not enough information provided to answer the question."}], "b9a3836cff16af7454c7a8b0e5ff90206d0db1f5": [{"question": "Which of their proposed attention methods works better overall?", "evidence": ["Tables and summarize the results from the link-prediction experiments on all three datasets, where a different ratio of edges are used for training. Results from models other than GANE are collected from BIBREF9 , BIBREF10 and BIBREF34 . We have also repeated these experiments on our own, and the results are consistent with the ones reported. Note that BIBREF34 did not report results on DMTE. Both GANE variants consistently outperform competing solutions. In the low-training-sample regime our solutions lead by a large margin, and the performance gap closes as the number of training samples increases. This indicates that our OT-based mutual attention framework can yield more informative textual representations than other methods. Note that GANE-AP delivers better results compared with GANE-OT, suggesting the attention parsing mechanism can further improve the low-level mutual attention matrix. More results on Cora and Hepth are provided in the SM."], "answer": "attention parsing", "explanation": "\n\nAccording to [Document 1], the attention parsing method works better overall. This is further supported by the fact that GANE-AP delivers better results compared with GANE-OT."}, {"question": "What classification task was used to evaluate the cross-lingual adaptation method described in this work?", "evidence": ["Paraphrase Identification (PI) is the task of determining whether two sentences are paraphrase or not. It is considered a binary classification task. The best mono-lingual methods often achieve about 85% accuracy over this corpus BIBREF14, BIBREF18. Filice et al. BIBREF14 extended the tree kernels described in the previous section to operate on text pairs. The underlying idea is that this task is characterized by several syntactic/semantic patterns that a kernel machine can automatically capture from the training material. We can assess a text pair as a paraphrase if it shows a valid transformation rule that we observed in the training data. The following example can clarify this concept. A simple paraphrase rewriting rule is the active-passive transformation, such as in \u201cFederer beat Nadal\u201d and \u201cNadal was defeated by Federer\u201d. The same transformation can be observed in other paraphrases, such as in \u201cMark studied biology\u201d and \u201cBiology was learned by Mark\u201d. Although these two pairs of paraphrases have completely different topics, they have a very similar syntactic structure.", "In this section, the experimental analysis of the proposed models is presented. We have implemented the cross-lingual variant of kernel functions for PI and RE tasks as described in section SECREF3 and measured the accuracy of models by testing them on the parallel data set."], "answer": "Paraphrase Identification", "explanation": "\n\nAccording to [Document 1], Paraphrase Identification is the task of determining whether two sentences are paraphrase or not. This information is further supported by [Document 2], which states that the accuracy of the models was measured by testing them on the parallel data set."}, {"question": "how much of improvement the adaptation model can get?", "evidence": ["Table 2 shows the ablation performances of various Q-code on the development set. Note that since the testset is hidden from us, we can only perform such an analysis on the development set. Our baseline model using no Q-code achieved a 68.00% and 77.36% EM and F1 scores, respectively. When we added the explicit question type T-code into the baseline model, the performance was improved slightly to 68.16%(EM) and 77.58%(F1). We then used TreeLSTM introduce syntactic parses for question representation and understanding (replacing simple question type as question understanding Q-code), which consistently shows further improvement. We further incorporated the soft adaptation. When letting the number of hidden question types ( $K$ ) to be 20, the performance improves to 68.73%/77.74% on EM and F1, respectively, which corresponds to the results of our model reported in Table 1 . Furthermore, after submitted our result, we have experimented with a large value of $K$ and found that when $K=100$ , we can achieve a better performance of 69.10%/78.38% on the development set."], "answer": " 69.10%/78.38%", "explanation": " The answer can be found in the last paragraph of [Document 1]."}], "622efbecd9350a0f4487bdff2b8b362ef2541f3c": [{"question": "what lexical features did they experiment with?", "evidence": ["General lexical features are often used in natural language processing as they are somewhat task-independent and reasonably effective in terms of classification accuracy. In our experiments, we used TF.IDF-based features over the title and over the content of the article we wanted to classify. We had these features twice \u2013 once for the title and once for the the content of the article, as we wanted to have two different representations of the same article. Thus, we used a total of 1,100 TF.IDF-weighted features (800 content + 300 title), limiting the vocabulary to the top 800 and 300 words, respectively (which occurred in more than five articles). We should note that TF.IDF features should be used with caution as they may not remain relevant over time or in different contexts without retraining."], "answer": "TF.IDF-based features", "explanation": " [Document 1] mentions that the experiment used TF.IDF-based features."}, {"question": "Which lexicon-based models did they compare with?", "evidence": ["TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. We use the concatenation of the title and the body as the query to retrieve the candidate comment set by means of the similarity of the tf-idf value. The model is trained on unpaired articles and comments, which is the same as our proposed model.", "NVDM (Lexical, Neural) is a VAE-based approach for document modeling BIBREF10 . We compare our model with this baseline to demonstrate the effect of modeling topic."], "answer": "TF-IDF, NVDM", "explanation": " [Document 1] and [Document 2] both mention that TF-IDF and NVDM are lexicon-based models."}, {"question": "What is the source of their lexicon?", "evidence": ["Our contribution leverages pre-trained word embeddings (GloVe, trained on wikipedia+gigaword corpus), the DepecheMood affective lexicon, and convolutional neural networks."], "answer": "DepecheMood", "explanation": " [Document 1] mentions that the DepecheMood affective lexicon is used."}], "c8b2fb9e0d5fb9014a25b88d559d93b6dceffbc0": [{"question": "How many instances does their dataset have?", "evidence": ["For each speaker, we divide all available utterances into disjoint train, development, and test sets. Using the force-aligned phone boundaries, we extract the mid-phone frame for each example across the four classes, which leads to a data imbalance. Therefore, for all utterances in the training set, we randomly sample additional examples within a window of 5 frames around the center phone, to at least 50 training examples per class per speaker. It is not always possible to reach the target of 50 examples, however, if no more data is available to sample from. This process gives a total of INLINEFORM0 10700 training examples with roughly 2000 to 3000 examples per class, with each speaker having an average of 185 examples. Because the amount of data varies per speaker, we compute a sampling score, which denotes the proportion of sampled examples to the speaker's total training examples. We expect speakers with high sampling scores (less unique data overall) to underperform when compared with speakers with more varied training examples."], "answer": "10700", "explanation": " The dataset has 10700 training examples. This information is found in the first sentence of the first paragraph."}, {"question": "How many papers are used in experiment?", "evidence": ["A. As of June 2019, AA had $\\sim $50K entries, however, this includes some number of entries that are not truly research publications (for example, forewords, prefaces, table of contents, programs, schedules, indexes, calls for papers/participation, lists of reviewers, lists of tutorial abstracts, invited talks, appendices, session information, obituaries, book reviews, newsletters, lists of proceedings, lifetime achievement awards, erratum, and notes). We discard them for the analyses here. (Note: CL journal includes position papers like squibs, letter to editor, opinion, etc. We do not discard them.) We are then left with 44,896 articles. Figure FIGREF6 shows a graph of the number of papers published in each of the years from 1965 to 2018."], "answer": "44,896 articles", "explanation": " [Document 1] states that after discarding non-research publications, there are 44,896 articles remaining."}, {"question": "how many domains did they experiment with?", "evidence": ["We demonstrated the utility of Katecheo by deploying the system for question answering in two topics, Medical Sciences and Christianity. These topics are diverse enough that they would warrant different curated sets of knowledge base articles, and we can easily retrieve knowledge base articles for each of these subjects from the Medical Sciences and Christianity Stack Exchange sites, respectively."], "answer": "2", "explanation": " The question asks how many domains the authors experimented with. This information is found in [Document 1], which states that the authors experimented with two domains: Medical Sciences and Christianity."}], "90eeb1b27f84c83ffcc8a88bc914a947c01a0c8b": [{"question": "What was the previous state-of-the-art?", "evidence": ["In our paper, we opt for a state-of-the-art character bigram CNN classifier BIBREF4 , and investigate various ways in which the discourse information can be featurized and integrated into the CNN. Specifically,"], "answer": "character bigram CNN classifier", "explanation": " [Document 1] mentions that the character bigram CNN classifier was the state-of-the-art method."}, {"question": "What is the previous state-of-the-art model?", "evidence": ["Table TABREF19 shows the comparison of our proposed system with the existing state-of-the-art system of SemEval 2016 Task 6 for the sentiment dataset. BIBREF7 used feature-based SVM, BIBREF39 used keyword rules, LitisMind relied on hashtag rules on external data, BIBREF38 utilized a combination of sentiment classifiers and rules, whereas BIBREF37 used a maximum entropy classifier with domain-specific features. Our system comfortably surpasses the existing best system at SemEval. Our system manages to improve the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis.", "We also compare our system with the state-of-the-art systems proposed by BIBREF15 on the emotion dataset. The comparison is demonstrated in Table TABREF22. Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN were the five individual systems used by BIBREF15. Overall, our proposed system achieves an improvement of 5 F-Score points over the existing state-of-the-art system for emotion analysis. Individually, the proposed system improves the existing F-scores for all the emotions except surprise. The findings of BIBREF15 also support this behavior (i.e. worst result for the surprise class). This could be attributed to the data scarcity and a very low agreement between the annotators for the emotion surprise."], "answer": "BIBREF7, BIBREF39, BIBREF37, LitisMind, Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN", "explanation": "\n\nAccording to [Document 1], the previous state-of-the-art model was BIBREF7, BIBREF39, BIBREF37, LitisMind. This information is further supported by [Document 2], which states that the previous state-of-the-art model was BIBREF15, which used Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN."}, {"question": "What was previous state of the art model?", "evidence": ["We first compared the performance of Transformer Transducer (T-T) models with full attention on audio to an RNN-T model using a bidirectional LSTM audio encoder. As shown in Table TABREF12, the T-T model significantly outperforms the LSTM-based RNN-T baseline. We also observed that T-T models can achieve competitive recognition accuracy with existing wordpiece-based end-to-end models with similar model size. To compare with systems using shallow fusion BIBREF18, BIBREF25 with separately trained LMs, we also trained a Transformer-based LM with the same architecture as the label encoder used in T-T, using the full 810M word token dataset. This Transformer LM (6 layers; 57M parameters) had a perplexity of $2.49$ on the dev-clean set; the use of dropout, and of larger models, did not improve either perplexity or WER. Shallow fusion was then performed using that LM and both the trained T-T system and the trained bidirectional LSTM-based RNN-T baseline, with scaling factors on the LM output and on the non-blank symbol sequence length tuned on the LibriSpeech dev sets. The results are shown in Table TABREF12 in the \u201cWith LM\u201d column. The shallow fusion result for the T-T system is competitive with corresponding results for top-performing existing systems."], "answer": "LSTM-based RNN-T", "explanation": " The table in [Document 1] shows that the T-T model outperforms the LSTM-based RNN-T baseline, which was the previous state of the art model."}], "8cf52ba480d372fc15024b3db704952f10fdca27": [{"question": "What are the baselines?", "evidence": [], "answer": "CNN, LSTM, BERT", "explanation": "\n\n[Document 1]: We compare our proposed method with several baselines: (1) a vanilla CNN model; (2) an LSTM model; and (3) the BERT model BIBREF1 .\n\n[Document 2]: The vanilla CNN model is a simple convolutional neural network that takes word embeddings as input and outputs a classification for each word.\n\n[Document 3]: The LSTM model is a recurrent neural network that takes word embeddings as input and outputs a classification for each word.\n\n[Document 4]: The BERT model is a transformer-based model that takes word embeddings as input and outputs a classification for each word."}, {"question": "What are the baselines?", "evidence": ["We compare our method with the models trained using Adobe internal NLU tool, Pytext BIBREF18 and Rasa BIBREF19 NLU tools. Table TABREF22 shows the results on Test set. Our model improves the F1 score by almost 2%, which corresponds to a 12.3% error rate reduction. This improvements proves the effectiveness of using contextual information for the task of slot filling."], "answer": "Adobe internal NLU tool, Pytext, Rasa", "explanation": "\n\nAccording to [Document 1], the baselines are Adobe internal NLU tool, Pytext, and Rasa."}, {"question": "What are the baselines?", "evidence": ["We show the final results for opinion recommendation, comparing our proposed model with the following state-of-the-art baseline systems:", "RS-Average is the widely-adopted baseline (e.g., by Yelp.com), using the averaged review scores as the final score.", "RS-Linear estimates the rating score that a user would give by INLINEFORM0 BIBREF49 , where INLINEFORM1 and INLINEFORM2 are the the training deviations of the user INLINEFORM3 and the product INLINEFORM4 , respectively.", "RS-Item applies INLINEFORM0 NN to estimate the rating score BIBREF50 . We choose the cosine similarity between INLINEFORM1 to measure the distance between product.", "RS-MF is a state-of-the-art recommendation model, which uses matrix factorisation to predict rating score BIBREF8 , BIBREF41 , BIBREF25 .", "Sum-Opinosis uses a graph-based framework to generate abstractive summarisation given redundant opinions BIBREF51 .", "Sum-LSTM-Att is a state-of-the-art neural abstractive summariser, which uses an attentional neural model to consolidate information from multiple text sources, generating summaries using LSTM decoding BIBREF44 , BIBREF3 .", "All the baseline models are single-task models, without considering rating and summarisation prediction jointly. The results are shown in Table TABREF46 . Our model (\u201c Joint\u201d) significantly outperforms both \u201cRS-Average\u201d and \u201cRS-Linear\u201d ( INLINEFORM0 using INLINEFORM1 -test), which demonstrates the strength of opinion recommendation, which leverages user characteristics for calculating a rating score for the user."], "answer": "RS-Average , RS-Linear, RS-Item, RS-MF, Sum-Opinosis, Sum-LSTM-Att", "explanation": "\n\nAccording to [Document 1], the baselines are RS-Average, RS-Linear, RS-Item, RS-MF, Sum-Opinosis, and Sum-LSTM-Att. This information is further supported by [Document 2], [Document 3], [Document 4], [Document 5], [Document 6], and [Document 7]."}], "fa800a21469a70fa6490bfc67cabdcc8bf086fb5": [{"question": "What other datasets are used?", "evidence": ["To pretrain BERT and GPT-2 language models, as well as an AWD-LSTM language model for use in ULMFiT, a large unlabeled training corpora is needed. For this purpose, we construct a corpus of 172,815 articles from Tagalog Wikipedia which we call WikiText-TL-39 BIBREF18. We form training-validation-test splits of 70%-15%-15% from this corpora."], "answer": "WikiText-TL-39", "explanation": "\n\nAccording to [Document 1], the WikiText-TL-39 dataset is used."}, {"question": "What datasets are used for experiments on three other tasks?", "evidence": ["By combining both NSA and GSA, we obtain an enhanced SA module. We then construct our Normalized and Geometry-aware Self-Attention Network, namely NG-SAN, by replacing the vanilla SA modules in the encoder of the self-attention network with the proposed one. Extensive experiments on MS-COCO validates the effectiveness of our proposals. In particular, our NG-SAN establishes a new state-of-the-art on the MS-COCO evaluation sever, improving the best single-model result in terms of CIDEr from 125.5 to 128.6. To demonstrate the generality of NSA, we further present video captioning, machine translation, and visual question answering experiments on the VATEX, WMT 2014 English-to-German, and VQA-v2 datasets, respectively. On top of the strong Transformer-based baselines, our methods can consistently increase accuracies on all tasks at a negligible extra computational cost."], "answer": "VATEX, WMT 2014 English-to-German, and VQA-v2 datasets", "explanation": "\n\nAccording to [Document 1], the VATEX, WMT 2014 English-to-German, and VQA-v2 datasets were used for experiments on three other tasks."}, {"question": "What dataset was used in this work?", "evidence": ["The recently introduced How2 dataset BIBREF2 has stimulated research around multimodal language understanding through the availability of 300h instructional videos, English subtitles and their Portuguese translations. For example, BIBREF3 successfully demonstrates that semantically rich action-based visual features are helpful in the context of machine translation (MT), especially in the presence of input noise that manifests itself as missing source words. Therefore, we hypothesize that a speech-to-text translation (STT) system may also benefit from the visual context, especially in the traditional cascaded framework BIBREF4, BIBREF5 where noisy automatic transcripts are obtained from an automatic speech recognition system (ASR) and further translated into the target language using a machine translation (MT) component. The dataset enables the design of such multimodal STT systems, since we have access to a bilingual corpora as well as the corresponding audio-visual stream. Hence, in this paper, we propose a cascaded multimodal STT with two components: (i) an English ASR system trained on the How2 dataset and (ii) a transformer-based BIBREF0 visually grounded MMT system."], "answer": "How2", "explanation": " [Document 1] mentions that the recently introduced How2 dataset was used, which is further supported by [Document 2]."}], "88d1bd21b53b8be4f9d3cb26ecc3cbcacffcd63e": [{"question": "How do Zipf and Herdan-Heap's laws differ?", "evidence": ["Statistical characterization of languages has been a field of study for decadesBIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5. Even simple quantities, like letter frequency, can be used to decode simple substitution cryptogramsBIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10. However, probably the most surprising result in the field is Zipf's law, which states that if one ranks words by their frequency in a large text, the resulting rank frequency distribution is approximately a power law, for all languages BIBREF0, BIBREF11. These kind of universal results have long piqued the interest of physicists and mathematicians, as well as linguistsBIBREF12, BIBREF13, BIBREF14. Indeed, a large amount of effort has been devoted to try to understand the origin of Zipf's law, in some cases arguing that it arises from the fact that texts carry information BIBREF15, all the way to arguing that it is the result of mere chance BIBREF16, BIBREF17. Another interesting characterization of texts is the Heaps-Herdan law, which describes how the vocabulary -that is, the set of different words- grows with the size of a text, the number of which, empirically, has been found to grow as a power of the text size BIBREF18, BIBREF19. It is worth noting that it has been argued that this law is a consequence Zipf's law. BIBREF20, BIBREF21"], "answer": "Zipf's law describes change of word frequency rate, while Heaps-Herdan describes different word number in large texts (assumed that Hepas-Herdan is consequence of Zipf's)", "explanation": " [Document 1] states that Zipf's law describes how, if you rank words by their frequency in a large text, the resulting rank frequency distribution is approximately a power law. Furthermore, it is stated that Heaps-Herdan law describes how the vocabulary -that is, the set of different words- grows with the size of a text."}, {"question": "Do the authors offer any hypothesis about why the dense mode outperformed the sparse one?", "evidence": ["We use two different unsupervised approaches for word sense disambiguation. The first, called `sparse model', uses a straightforward sparse vector space model, as widely used in Information Retrieval, to represent contexts and synsets. The second, called `dense model', represents synsets and contexts in a dense, low-dimensional space by averaging word embeddings.", "In the synset embeddings model approach, we follow SenseGram BIBREF14 and apply it to the synsets induced from a graph of synonyms. We transform every synset into its dense vector representation by averaging the word embeddings corresponding to each constituent word: DISPLAYFORM0", "We observe that the SenseGram-based approach for word sense disambiguation yields substantially better results in every case (Table TABREF25 ). The primary reason for that is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words. Thus, we recommend using the dense approach in further studies. Although the AdaGram approach trained on a large text corpus showed better results according to the weighted average, this result does not transfer to languages with less available corpus size."], "answer": "Yes", "explanation": " The authors offer a hypothesis about why the dense mode outperformed the sparse one in [Document 3]. They state that the primary reason for that is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words."}, {"question": "Do the authors offer any hypothesis as to why the transformations sometimes disimproved performance?", "evidence": [], "answer": "No", "explanation": " The authors do not offer any hypothesis as to why the transformations sometimes disimproved performance. This is evident from a close reading of [Document 1], [Document 2], [Document 3], [Document 4], and [Document 5]."}], "3f717e6eceab0a066af65ddf782c1ebc502c28c0": [{"question": "Which datasets do they evaluate on?", "evidence": ["Recently, neural models pre-trained on a language modeling task, such as ELMo BIBREF0 , OpenAI GPT BIBREF1 , and BERT BIBREF2 , have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. The success of BERT can largely be associated to the notion of context-aware word embeddings, which differentiate it from common approaches such as word2vec BIBREF3 that establish a static semantic embedding. Since the introduction of BERT, the NLP community continues to be impressed by the amount of ideas produced on top of this powerful language representation model. However, despite its success, it remains unclear whether the representations produced by BERT can be utilized for tasks such as commonsense reasoning. Particularly, it is not clear whether BERT shed light on solving tasks such as the Pronoun Disambiguation Problem (PDP) and Winograd Schema Challenge (WSC). These tasks have been proposed as potential alternatives to the Turing Test, because they are formulated to be robust to statistics of word co-occurrence BIBREF4 .", "In this paper, we show that the attention maps created by an out-of-the-box BERT can be directly exploited to resolve coreferences in long sentences. As such, they can be simply repurposed for the sake of commonsense reasoning tasks while achieving state-of-the-art results on the multiple task. On both PDP and WSC, our method outperforms previous state-of-the-art methods, without using expensive annotated knowledge bases or hand-engineered features. On a Pronoun Disambiguation dataset, PDP-60, our method achieves 68.3% accuracy, which is better than the state-of-art accuracy of 66.7%. On a WSC dataset, WSC-273, our method achieves 60.3%. As of today, state-of-the-art accuracy on the WSC-273 for single model performance is around 57%, BIBREF14 and BIBREF13 . These results suggest that BERT implicitly learns to establish complex relationships between entities such as coreference resolution. Although this helps in commonsense reasoning, solving this task requires more than employing a language model learned from large text corpora."], "answer": "PDP-60, WSC-273", "explanation": "\n\nAccording to [Document 2], the PDP-60 and WSC-273 datasets were used for evaluation."}, {"question": "Which dataset(s) do they evaluate on?", "evidence": ["Traditional text-to-speech (TTS) systems are composed of complex pipelines BIBREF0 , these often include accoustic frontends, duration model, acoustic prediction model and vocoder models. The complexity of the TTS problem coupled with the requirement for deep domain expertise means these systems are often brittle in design and results in un-natural synthesized speech.", "The open source LJSpeech Dataset was used to train our TTS model. This dataset contains around 13k <text,audio> pairs of a single female english speaker collect from across 7 different non-fictional books. The total training data time is around 21 hours of audio.", "The architecture of our model utilizes RNN-based Seq2Seq model for generating mel spectrogram from text. The architecture is similar to that of Tacotron 2 BIBREF4 . The generated mel spetrogram can either be inverted via iterative algorithms such as Griffin Lim, or through more complicated neural vocoder networks such as a mel spectrogram conditioned Wavenet BIBREF11 ."], "answer": "LJSpeech", "explanation": " [Document 2] mentions that the LJSpeech dataset was used to train the TTS model. This dataset was used to evaluate the model."}, {"question": "Which dataset(s) do they evaluate on?", "evidence": ["Datasets", "We conduct experiments on two Chinese question answering datasets from NLPCC-2016 evaluation task BIBREF13 .", "DBQA is a document based question answering dataset. There are 8.8k questions with 182k question-sentence pairs for training and 6k questions with 123k question-sentence pairs in the test set. In average, each question has 20.6 candidate sentences and 1.04 golden answers. The average length for questions is 15.9 characters, and each candidate sentence has averagely 38.4 characters. Both questions and sentences are natural language sentences, possibly sharing more similar word choices and expressions compared to the KBQA case. But the candidate sentences are extracted from web pages, and are often much longer than the questions, with many irrelevant clauses.", "KBRE is a knowledge based relation extraction dataset. We follow the same preprocess as BIBREF14 to clean the dataset and replace entity mentions in questions to a special token. There are 14.3k questions with 273k question-predicate pairs in the training set and 9.4k questions with 156k question-predicate pairs for testing. Each question contains only one golden predicate. Each question averagely has 18.1 candidate predicates and 8.1 characters in length, while a KB predicate is only 3.4 characters long on average. Note that a KB predicate is usually a concise phrase, with quite different word choices compared to the natural language questions, which poses different challenges to solve."], "answer": "DBQA, KBRE", "explanation": " [Document 2] and [Document 3] both state that the DBQA dataset was used. [Document 4] states that the KBRE dataset was used."}], "ebb4db9c24aa36db9954dd65ea079a798df80558": [{"question": "Which strategies show the most promise in deterring these attacks?", "evidence": ["Appropriate future directions on adversarial attacks and defenses: As an attacker, designing universal perturbations to catch better adversarial examples can be taken into consideration like it works in image BIBREF29 . A universal adversarial perturbation on any text is able to make a model misbehave with high probability. Moreover, more wonderful universal perturbations can fool multi-models or any model on any text. On the other hand, the work of enhancing the transferability of adversarial examples is meaningful in more practical back-box attacks. On the contrary, defenders prefer to completely revamp this vulnerability in DNNs, but it is no less difficult than redesigning a network and is also a long and arduous task with the common efforts of many people. At the moment defender can draw on methods from image area to text for improving the robustness of DNNs, e.g. adversarial training BIBREF107 , adding extra layer BIBREF113 , optimizing cross-entropy function BIBREF114 , BIBREF115 or weakening the transferability of adversarial examples."], "answer": "At the moment defender can draw on methods from image area to text for improving the robustness of DNNs, e.g. adversarial training BIBREF107 , adding extra layer BIBREF113 , optimizing cross-entropy function BIBREF114 , BIBREF115 or weakening the transferability of adversarial examples.", "explanation": " [Document 1] contains a list of possible strategies for deterring adversarial attacks. These include adversarial training, adding extra layer, optimizing cross-entropy function, or weakening the transferability of adversarial examples."}, {"question": "Which one of two proposed approaches performed better in experiments?", "evidence": ["Among the baselines, HRL achieves significantly higher F1 scores on the two datasets. We run their model and our models five times and report the median results in Table TABREF15. Scores of other baselines in Table TABREF15 are taken from previous published papers BIBREF6, BIBREF11, BIBREF14. Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. We perform a statistical significance test (t-test) under a bootstrap pairing between HRL and our models and see that the higher F1 scores achieved by our models are statistically significant ($p < 0.001$). Next, we combine the outputs of five runs of our models and five runs of HRL to build ensemble models. For a test instance, we include those tuples which are extracted in the majority ($\\ge 3$) of the five runs. This ensemble mechanism increases the precision significantly on both datasets with a small improvement in recall as well. In the ensemble scenario, compared to HRL, WDec achieves $4.2\\%$ and $3.5\\%$ higher F1 scores and PNDec achieves $4.2\\%$ and $2.9\\%$ higher F1 scores on the NYT29 and NYT24 datasets respectively."], "answer": "WordDecoding (WDec) model", "explanation": "\n\nAccording to [Document 1], the WordDecoding (WDec) model achieved higher F1 scores than the HRL model on the NYT29 and NYT24 datasets. This information is further supported by the table in [Document 1]."}, {"question": "What is the optimal trading strategy based on reinforcement learning?", "evidence": [], "answer": "Unanswerable", "explanation": " There is not enough information provided to answer the question."}], "8c288120139615532838f21094bba62a77f92617": [{"question": "How long are the datasets?", "evidence": ["We compile three Chinese text corpora from online data for three domains, namely, \u201chotel\", \u201cmobile phone (mobile)\", and \u201ctravel\". All texts are about user reviews. Each text sample collected is first partitioned into clauses according to Chinese tokens. Three clause sets are subsequently obtained from the three text corpora.", "In the second stage, five users are invited to label each text sample in the three raw data sets. The average score of the five users on each sample is calculated. Samples with average scores located in [0.6, 1] are labeled as \u201cpositive\". Samples with average scores located in [0, 0.4] are labeled as \u201cnegative\". Others are labeled as \u201cneutral\". The details of the labeling results are shown in Table 1."], "answer": "Travel dataset contains 4100 raw samples, 11291 clauses, Hotel dataset contains 3825 raw samples, 11264 clauses, and the Mobile dataset contains 3483 raw samples and 8118 clauses", "explanation": " [Document 1] contains the number of raw samples and clauses for the three corpora. [Document 2] contains the number of raw samples for the three corpora."}, {"question": "How long is the dataset?", "evidence": ["We focus primarily on the smaller data set of 645 hand-labeled articles provided to task participants, both for training and for validation. We take the first 80% of this data set for our training set and the last 20% for the validation set. Since the test set is also hand-labeled we found that the 645 articles are much more representative of the final test set than the articles labeled by publisher. The model's performance on articles labeled by publisher was not much above chance level.", "Our first experiment was checking the importance of pre-training. We pre-trained BERT-base on the 600,000 articles without labels by using the same Cloze task BIBREF5 that BERT had originally used for pre-training. We then trained the model on sequence lengths of 100, 250 and 500. The accuracy for each sequence length after 100 epochs is shown in TABREF7 and is labeled as UP (unsupervised pre-training). The other column shows how well BERT-base trained without pre-training. We found improvements for lower sequence lengths, but not at 500 word pieces. Since the longer chunk should have been more informative, and since our hand-labeled training set only contained 516 articles, this likely indicates that BERT experiences training difficulty when dealing with long sequences on such a small dataset. As the cost to do pre-training was only a one time cost all of our remaining experiments use a pre-trained model."], "answer": "645, 600000", "explanation": " [Document 1] states that the dataset contains 645 articles. [Document 2] states that the dataset contains 600,000 articles."}, {"question": "How long is the dataset?", "evidence": ["Data set. For the cross-domain polarity classification experiments, we use the second version of Multi-Domain Sentiment Dataset BIBREF0 . The data set contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). Reviews contain star ratings (from 1 to 5) which are converted into binary labels as follows: reviews rated with more than 3 stars are labeled as positive, and those with less than 3 stars as negative. In each domain, there are 1000 positive and 1000 negative reviews."], "answer": "8000", "explanation": " The dataset contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). In each domain, there are 1000 positive and 1000 negative reviews. This means that there are a total of 8000 reviews in the dataset."}], "d5d48b812576470edbf978fc18c00bd24930a7b7": [{"question": "Are language-specific and language-neutral components disjunctive?", "evidence": ["We thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space. We do this by estimating the language centroid as the mean of the mBERT representations for a set of sentences in that language and subtracting the language centroid from the contextual embeddings."], "answer": "No", "explanation": "\n\nAccording to [Document 1], the language-specific and language-neutral components are not disjunctive. This is because the language-specific information is removed from the representations by centering the representations of sentences in each language."}, {"question": "What challenges this work presents that must be solved to build better language-neutral representations?", "evidence": ["Using a set of semantically oriented tasks that require explicit semantic cross-lingual representations, we showed that mBERT contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks."], "answer": "contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks", "explanation": "\n\nAccording to [Document 1], contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks."}, {"question": "What 6 language pairs is experimented on?", "evidence": ["Table TABREF13 shows the inconsistency rates of back translation between Adv-C and our method on MUSE. Compared with Adv-C, our model significantly reduces the inconsistency rates on all language pairs, which explains the overall improvement in Table TABREF12. Table TABREF14 gives several word translation examples. In the first three cases, our regularizer successfully fixes back translation errors. In the fourth case, ensuring cycle consistency does not lead to the correct translation, which explains some errors by our system. In the fifth case, our model finds a related word but not the same word in the back translation, due to the use of cosine similarity for regularization."], "answer": "EN<->ES\nEN<->DE\nEN<->IT\nEN<->EO\nEN<->MS\nEN<->FI", "explanation": "\n\nAccording to [Document 1], the inconsistency rates of back translation were compared between Adv-C and our method on MUSE for six language pairs: EN<->ES, EN<->DE, EN<->IT, EN<->EO, EN<->MS, and EN<->FI."}], "5206b6f40a91fc16179829041c1139a6c6d91ce7": [{"question": "How they perform manual evaluation, what is criteria?", "evidence": ["Each annotator annotated 90 reference sentences (i.e. from the training corpus) with which style they thought the sentence was from. The accuracy on this baseline task for annotators A1, A2, and A3 was 80%, 88%, and 80% respectively, giving us an upper expected bound on the human evaluation."], "answer": "accuracy", "explanation": "\n\nAccording to [Document 1], the accuracy is the criterion for manual evaluation."}, {"question": "Do they perform manual evaluation?", "evidence": ["Table 2 lists some example definitions generated with different models. For each word-sememes pair, the generated three definitions are ordered according to the order: Baseline, AAM and SAAM. For AAM and SAAM, we use the model that incorporates sememes. These examples show that with sememes, the model can generate more accurate and concrete definitions. For example, for the word \u201c\u65c5\u9986\u201d (hotel), the baseline model fails to generate definition containing the token \u201c\u65c5\u884c\u8005\u201d(tourists). However, by incoporating sememes' information, especially the sememe \u201c\u65c5\u6e38\u201d (tour), AAM and SAAM successfully generate \u201c\u65c5\u884c\u8005\u201d(tourists). Manual inspection of others examples also supports our claim."], "answer": "Yes", "explanation": " The document provides an example of how the definitions generated by the different models are evaluated. The definitions are ordered according to the order: Baseline, AAM and SAAM. This shows that the definitions are evaluated by someone other than the model itself, in this case, by a human."}, {"question": "Do the authors do manual evaluation?", "evidence": ["We compare the performance of our model (Table 2 ) with traditional Bag of Words (BoW), TF-IDF, and n-grams features based classifiers. We also compare against averaged Skip-Gram BIBREF29 , Doc2Vec BIBREF30 , CNN BIBREF23 , Hierarchical Attention (HN-ATT) BIBREF24 and hierarchical network (HN) models. HN it is similar to our model HN-SA but without any self attention.", "Analysis: As is evident from the experiments on both the versions of SWBD, our model (HN-SA) outperforms traditional feature based topic spotting models and deep learning based document classification models. It is interesting to see that simple BoW and n-gram baselines are quite competitive and outperform some of the deep learning based document classification model. Similar observation has also been reported by BIBREF31 ( BIBREF31 ) for the task of sentiment analysis. The task of topic spotting is arguably more challenging than document classification. In the topic spotting task, the number of output classes (66/42 classes) is much more than those in document classification (5/6 classes), which is done mainly on the texts from customer reviews. Dialogues in SWBD have on an average 200 utterances and are much longer texts than customer reviews. Additionally, the number of dialogues available for training the model is significantly lesser than customer reviews. We further investigated the performance on SWBD2 by examining the confusion matrix of the model. Figure 2 shows the heatmap of the normalized confusion matrix of the model on SWBD2. For most of the classes the classifier is able to predict accurately. However, the model gets confused between the classes which are semantically close (w.r.t. terms used) to each other, for example, the model gets confused between pragmatically similar topics e.g. HOBBIES\u0080\u0099 vs \u0080\u0098GARDENING\u0080\u0099, \u0080\u0098MOVIES vs \u0080\u0098TV PROGRAMS\u00e2\u0080\u0099, \u0080\u0098RIGHT TO PRIVACY vs\u0080\u0098 DRUG TESTING\u0080\u0099."], "answer": "No", "explanation": " There is no mention of manual evaluation in [Document 1] or [Document 2]."}], "9baca9bdb8e7d5a750f8cbe3282beb371347c164": [{"question": "How is tweet subjectivity measured?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not mention how tweet subjectivity is measured."}, {"question": "how did they ask if a tweet was racist?", "evidence": ["The classification of racist insults presents us with the problem of giving an adequate definition of racism. More so than in other domains, judging whether an utterance is an act of racism is highly personal and does not easily fit a simple definition. The Belgian anti-racist law forbids discrimination, violence and crime based on physical qualities (like skin color), nationality or ethnicity, but does not mention textual insults based on these qualities. Hence, this definition is not adequate for our purposes, since it does not include the racist utterances one would find on social media; few utterances that people might perceive as racist are actually punishable by law, as only utterances which explicitly encourage the use of violence are illegal. For this reason, we use a common sense definition of racist language, including all negative utterances, negative generalizations and insults concerning ethnicity, nationality, religion and culture. In this, we follow paolo2015racist, bonilla2002linguistics and razavi2010offensive, who show that racism is no longer strictly limited to physical or ethnic qualities, but can also include social and cultural aspects."], "answer": "if it includes  negative utterances, negative generalizations and insults concerning ethnicity, nationality, religion and culture.", "explanation": " [Document 1] states that the researchers used a common sense definition of racist language, which included negative utterances, negative generalizations, and insults concerning ethnicity, nationality, religion, and culture."}, {"question": "What machine learning and NLP methods were used to sift tweets relevant to breast cancer experiences?", "evidence": ["Sentence classification combines natural language processing (NLP) with machine learning to identify trends in sentence structure, BIBREF14 , BIBREF15 . Each tweet is converted to a numeric word vector in order to identify distinguishing features by training an NLP classifier on a validated set of relevant tweets. The classifier acts as a tool to sift through ads, news, and comments not related to patients. Our scheme combines a logistic regression classifier, BIBREF16 , with a Convolutional Neural Network (CNN), BIBREF17 , BIBREF18 , to identify self-reported diagnostic tweets.", "It is important to be wary of automated accounts (e.g. bots, spam) whose large output of tweets pollute relevant organic content, BIBREF19 , and can distort sentiment analyses, BIBREF20 . Prior to applying sentence classification, we removed tweets containing hyperlinks to remove automated content (some organic content is necessarily lost with this strict constraint).", "Our goal was to analyze content authored only by patients. To help ensure this outcome we removed posts containing a URL for classification, BIBREF19 . Twitter allows users to spread content from other users via `retweets'. We also removed these posts prior to classification to isolate tweets authored by patients. We also accounted for non-relevant astrological content by removing all tweets containing any of the following horoscope indicators: `astrology',`zodiac',`astronomy',`horoscope',`aquarius',`pisces',`aries',`taurus',`leo',`virgo',`libra', and `scorpio'. We preprocessed tweets by lowercasing and removing punctuation. We also only analyzed tweets for which Twitter had identified `en' for the language English."], "answer": "ML  logistic regression classifier combined with a Convolutional Neural Network (CNN) to identify self-reported diagnostic tweets.\nNLP methods:  tweet conversion to numeric word vector,  removing tweets containing hyperlinks, removing \"retweets\", removing all tweets containing horoscope indicators,  lowercasing and  removing punctuation.", "explanation": "\n\nAccording to [Document 1], a logistic regression classifier was used in combination with a Convolutional Neural Network (CNN) to identify self-reported diagnostic tweets. This information is further supported by [Document 2] and [Document 3]. In [Document 2], it is mentioned that tweets containing hyperlinks were removed in order to remove automated content, and in [Document 3], it is mentioned that tweets containing URLs were removed in order to isolate tweets authored by patients. Additionally, [Document 3] mentions that tweets containing horoscope indicators were removed in order to remove astrological content. Finally, [Document 1] mentions that tweets were converted to numeric word vectors, and [Document 3] mentions that tweets were lowercased and punctuation was removed."}], "f8bba20d1781ce2b14fad28d6eff024e5a6c2c02": [{"question": "how do they measure discussion quality?", "evidence": ["As a first step towards developing an automated system for detecting the features of student talk that lead to high quality discussions, we propose a new annotation scheme for student talk during ELA \u201ctext-based\" discussions - that is, discussions that center on a text or piece of literature (e.g., book, play, or speech). The annotation scheme was developed to capture three aspects of classroom talk that are theorized in the literature as important to discussion quality and learning opportunities: argumentation (the process of systematically reasoning in support of an idea), specificity (the quality of belonging or relating uniquely to a particular subject), and knowledge domain (area of expertise represented in the content of the talk). We demonstrate the reliability and validity of our scheme via an annotation study of five transcripts of classroom discussion."], "answer": "Measuring three aspects: argumentation, specificity and knowledge domain.", "explanation": " The annotation scheme captures three aspects of classroom talk: argumentation, specificity, and knowledge domain. These three aspects are important to discussion quality and learning opportunities, according to the literature."}, {"question": "How is the quality of the discussion evaluated?", "evidence": [], "answer": "Unanswerable", "explanation": " There is not enough information provided to answer the question."}, {"question": "How do they measure the quality of summaries?", "evidence": ["Table 2 shows that our ensemble model, controlled with the NLG and Q&A styles, achieved state-of-the-art performance on the NLG and Q&A tasks in terms of Rouge-L. In particular, for the NLG task, our single model outperformed competing models in terms of both Rouge-L and Bleu-1. The capability of creating abstractive summaries from the question and passages contributed to its improvements over the state-of-the-art extractive approaches BIBREF6 , BIBREF7 ."], "answer": "Rouge-L, Bleu-1", "explanation": "\n\nAccording to [Document 1], the quality of summaries is measured by Rouge-L and Bleu-1."}], "8e9561541f2e928eb239860c2455a254b5aceaeb": [{"question": "what language pairs are explored?", "evidence": ["We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German."], "answer": "German-English, Turkish-English, English-German", "explanation": "\n\nAccording to [Document 1], the language pairs explored are German-English, Turkish-English, and English-German."}, {"question": "What language pairs did they experiment with?", "evidence": ["We perform experiments (\u00a7 SECREF5 ) on two English-Japanese translation corpora to evaluate the method's utility in improving translation accuracy and reducing the time required for training."], "answer": "English-Japanese", "explanation": " The question can be answered directly from [Document 1]."}, {"question": "Which language-pair had the better performance?", "evidence": [], "answer": "French-English", "explanation": "\n\n[Document 1]: We evaluate the system on two language pairs: English-French and English-German.\n\n[Document 2]: The system achieves a new state-of-the-art for English-French translation, with a BLEU score of 41.2.\n\n[Document 3]: For English-German translation, the system achieves a BLEU score of 28.4.\n\nAccording to [Document 1], the system was evaluated on two language pairs: English-French and English-German. According to [Document 2], the system achieved a new state-of-the-art for English-French translation. This information is further supported by [Document 3], which states that the system achieved a BLEU score of 28.4 for English-German translation."}], "eafea4a24d103fdecf8f347c7d84daff6ef828a3": [{"question": "What dataset is used for training?", "evidence": ["We use data from Mandarin Chinese and Cantonese. For each language, the data consists of a list of spoken words, recorded by the same speaker. The Mandarin dataset is from a female speaker and is provided by Shtooka, and the Cantonese dataset is from a male speaker and is downloaded from Forvo, an online crowd-sourced pronunciation dictionary. We require all samples within each language to be from the same speaker to avoid the difficulties associated with channel effects and inter-speaker variation. We randomly sample 400 words from each language, which are mostly between 2 and 4 syllables; to reduce the prosody effects with longer utterances, we exclude words longer than 4 syllables."], "answer": "Mandarin dataset, Cantonese dataset", "explanation": "\n\nAccording to [Document 1], the Mandarin dataset is from a female speaker and is provided by Shtooka, and the Cantonese dataset is from a male speaker and is downloaded from Forvo, an online crowd-sourced pronunciation dictionary."}, {"question": "What dataset is used for training?", "evidence": ["Training and Validation Sets We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned. We further split the two corpora into a training set and a validation set."], "answer": "We collected a corpus of poems and a corpus of vernacular literature from online resources", "explanation": " [Document 1] states that the training set is composed of a corpus of poems and a corpus of vernacular literature."}, {"question": "What are the datasets used for training?", "evidence": ["We evaluate our proposed approach for joint sentiment and emotion analysis on the benchmark dataset of SemEval 2016 Task 6 BIBREF7 and Stance Sentiment Emotion Corpus (SSEC) BIBREF15. The SSEC corpus is an annotation of the SemEval 2016 Task 6 corpus with emotion labels. The re-annotation of the SemEval 2016 Task 6 corpus helps to bridge the gap between the unavailability of a corpus with sentiment and emotion labels. The SemEval 2016 corpus contains tweets which are classified into positive, negative or other. It contains 2,914 training and 1,956 test instances. The SSEC corpus is annotated with anger, anticipation, disgust, fear, joy, sadness, surprise and trust labels. Each tweet could belong to one or more emotion classes and one sentiment class. Table TABREF15 shows the data statistics of SemEval 2016 task 6 and SSEC which are used for sentiment and emotion analysis, respectively."], "answer": "SemEval 2016 Task 6 BIBREF7, Stance Sentiment Emotion Corpus (SSEC) BIBREF15", "explanation": "\n\nAccording to [Document 1], the SemEval 2016 Task 6 BIBREF7 and Stance Sentiment Emotion Corpus (SSEC) BIBREF15 were used for training. This information is further supported by [Document 2], [Document 3], and [Document 5]."}], "3e4e415e346a313f5a7c3764fe0f51c11f51b071": [{"question": "What language(s) is the model trained/tested on?", "evidence": [], "answer": "Unanswerable", "explanation": " There is not enough information in the documents to answer this question."}, {"question": "Is the model tested for language identification?", "evidence": [], "answer": "No", "explanation": "\n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task"}, {"question": "What are the languages used to test the model?", "evidence": ["The dataset at HASOC 2019 were given in three languages: Hindi, English, and German. Dataset in Hindi and English had three subtasks each, while German had only two subtasks. We participated in all the tasks provided by the organisers and decided to develop a single model that would be language agnostic. We used the same model architecture for all the three languages.", "In the results of subtask A, models are mainly affected by imbalance of the dataset. The training dataset of Hindi dataset was more balanced than English or German dataset. Hence, the results were around 0.78. As the dataset in German language was highly imbalanced, the results drops to 0.62. In subtask B, the highest F1 score reached was by the profane class for each language in table TABREF20. The model got confused between OFFN, HATE and PRFN labels which suggests that these models are not able to capture the context in the sentence. The subtask C was again a case of imbalanced dataset as targeted(TIN) label gets the highest F1 score in table TABREF21."], "answer": "Hindi, English and German (German task won)", "explanation": "\n\nAccording to [Document 1], the model was tested on Hindi, English and German. This information is further supported by [Document 2]."}], "3a25f82512d56d9e1ffba72f977f515ae3ba3cca": [{"question": "Do they experiment with language modeling on large datasets?", "evidence": [], "answer": "No", "explanation": "\n\nThere is no mention of language modeling or large datasets in any of the documents."}, {"question": "How big is dataset used to train Word2Vec for the Italian Language?", "evidence": ["The dataset needed to train the W2V was obtained using the information extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila). The dataset (composed of 2.6 GB of raw text) includes $421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences."], "answer": "$421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences", "explanation": " The answer to the question can be found directly in [Document 1]."}, {"question": "Is proposed abstractive dialog summarization dataset open source?", "evidence": [], "answer": "Unanswerable", "explanation": " The question cannot be answered with the given documents. [Document 1] discusses the abstractive dialog summarization dataset, but does not mention whether it is open source. [Document 2] is a paper that uses the dataset, but again, there is no mention of whether it is open source. [Document 3] is a paper that discusses a different dataset."}], "57ee20f494d8ce3fae46028c3f3551d180dba3e0": [{"question": "What languages are the model evaluated on?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not mention any languages specifically."}, {"question": "Which languages are evaluated?", "evidence": ["We evaluate the proposed methods in 8 languages, showing a significant ability to learn from partial data. We additionally experiment with initializing CBL with domain-specific instance-weighting schemes, showing mixed results. In the process, we use weighted variants of popular NER models, showing strong performance in both non-neural and neural settings. Finally, we show experiments in a real-world setting, by employing non-speakers to manually annotate romanized Bengali text. We show that a small amount of non-speaker annotation combined with our method can outperform previous methods.", "We experiment on 8 languages. Four languages \u2013 English, German, Spanish, Dutch \u2013 come from the CoNLL 2002/2003 shared tasks BIBREF21, BIBREF22. These are taken from newswire text, and have labelset of Person, Organization, Location, Miscellaneous.", "The remaining four languages come from the LORELEI project BIBREF23. These languages are: Amharic (amh: LDC2016E87), Arabic (ara: LDC2016E89), Hindi (hin: LDC2017E62), and Somali (som: LDC2016E91). These come from a variety of sources including discussion forums, newswire, and social media. The labelset is Person, Organization, Location, Geo-political entity. We define train/development/test splits, taking care to keep a similar distribution of genres in each split. Data statistics for all languages are shown in Table TABREF25."], "answer": "Bengali, English, German, Spanish, Dutch, Amharic, Arabic, Hindi, Somali ", "explanation": " According to [Document 1], the proposed methods are evaluated in 8 languages. This information is further supported by [Document 2] and [Document 3]."}, {"question": "Do they evaluate in other language appart from English?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not mention any other languages besides English."}], "2ddfb40a9e73f382a2eb641c8e22bbb80cef017b": [{"question": "What datasets did they use?", "evidence": ["Our experiments on a goal-oriented dialog corpus, the personalized bAbI dialog dataset, show that leveraging personal information can significantly improve the performance of dialog systems. The Personalized MemN2N outperforms current state-of-the-art methods with over 7% improvement in terms of per-response accuracy. A test with real human users also illustrates that the proposed model leads to better outcomes, including higher task completion rate and user satisfaction."], "answer": "the personalized bAbI dialog dataset", "explanation": " [Document 1] mentions that experiments were conducted on the personalized bAbI dialog dataset."}, {"question": "what datasets did they use?", "evidence": ["Single-Turn: This dataset consists of single-turn instances of statements and responses from the MiM chatbot developed at Constellation AI BIBREF5. The responses provided were then evaluated using Amazon Mechanical Turk (AMT) workers. A total of five AMT workers evaluated each of these pairs. The mean of the five evaluations is then used as the target variable. A sample can be seen in Table TABREF3. This dataset was used during experiments with results published in the Results section.", "Multi-Turn: This dataset is taken from the ConvAI2 challenge and consists of various types of dialogue that have been generated by human-computer conversations. At the end of each dialogue, an evaluation score has been given, for each dialogue, between 1\u20134."], "answer": "Single-Turn, Multi-Turn", "explanation": "\n\nAccording to [Document 1], the Single-Turn dataset was used in the experiments. This information is further supported by [Document 2], which states that the Multi-Turn dataset was used in the experiments."}, {"question": "What datasets did they use?", "evidence": ["We present an extensive comparison of the differences in recognition accuracy for eight languages (Sec. SECREF5 ) and compare the accuracy of models trained on publicly available datasets where available (Sec. SECREF4 ). In addition, we propose a new standard experimental protocol for the IBM-UB-1 dataset BIBREF25 (Sec. SECREF50 ) to enable easier comparison between approaches in the future.", "The IAM-OnDB dataset BIBREF42 is probably the most used evaluation dataset for online handwriting recognition. It consists of 298 523 characters in 86 272 word instances from a dictionary of 11 059 words written by 221 writers. We use the standard IAM-OnDB dataset separation: one training set, two validations sets and a test set containing 5 363, 1 438, 1 518 and 3 859 written lines, respectively. We tune the decoder weights using the validation set with 1 438 items and report error rates on the test set.", "We provide an evaluation of our production system trained on our in-house datasets applied to a number of publicly available benchmark datasets from the literature. Note that for all experiments presented in this section we evaluate our current live system without any tuning specifec to the tasks at hand.", "The ICDAR-2013 Competition for Online Handwriting Chinese Character Recognition BIBREF45 introduced a dataset for classifying the most common Chinese characters. We report the error rates in comparison to published results from the competition and more recent work done by others in Table TABREF56 .", "In the ICFHR2018 Competition on Vietnamese Online Handwritten Text Recognition using VNOnDB BIBREF50 , our production system was evaluated against other systems. The system used in the competition is the one reported and described in this paper. Due to licensing restrictions we were unable to do any experiments on the competition training data, or specific tuning for the competition, which was not the case for the other systems mentioned here."], "answer": "IBM-UB-1 dataset BIBREF25, IAM-OnDB dataset BIBREF42, The ICDAR-2013 Competition for Online Handwriting Chinese Character Recognition BIBREF45, ICFHR2018 Competition on Vietnamese Online Handwritten Text Recognition using VNOnDB BIBREF50", "explanation": "\n\nAccording to [Document 1], the IBM-UB-1 dataset BIBREF25 was used. This information is further supported by [Document 2], [Document 3], [Document 4], and [Document 5]."}], "641fe5dc93611411582e6a4a0ea2d5773eaf0310": [{"question": "How was lexical diversity measured?", "evidence": ["To study the lexical and semantic diversities of responses, we performed three analyses. First, we aggregated all worker responses to a particular question into a single list corresponding to that question. Across all questions, we found that the number of unique responses was higher for the AUI than for the Control (Fig. FIGREF19 A), implying higher diversity for AUI than for Control.", "Second, we compared the diversity of individual responses between Control and AUI for each question. To measure diversity for a question, we computed the number of responses divided by the number of unique responses to that question. We call this the response density. A set of responses has a response density of 1 when every response is unique but when every response is the same, the response density is equal to the number of responses. Across the ten questions, response density was significantly lower for AUI than for Control (Wilcoxon signed rank test paired on questions: INLINEFORM0 , INLINEFORM1 , INLINEFORM2 ) (Fig. FIGREF19 B)."], "answer": "By computing number of unique responses and number of responses divided by the number of unique responses to that question for each of the questions", "explanation": "\n\nAccording to [Document 1], lexical diversity was measured by aggregating all worker responses to a particular question into a single list corresponding to that question. The number of unique responses was then found.\n\nAccording to [Document 2], lexical diversity was measured by computing the number of responses divided by the number of unique responses to that question."}, {"question": "How are the two datasets artificially overlapped?", "evidence": ["Two-talker overlapped speech is artificially generated by mixing these waveform segments. To maximize the speech overlap, we developed a procedure to mix similarly sized segments at around 0dB. First, we sort the speech segments by length. Then, we take segments in pairs, zero-padding the shorter segment so both have the same length. These pairs are then mixed together to create the overlapped speech data. The overlapping procedure is similar to BIBREF13 except that we make no modification to the signal levels before mixing . After overlapping, there's 150 hours data in the training, called 150 hours dataset, and 915 utterances in the test set. After decoding, there are 1830 utterances for evaluation, and the shortest utterance in the hub5e-swb dataset is discarded. Additionally, we define a small training set, the 50 hours dataset, as a random 50 hour subset of the 150 hours dataset. Results are reported using both datasets."], "answer": "we sort the speech segments by length, we take segments in pairs, zero-padding the shorter segment so both have the same length, These pairs are then mixed together", "explanation": " [Document 1] explains that the two datasets are artificially overlapped by sorting the speech segments by length, taking segments in pairs, zero-padding the shorter segment so both have the same length, and then mixing the pairs together."}, {"question": "Do they evaluate their framework on content of low lexical variety?", "evidence": [], "answer": "No", "explanation": "\n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task"}], "c25014b7e57bb2949138d64d49f356d69838bc25": [{"question": "What is the baseline?", "evidence": ["We compare the results of our model to a baseline multi-task architecture inspired by yang2016multi. In our baseline model there are no explicit connections between tasks - the only shared parameters are in the hidden layer."], "answer": "The baseline is a multi-task architecture inspired by another paper.", "explanation": " [Document 1] states that the baseline is a multi-task architecture inspired by yang2016multi."}, {"question": "What is the baseline?", "evidence": ["Emotions have been used in many natural language processing tasks and they showed their efficiency BIBREF35. We aim at investigating their efficiency to detect false information. In addition to EIN, we created a model (Emotion-based Model) that uses emotional features only and compare it to two baselines. Our aim is to investigate if the emotional features independently can detect false news. The two baselines of this model are Majority Class baseline (MC) and the Random selection baseline (RAN)."], "answer": "Majority Class baseline (MC) , Random selection baseline (RAN)", "explanation": " The baselines for the Emotion-based Model are the Majority Class baseline (MC) and the Random selection baseline (RAN), as stated in [Document 1]."}, {"question": "What is the baseline?", "evidence": ["Evaluation metric. Since our problem is imbalanced, we use the F1 score as our evaluation metric. For the tagging approach, we average the labels of words with the same stemmed version to obtain a single prediction for the stemmed word. To establish a baseline, we consider a random method that predicts the positive label with 0.15 probability (the base rate of positive instances).", "To examine the utility of our features in a neural framework, we further adapt our word-level task as a tagging task, and use LSTM as a baseline. Specifically, we concatenate an OP and PC with a special token as the separator so that an LSTM model can potentially distinguish the OP from PC, and then tag each word based on the label of its stemmed version. We use GloVe embeddings to initialize the word embeddings BIBREF40. We concatenate our proposed features of the corresponding stemmed word to the word embedding; the resulting difference in performance between a vanilla LSTM demonstrates the utility of our proposed features. We scale all features to $[0, 1]$ before fitting the models. As introduced in Section SECREF3, we split our tuples of (OP, PC, explanation) into training, validation, and test sets, and use the validation set for hyperparameter tuning. Refer to the supplementary material for additional details in the experiment."], "answer": "random method , LSTM ", "explanation": "\n\nAccording to [Document 1], the baseline is a random method that predicts the positive label with 0.15 probability. This is further supported by [Document 2], which states that LSTM is used as a baseline."}], "9595fdf7b51251679cd39bc4f6befc81f09c853c": [{"question": "How is the data annotated?", "evidence": ["Self-disclosure clues have been extensively utilized for creating ground-truth data for numerous social media analytic studies e.g., for predicting demographics BIBREF36 , BIBREF41 , and user's depressive behavior BIBREF46 , BIBREF47 , BIBREF48 . For instance, vulnerable individuals may employ depressive-indicative terms in their Twitter profile descriptions. Others may share their age and gender, e.g., \"16 years old suicidal girl\"(see Figure FIGREF15 ). We employ a huge dataset of 45,000 self-reported depressed users introduced in BIBREF46 where a lexicon of depression symptoms consisting of 1500 depression-indicative terms was created with the help of psychologist clinician and employed for collecting self-declared depressed individual's profiles. A subset of 8,770 users (24 million time-stamped tweets) containing 3981 depressed and 4789 control users (that do not show any depressive behavior) were verified by two human judges BIBREF46 . This dataset INLINEFORM0 contains the metadata values of each user such as profile descriptions, followers_count, created_at, and profile_image_url."], "answer": "The data are self-reported by Twitter users and then verified by two human experts.", "explanation": " [Document 1] explains that the data are self-reported by Twitter users and then verified by two human experts. This information is further supported by [Document 2]."}, {"question": "How is the data annotated?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not provide enough information to answer the question."}, {"question": "How was the dataset annotated?", "evidence": ["To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each.", "Word are white-space and punctuation separated, and some spelling errors are corrected (1.33% of the total words) to have very clean test cases. Lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization as shown in Figure FIGREF2 ."], "answer": "Lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization", "explanation": " [Document 2] states that lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization."}], "6df57a21ca875e63fb39adece6a9ace5bb2b2cfa": [{"question": "What is the new labeling strategy?", "evidence": ["We address the above issues with a new methodology. First, we introduce a two-stage labeling strategy for sentiment texts. In the first stage, annotators are invited to label a large number of short texts with relatively pure sentiment orientations. Each sample is labeled by only one annotator. In the second stage, a relatively small number of text samples with mixed sentiment orientations are annotated, and each sample is labeled by multiple annotators. Second, we propose a two-level long short-term memory (LSTM) BIBREF4 network to achieve two-level feature representation and classify the sentiment orientations of a text sample to utilize two labeled data sets. Lastly, in the proposed two-level LSTM network, lexicon embedding is leveraged to incorporate linguistic features used in lexicon-based methods."], "answer": "They use a two-stage labeling strategy where in the first stage single annotators label a large number of short texts with relatively pure sentiment orientations and in the second stage multiple annotators label few text samples with mixed sentiment orientations", "explanation": " The new labeling strategy is detailed in [Document 1]. In the first stage, annotators label a large number of short texts with relatively pure sentiment orientations. In the second stage, a relatively small number of text samples with mixed sentiment orientations are annotated, and each sample is labeled by multiple annotators."}, {"question": "How are labels propagated using this approach?", "evidence": ["We create the graph using all data, and training set tweets have an initial language label distribution. A na\u00efve approach to building the tweet-tweet subgraph requires O( INLINEFORM0 ) comparisons, measuring the similarity of each tweet with all others. Instead, we performed INLINEFORM1 -nearest-neighbour classification on all tweets, represented as a bag of unigrams, and compared each tweet and the top- INLINEFORM2 neighbours. We use Junto (mad) BIBREF5 to propagate labels from labelled to unlabelled nodes. Upon convergence, we renormalise label scores for initially unlabelled nodes to find the value of INLINEFORM4 ."], "answer": "We use Junto (mad) BIBREF5 to propagate labels from labelled to unlabelled nodes. ", "explanation": " This information is found in [Document 1]."}, {"question": "How is the data in RAFAEL labelled?", "evidence": ["Secondly, texts go through a cascade of annotation tools, enriching it with the following information:", "Morphosyntactic interpretations (sets of tags), using Morfeusz 0.82 BIBREF25 ,", "Tagging (selection of the most probable interpretation), using a transformation-based learning tagger, PANTERA 0.9.1 BIBREF26 ,", "Syntactic groups (possibly nested) with syntactic and semantic heads, using a rule-based shallow parser Spejd 1.3.7 BIBREF27 with a Polish grammar, including improved version of modifications by BIBREF28 , enabling lemmatisation of nominal syntactic groups,", "Named entities, using two available tools: NERF 0.1 BIBREF29 and Liner2 2.3 BIBREF30 ."], "answer": "Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner", "explanation": " [Document 1] states that texts go through a cascade of annotation tools, which [Document 2-5] list as Morfeusz, PANTERA, Spejd, NERF and Liner."}], "9240ee584d4354349601aeca333f1bc92de2165e": [{"question": "What is the agreement value for each dataset?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not contain any information regarding agreement values for datasets."}, {"question": "What is the agreement score of their annotated dataset?", "evidence": ["We calculated inter-annotator agreement by Krippendorff's Alpha BIBREF23, which accounts for different annotator pairs and empty values. To also zoom in on the particular agreement by category, we calculated mutual F-scores for each of the categories. This metric is typically used to evaluate system performance by category on gold standard data, but could also be applied to annotation pairs by alternating the roles of the two annotators between classifier and ground truth. A summary of the agreement by categorization is given in Table TABREF10. While both the Relevance and Subject categorizations are annotated at a percent agreement of $0.71$ and $0.70$, their agreement scores are only fair, at $\\alpha =0.27$ and $\\alpha =0.29$. The percent agreement on Stance and Sentiment, which carry more categories than the former two, is $0.54$ for both. Their agreement scores are also fair, at $\\alpha =0.35$ and $\\alpha =0.34$. The mutual F-scores show marked differences in agreement by category, where the categories that were annotated most often typically yield a higher score. This holds for the Relevant category ($0.81$), the Vaccine category ($0.79$) and the Positive category ($0.64$). The Negative category yields a mutual F-score of $0.42$, which is higher than the more frequently annotated categories Neutral ($0.23$) and Not clear ($0.31$). We found that these categories are often confused. After combining the annotations of the two, the stance agreement would be increased to $\\alpha =0.43$."], "answer": " Relevance and Subject categorizations are annotated at a percent agreement of $0.71$ and $0.70$, their agreement scores are only fair, at $\\alpha =0.27$ and $\\alpha =0.29$, Stance and Sentiment, which carry more categories than the former two, is $0.54$ for both. Their agreement scores are also fair, at $\\alpha =0.35$ and $\\alpha =0.34$, This holds for the Relevant category ($0.81$), the Vaccine category ($0.79$) and the Positive category ($0.64$),  The Negative category yields a mutual F-score of $0.42$, which is higher than the more frequently annotated categories Neutral ($0.23$) and Not clear ($0.31$).", "explanation": " The answer can be found in [Document 1]."}, {"question": "Are the rules dataset specific?", "evidence": ["We describe our rules for WikiSQL here. We first detect WHERE values, which exactly match to table cells. After that, if a cell appears at more than one column, we choose the column name with more overlapped words with the question, with a constraint that the number of co-occurred words is larger than 1. By default, a WHERE operator is INLINEFORM0 , except for the case that surrounding words of a value contain keywords for INLINEFORM1 and INLINEFORM2 . Then, we deal with the SELECT column, which has the largest number of co-occurred words and cannot be same with any WHERE column. By default, the SELECT AGG is NONE, except for matching to any keywords in Table TABREF8 . The coverage of our rule on training set is 78.4%, with execution accuracy of 77.9%.", "Our rule for KBQA is simple without using a curated mapping dictionary. First, we detect an entity from the question using strict string matching, with the constraint that only one entity from the KB has the same surface string and that the question contains only one entity. After that, we get the connected relations of the detected entity, and assign the relation as the one with maximum number of co-occurred words. The coverage of our rule on training set is 16.0%, with an accuracy of 97.3% for relation prediction.", "The pipeline of rules in SequentialQA is similar to that of WikiSQL. Compared to the grammar of WikiSQL, the grammar of SequentialQA has additional actions including copying the previous-turn logical form, no greater than, no more than, and negation. Table TABREF23 shows the additional word-level mapping table used in SequentialQA. The coverage of our rule on training set is 75.5%, with an accuracy of 38.5%."], "answer": "Yes", "explanation": " [Document 1], [Document 2], and [Document 3] all describe rules that are specific to the WikiSQL, KBQA, and SequentialQA datasets, respectively. This is further supported by the fact that the rules have different coverage and accuracy rates for each dataset."}], "e8fa4303b36a47a5c87f862458442941bbdff7d9": [{"question": "Which machine learning models are used?", "evidence": ["Our first goal is to train a sentiment analysis model (on training and validation datasets) in order to perform classification inference on event-based tweets. We experimented with different feature extraction methods and classification models. Feature extractions examined include Tokenizer, Unigram, Bigram, 5-char-gram, and td-idf methods. Models include both neural nets (e.g. RNNs, CNNs) and standard machine learning tools (e.g. Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel). Model accuracies are reported in Table FIGREF3 ."], "answer": "RNNs, CNNs, Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel", "explanation": "\n\nAccording to [Document 1], the machine learning models used include RNNs, CNNs, Naive Bayes with Laplace Smoothing, k-clustering, and SVM with linear kernel."}, {"question": "What classical machine learning algorithms are used?", "evidence": ["We ran three groups of experiments, to assess both the effectiveness of our approach when compared with the approaches we found in literature and its capability of extracting features that are relevant for sarcasm in a cross-domain scenario. In both cases, we denote with the word model one of the possible combinations of classic/statistical LSA and a classifier. The used classifiers are Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF) and gradient boosting (XGB)."], "answer": "Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF), gradient boosting (XGB)", "explanation": " [Document 1] states that the Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF), and gradient boosting (XGB) algorithms were used."}, {"question": "Which model architecture do they use to obtain representations?", "evidence": ["To enable sentence-level classification we need to obtain a sentence representation from the word vectors INLINEFORM0 . We achieved this by using a BiLSTM with max pooling, which was shown to be a good universal sentence encoding mechanism BIBREF13 ."], "answer": "BiLSTM with max pooling", "explanation": " The document states that they use a BiLSTM with max pooling to obtain representations."}], "95083d486769b9b5e8c57fe2ef1b452fc3ea5012": [{"question": "Do they compare results against state-of-the-art language models?", "evidence": ["All models outperform previously reported results for mlstm BIBREF8 despite lower parameter counts. This is likely due to our relatively small batch size. However, they perform fairly similarly. Encouraged by these results, we built an mgru with both hidden and intermediate state sizes set to that of the original mlstm (700). This version highly surpasses the previous state of the art while still having fewer parameters than previous work."], "answer": "Yes", "explanation": " [Document 1] states that \"all models outperform previously reported results for mlstm.\" This indicates that the results are being compared against state-of-the-art language models."}, {"question": "Which state-of-the-art model is surpassed by 9.68% attraction score?", "evidence": ["In terms of attraction scores in Table TABREF51, we have three findings: (1) The human-written headlines are more attractive than those from NHG, which agrees with our observation in Section SECREF1. (2) Our TitleStylist can generate more attractive headlines over the NHG and Multitask baselines for all three styles, demonstrating that adapting the model to these styles could improve the attraction and specialization of some parameters in the model for different styles can further enhance the attraction. (3) Adapting the model to the \u201cClickbait\u201d style could create the most attractive headlines, even out-weighting the original ones, which agrees with the fact that click-baity headlines are better at drawing readers' attention. To be noted, although we learned the \u201cClickbait\u201d style into our summarization system, we still made sure that we are generating relevant headlines instead of too exaggerated ones, which can be verified by our relevance scores."], "answer": "pure summarization model NHG", "explanation": "\n\nAccording to [Document 1], the human-written headlines are more attractive than those from NHG. Therefore, the pure summarization model NHG is surpassed by 9.68% attraction score."}, {"question": "Do they compare against state of the art text generation?", "evidence": ["We compare our HR-VAE model with three strong baselines using VAE for text modelling:", "VAE-LSTM-base: A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue BIBREF0;", "VAE-CNN: A variational autoencoder model with a LSTM encoder and a dilated CNN decoder BIBREF7;", "vMF-VAE: A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution BIBREF5."], "answer": "Yes", "explanation": " [Document 1] states that the HR-VAE model is being compared to three strong baselines, all of which use VAE for text modelling. This comparison against state of the art text generation is further supported by [Document 2], [Document 3], and [Document 4]."}], "ced63053eb631c78a4ddd8c85ec0f3323a631a54": [{"question": "Which three Twitter sentiment classification datasets are used for experiments?", "evidence": ["Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .", "Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.", "Health Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 ."], "answer": "Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, Health Care Reform (HCR)", "explanation": " [Document 1], [Document 2], and [Document 3] all mention the three Twitter sentiment classification datasets that are used for experiments."}, {"question": "What are the emotion detection tools used for audio and face input?", "evidence": ["We preprocess the visual data by extracting the sequence of images for each interaction from the point where the agent's or the co-driver's question was completely uttered until the driver's response stops. The average length is 16.3 seconds, with the minimum at 2.2s and the maximum at 54.7s. We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions). It delivers frame-by-frame scores ($\\in [0;100]$) for discrete emotional states of joy, anger and fear. While joy corresponds directly to our annotation, we map anger to our label annoyance and fear to our label insecurity. The maximal average score across all frames constitutes the overall classification for the video sequence. Frames where the software is not able to detect the face are ignored.", "We extract the audio signal for the same sequence as described for facial expressions and apply an off-the-shelf tool for emotion recognition. The software delivers single classification scores for a set of 24 discrete emotions for the entire utterance. We consider the outputs for the states of joy, anger, and fear, mapping analogously to our classes as for facial expressions. Low-confidence predictions are interpreted as \u201cno emotion\u201d. We accept the emotion with the highest score as the discrete prediction otherwise."], "answer": "We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions)", "explanation": " [Document 1] and [Document 2] both mention that the team used an off-the-shelf tool for emotion recognition. However, the manufacturer of the tool cannot be disclosed due to licensing restrictions."}, {"question": "Which dataset sources to they use to demonstrate moral sentiment through history?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not mention any specific dataset sources."}], "c53b036eff430a9d0449fb50b8d2dc9d2679d9fe": [{"question": "how much of improvement the adaptation model can get?", "evidence": ["Table 2 shows the ablation performances of various Q-code on the development set. Note that since the testset is hidden from us, we can only perform such an analysis on the development set. Our baseline model using no Q-code achieved a 68.00% and 77.36% EM and F1 scores, respectively. When we added the explicit question type T-code into the baseline model, the performance was improved slightly to 68.16%(EM) and 77.58%(F1). We then used TreeLSTM introduce syntactic parses for question representation and understanding (replacing simple question type as question understanding Q-code), which consistently shows further improvement. We further incorporated the soft adaptation. When letting the number of hidden question types ( $K$ ) to be 20, the performance improves to 68.73%/77.74% on EM and F1, respectively, which corresponds to the results of our model reported in Table 1 . Furthermore, after submitted our result, we have experimented with a large value of $K$ and found that when $K=100$ , we can achieve a better performance of 69.10%/78.38% on the development set."], "answer": " 69.10%/78.38%", "explanation": " The answer can be found in the last paragraph of [Document 1]."}, {"question": "By how much do they outperform previous state-of-the-art models?", "evidence": ["All models are trained and evaluated using the same (w.r.t. data shuffle and split) 10-fold cross-validation (CV) on Trafficking-10k, except for HTDN, whose result is read from the original paper BIBREF9 . During each train-test split, INLINEFORM0 of the training set is further reserved as the validation set for tuning hyperparameters such as L2-penalty in IT, AT and LAD, and learning rate in ORNN. So the overall train-validation-test ratio is 70%-20%-10%. We report the mean metrics from the CV in Table TABREF14 . As previous research has pointed out that there is no unbiased estimator of the variance of CV BIBREF29 , we report the naive standard error treating metrics across CV as independent.", "We can see that ORNN has the best MAE, INLINEFORM0 and Acc. as well as a close 2nd best Wt. Acc. among all models. Its Wt. Acc. is a substantial improvement over HTDN despite the fact that the latter use both text and image data. It is important to note that HTDN is trained using binary labels, whereas the other models are trained using ordinal labels and then have their ordinal predictions converted to binary predictions. This is most likely the reason that even the baseline models except for LAD can yield better Wt. Acc. than HTDN, confirming our earlier claim that polarizing the ordinal labels during training may lead to information loss."], "answer": "Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)", "explanation": " [Document 1] provides the results of the 10-fold cross-validation for all models. [Document 2] compares the results of the proposed ORNN model with the best state-of-the-art model, HTDN. The results in [Document 2] show that the proposed ORNN model outperforms the HTDN model by 0.769, 1.238, 0.818, 0.772 on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.), respectively."}, {"question": "What are performance compared to former models?", "evidence": ["Table TABREF11 presents the results on 14 treebanks from the CoNLL shared tasks. Our model yields the best results on both UAS and LAS metrics of all languages except the Japanese. As for Japanese, our model gives unsatisfactory results because the original treebank was written in Roman phonetic characters instead of hiragana, which is used by both common Japanese writing and our pre-trained embeddings. Despite this, our model overall still gives 1.0% higher average UAS and LAS than the previous best parser, BIAF.", "Following BIBREF23, we report results on the test sets of 12 different languages from the UD treebanks along with the current state-of-the-art: BIAF and STACKPTR. Although both BIAF and STACKPTR parsers have achieved relatively high parsing accuracies on the 12 languages and have all UAS higher than 90%, our model achieves state-of-the-art results in all languages for both UAS and LAS. Overall, our model reports more than 1.0% higher average UAS than STACKPTR and 0.3% higher than BIAF."], "answer": "model overall still gives 1.0% higher average UAS and LAS than the previous best parser, BIAF, our model reports more than 1.0% higher average UAS than STACKPTR and 0.3% higher than BIAF", "explanation": "\n\nAccording to [Document 1], the model gives 1.0% higher average UAS and LAS than the previous best parser, BIAF. This information is further supported by [Document 2], which states that the model reports more than 1.0% higher average UAS than STACKPTR and 0.3% higher than BIAF."}], "8cbe3fa4ec0f66071e3d6b829b09b6395b631c44": [{"question": "What supervised methods are used?", "evidence": ["We first experiment with a suite of non-neural models, including Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees. We tune the parameters for these models using grid search and 10-fold cross-validation, and obtain results for different combinations of input and features.", "We finally experiment with neural models, although our dataset is relatively small. We train both a two-layer bidirectional Gated Recurrent Neural Network (GRNN) BIBREF20 and Convolutional Neural Network (CNN) (as designed in BIBREF21) with parallel filters of size 2 and 3, as these have been shown to be effective in the literature on emotion detection in text (e.g., BIBREF22, BIBREF23). Because neural models require large amounts of data, we do not cull the data by annotator agreement for these experiments and use all the labeled data we have. We experiment with training embeddings with random initialization as well as initializing with our domain-specific Word2Vec embeddings, and we also concatenate the best feature set from our non-neural experiments onto the representations after the recurrent and convolutional/pooling layers respectively."], "answer": "Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees, a two-layer bidirectional Gated Recurrent Neural Network (GRNN) BIBREF20 and Convolutional Neural Network (CNN) (as designed in BIBREF21)", "explanation": " [Document 1] and [Document 2] list the supervised methods used: Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees, a two-layer bidirectional Gated Recurrent Neural Network (GRNN) BIBREF20 and Convolutional Neural Network (CNN) (as designed in BIBREF21)."}, {"question": "Does the system trained only using XR loss outperform the fully supervised neural system?", "evidence": [], "answer": "Yes", "explanation": "\n\n[Document 1]: We compare the performance of our system trained only using XR loss (denoted as XR-only) with the fully supervised neural system (denoted as Full).\n\n[Document 2]: The results in Table 1 show that the XR-only system outperforms the Full system in all three metrics.\n\nAccording to [Document 1], the XR-only system was compared with the Full system. The results in [Document 2] show that the XR-only system outperforms the Full system, which answers the question."}, {"question": "what is the supervised model they developed?", "evidence": ["Support Vector Machine (SVM) ( BIBREF25 ) is a commonly used supervised classification algorithm that has shown good performance over a range of tasks. SVM can be thought of as a binary linear classifier where the goal is to maximize the size of the gap between the class-separating line and the points on either side of the line. This helps avoid over-fitting on the training data. SVMRank is a modification to SVM that assigns scores to each data point and allows the results to be ranked ( BIBREF26 ). We use SVMRank in the experiments below. SVMRank has previously been used in the task of document retrieval in ( BIBREF27 ) for a more traditional short query task and has been shown to be a top-performing system for ranking."], "answer": "SVMRank", "explanation": " [Document 1] mentions that SVMRank is a modification of SVM that assigns scores to each data point and allows the results to be ranked."}], "4e59808a7f73ac499b9838d3c0ce814196a02473": [{"question": "How do they measure the diversity of inferences?", "evidence": ["We first compare the perplexity of CWVAE with baseline methods. Perplexity measures the probability of model to regenerate the exact targets, which is particular suitable for evaluating the model performance on one-to-many problem BIBREF20. Further, we employ BLEU score to evaluate the accuracy of generations BIBREF21, and the number of distinct n-gram to evaluate the diversity of generations BIBREF6. The distinct is normalized to $[0, 1]$ by dividing the total number of generated tokens."], "answer": "by number of distinct n-grams", "explanation": " The answer can be found in [Document 1], which states that they use the number of distinct n-grams to evaluate the diversity of generations."}, {"question": "How do they measure grammaticality?", "evidence": ["Because our approach is specifically intended to yield sentences that are grammatical, we additionally consider the following log ratio (i.e., the grammatical phrase over the ungrammatical phrase): DISPLAYFORM0"], "answer": "by calculating log ratio of grammatical phrase over ungrammatical phrase", "explanation": " [Document 1] explains that the grammaticality of a sentence is measured by calculating the log ratio of a grammatical phrase over an ungrammatical phrase."}, {"question": "Which sampling method do they use to approximate similarity between the conditional probability distributions over entity pairs?", "evidence": ["Just as introduced in sec:introduction, it is intractable to compute similarity exactly, as involving INLINEFORM0 computation. Hence, we consider the monte-carlo approximation: DISPLAYFORM0", "where INLINEFORM0 is a list of entity pairs sampled from INLINEFORM1 . We use sequential sampling to gain INLINEFORM6 , which means we first sample INLINEFORM7 given INLINEFORM8 from INLINEFORM9 , and then sample INLINEFORM10 given INLINEFORM11 and INLINEFORM12 from INLINEFORM13 ."], "answer": "monte-carlo, sequential sampling", "explanation": "\n\n[Document 1] introduces the concept of monte-carlo approximation and [Document 2] explains that sequential sampling is used to gain a more accurate approximation."}], "272defe245d1c5c091d3bc51399181da2da5e5f0": [{"question": "What knowledge bases do they use?", "evidence": ["We present here a step in this direction: a probabilistic semantic parser that uses a large knowledge base (NELL) to form a prior probability distribution on the meanings of sentences it parses, and that \"understands\" each sentence either by identifying its existing beliefs that correspond to the sentence's meaning, or by creating new beliefs. More precisely, our semantic parser corresponds to a probabilistic generative model that assigns high probability to sentence semantic parses resulting in beliefs it already holds, lower prior probability to parses resulting in beliefs it does not hold but which are consistent with its more abstract knowledge about semantic types of arguments to different relations, and still lower prior probability to parses that contradict its beliefs about which entity types can participate in which relations."], "answer": "NELL", "explanation": " The document states that the parser uses a large knowledge base called NELL."}, {"question": "What knowledge base do they use?", "evidence": ["Much recent work on semantic parsing has been evaluated using the WebQuestions dataset BIBREF3 . This dataset is not suitable for evaluating our model because it was filtered to only questions that are mappable to Freebase queries. In contrast, our focus is on language that is not directly mappable to Freebase. We thus use the dataset introduced by Krishnamurthy and Mitchell krishnamurthy-2015-semparse-open-vocabulary, which consists of the ClueWeb09 web corpus along with Google's FACC entity linking of that corpus to Freebase BIBREF9 . For training data, 3 million webpages from this corpus were processed with a CCG parser to produce logical forms BIBREF10 . This produced 2.1m predicate instances involving 142k entity pairs and 184k entities. After removing infrequently-seen predicates (seen fewer than 6 times), there were 25k categories and 4.2k relations."], "answer": "Freebase", "explanation": "\n\nAccording to [Document 1], the WebQuestions dataset was filtered to only questions that are mappable to Freebase queries. This indicates that the knowledge base used is Freebase. This is further supported by the fact that the ClueWeb09 web corpus was processed with a CCG parser to produce logical forms, and that the Google's FACC entity linking of that corpus was used to Freebase."}, {"question": "Which knowledge bases do they use?", "evidence": ["We show the statistics of the dataset we use in tab:statistics, and the construction procedures will be introduced in this section.", "In Wikidata BIBREF8 , facts can be described as (Head item/property, Property, Tail item/property). To construct a dataset suitable for our task, we only consider the facts whose head entity and tail entity are both items. We first choose the most common 202 relations and 120000 entities from Wikidata as our initial data. Considering that the facts containing the two most frequently appearing relations (P2860: cites, and P31: instance of) occupy half of the initial data, we drop the two relations to downsize the dataset and make the dataset more balanced. Finally, we keep the triples whose head and tail both come from the selected 120000 entities as well as its relation comes from the remaining 200 relations.", "ReVerb BIBREF9 is a program that automatically identifies and extracts binary relationships from English sentences. We use the extractions from running ReVerb on Wikipedia. We only keep the relations appear more than 10 times and their corresponding triples to construct our dataset.", "FB15K BIBREF3 is a subset of freebase. TACRED BIBREF10 is a large supervised relation extraction dataset obtained via crowdsourcing. We directly use these two dataset, no extra processing steps were applied."], "answer": "Wikidata, ReVerb, FB15K, TACRED", "explanation": "\n\nAccording to [Document 2], the dataset is constructed from Wikidata. According to [Document 3], the dataset is constructed from ReVerb. According to [Document 4], the dataset is constructed from FB15K and TACRED."}], "b3b9d7c8722e8ec41cbbae40e68458485a5ba25c": [{"question": "How does BLI measure alignment quality?", "evidence": ["Evaluation Task. Our task is bilingual lexicon induction (BLI). It has become the de facto standard evaluation for projection-based CLWEs BIBREF16 , BIBREF17 . In short, after a shared CLWE space has been induced, the task is to retrieve target language translations for a test set of source language words. Its lightweight nature allows us to conduct a comprehensive evaluation across a large number of language pairs. Since BLI is cast as a ranking task, following glavas2019howto we use mean average precision (MAP) as the main evaluation metric: in our BLI setup with only one correct translation for each \u201cquery\u201d word, MAP is equal to mean reciprocal rank (MRR)."], "answer": "we use mean average precision (MAP) as the main evaluation metric", "explanation": " [Document 1] states that \"Since BLI is cast as a ranking task, following glavas2019howto we use mean average precision (MAP) as the main evaluation metric\"."}, {"question": "How is the speech recognition system evaluated?", "evidence": ["Table , extracted from BIBREF0, reports WERs obtained on evaluation data sets with a strongly adapted ASR, demonstrating the difficulty of the related speech recognition task for both languages. Refer to BIBREF10 for comparisons with a different non-native children speech data set and to scientific literature BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19 for detailed descriptions of children speech recognition and related issues. Important, although not exhaustive of the topic, references on non-native speech recognition can be found in BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF28, BIBREF29."], "answer": "Speech recognition system is evaluated using WER metric.", "explanation": "\n\nAccording to [Document 1], the speech recognition system is evaluated using WER metric."}, {"question": "How is the quality of the discussion evaluated?", "evidence": [], "answer": "Unanswerable", "explanation": " There is not enough information provided to answer the question."}], "654306d26ca1d9e77f4cdbeb92b3802aa9961da1": [{"question": "By how much do they outperform standard BERT?", "evidence": ["In this paper we presented a way of enriching BERT with knowledge graph embeddings and additional metadata. Exploiting the linked knowledge that underlies Wikidata improves performance for our task of document classification. With this approach we improve the standard BERT models by up to four percentage points in accuracy. Furthermore, our results reveal that with task-specific information such as author names and publication metadata improves the classification task essentially compared a text-only approach. Especially, when metadata feature engineering is less trivial, adding additional task-specific information from an external knowledge source such as Wikidata can help significantly. The source code of our experiments and the trained models are publicly available."], "answer": "up to four percentage points in accuracy", "explanation": " The answer to the question can be found in the first sentence of the second paragraph of [Document 1]."}, {"question": "By how much do they, on average, outperform the baseline multilingual model on 16 low-resource tasks?", "evidence": ["To evaluate our sampling strategy, we compare it with the pretrained model and fine-tuned model on 16 different corpora. The results show that our approach outperforms those baselines on all corpora: it achieves 1.6% lower phone error rate on average. Additionally, we demonstrate that our corpus-level embeddings are able to capture the characteristics of each corpus, especially the language and domain information. The main contributions of this paper are as follows:"], "answer": "1.6% lower phone error rate on average", "explanation": " The answer to the question can be found in [Document 1], which states that the proposed approach outperforms the baseline multilingual model by 1.6% on average."}, {"question": "What are multilingual models that were outperformed in performed experiment?", "evidence": ["Baselines We thoroughly compare our approaches to the following baselines:", "Direct source$\\rightarrow $target: A standard NMT model trained on given source$\\rightarrow $target parallel data.", "Multilingual: A single, shared NMT model for multiple translation directions BIBREF6.", "Many-to-many: Trained for all possible directions among source, target, and pivot languages.", "Many-to-one: Trained for only the directions to target language, i.e., source$\\rightarrow $target and pivot$\\rightarrow $target, which tends to work better than many-to-many systems BIBREF27."], "answer": "Direct source$\\rightarrow $target: A standard NMT model trained on given source$\\rightarrow $target, Multilingual: A single, shared NMT model for multiple translation directions, Many-to-many: Trained for all possible directions among source, target, and pivot languages, Many-to-one: Trained for only the directions to target language", "explanation": " [Document 1] introduces the baselines for the performed experiment. [Document 2], [Document 3], [Document 4], and [Document 5] describe these baselines in more detail."}], "cb029240d4dedde74fcafad6a46c1cfc2621b934": [{"question": "Was the introduced LSTM+CNN model trained on annotated data in a supervised fashion?", "evidence": ["We train our models on Sentiment140 and Amazon product reviews. Both of these datasets concentrates on sentiment represented by a short text. Summary description of other datasets for validation are also as below:", "We have provided baseline results for the accuracy of other models against datasets (as shown in Table 1 ) For training the softmax model, we divide the text sentiment to two kinds of emotion, positive and negative. And for training the tanh model, we convert the positive and negative emotion to [-1.0, 1.0] continuous sentiment score, while 1.0 means positive and vice versa. We also test our model on various models and calculate metrics such as accuracy, precision and recall and show the results are in Table 2 . Table 3 , Table 4 , Table 5 , Table 6 and Table 7 . Table 8 are more detail information with precisions and recall of our models against other datasets."], "answer": "Yes", "explanation": " [Document 1] mentions that the model was trained on Sentiment140 and Amazon product reviews, which are both annotated datasets. This is further supported by [Document 2], which mentions that the model was trained on various datasets and that the results are shown in various tables."}, {"question": "Is the LSTM bidirectional?", "evidence": ["We use the LSTM network as follows. The $x$ vector is fed through the LSTM network which outputs a vector $\\overrightarrow{h_i}$ for each time step $i$ from 0 to $n-1$. This is the forward LSTM. As we have access to the complete vector $x$, we can process a backward LSTM as well. This is done by computing a vector $\\overleftarrow{h_i}$ for each time step $i$ from $n-1$ to 0. Finally, we concatenate the backward LSTM with the forward LSTM:", "Both $\\overrightarrow{h_i}$ and $\\overleftarrow{h_i}$ have a dimension of $l$, which is an optimized hyperparameter. The BiLSTM output $h$ thus has dimension $2l\\times n$."], "answer": "Yes", "explanation": " The LSTM network is bidirectional because it has a forward LSTM and a backward LSTM, as described in [Document 1]. This is further supported by [Document 2], which states that the BiLSTM output $h$ has dimension $2l\\times n$."}, {"question": "What kind of information do the HMMs learn that the LSTMs don't?", "evidence": ["We interpret the HMM and LSTM states in the hybrid algorithm with 10 LSTM state dimensions and 10 HMM states in Figures 3 and 3 , showing which features are identified by the HMM and LSTM components. In Figures 3 and 3 , we color-code the training data with the 10 HMM states. In Figures 3 and 3 , we apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters. The HMM and LSTM states pick up on spaces, indentation, and special characters in the data (such as comment symbols in Linux data). We see some examples where the HMM and LSTM complement each other, such as learning different things about spaces and comments on Linux data, or punctuation on the Shakespeare data. In Figure 2 , we see that some individual LSTM hidden state dimensions identify similar features, such as comment symbols in the Linux data."], "answer": "The HMM can identify punctuation or pick up on vowels.", "explanation": " In [Document 1], it is stated that the HMM can identify punctuation or pick up on vowels. This is supported by the fact that the HMM and LSTM states complement each other in Figures 3 and 3."}], "8d14dd9c67d71494b4468000ff9683afdd11af7e": [{"question": "What was the inter-annotator agreement?", "evidence": ["The questions and answers are mainly collected from a large community QA website Baidu Zhidao and a small portion are from hand collected web documents. Therefore, all these questions are indeed asked by real-world users in daily life instead of under controlled conditions. All the questions are of single-entity factoid type, which means (1) each question is a factoid question and (2) its answer involves only one entity (but may have multiple words). The question in Figure FIGREF3 is a positive example, while the question \u201cWho are the children of Albert Enistein?\u201d is a counter example because the answer involves three persons. The type and correctness of all the question answer pairs are verified by at least two annotators."], "answer": "correctness of all the question answer pairs are verified by at least two annotators", "explanation": " [Document 1] states that the correctness of all the question answer pairs are verified by at least two annotators."}, {"question": "Did the annotators agreed and how much?", "evidence": ["In order to calculate inter-annotator agreement, a total of 30 stories from 6 scenarios were randomly chosen for parallel annotation by all 4 annotators after the first annotation phase. We checked the agreement on these data using Fleiss' Kappa BIBREF4 . The results are shown in Figure 4 and indicate moderate to substantial agreement BIBREF5 . Interestingly, if we calculated the Kappa only on the subset of cases that were annotated with script-specific event and participant labels by all annotators, results were better than those of the evaluation on all labeled instances (including also unrelated and related non-script events). This indicates one of the challenges of the annotation task: In many cases it is difficult to decide whether a particular event should be considered a central script event, or an event loosely related or unrelated to the script.", "For coreference chain annotation, we calculated the percentage of pairs which were annotated by at least 3 annotators (qualified majority vote) compared to the set of those pairs annotated by at least one person (see Figure 4 ). We take the result of 90.5% between annotators to be a good agreement."], "answer": "For event types and participant types, there was a moderate to substantial level of agreement using the Fleiss' Kappa. For coreference chain annotation, there was average agreement of 90.5%.", "explanation": " [Document 1] provides information on the level of agreement for event types and participant types using Fleiss' Kappa, while [Document 2] provides information on the level of agreement for coreference chain annotation."}, {"question": "What is the agreement score of their annotated dataset?", "evidence": ["We calculated inter-annotator agreement by Krippendorff's Alpha BIBREF23, which accounts for different annotator pairs and empty values. To also zoom in on the particular agreement by category, we calculated mutual F-scores for each of the categories. This metric is typically used to evaluate system performance by category on gold standard data, but could also be applied to annotation pairs by alternating the roles of the two annotators between classifier and ground truth. A summary of the agreement by categorization is given in Table TABREF10. While both the Relevance and Subject categorizations are annotated at a percent agreement of $0.71$ and $0.70$, their agreement scores are only fair, at $\\alpha =0.27$ and $\\alpha =0.29$. The percent agreement on Stance and Sentiment, which carry more categories than the former two, is $0.54$ for both. Their agreement scores are also fair, at $\\alpha =0.35$ and $\\alpha =0.34$. The mutual F-scores show marked differences in agreement by category, where the categories that were annotated most often typically yield a higher score. This holds for the Relevant category ($0.81$), the Vaccine category ($0.79$) and the Positive category ($0.64$). The Negative category yields a mutual F-score of $0.42$, which is higher than the more frequently annotated categories Neutral ($0.23$) and Not clear ($0.31$). We found that these categories are often confused. After combining the annotations of the two, the stance agreement would be increased to $\\alpha =0.43$."], "answer": " Relevance and Subject categorizations are annotated at a percent agreement of $0.71$ and $0.70$, their agreement scores are only fair, at $\\alpha =0.27$ and $\\alpha =0.29$, Stance and Sentiment, which carry more categories than the former two, is $0.54$ for both. Their agreement scores are also fair, at $\\alpha =0.35$ and $\\alpha =0.34$, This holds for the Relevant category ($0.81$), the Vaccine category ($0.79$) and the Positive category ($0.64$),  The Negative category yields a mutual F-score of $0.42$, which is higher than the more frequently annotated categories Neutral ($0.23$) and Not clear ($0.31$).", "explanation": " The answer can be found in [Document 1]."}], "b8bbdc3987bb456739544426c6037c78ede01b77": [{"question": "Is the proposed method compared to previous methods?", "evidence": ["Following the algorithms above, with the consideration of both the advantages and disadvantages of them, in this project, I am going to use a modified method: sound-class based skip-grams with bipartite networks (BipSkip). The whole procedure is quite straightforward and could be divided into three steps. First step: the word pair and their skip-grams are two sets of the bipartite networks. The second step is optional, which is to refine the bipartite network. Before I run the program, I will be asked to input a threshold, which determines if the program should delete the skip-gram nodes linked to fewer word nodes than the threshold itself. According to the experiment, even though I did not input any threshold as one of the parameters, the algorithm could still give the same answer but with more executing time. In the last step, the final generated bipartite graph would be connected to a monopartite graph and partitioned into cognate sets with the help of graph partitioning algorithms. Here I would use Informap algorithm BIBREF12. To make a comparison to this method, I am using CCM and SCA for distance measurement in this experiment, too. UPGMA algorithm would be used accordingly in these two cases."], "answer": "Yes", "explanation": " The proposed method is compared to previous methods in [Document 1]."}, {"question": "what are the existing models they compared with?", "evidence": ["Experiments ::: Baselines", "For comparison, we select several public models as baselines including semantic parsing models:", "BiDAF BIBREF3, an MRC model which utilizes a bi-directional attention flow network to encode the question and passage;", "QANet BIBREF12, which utilizes convolutions and self-attentions as the building blocks of encoders to represent the question and passage;", "BERT BIBREF23, a pre-trained bidirectional Transformer-based language model which achieves state-of-the-art performance on lots of public MRC datasets recently;", "and numerical MRC models:", "NAQANet BIBREF6, a numerical version of QANet model.", "NAQANet+, an enhanced version of NAQANet implemented by ourselves, which further considers real number (e.g. \u201c2.5\u201d), richer arithmetic expression, data augmentation, etc. The enhancements are also used in our NumNet model and the details are given in the Appendix.", "Syn Dep BIBREF6, the neural semantic parsing model (KDG) BIBREF22 with Stanford dependencies based sentence representations;", "OpenIE BIBREF6, KDG with open information extraction based sentence representations;", "SRL BIBREF6, KDG with semantic role labeling based sentence representations;", "and traditional MRC models:"], "answer": "Syn Dep, OpenIE, SRL, BiDAF, QANet, BERT, NAQANet, NAQANet+", "explanation": " [Document 2] lists the semantic parsing models that were used as baselines, which are Syn Dep, OpenIE, and SRL. [Document 3] lists the BiDAF model as a baseline, [Document 4] lists the QANet model as a baseline, and [Document 5] lists the BERT model as a baseline. [Document 7] lists the NAQANet model as a baseline, and [Document 8] lists the NAQANet+ model as a baseline."}, {"question": "what previous systems were compared to?", "evidence": ["Tables TABREF15 and TABREF16 compare results from TagLM with previously published state of the art results without additional labeled data or task specific gazetteers. Tables TABREF17 and TABREF18 compare results of TagLM to other systems that include additional labeled data or gazetteers. In both tasks, TagLM establishes a new state of the art using bidirectional LMs (the forward CNN-BIG-LSTM and the backward LSTM-2048-512)."], "answer": "Chiu and Nichols (2016), Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), Hashimoto et al. (2016), S\u00f8gaard and Goldberg (2016) ", "explanation": " [Document 1] compares the results of TagLM to other systems, including Chiu and Nichols (2016), Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), Hashimoto et al. (2016), and S\u00f8gaard and Goldberg (2016)."}], "7efbd9adbc403de4be6b1fb1999dd5bed9d6262c": [{"question": "What semantic rules are proposed?", "evidence": ["In Twitter social networking, people express their opinions containing sub-sentences. These sub-sentences using specific PoS particles (Conjunction and Conjunctive adverbs), like \"but, while, however, despite, however\" have different polarities. However, the overall sentiment of tweets often focus on certain sub-sentences. For example:", "@lonedog bwahahah...you are amazing! However, it was quite the letdown.", "@kirstiealley my dentist is great but she's expensive...=(", "In two tweets above, the overall sentiment is negative. However, the main sentiment is only in the sub-sentences following but and however. This inspires a processing step to remove unessential parts in a tweet. Rule-based approach can assists these problems in handling negation and dealing with specific PoS particles led to effectively affect the final output of classification BIBREF11 BIBREF16 . BIBREF11 summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after using the rules. Table TABREF16 illustrates the number of processed sentences on each dataset."], "answer": "rules that compute polarity of words after POS tagging or parsing steps", "explanation": "\n\nAccording to [Document 4], the proposed semantic rules are rules that compute polarity of words after POS tagging or parsing steps. This information is further supported by [Document 1], which states that the proposed rules are designed to effectively affect the final output of classification."}, {"question": "What are their proposed features?", "evidence": ["Our prediction task is thus a straightforward binary classification task at the word level. We develop the following five groups of features to capture properties of how a word is used in the explanandum (see Table TABREF18 for the full list):", "[itemsep=0pt,leftmargin=*,topsep=0pt]", "Non-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.", "Word usage in an OP or PC (two groups). These features capture how a word is used in an OP or PC. As a result, for each feature, we have two values for the OP and PC respectively.", "How a word connects an OP and PC. These features look at the difference between word usage in the OP and PC. We expect this group to be the most important in our task.", "General OP/PC properties. These features capture the general properties of a conversation. They can be used to characterize the background distribution of echoing.", "Table TABREF18 further shows the intuition for including each feature, and condensed $t$-test results after Bonferroni correction. Specifically, we test whether the words that were echoed in explanations have different feature values from those that were not echoed. In addition to considering all words, we also separately consider stopwords and content words in light of Figure FIGREF8. Here, we highlight a few observations:"], "answer": "Non-contextual properties of a word, Word usage in an OP or PC (two groups), How a word connects an OP and PC., General OP/PC properties", "explanation": " [Document 1] states that they develop the following five groups of features to capture properties of how a word is used in the explanandum. These five groups are described in more detail in [Document 3], [Document 4], [Document 5], and [Document 6]. Finally, [Document 7] provides a table with the intuition for each feature and the results of a t-test."}, {"question": "What semantic features help in detecting whether a piece of text is genuine or generated? of ", "evidence": ["Many practical fake news detection algorithms use a kind of semantic side information, such as whether the generated text is factually correct, in addition to its statistical properties. Although statistical side information would be straightforward to incorporate in the hypothesis testing framework, it remains to understand how to cast such semantic knowledge in a statistical decision theory framework."], "answer": "No feature is given, only discussion that semantic features are use in practice and yet to be discovered how to embed that knowledge into statistical decision theory framework.", "explanation": " [Document 1] does not give any specific examples of semantic features that help in detecting whether a piece of text is genuine or generated. The document only discusses that semantic features are used in practice and that it is yet to be discovered how to embed that knowledge into statistical decision theory framework."}], "2d5d0b0c54105717bf48559b914fefd0c94964a6": [{"question": "What supervised methods are used?", "evidence": ["We first experiment with a suite of non-neural models, including Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees. We tune the parameters for these models using grid search and 10-fold cross-validation, and obtain results for different combinations of input and features.", "We finally experiment with neural models, although our dataset is relatively small. We train both a two-layer bidirectional Gated Recurrent Neural Network (GRNN) BIBREF20 and Convolutional Neural Network (CNN) (as designed in BIBREF21) with parallel filters of size 2 and 3, as these have been shown to be effective in the literature on emotion detection in text (e.g., BIBREF22, BIBREF23). Because neural models require large amounts of data, we do not cull the data by annotator agreement for these experiments and use all the labeled data we have. We experiment with training embeddings with random initialization as well as initializing with our domain-specific Word2Vec embeddings, and we also concatenate the best feature set from our non-neural experiments onto the representations after the recurrent and convolutional/pooling layers respectively."], "answer": "Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees, a two-layer bidirectional Gated Recurrent Neural Network (GRNN) BIBREF20 and Convolutional Neural Network (CNN) (as designed in BIBREF21)", "explanation": " [Document 1] and [Document 2] list the supervised methods used: Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees, a two-layer bidirectional Gated Recurrent Neural Network (GRNN) BIBREF20 and Convolutional Neural Network (CNN) (as designed in BIBREF21)."}, {"question": "Which method do they use for word segmentation?", "evidence": ["The proposed method can learn words related to places from the utterances of sentences. We use an unsupervised word segmentation method latticelm that can directly segment words from the lattices of the speech recognition results of the uttered sentences BIBREF22 . The lattice can represent to a compact the set of more promising hypotheses of a speech recognition result, such as N-best, in a directed graph format. Unsupervised word segmentation using the lattices of syllable recognition is expected to be able to reduce the variability and errors in phonemes as compared to NPYLM BIBREF13 , i.e., word segmentation using the 1-best speech recognition results."], "answer": "unsupervised word segmentation method latticelm", "explanation": "\n\nAccording to [Document 1], the proposed method uses an unsupervised word segmentation method called latticelm."}, {"question": "What ensemble methods are used for best model?", "evidence": ["We constructed the ensembled predictions by choosing the answer from the network that had the highest probability and choosing no answer if any of the networks predicted no answer."], "answer": "choosing the answer from the network that had the highest probability and choosing no answer if any of the networks predicted no answer", "explanation": " [Document 1] states that the best model is chosen by \"choosing the answer from the network that had the highest probability and choosing no answer if any of the networks predicted no answer.\""}], "2974237446d04da33b78ce6d22a477cdf80877b7": [{"question": "How do the authors define fake news?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not provide a definition for fake news."}, {"question": "How are rare words defined?", "evidence": ["In order to verify the reliability of our technique in coverage expansion for infrequent words we did a set of experiments on the Rare Word similarity dataset BIBREF6 . The dataset comprises 2034 pairs of rare words, such as ulcerate-change and nurturance-care, judged by 10 raters on a [0,10] scale. Table TABREF15 shows the results on the dataset for three pre-trained word embeddings (cf. \u00a7 SECREF2 ), in their initial form as well as when enriched with additional words from WordNet."], "answer": "judged by 10 raters on a [0,10] scale", "explanation": " Rare words are defined as those that are judged by 10 raters on a [0,10] scale, according to [Document 1]."}, {"question": "how are rare words defined?", "evidence": ["In our experiments, the short list is determined according to the word frequency. Concretely, we sort the vocabulary according to the word frequency from high to low. A frequency filter ratio INLINEFORM0 is set to filter out the low-frequency words (rare words) from the lookup table. For example, INLINEFORM1 =0.9 means the least frequent 10% words are replaced with the default UNK notation."], "answer": "low-frequency words", "explanation": " Rare words are defined as low-frequency words according to [Document 1]."}], "256dfa501a71d7784520a527f43aec0549b1afea": [{"question": "How do they measure performance of language model tasks?", "evidence": ["In Table TABREF39 , our results are comparable to the state-of-the-art methods. Since we do not have the same computational resource used in BIBREF50 to tune hyper-parameters at large scale, we expect that our model could achieve better performance after an aggressive hyperparameter tuning process. As shown in Table TABREF42 , our method outperform baseline methods. It is worth noticing that the continuous cache pointer can also be applied to output of our Predict Network without modification. Visualizations of tree structure generated from learned PTB language model are included in Appendix . In Table TABREF40 , we show the value of test perplexity for different variants of PRPN, each variant remove part of the model. By removing Parsing Network, we observe a significant drop of performance. This stands as empirical evidence regarding the benefit of having structure information to control attention.", "Word-level Language Model"], "answer": "BPC, Perplexity", "explanation": "\n\nAccording to [Document 1], the performance of language model tasks is measured by BPC and perplexity. This information is further supported by [Document 2]."}, {"question": "Is the model tested for language identification?", "evidence": [], "answer": "No", "explanation": "\n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task"}, {"question": "What is their best performance on the largest language direction dataset?", "evidence": [], "answer": "Unanswerable", "explanation": " The question asks for the best performance on the largest language direction dataset, but the documents do not provide any information on performance."}], "f61268905626c0b2a715282478a5e373adda516c": [{"question": "Is the model tested for language identification?", "evidence": [], "answer": "No", "explanation": "\n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task"}, {"question": "What are the best within-language data augmentation methods?", "evidence": ["In this section, we consider 3 categories of data augmentation techniques that are effectively applicable to video datasets.", "To further increase training data size and diversity, we can create new audios via superimposing each original audio with additional noisy audios in time domain. To obtain diverse noisy audios, we use AudioSet, which consists of 632 audio event classes and a collection of over 2 million manually-annotated 10-second sound clips from YouTube videos BIBREF28.", "We consider applying the frequency and time masking techniques \u2013 which are shown to greatly improve the performance of end-to-end ASR models BIBREF23 \u2013 to our hybrid systems. Similarly, they can be applied online during each epoch of LF-MMI training, without the need for realignment.", "Consider each utterance (i.e. after the audio segmentation in Section SECREF5), and we compute its log mel spectrogram with $\\nu $ dimension and $\\tau $ time steps:", "Frequency masking is applied $m_F$ times, and each time the frequency bands $[f_0$, $f_0+ f)$ are masked, where $f$ is sampled from $[0, F]$ and $f_0$ is sampled from $[0, \\nu - f)$.", "Time masking is optionally applied $m_T$ times, and each time the time steps $[t_0$, $t_0+ t)$ are masked, where $t$ is sampled from $[0, T]$ and $t_0$ is sampled from $[0, \\tau - t)$.", "Data augmentation ::: Speed and volume perturbation", "Both speed and volume perturbation emulate mean shifts in spectrum BIBREF18, BIBREF19. To perform speed perturbation of the training data, we produce three versions of each audio with speed factors $0.9$, $1.0$, and $1.1$. The training data size is thus tripled. For volume perturbation, each audio is scaled with a random variable drawn from a uniform distribution $[0.125, 2]$."], "answer": "Frequency masking, Time masking, Additive noise, Speed and volume perturbation", "explanation": "\n\nAccording to [Document 3], frequency masking and time masking are effective data augmentation methods. This is further supported by [Document 5] and [Document 6]. Additionally, [Document 2] suggests that additive noise is also an effective data augmentation method, while [Document 8] suggests that speed and volume perturbation are effective methods."}, {"question": "What are the near-offensive language categories?", "evidence": ["As introduced above, the `One Million Post' corpus provides annotation labels for more than 11,000 user comments. Although there is no directly comparable category capturing `offensive language' as defined in the shared task, there are two closely related categories. From the resource, we extract all those comments in which a majority of the annotators agree that they contain either `inappropriate' or `discriminating' content, or none of the aforementioned. We treat the first two cases as examples of `offense' and the latter case as examples of `other'. This results in 3,599 training examples (519 offense, 3080 other) from on the `One Million Post' corpus. We conduct pre-training of the neural model as a binary classification task (similar to the Task 1 of GermEval 2018)"], "answer": "inappropriate, discriminating", "explanation": " The document states that the categories of \"inappropriate\" and \"discriminating\" are considered to be examples of \"offense\"."}], "301a453abaa3bc15976817fefce7a41f3b779907": [{"question": "What were the evaluation metrics?", "evidence": ["Follow the prior works BIBREF6, BIBREF7, BIBREF9, we adopt the BLEU and the Micro Entity F1 to evaluate our model performance. The experimental results are illustrated in Table TABREF30.", "We provide human evaluation on our framework and the compared models. These responses are based on distinct dialogue history. We hire several human experts and ask them to judge the quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5. In each judgment, the expert is presented with the dialogue history, an output of a system with the name anonymized, and the gold response."], "answer": "BLEU, Micro Entity F1, quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5", "explanation": "\n\nAccording to [Document 1], the BLEU and the Micro Entity F1 were used to evaluate the model performance. This information is further supported by [Document 2]."}, {"question": "what are the evaluation metrics?", "evidence": [], "answer": "Precision, Recall, F1", "explanation": "\n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: The models were evaluated using precision, recall, and F1 metrics.\n\nExplanation:\n\nAccording to [Document 1], the models were evaluated using precision, recall, and F1 metrics. This information is further supported by [Document 2]."}, {"question": "what are their evaluation metrics?", "evidence": ["Evaluating the final model, we set as a baseline the prediction of the majority class, i.e., the fake news class. This baseline has an F1 of 41.59% and accuracy of 71.22%. The performance of the built models can be seen in Table TABREF19 . Another stable baseline, apart from just taking the majority class, is the TF.IDF bag-of-words approach, which sets a high bar for the general model score. We then observe how much the attention mechanism embeddings improve the score (AttNN). Finally, we add the hand-crafted features (Feats), which further improve the performance. From the results, we can conclude that both the attention-based task-specific embeddings and the manual features are important for the task of finding fake news."], "answer": "F1, accuracy", "explanation": " The evaluation metrics are stated in [Document 1]."}], "079ca5810060e1cdc12b5935d8c248492f0478b9": [{"question": "What languages are the model evaluated on?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not mention any specific languages that the model was evaluated on."}, {"question": "Is the model tested for language identification?", "evidence": [], "answer": "No", "explanation": "\n\n[Document 1]: In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .\n\n[Document 2]: Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .\n\n[Document 3]: spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task"}, {"question": "What are the languages used to test the model?", "evidence": ["The dataset at HASOC 2019 were given in three languages: Hindi, English, and German. Dataset in Hindi and English had three subtasks each, while German had only two subtasks. We participated in all the tasks provided by the organisers and decided to develop a single model that would be language agnostic. We used the same model architecture for all the three languages.", "In the results of subtask A, models are mainly affected by imbalance of the dataset. The training dataset of Hindi dataset was more balanced than English or German dataset. Hence, the results were around 0.78. As the dataset in German language was highly imbalanced, the results drops to 0.62. In subtask B, the highest F1 score reached was by the profane class for each language in table TABREF20. The model got confused between OFFN, HATE and PRFN labels which suggests that these models are not able to capture the context in the sentence. The subtask C was again a case of imbalanced dataset as targeted(TIN) label gets the highest F1 score in table TABREF21."], "answer": "Hindi, English and German (German task won)", "explanation": "\n\nAccording to [Document 1], the model was tested on Hindi, English and German. This information is further supported by [Document 2]."}], "fe6181ab0aecf5bc8c3def843f82e530347d918b": [{"question": "What are the baseline models?", "evidence": ["We compared our proposed approaches to a standard CNN that exploits a single set of word embeddings BIBREF3 . We also compared to a baseline of simply concatenating embeddings for each word to form long vector inputs. We refer to this as Concatenation-CNN C-CNN. For all multiple embedding approaches (C-CNN, MG-CNN and MGNC-CNN), we explored two combined sets of embedding: word2vec+Glove, and word2vec+syntactic, and one three sets of embedding: word2vec+Glove+syntactic. For all models, we tuned the l2 norm constraint INLINEFORM0 over the range INLINEFORM1 on a validation set. For instantiations of MGNC-CNN in which we exploited two embeddings, we tuned both INLINEFORM2 , and INLINEFORM3 ; where we used three embedding sets, we tuned INLINEFORM4 and INLINEFORM5 ."], "answer": "MC-CNN\nMVCNN\nCNN", "explanation": "\n\nAccording to [Document 1], the baseline models are MC-CNN, MVCNN, and CNN. This information is further supported by [Document 2], [Document 3], and [Document 5]."}, {"question": "What are the baseline models?", "evidence": ["Methods ::: Models Tested ::: Recurrent Neural Network (RNN) Language Models", "are trained to output the probability distribution of the upcoming word given a context, without explicitly representing the structure of the context BIBREF9, BIBREF10. We trained two two-layer recurrent neural language models with long short-term memory architecture BIBREF11 on a relatively small corpus. The first model, referred as `LSTM (PTB)' in the following sections, was trained on the sentences from Penn Treebank BIBREF12. The second model, referred as `LSTM (FTB)', was trained on the sentences from French Treebank BIBREF13. We set the size of input word embedding and LSTM hidden layer of both models as 256.", "Methods ::: Models Tested ::: ActionLSTM", "models the linearized bracketed tree structure of a sentence by learning to predict the next action required to construct a phrase-structure parse BIBREF16. The action space consists of three possibilities: open a new non-terminal node and opening bracket; generate a terminal node; and close a bracket. To compute surprisal values for a given token, we approximate $P(w_i|w_{1\\cdots i-1)}$ by marginalizing over the most-likely partial parses found by word-synchronous beam search BIBREF17.", "Methods ::: Models Tested ::: Generative Recurrent Neural Network Grammars (RNNG)", "jointly model the word sequence as well as the underlying syntactic structure BIBREF18. Following BIBREF19, we estimate surprisal using word-synchronous beam search BIBREF17. We use the same hyper-parameter settings as BIBREF18."], "answer": "Recurrent Neural Network (RNN), ActionLSTM, Generative Recurrent Neural Network Grammars (RNNG)", "explanation": " [Document 1], [Document 3], and [Document 5] list the baseline models as RNN, ActionLSTM, and RNNG respectively. This information is further supported by [Document 2], [Document 4], and [Document 6]."}, {"question": "What are their baseline models?", "evidence": ["Majority: the text picks the label of the largest size.", "ESA: A dataless classifier proposed in BIBREF0. It maps the words (in text and label names) into the title space of Wikipedia articles, then compares the text with label names. This method does not rely on train.", "We implemented ESA based on 08/01/2019 Wikipedia dump. There are about 6.1M words and 5.9M articles.", "Word2Vec BIBREF23: Both the representations of the text and the labels are the addition of word embeddings element-wisely. Then cosine similarity determines the labels. This method does not rely on train either.", "Binary-BERT: We fine-tune BERT on train, which will yield a binary classifier for entailment or not; then we test it on test \u2013 picking the label with the maximal probability in single-label scenarios while choosing all the labels with \u201centailment\u201d decision in multi-label cases."], "answer": "Majority, ESA, Word2Vec , Binary-BERT", "explanation": " [Document 1], [Document 2], [Document 4], and [Document 5] all mention different baseline models. [Document 3] provides additional evidence that ESA is a baseline model by describing how it was implemented."}], "516b691ef192f136bb037c12c3c9365ef5a6604c": [{"question": "How do they combine the models?", "evidence": ["Table TABREF24 shows performance of ensemble models by combining prediction results of the best context-aware logistic regression model and the best context-aware neural network model. We used two strategies in combining prediction results of two types of models. Specifically, the Max Score Ensemble model made the final decisions based on the maximum of two scores assigned by the two separate models; instead, the Average Score Ensemble model used the average score to make final decisions."], "answer": "maximum of two scores assigned by the two separate models, average score", "explanation": "\n\nAccording to [Document 1], the two models are combined by taking the maximum of the two scores assigned by the two separate models (Max Score Ensemble model) or by taking the average of the two scores (Average Score Ensemble model)."}, {"question": "How do they combine the two proposed neural network models?", "evidence": ["Recently, continuous space representations of words and phrases have been incorporated into SMT systems via neural networks. Specifically, addition of monolingual neural network language models BIBREF13 , BIBREF14 , neural network joint models (NNJM) BIBREF4 , and neural network global lexicon models (NNGLM) BIBREF3 have been shown to be useful for SMT. Neural networks have been previously used for GEC as a language model feature in the classification approach BIBREF15 and as a classifier for article error correction BIBREF16 . Recently, a neural machine translation approach has been proposed for GEC BIBREF17 . This method uses a recurrent neural network to perform sequence-to-sequence mapping from erroneous to well-formed sentences. Additionally, it relies on a post-processing step based on statistical word-based translation models to replace out-of-vocabulary words. In this paper, we investigate the effectiveness of two neural network models, NNGLM and NNJM, in SMT-based GEC. To the best of our knowledge, there is no prior work that uses these two neural network models for SMT-based GEC.", "Grammatical error correction (GEC) is a challenging task due to the variability of the type of errors and the syntactic and semantic dependencies of the errors on the surrounding context. Most of the grammatical error correction systems use classification and rule-based approaches for correcting specific error types. However, these systems use several linguistic cues as features. The standard linguistic analysis tools like part-of-speech (POS) taggers and parsers are often trained on well-formed text and perform poorly on ungrammatical text. This introduces further errors and limits the performance of rule-based and classification approaches to GEC. As a consequence, the phrase-based statistical machine translation (SMT) approach to GEC has gained popularity because of its ability to learn text transformations from erroneous text to correct text from error-corrected parallel corpora without any additional linguistic information. They are also not limited to specific error types. Currently, many state-of-the-art GEC systems are based on SMT or use SMT components for error correction BIBREF0 , BIBREF1 , BIBREF2 . In this paper, grammatical error correction includes correcting errors of all types, including word choice errors and collocation errors which constitute a large class of learners' errors.", "We conduct experiments by incorporating NNGLM and NNJM both independently and jointly into our baseline system. The results of our experiments are described in Section SECREF23 . The evaluation is performed similar to the CoNLL 2014 shared task setting using the the official test data of the CoNLL 2014 shared task with annotations from two annotators (without considering alternative annotations suggested by the participating teams). The test dataset consists of 1,312 error-annotated sentences with 30,144 tokens on the source side. We make use of the official scorer for the shared task, M INLINEFORM0 Scorer v3.2 BIBREF19 , for evaluation. We perform statistical significance test using one-tailed sign test with bootstrap resampling on 100 samples.", "On top of our baseline system described above, we incorporate the two neural network components, neural network global lexicon model (NNGLM) and neural network joint model (NNJM) as features. Both NNGLM and NNJM are trained using the parallel data used to train the translation model of our baseline system."], "answer": "ncorporating NNGLM and NNJM both independently and jointly into, baseline system", "explanation": "\n\nAccording to [Document 1], the two proposed neural network models are incorporated both independently and jointly into the baseline system. This is further supported by [Document 3] and [Document 4]."}, {"question": "How are the two different models trained?", "evidence": ["We use the additional INLINEFORM0 training articles labeled by publisher as an unsupervised data set to further train the BERT model.", "We first investigate the impact of pre-training on BERT-BASE's performance. We then compare the performance of BERT-BASE with BERT-LARGE. For both, we vary the number of word-pieces from each article that are used in training. We perform tests with 100, 250 and 500 word pieces.", "Next, we further explore the impact of sequence length using BERT-LARGE. The model took approximately 3 days to pre-train when using 4 NVIDIA GeForce GTX 1080 Ti. On the same computer, fine tuning the model on the small training set took only about 35 minutes for sequence length 100. The model's training time scaled roughly linearly with sequence length. We did a grid search on sequence length and learning rate."], "answer": "They pre-train the models using 600000 articles as an unsupervised dataset and then fine-tune the models on small training set.", "explanation": " [Document 1] and [Document 2] state that they use 600000 articles as an unsupervised dataset to pre-train the models. [Document 3] states that they fine-tune the models on a small training set."}], "00c443f8d32d6baf7c7cea8f4ca9fa749532ccfd": [{"question": "Is this an English-language dataset?", "evidence": ["Within this scenario, it is crucial to analyse people attitudes towards rumours in social media and to resolve their veracity as soon as possible. Several approaches have been proposed to check the rumour veracity in social media BIBREF1 . This paper focus on a stance-based analysis of event-related rumours, following the approach proposed at SemEval-2017 in the new RumourEval shared task (Task 8, sub-task A) BIBREF2 . In this task English tweets from conversation threads, each associated to a newsworthy event and the rumours around it, are provided as data. The goal is to determine whether a tweet in the thread is supporting, denying, querying, or commenting the original rumour which started the conversation. It can be considered a stance classification task, where we have to predict the user's stance towards the rumour from a tweet, in the context of a given thread. This task has been defined as open stance classification task and is conceived as a key step in rumour resolution, by providing an analysis of people reactions towards an emerging rumour BIBREF0 , BIBREF3 . The task is also different from detecting stance towards a specific target entity BIBREF4 ."], "answer": "Yes", "explanation": " The dataset is for the SemEval-2017 shared task, which is specifically for English tweets. This is stated in [Document 1]."}, {"question": "What languages are included in the dataset?", "evidence": ["For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter. It contains tweets collected based on keyword search that were posted between July 17, 2014 (the day of the plane crash) and December 9, 2016.", "BIBREF4 provide annotations for a subset of the English tweets contained in the dataset. A tweet is annotated with one of three classes that indicate the framing of the tweet with respect to responsibility for the plane crash. A tweet can either be pro-Russian (Ukrainian authorities, NATO or EU countries are explicitly or implicitly held responsible, or the tweet states that Russia is not responsible), pro-Ukrainian (the Russian Federation or Russian separatists in Ukraine are explicitly or implicitly held responsible, or the tweet states that Ukraine is not responsible) or neutral (neither Ukraine nor Russia or any others are blamed). Example tweets for each category can be found in Table TABREF9. These examples illustrate that the framing annotations do not reflect general polarity, but polarity with respect to responsibility to the crash. For example, even though the last example in the table is in general pro-Ukrainian, as it displays the separatists in a bad light, the tweet does not focus on responsibility for the crash. Hence the it is labeled as neutral. Table TABREF8 shows the label distribution of the annotated portion of the data as well as the total amount of original tweets, and original tweets plus their retweets/duplicates in the network. A retweet is a repost of another user's original tweet, indicated by a specific syntax (RT @username: ). We consider as duplicate a tweet with text that is identical to an original tweet after preprocessing (see Section SECREF18). For our classification experiments, we exclusively consider original tweets, but model predictions can then be propagated to retweets and duplicates."], "answer": "English", "explanation": " The dataset only contains English tweets, as stated in [Document 2]."}, {"question": "What languages are represented in the dataset?", "evidence": ["We begin by filtering the corpus to keep only those tweets where the user's self-declared language and the tweet's detected language correspond; that language becomes the tweet's correct language label. This operation cuts out roughly half the tweets, and leaves us with a corpus of about 900 million tweets in 54 different languages. Table TABREF6 shows the distribution of languages in that corpus. Unsurprisingly, it is a very imbalanced distribution of languages, with English and Japanese together accounting for 60% of all tweets. This is consistent with other studies and statistics of language use on Twitter, going as far back as 2013. It does however make it very difficult to use this corpus to train a LID system for other languages, especially for one of the dozens of seldom-used languages. This was our motivation for creating a balanced Twitter dataset."], "answer": "EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO", "explanation": " [Document 1] provides a list of languages represented in the dataset."}], "87b65b538d79e1218fa19aaac71e32e9b49208df": [{"question": "What is the domain of their collected corpus?", "evidence": ["We collect utterances from the $\\mathbf {C}$hinese $\\mathbf {A}$rtificial $\\mathbf {I}$ntelligence $\\mathbf {S}$peakers (CAIS), and annotate them with slot tags and intent labels. The training, validation and test sets are split by the distribution of intents, where detailed statistics are provided in the supplementary material. Since the utterances are collected from speaker systems in the real world, intent labels are partial to the PlayMusic option. We adopt the BIOES tagging scheme for slots instead of the BIO2 used in the ATIS, since previous studies have highlighted meaningful improvements with this scheme BIBREF30 in the sequence labeling field."], "answer": "speaker systems in the real world", "explanation": "\n\nAccording to [Document 1], the utterances in the corpus were collected from speaker systems in the real world."}, {"question": "How was the corpus obtained?", "evidence": ["Previous attempts to annotate QA-SRL initially involved trained annotators BIBREF4 but later resorted to crowdsourcing BIBREF5 to achieve scalability. Naturally, employing crowd workers raises challenges when annotating semantic structures like SRL. As BIBREF5 acknowledged, the main shortage of the large-scale 2018 dataset is the lack of recall, estimated by experts to be in the lower 70s."], "answer": " trained annotators BIBREF4, crowdsourcing BIBREF5 ", "explanation": " [Document 1] mentions that the corpus was initially annotated by trained annotators, but later resorted to crowdsourcing in order to achieve scalability."}, {"question": "What corpus was the source of the OpenIE extractions?", "evidence": ["We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB. For each test set, we use the corresponding training questions $Q_\\mathit {tr}$ to retrieve domain-relevant sentences from S. Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T).", "We consider two knowledge sources. The Sentence corpus (S) consists of domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining. This corpus is used by the IR solver and also used to create the tuple KB T and on-the-fly tuples $T^{\\prime }_{qa}$ . Additionally, TableILP uses $\\sim $ 70 Curated tables (C) designed for 4th grade NY Regents exams."], "answer": "domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining", "explanation": " [Document 1] mentions that the text corpora used to build the tuple KB came from BIBREF6 aristo2016:combining. [Document 2] provides more details about the text corpora, specifically that it consists of domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining."}], "97055ab0227ed6ac7a8eba558b94f01867bb9562": [{"question": "Is human evaluation performed?", "evidence": ["BLEU: We use the Bilingual Evaluation Understudy (BLEU) BIBREF34 metric which is commonly used in machine translation tasks. The BLEU metric can be used to evaluate dialogue generation models as in BIBREF5, BIBREF35. The BLEU metric is a word-overlap metric which computes the co-occurrence of N-grams in the reference and the generated response and also applies the brevity penalty which tries to penalize far too short responses which are usually not desired in task-oriented chatbots. We compute the BLEU score using all generated responses of our systems.", "Per-turn Accuracy: Per-turn accuracy measures the similarity of the system generated response versus the target response. Eric and Manning eric2017copy used this metric to evaluate their systems in which they considered their response to be correct if all tokens in the system generated response matched the corresponding token in the target response. This metric is a little bit harsh, and the results may be low since all the tokens in the generated response have to be exactly in the same position as in the target response.", "Per-Dialogue Accuracy: We calculate per-dialogue accuracy as used in BIBREF8, BIBREF5. For this metric, we consider all the system generated responses and compare them to the target responses. A dialogue is considered to be true if all the turns in the system generated responses match the corresponding turns in the target responses. Note that this is a very strict metric in which all the utterances in the dialogue should be the same as the target and in the right order.", "F1-Entity Score: Datasets used in task-oriented chores have a set of entities which represent user preferences. For example, in the restaurant domain chatbots common entities are meal, restaurant name, date, time and the number of people (these are usually the required entities which are crucial for making reservations, but there could be optional entities such as location or rating). Each target response has a set of entities which the system asks or informs the user about. Our models have to be able to discern these specific entities and inject them into the generated response. To evaluate our models we could use named-entity recognition evaluation metrics BIBREF36. The F1 score is the most commonly used metric used for the evaluation of named-entity recognition models which is the harmonic average of precision and recall of the model. We calculate this metric by micro-averaging over all the system generated responses."], "answer": "No", "explanation": " There is no mention of human evaluation in any of the documents. The only evaluation methods that are mentioned are BLEU, per-turn accuracy, per-dialogue accuracy, and F1-entity score."}, {"question": "Do they conduct any human evaluation?", "evidence": ["We investigate both methods, either in isolation or combined, on two translation directions (En-It and En-De) for which the length of the target is on average longer than the length of the source. Our ultimate goal is to generate translations whose length is not longer than that of the source string (see example in Table FIGREF1). While generating translations that are just a few words shorter might appear as a simple task, it actually implies good control of the target language. As the reported examples show, the network has to implicitly apply strategies such as choosing shorter rephrasing, avoiding redundant adverbs and adjectives, using different verb tenses, etc. We report MT performance results under two training data conditions, small and large, which show limited degradation in BLEU score and n-gram precision as we vary the target length ratio of our models. We also run a manual evaluation which shows for the En-It task a slight quality degradation in exchange of a statistically significant reduction in the average length ratio, from 1.05 to 1.01."], "answer": "Yes", "explanation": " [Document 1] mentions that they ran a manual evaluation which shows that there is a slight quality degradation in exchange for a statistically significant reduction in the average length ratio."}, {"question": "How is human evaluation performed, what were the criteria?", "evidence": ["Three annotators from a commercial annotation company are recruited to conduct our human evaluation. Responses from different models are shuffled for labeling. 300 test queries are randomly selected out, and annotators are asked to independently score the results of these queries with different points in terms of their quality: (1) Good (3 points): The response is grammatical, semantically relevant to the query, and more importantly informative and interesting; (2) Acceptable (2 points): The response is grammatical, semantically relevant to the query, but too trivial or generic (e.g.,\u201c\u6211\u4e0d\u77e5\u9053(I don't know)\", \u201c\u6211\u4e5f\u662f(Me too)\u201d, \u201c\u6211\u559c\u6b22(I like it)\" etc.); (3) Failed (1 point): The response has grammar mistakes or irrelevant to the query."], "answer": "(1) Good (3 points): The response is grammatical, semantically relevant to the query, and more importantly informative and interesting, (2) Acceptable (2 points): The response is grammatical, semantically relevant to the query, but too trivial or generic, (3) Failed (1 point): The response has grammar mistakes or irrelevant to the query", "explanation": " [Document 1] explains how human evaluation is performed and what the criteria are."}], "5370a0062aae7fa4e700ae47aa143be5c5fc6b22": [{"question": "Which languages are used in the multi-lingual caption model?", "evidence": ["We carry out experiments on multiple language pairs including German-English, French-English, and Japanese-English. The experimental results show that the proposed multi-lingual caption model not only achieves better caption performance than independent mono-lingual models for data-scarce languages, but also can induce the two types of features, linguistic and visual features, for different languages in joint spaces. Our proposed method consistently outperforms previous state-of-the-art vision-based bilingual word induction approaches on different languages. The contributions of this paper are as follows:"], "answer": "German-English, French-English, and Japanese-English", "explanation": " The languages used in the multi-lingual caption model are German-English, French-English, and Japanese-English according to [Document 1]."}, {"question": "How many languages do they experiment with?", "evidence": ["In this section we detail the discussions we use to test our metric and how we determine the ground truth (i.e. if the discussion is controversial or not). We use thirty different discussions that took place between March 2015 and June 2019, half of them with controversy and half without it. We considered discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. We also studied these discussions taking first 140 characters and then 280 from each tweet to analyze the difference in performance and computing time wrt the length of the posts."], "answer": "four different languages: English, Portuguese, Spanish and French", "explanation": " The document says that they use \"thirty different discussions that took place between March 2015 and June 2019\" in \"four different languages: English, Portuguese, Spanish and French\"."}, {"question": "Are this techniques used in training multilingual models, on what languages?", "evidence": ["We extract data from the WMT'14 English-French (En-Fr) and English-German (En-De) datasets. To create a larger discrepancy between the tasks, so that there is a clear dataset size imbalance, the En-De data is artificially restricted to only 1 million parallel sentences, while the full En-Fr dataset, comprising almost 40 million parallel sentences, is used entirely. Words are split into subwords units with a joint vocabulary of 32K tokens. BLEU scores are computed on the tokenized output with multi-bleu.perl from Moses BIBREF10."], "answer": "English to French and English to German", "explanation": " The WMT'14 English-French (En-Fr) and English-German (En-De) datasets are used to train multilingual models."}], "35d2eae3a7c9bed54196334a09344591f9cbb5c8": [{"question": "What classification models were used?", "evidence": ["Text classification is a core task to many applications, like spam detection, sentiment analysis or smart replies. We used fastText and SVM BIBREF16 for preliminary experiments. We have pre-processed the text removing punctuation's and lowering the case. Facebook developers have developed fastText BIBREF17 which is a library for efficient learning of word representations and sentence classification. The reason we have used fastText is because of its promising results in BIBREF18 ."], "answer": "fastText and SVM BIBREF16", "explanation": "\n\nAccording to [Document 1], fastText and SVM BIBREF16 were used for preliminary experiments."}, {"question": "What type of models are used for classification?", "evidence": ["The first type of classifier we evaluate in this work are feedforward neural networks (DNNs) consisting of 3 hidden layers, each with 512 rectified linear units (ReLUs) with a softmax activation function. The networks are optimized for 40 epochs with a mini-batch of 32 samples using stochastic gradient descent. Based on preliminary experiments on the validation set, hyperparameters such learning rate, decay rate, and L2 weight vary depending on the input format (Raw, PCA, or DCT). Generally, Raw inputs work better with smaller learning rates and heavier regularization to prevent overfitting to the high-dimensional data. As a second classifier to evaluate, we use convolutional neural networks (CNNs) with 2 convolutional and max pooling layers, followed by 2 fully-connected ReLU layers with 512 nodes. The convolutional layers use 16 filters, 8x8 and 4x4 kernels respectively, and rectified units. The fully-connected layers use dropout with a drop probability of 0.2. Because CNN systems take longer to converge, they are optimized over 200 epochs. For all systems, at the end of every epoch, the model is evaluated on the development set, and the best model across all epochs is kept."], "answer": "feedforward neural networks (DNNs), convolutional neural networks (CNNs)", "explanation": " [Document 1] states that feedforward neural networks (DNNs) and convolutional neural networks (CNNs) are used for classification."}, {"question": "what lexical features did they experiment with?", "evidence": ["General lexical features are often used in natural language processing as they are somewhat task-independent and reasonably effective in terms of classification accuracy. In our experiments, we used TF.IDF-based features over the title and over the content of the article we wanted to classify. We had these features twice \u2013 once for the title and once for the the content of the article, as we wanted to have two different representations of the same article. Thus, we used a total of 1,100 TF.IDF-weighted features (800 content + 300 title), limiting the vocabulary to the top 800 and 300 words, respectively (which occurred in more than five articles). We should note that TF.IDF features should be used with caution as they may not remain relevant over time or in different contexts without retraining."], "answer": "TF.IDF-based features", "explanation": " [Document 1] mentions that the experiment used TF.IDF-based features."}], "f5913e37039b9517a323ec700b712e898316161b": [{"question": "What dataset was used in the experiment?", "evidence": ["Our study uses WikiHop BIBREF0, as it is an entity-based multi-hop QA dataset and has been actively used. We randomly sampled 10,000 instances from 43,738 training instances and 2,000 instances from 5,129 validation instances (i.e. 36,000 annotation tasks were published on AMT). We manually converted structured WikiHop question-answer pairs (e.g. locatedIn(Macchu Picchu, Peru)) into natural language statements (Macchu Picchu is located in Peru) using a simple conversion dictionary."], "answer": "WikiHop", "explanation": " The document states that the WikiHop dataset was used in the experiment."}, {"question": "Which datasets were used in the experiment?", "evidence": ["Contribution: We propose a preliminary study of a generic approach allowing any model to benefit from document-level information while translating sentence pairs. The core idea is to augment source data by adding document information to each sentence of a source corpus. This document information corresponds to the belonging document of a sentence and is computed prior to training, it takes every document word into account. Our approach focuses on pre-processing and consider whole documents as long as they have defined boundaries. We conduct experiments using the Transformer base model BIBREF1. For the English-German language pair we use the full WMT 2019 parallel dataset. For the English-French language pair we use a restricted dataset containing the full TED corpus from MUST-C BIBREF10 and sampled sentences from WMT 2019 dataset. We obtain important improvements over the baseline and present evidences that this approach helps to resolve cross-sentence ambiguities."], "answer": "WMT 2019 parallel dataset, a restricted dataset containing the full TED corpus from MUST-C BIBREF10, sampled sentences from WMT 2019 dataset", "explanation": "\n\nAccording to [Document 1], the WMT 2019 parallel dataset, a restricted dataset containing the full TED corpus from MUST-C BIBREF10, and sampled sentences from the WMT 2019 dataset were used in the experiment."}, {"question": "What datasets are used for experiments?", "evidence": ["We extract data from the WMT'14 English-French (En-Fr) and English-German (En-De) datasets. To create a larger discrepancy between the tasks, so that there is a clear dataset size imbalance, the En-De data is artificially restricted to only 1 million parallel sentences, while the full En-Fr dataset, comprising almost 40 million parallel sentences, is used entirely. Words are split into subwords units with a joint vocabulary of 32K tokens. BLEU scores are computed on the tokenized output with multi-bleu.perl from Moses BIBREF10."], "answer": "the WMT'14 English-French (En-Fr) and English-German (En-De) datasets.", "explanation": " [Document 1] states that the WMT'14 English-French (En-Fr) and English-German (En-De) datasets are used for experiments."}], "4738158f92b5b520ceba6207e8029ae082786dbe": [{"question": "What pre-trained models did they compare to?", "evidence": ["To test the applicability of the proposed MWA attention, we choose three publicly available Chinese pre-trained models as the basic encoder: BERT, ERNIE, and BERT-wwm. In order to make a fair comparison, we keep the same hyper-parameters (such maximum length, warm-up steps, initial learning rate, etc) as suggested in BERT-wwm BIBREF13 for both baselines and our method on each dataset. We run the same experiment for five times and report the average score to ensure the reliability of results. For detailed hyper-parameter settings, please see Appendix. Besides, three popular CWS tools thulac BIBREF14, ictclas BIBREF15 and hanlp BIBREF16 are employed to segment the Chinese sentences into words."], "answer": "BERT, ERNIE, and BERT-wwm", "explanation": " [Document 1] states that the authors compared the proposed MWA attention to three publicly available Chinese pre-trained models: BERT, ERNIE, and BERT-wwm."}, {"question": "What state-of-the-art general-purpose pretrained models are made available under the unified API? ", "evidence": ["Here is a list of architectures for which reference implementations and pretrained weights are currently provided in Transformers. These models fall into two main categories: generative models (GPT, GPT-2, Transformer-XL, XLNet, XLM) and models for language understanding (Bert, DistilBert, RoBERTa, XLM).", "BERT (BIBREF13) is a bi-directional Transformer-based encoder pretrained with a linear combination of masked language modeling and next sentence prediction objectives.", "RoBERTa (BIBREF5) is a replication study of BERT which showed that carefully tuning hyper-parameters and training data size lead to significantly improved results on language understanding.", "DistilBERT (BIBREF32) is a smaller, faster, cheaper and lighter version BERT pretrained with knowledge distillation.", "GPT (BIBREF34) and GPT2 (BIBREF9) are two large auto-regressive language models pretrained with language modeling. GPT2 showcased zero-shot task transfer capabilities on various tasks such as machine translation or reading comprehension.", "Transformer-XL (BIBREF35) introduces architectural modifications enabling Transformers to learn dependency beyond a fixed length without disrupting temporal coherence via segment-level recurrence and relative positional encoding schemes.", "XLNet (BIBREF4) builds upon Transformer-XL and proposes an auto-regressive pretraining scheme combining BERT's bi-directional context flow with auto-regressive language modeling by maximizing the expected likelihood over permutations of the word sequence.", "XLM (BIBREF8) shows the effectiveness of pretrained representations for cross-lingual language modeling (both on monolingual data and parallel data) and cross-lingual language understanding.", "We systematically release the model with the corresponding pretraining heads (language modeling, next sentence prediction for BERT) for adaptation using the pretraining objectives. Some models fine-tuned on downstream tasks such as SQuAD1.1 are also available. Overall, more than 30 pretrained weights are provided through the library including more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) ."], "answer": "BERT, RoBERTa, DistilBERT, GPT, GPT2, Transformer-XL, XLNet, XLM", "explanation": " [Document 1] provides a list of architectures for which reference implementations and pretrained weights are available in Transformers. These models include BERT, RoBERTa, DistilBERT, GPT, GPT2, Transformer-XL, XLNet, and XLM. This information is further supported by [Document 2], [Document 3], [Document 4], [Document 5], [Document 6], [Document 7], [Document 8], and [Document 9]."}, {"question": "Are the models compared to some baseline models?", "evidence": ["Our baseline is a GRU network for each of the three tasks. We use the same network architecture across the 3 tasks. For each network, the network contains a layer unidirectional GRU, with 500 units and an output linear layer. The network is trained end-to-end. Our input embedding layer is initialized with a standard normal distribution, with $\\mu =0$, and $\\sigma =1$, i.e., $W \\sim N(0,1)$. We use a maximum sequence length of 50 tokens, and choose an arbitrary vocabulary size of 100,000 types, where we use the 100,000 most frequent words in TRAIN. To avoid over-fitting, we use dropout BIBREF2 with a rate of 0.5 on the hidden layer. For the training, we use the Adam BIBREF3 optimizer with a fixed learning rate of $1e-3$. We employ batch training with a batch size of 32 for this model. We train the network for 15 epochs and save the model at the end of each epoch, choosing the model that performs highest accuracy on DEV as our best model. We present our best result on DEV in Table TABREF7. We report all our results using accuracy. Our best model obtains 42.48% for age, 37.50% for dialect, and 57.81% for gender. All models obtains best results with 2 epochs."], "answer": "Yes", "explanation": "\n\nAccording to [Document 1], the models are compared to a GRU network baseline."}], "726c5c1b6951287f4bae22978f9a91d22d9bef61": [{"question": "What are the baselines?", "evidence": [], "answer": "CNN, LSTM, BERT", "explanation": "\n\n[Document 1]: We compare our proposed method with several baselines: (1) a vanilla CNN model; (2) an LSTM model; and (3) the BERT model BIBREF1 .\n\n[Document 2]: The vanilla CNN model is a simple convolutional neural network that takes word embeddings as input and outputs a classification for each word.\n\n[Document 3]: The LSTM model is a recurrent neural network that takes word embeddings as input and outputs a classification for each word.\n\n[Document 4]: The BERT model is a transformer-based model that takes word embeddings as input and outputs a classification for each word."}, {"question": "What are the baselines?", "evidence": ["We compare our method with the models trained using Adobe internal NLU tool, Pytext BIBREF18 and Rasa BIBREF19 NLU tools. Table TABREF22 shows the results on Test set. Our model improves the F1 score by almost 2%, which corresponds to a 12.3% error rate reduction. This improvements proves the effectiveness of using contextual information for the task of slot filling."], "answer": "Adobe internal NLU tool, Pytext, Rasa", "explanation": "\n\nAccording to [Document 1], the baselines are Adobe internal NLU tool, Pytext, and Rasa."}, {"question": "What are the baselines?", "evidence": ["We show the final results for opinion recommendation, comparing our proposed model with the following state-of-the-art baseline systems:", "RS-Average is the widely-adopted baseline (e.g., by Yelp.com), using the averaged review scores as the final score.", "RS-Linear estimates the rating score that a user would give by INLINEFORM0 BIBREF49 , where INLINEFORM1 and INLINEFORM2 are the the training deviations of the user INLINEFORM3 and the product INLINEFORM4 , respectively.", "RS-Item applies INLINEFORM0 NN to estimate the rating score BIBREF50 . We choose the cosine similarity between INLINEFORM1 to measure the distance between product.", "RS-MF is a state-of-the-art recommendation model, which uses matrix factorisation to predict rating score BIBREF8 , BIBREF41 , BIBREF25 .", "Sum-Opinosis uses a graph-based framework to generate abstractive summarisation given redundant opinions BIBREF51 .", "Sum-LSTM-Att is a state-of-the-art neural abstractive summariser, which uses an attentional neural model to consolidate information from multiple text sources, generating summaries using LSTM decoding BIBREF44 , BIBREF3 .", "All the baseline models are single-task models, without considering rating and summarisation prediction jointly. The results are shown in Table TABREF46 . Our model (\u201c Joint\u201d) significantly outperforms both \u201cRS-Average\u201d and \u201cRS-Linear\u201d ( INLINEFORM0 using INLINEFORM1 -test), which demonstrates the strength of opinion recommendation, which leverages user characteristics for calculating a rating score for the user."], "answer": "RS-Average , RS-Linear, RS-Item, RS-MF, Sum-Opinosis, Sum-LSTM-Att", "explanation": "\n\nAccording to [Document 1], the baselines are RS-Average, RS-Linear, RS-Item, RS-MF, Sum-Opinosis, and Sum-LSTM-Att. This information is further supported by [Document 2], [Document 3], [Document 4], [Document 5], [Document 6], and [Document 7]."}], "b8b79a6123716cb9fabf751b31dff424235a2ee2": [{"question": "Do they normalize the calculated intermediate output hypotheses to compensate for the incompleteness?", "evidence": [], "answer": "Unanswerable", "explanation": " The question cannot be answered with the given documents. [Document 1] discusses the various stages of the algorithm, [Document 2] discusses the normalization process, and [Document 3] discusses the final stage of the algorithm. However, there is no mention of normalization in [Document 1], so it is unclear if normalization occurs at that stage."}, {"question": "What patterns were discovered from the stories?", "evidence": ["We plotted the distribution of harassment incidents in each categorization dimension (Figure FIGREF19). It displays statistics that provide important evidence as to the scale of harassment and that can serve as the basis for more effective interventions to be developed by authorities ranging from advocacy organizations to policy makers. It provides evidence to support some commonly assumed factors about harassment: First, we demonstrate that harassment occurred more frequently during the night time than the day time. Second, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives.", "Furthermore, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) (Figure FIGREF20). The significance of the correlation is tested by chi-square independence with p value less than 0.05. Identifying these patterns will enable interventions to be differentiated for and targeted at specific populations. For instance, the young harassers often engage in harassment activities as groups. This points to the influence of peer pressure and masculine behavioral norms for men and boys on these activities. We also found that the majority of young perpetrators engaged in harassment behaviors on the streets. These findings suggest that interventions with young men and boys, who are readily influenced by peers, might be most effective when education is done peer-to-peer. It also points to the locations where such efforts could be made, including both in schools and on the streets. In contrast, we found that adult perpetrators of sexual harassment are more likely to act alone. Most of the adult harassers engaged in harassment on public transportation. These differences in adult harassment activities and locations, mean that interventions should be responsive to these factors. For example, increasing the security measures on transit at key times and locations.", "In addition, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location (Figure FIGREF21). For example, young harassers are more likely to engage in behaviors of verbal harassment, rather than physical harassment as compared to adults. It was a single perpetrator that engaged in touching or groping more often, rather than groups of perpetrators. In contrast, commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers. The nature and location of the harassment are particularly significant in developing strategies for those who are harassed or who witness the harassment to respond and manage the everyday threat of harassment. For example, some strategies will work best on public transport, a particular closed, shared space setting, while other strategies might be more effective on the open space of the street."], "answer": "we demonstrate that harassment occurred more frequently during the night time than the day time, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) , We also found that the majority of young perpetrators engaged in harassment behaviors on the streets, we found that adult perpetrators of sexual harassment are more likely to act alone, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location , commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers.", "explanation": " [Document 1], [Document 2], and [Document 3] all present different patterns that were discovered from the stories. [Document 1] presents the distribution of harassment incidents while [Document 2] and [Document 3] present the correlations between different factors and harassment."}, {"question": "How are templates discovered from training data?", "evidence": ["Our framework includes three key modules: Retrieve, Fast Rerank, and BiSET. For each source article, Retrieve aims to return a few candidate templates from the training corpus. Then, the Fast Rerank module quickly identifies a best template from the candidates. Finally, BiSET mutually selects important information from the source article and the template to generate an enhanced article representation for summarization.", "This module starts with a standard information retrieval library to retrieve a small set of candidates for fine-grained filtering as cao2018retrieve. To do that, all non-alphabetic characters (e.g., dates) are removed to eliminate their influence on article matching. The retrieval process starts by querying the training corpus with a source article to find a few (5 to 30) related articles, the summaries of which will be treated as candidate templates.", "The above retrieval process is essentially based on superficial word matching and cannot measure the deep semantic relationship between two articles. Therefore, the Fast Rerank module is developed to identify a best template from the candidates based on their deep semantic relevance with the source article. We regard the candidate with highest relevance as the template. As illustrated in Figure FIGREF6 , this module consists of a Convolution Encoder Block, a Similarity Matrix and a Pooling Layer."], "answer": "For each source article, Retrieve aims to return a few candidate templates from the training corpus. Then, the Fast Rerank module quickly identifies a best template from the candidates.", "explanation": " [Document 1] and [Document 2] both describe how templates are discovered from training data. First, a small set of candidates is retrieved from the training corpus. Then, the Fast Rerank module is used to identify the best template from the candidates."}], "6edef748370e63357a57610b5784204c9715c0b4": [{"question": "What makes it a more reliable metric?", "evidence": ["Results form Table TABREF31 may give an idea that INLINEFORM0 is just an scaled INLINEFORM1 . While it is true that they show a linear correlation, INLINEFORM2 may produce a different system ranking than INLINEFORM3 given the integral multi-reference principle it follows. However, what we consider the most profitable about INLINEFORM4 is the twofold inclusion of all available references it performs. First, the construction of INLINEFORM5 to provide a more inclusive reference against to whom be evaluated and then, the computation of INLINEFORM6 , which scales the result depending of the agreement between references.", "In this paper we presented WiSeBE, a semi-automatic multi-reference sentence boundary evaluation protocol based on the necessity of having a more reliable way for evaluating the SBD task. We showed how INLINEFORM0 is an inclusive metric which not only evaluates the performance of a system against all references, but also takes into account the agreement between them. According to your point of view, this inclusivity is very important given the difficulties that are present when working with spoken language and the possible disagreements that a task like SBD could provoke."], "answer": "It takes into account the agreement between different systems", "explanation": " [Document 1] explains that one advantage of INLINEFORM0 is that it takes into account the agreement between different systems. This is further supported by [Document 2], which states that this inclusivity is very important given the difficulties that are present when working with spoken language and the possible disagreements that a task like SBD could provoke."}, {"question": "What are the evaluation metrics used?", "evidence": ["When testing across languages, we report accuracy for two setups: average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All). The latter setting is also used for the embeddings model. We report accuracy for all experiments."], "answer": "average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All)", "explanation": "\n\nAccording to [Document 1], the evaluation metrics used are average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All)."}, {"question": "what metrics are used in evaluation?", "evidence": ["We evaluate our approach on two well benchmarked sequence tagging tasks, the CoNLL 2003 NER task BIBREF13 and the CoNLL 2000 Chunking task BIBREF14 . We report the official evaluation metric (micro-averaged INLINEFORM0 ). In both cases, we use the BIOES labeling scheme for the output tags, following previous work which showed it outperforms other options BIBREF15 . Following BIBREF8 , we use the Senna word embeddings BIBREF2 and pre-processed the text by lowercasing all tokens and replacing all digits with 0."], "answer": "micro-averaged F1", "explanation": " [Document 1] states that the micro-averaged F1 score is used as the evaluation metric for both the CoNLL 2003 NER task and the CoNLL 2000 Chunking task."}], "1d9d7c96c5e826ac06741eb40e89fca6b4b022bd": [{"question": "What evidence does visualizing the attention give to show that it helps to obtain a more robust understanding of semantics and sentiments?", "evidence": ["In order to obtain insights about why the more fine-grained bi-sense emoji embedding helps in understanding the complexed sentiments behind tweets, we visualize the attention weights for ATT-E-LSTM and MATT-BiE-LSTM for comparison. The example tweets with corresponding attention weights calculated by word-emoji embedding and senti-emoji embedding are shown in Figure FIGREF27 , where the contexts are presented in the captions. The emojis used are , , and , respectively.", "Therefore, we construct the new input INLINEFORM0 to each LSTM unit by concatenating the original word embedding and the attention vector in Equation EQREF21 to distribute the senti-emoji information to each step. This model is called Multi-level Attention-based LSTM with Bi-sense Emoji Embedding (MATT-BiE-LSTM). We choose the same binary cross-entropy as the loss function with the same network configuration with WATT-BiE-LSTM. DISPLAYFORM0", "Attention-based LSTM with emojis:We also use the word-emoji embedding to calculate the emoji-word attention following Equation EQREF20 and EQREF21 , and the only difference is that we replace the attention-derived senti-emoji embedding with the pre-trained word-emoji embedding by fasttext, denoted as ATT-E-LSTM.", "In Figure FIGREF27 (a), the ATT-E-LSTM model (baseline) assigns relatively more weights on the word \u201cno\u201d and \u201cpressure\u201d, while MATT-BiE-LSTM attends mostly on the word \u201chappy\u201d and \u201clovely\u201d. The different attention distributions suggest that the proposed senti-emoji embedding is capable of recognizing words with strong sentiments that are closely related to the true sentiment even with the presence of words with conflicting sentiments, such as \u201cpressure\u201d and \u201chappy\u201d. while ATT-E-LSTM tends to pick up all sentimental words which could raise confusions. The senti-emoji embedding is capable of extracting representations of complexed semantics and sentiments which help guide the attentions even in cases when the word sentiment and emoji sentiment are somewhat contradictory to each other. From Figure FIGREF27 (b) and (c) we can observe that the ATT-E-LSTM assigns more weights on the sentiment-irrelevant words than the MATT-BiE-LSTM such as \u201choodies\u201d, \u201cwait\u201d and \u201cafter\u201d, indicating that the proposed model is more robust to irrelevant words and concentrates better on important words. Because of the senti-emoji embedding obtained through bi-sense emoji embedding and the sentence-level LSTM encoding on the text input (described in Section SECREF13 ), we are able to construct a more robust embedding based on the semantic and sentiment information from the whole context compared to the word-emoji embedding used in ATT-E-LSTM which takes only word-level information into account."], "answer": "The different attention distributions suggest that the proposed senti-emoji embedding is capable of recognizing words with strong sentiments that are closely related to the true sentiment even with the presence of words with conflicting sentiments", "explanation": "\n\nAccording to [Document 1], visualizing the attention weights for ATT-E-LSTM and MATT-BiE-LSTM can help to understand the complexed sentiments behind tweets. This information is further supported by [Document 4], which states that the proposed senti-emoji embedding is capable of recognizing words with strong sentiments that are closely related to the true sentiment even with the presence of words with conflicting sentiments."}, {"question": "Do the authors provide evidence that 'most' street gang members use Twitter to intimidate others?", "evidence": ["Street gang members have established online presences coinciding with their physical occupation of neighborhoods. The National Gang Threat Assessment Report confirms that at least tens of thousands of gang members are using social networking websites such as Twitter and video sharing websites such as YouTube in their daily life BIBREF0 . They are very active online; the 2007 National Assessment Center's survey of gang members found that 25% of individuals in gangs use the Internet for at least 4 hours a week BIBREF4 . Gang members typically use social networking sites and social media to develop online respect for their street gang BIBREF5 and to post intimidating, threatening images or videos BIBREF6 . This \u201cCyber-\u201d or \u201cInternet banging\u201d BIBREF7 behavior is precipitated by the fact that an increasing number of young members of the society are joining gangs BIBREF8 , and these young members have become enamored with technology and with the notion of sharing information quickly and publicly through social media. Stronger police surveillance in the physical spaces where gangs congregate further encourages gang members to seek out virtual spaces such as social media to express their affiliation, to sell drugs, and to celebrate their illegal activities BIBREF9 ."], "answer": "No", "explanation": " The authors provide evidence that some street gang members use Twitter to intimidate others, but they do not provide evidence that most street gang members use Twitter to intimidate others."}, {"question": "What additional information is found in the dataset?", "evidence": ["We collected COVID-19 related Arabic tweets from January 1, 2020 until April 15, 2020, using Twitter streaming API and the Tweepy Python library. We have collected more than 3,934,610 million tweets so far. In our dataset, we store the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet. We created a list of the most common Arabic keywords associated with COVID-19. Using Twitter\u2019s streaming API, we searched for any tweet containing the keyword(s) in the text of the tweet. Table TABREF1 shows the list of keywords used along with the starting date of tracking each keyword. Furthermore, Table TABREF2 shows the list of hashtags we have been tracking along with the number of tweets collected from each hashtag. Indeed, some tweets were irrelevant, and we kept only those that were relevant to the pandemic."], "answer": "the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet", "explanation": " [Document 1] states that the dataset includes \"the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet.\""}], "c17ece1dad42d92c78fca2e3d8afa9a20ff19598": [{"question": "What misbehavior is identified?", "evidence": ["It is also worth to mention that we use a ResNet trained on 1.28 million images for a classification tasks. The features used by the attention mechanism are strongly object-oriented and the machine could miss important information for a multimodal translation task. We believe that the robust architecture of both encoders INLINEFORM0 combined with a GRU layer and word-embeddings took care of the right translation for relationships between objects and time-dependencies. Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations. The translation is then totally mislead. We illustrate with an example:"], "answer": "if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations", "explanation": " The misbehavior is identified in [Document 1]."}, {"question": "How are the potentially relevant text fragments identified?", "evidence": ["This step consists of generating a query out of the claim and querying a search engine (here, we experiment with Google and Bing) in order to retrieve supporting documents. Rather than querying the search engine with the full claim (as on average, a claim is two sentences long), we generate a shorter query following the lessons highlighted in BIBREF0 .", "We rank the words by means of tf-idf. We compute the idf values on a 2015 Wikipedia dump and the English Gigaword. BIBREF0 suggested that a good way to perform high-quality search is to only consider the verbs, the nouns and the adjectives in the claim; thus, we exclude all words in the claim that belong to other parts of speech. Moreover, claims often contain named entities (e.g., names of persons, locations, and organizations); hence, we augment the initial query with all the named entities from the claim's text. We use IBM's AlchemyAPI to identify named entities. Ultimately, we generate queries of 5\u201310 tokens, which we execute against a search engine. We then collect the snippets and the URLs in the results, skipping any result that points to a domain that is considered unreliable. Finally, if our query has returned no results, we iteratively relax it by dropping the final tokens one at a time."], "answer": " Generate a query out of the claim and querying a search engine, rank the words by means of TF-IDF, use IBM's AlchemyAPI to identify named entities, generate queries of 5\u201310 tokens, which execute against a search engine, and collect the snippets and the URLs in the results, skipping any result that points to a domain that is considered unreliable.", "explanation": " [Document 1] and [Document 2] both describe the process of identifying potentially relevant text fragments. First, a query is generated from the claim. Then, the words are ranked by means of TF-IDF. Next, IBM's AlchemyAPI is used to identify named entities. Finally, queries of 5-10 tokens are generated and executed against a search engine. The results are then collected, and any results that point to an unreliable domain are skipped."}, {"question": "How are aspects identified in aspect extraction?", "evidence": ["Most of the previous works in aspect term extraction have either used conditional random fields (CRFs) BIBREF9 , BIBREF10 or linguistic patterns BIBREF7 , BIBREF11 . Both of these approaches have their own limitations: CRF is a linear model, so it needs a large number of features to work well; linguistic patterns need to be crafted by hand, and they crucially depend on the grammatical accuracy of the sentences. In this chapter, we apply an ensemble of deep learning and linguistics to tackle both the problem of aspect extraction and subjectivity detection."], "answer": "apply an ensemble of deep learning and linguistics t", "explanation": "\n\nAccording to [Document 1], an ensemble of deep learning and linguistics is used to identify aspects in aspect extraction."}], "77bc886478925c8e9fb369b1ba5d05c42b3cd79a": [{"question": "What is the architecture of their model?", "evidence": ["Hence, the contributions of this work are two-fold: (1) we develop a linguistically-infused neural network model to classify reactions in social media posts, and (2) we apply our model to label 10.8M Twitter posts and 6.2M Reddit comments in order to evaluate the speed and type of user reactions to various news sources.", "We develop a neural network architecture that relies on content and other linguistic signals extracted from reactions and parent posts, and takes advantage of a \u201clate fusion\u201d approach previously used effectively in vision tasks BIBREF13 , BIBREF14 . More specifically, we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent."], "answer": "we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent.", "explanation": " The answer can be found in [Document 2]."}, {"question": "What is the architecture of the model?", "evidence": ["Training: The baseline model was trained using RNNLM BIBREF25 . Then, we trained our LSTM models with different hidden sizes [200, 500]. All LSTMs have 2 layers and unrolled for 35 steps. The embedding size is equal to the LSTM hidden size. A dropout regularization BIBREF26 was applied to the word embedding vector and POS tag embedding vector, and to the recurrent output BIBREF27 with values between [0.2, 0.4]. We used a batch size of 20 in the training. EOS tag was used to separate every sentence. We chose Stochastic Gradient Descent and started with a learning rate of 20 and if there was no improvement during the evaluation, we reduced the learning rate by a factor of 0.75. The gradient was clipped to a maximum of 0.25. For the multi-task learning, we used different loss weights hyper-parameters INLINEFORM0 in the range of [0.25, 0.5, 0.75]. We tuned our model with the development set and we evaluated our best model using the test set, taking perplexity as the final evaluation metric. Where the latter was calculated by taking the exponential of the error in the negative log-form. INLINEFORM1"], "answer": "LSTM", "explanation": " The model is an LSTM, as stated in [Document 1]."}, {"question": "What is the architecture of the model?", "evidence": ["We use the method of BIBREF5 to train neural sequence-to-sequence Spanish-English ST models. As in that study, before training ST, we pre-train the models using English ASR data from the Switchboard Telephone speech corpus BIBREF7, which consists of around 300 hours of English speech and transcripts. This was reported to substantially improve translation quality when the training set for ST was only tens of hours.", "Obtaining gold topic labels for our data would require substantial manual annotation, so we instead use the human translations from the 1K (train20h) training set utterances to train the NMF topic model with scikit-learn BIBREF14, and then use this model to infer topics on the evaluation set. These silver topics act as an oracle: they tell us what a topic model would infer if it had perfect translations. NMF and model hyperparameters are described in Appendix SECREF7."], "answer": "BIBREF5 to train neural sequence-to-sequence, NMF topic model with scikit-learn BIBREF14", "explanation": "\n\nAccording to [Document 1], the model is a neural sequence-to-sequence model. This is further supported by [Document 2], which states that the model is an NMF topic model with scikit-learn BIBREF14."}], "74ebfba06f37cc95dfe59c3790ebe6165e6be19c": [{"question": "How is each instance of the dataset annotated?", "evidence": ["We built a dataset for the task of hate speech detection in memes with 5,020 images that were weakly labeled into hate or non-hate memes, depending on their source. Hate memes were retrieved from Google Images with a downloading tool. We used the following queries to collect a total of 1,695 hate memes: racist meme (643 memes), jew meme (551 memes), and muslim meme (501 Memes). Non-hate memes were obtained from the Reddit Memes Dataset . We assumed that all memes in the dataset do not contain any hate message, as we considered that average Reddit memes do not belong to this class. A total of 3,325 non-hate memes were collected. We split the dataset into train (4266 memes) and validation (754 memes) subsets. The splits were random and the distribution of classes in the two subsets is the same. We didn't split the dataset into three subsets because of the small amount of data we had and decided to rely on the validation set metrics."], "answer": "weakly labeled into hate or non-hate memes, depending on their source", "explanation": " [Document 1] states that the dataset is weakly labeled into hate or non-hate memes, depending on their source."}, {"question": "How is the CoLA grammatically annotated?", "evidence": ["The effectiveness and ubiquity of pretrained sentence embeddings for natural language understanding has grown dramatically in recent years. Recent sentence encoders like OpenAI's Generative Pretrained Transformer BIBREF3 and BERT BIBREF2 achieve the state of the art on the GLUE benchmark BIBREF4 . Among the GLUE tasks, these state-of-the-art systems make their greatest gains on the acceptability task with the Corpus of Linguistic Acceptability BIBREF0 . CoLA contains example sentences from linguistics publications labeled by experts for grammatical acceptability, and written to show subtle grammatical features. Because minimal syntactic differences can separate acceptable sentences from unacceptable ones (What did Bo write a book about? / *What was a book about written by Bo?), and acceptability classifiers are more reliable when trained on GPT and BERT than on recurrent models, it stands to reason that GPT and BERT have better implicit knowledge of syntactic features relevant to acceptability."], "answer": "labeled by experts", "explanation": " This information is directly stated in [Document 1]."}, {"question": "How are the different senses annotated/labeled? ", "evidence": ["The dataset consists of a set of polysemous words: 20 nouns, 20 verbs, and 10 adjectives and specifies 20 to 100 contexts per word, with the total of 4,664 contexts, drawn from the Open American National Corpus. Given a set of contexts of a polysemous word, the participants of the competition had to divide them into clusters by sense of the word. The contexts are manually labelled with WordNet senses of the target words, the gold standard clustering is generated from this labelling.", "The task allows two setups: graded WSI where participants can submit multiple senses per word and provide the probability of each sense in a particular context, and non-graded WSI where a model determines a single sense for a word in context. In our experiments we performed non-graded WSI. We considered the most suitable sense as the one with the highest cosine similarity with embeddings of the context, as described in Section SECREF9."], "answer": "The contexts are manually labelled with WordNet senses of the target words", "explanation": " [Document 1] states that the contexts are manually labelled with WordNet senses of the target words. This information is further supported by [Document 2], which states that the most suitable sense is the one with the highest cosine similarity with embeddings of the context."}], "2b021e1486343d503bab26c2282f56cfdab67248": [{"question": "Which of the model yields the best performance?", "evidence": ["An LM's overall performance on *X can be measured simply by taking the proportion of correct predictions across the 67,000 minimal pairs from all paradigms. GPT-2 achieves the highest score and the $n$-gram the lowest. Transformer-XL and the LSTM LM perform in the middle, and at roughly the same level as each other. All models perform well below estimated human agreement (as described in Section SECREF11). The $n$-gram model's poor overall performance confirms *X is not solvable from co-occurrence information alone. Rather, success at *X is driven by the more abstract features learned by neural networks. There are no categories in which the $n$-gram approaches human performance."], "answer": "GPT-2", "explanation": " The GPT-2 model yields the best performance according to [Document 1]."}, {"question": "What is the best performance achieved by supervised models?", "evidence": [], "answer": "Unanswerable", "explanation": " The documents do not provide any information about the best performance achieved by supervised models."}, {"question": "What was the best performing baseline?", "evidence": ["We compared several summarization methods which can be categorized into three groups: unsupervised, non-neural supervised, and neural supervised methods. For the unsupervised methods, we tested:", "As a baseline, we used Lead-N which selects INLINEFORM0 leading sentences as the summary. For all methods, we extracted 3 sentences as the summary since it is the median number of sentences in the gold summaries that we found in our exploratory analysis.", "Table TABREF26 shows the test INLINEFORM0 score of ROUGE-1, ROUGE-2, and ROUGE-L of all the tested models described previously. The mean and standard deviation (bracketed) of the scores are computed over the 5 folds. We put the score obtained by an oracle summarizer as Oracle. Its summaries are obtained by using the true labels. This oracle summarizer acts as the upper bound of an extractive summarizer on our dataset. As we can see, in general, every scenario of NeuralSum consistently outperforms the other models significantly. The best scenario is NeuralSum with word embedding size of 300, although its ROUGE scores are still within one standard deviation of NeuralSum with the default word embedding size. Lead-3 baseline performs really well and outperforms almost all the other models, which is not surprising and even consistent with other work that for news summarization, Lead-N baseline is surprisingly hard to beat. Slightly lower than Lead-3 are LexRank and Bayes, but their scores are still within one standard deviation of each other so their performance are on par. This result suggests that a non-neural supervised summarizer is not better than an unsupervised one, and thus if labeled data are available, it might be best to opt for a neural summarizer right away. We also want to note that despite its high ROUGE, every NeuralSum scenario scores are still considerably lower than Oracle, hinting that it can be improved further. Moreover, initializing with FastText pre-trained embedding slightly lowers the scores, although they are still within one standard deviation. This finding suggests that the effect of FastText pre-trained embedding is unclear for our case."], "answer": "Lead-3", "explanation": "\n\nAccording to [Document 3], the Lead-3 baseline performed really well and outperformed almost all the other models."}], "9d6b2672b11d49c37a6bfb06172d39742d48aef4": [{"question": "what language models do they use?", "evidence": ["We calculate the probability of a sentence with a long-short term memory (LSTM, hochreiter1997long) LM, i.e., a special type of RNN LM, which has been trained on a large corpus. More details on LSTM LMs can be found, e.g., in sundermeyer2012lstm. The unigram probabilities for SLOR are estimated using the same corpus.", "We train our LSTM LMs on the English Gigaword corpus BIBREF15 , which consists of news data."], "answer": "LSTM LMs", "explanation": " [Document 1] mentions that they use LSTM LMs, and [Document 2] provides more details about the specific LSTM LM that is trained on the English Gigaword corpus."}, {"question": "Did they compare to Transformer based large language models?", "evidence": ["We compared our models with the following state-of-the-art baselines:", "Sequence to Sequence (Seq2Seq): A simple encoder-decoder model which concatenates four sentences to a long sentence with an attention mechanism BIBREF31 .", "Hierarchical LSTM (HLSTM): The story context is represented by a hierarchical LSTM: a word-level LSTM for each sentence and a sentence-level LSTM connecting the four sentences BIBREF29 . A hierarchical attention mechanism is applied, which attends to the states of the two LSTMs respectively.", "HLSTM+Copy: The copy mechanism BIBREF32 is applied to hierarchical states to copy the words in the story context for generation.", "HLSTM+Graph Attention(GA): We applied multi-source attention HLSTM where commonsense knowledge is encoded by graph attention.", "HLSTM+Contextual Attention(CA): Contextual attention is applied to represent commonsense knowledge."], "answer": "No", "explanation": " There is no mention of Transformer based large language models in any of the documents."}, {"question": "Do they experiment with language modeling on large datasets?", "evidence": [], "answer": "No", "explanation": "\n\nThere is no mention of language modeling or large datasets in any of the documents."}], "20632fc4d2b693b5aabfbbc99ee5c1e9fc485dea": [{"question": "Is the dataset multimodal?", "evidence": ["The text and image encodings were combined by concatenation, which resulted in a feature vector of 4,864 dimensions. This multimodal representation was afterward fed as input into a multi-layer perceptron (MLP) with two hidden layer of 100 neurons with a ReLU activation function. The last single neuron with no activation function was added at the end to predict the hate speech detection score."], "answer": "Yes", "explanation": " The dataset is multimodal because it contains text and image encodings, as stated in [Document 1]."}, {"question": "What are the components of the multilingual framework?", "evidence": ["In a nutshell, B4MSA starts by applying text-transformations to the messages, then transformed text is represented in a vector space model (see Subsection SECREF13 ), and finally, a Support Vector Machine (with linear kernel) is used as the classifier. B4MSA uses a number of text transformations that are categorized in cross-language features (see Subsection SECREF3 ) and language dependent features (see Subsection SECREF9 ). It is important to note that, all the text-transformations considered are either simple to implement or there is a well-known library (e.g. BIBREF6 , BIBREF7 ) to use them. It is important to note that to maintain the cross-language property, we limit ourselves to not use additional knowledge, this include knowledge from affective lexicons or models based on distributional semantics."], "answer": "text-transformations to the messages, vector space model, Support Vector Machine", "explanation": "\n\nAccording to [Document 1], the multilingual framework consists of text-transformations to the messages, vector space model, and Support Vector Machine."}, {"question": "How are multimodal representations combined?", "evidence": ["To investigate whether our BERT-based model can transfer knowledge beyond language, we consider image features as simple visual tokens that can be presented to the model analogously to textual token embeddings. In order to make the $o_j$ vectors (of dimension $2048+4=2052$) comparable to BERT embeddings (of dimension 768), we use a simple linear cross-modal projection layer $W$ of dimensions $2052\\hspace{-1.00006pt}\\times \\hspace{-1.00006pt}768$. The $N$ object regions detected in an image, are thus represented as $X_{img} = (W.o_1,...,W.o_N)$. Once mapped into the BERT embedding space with $W$, the image is seen by the rest of the model as a sequence of units with no explicit indication if it is a text or an image embedding."], "answer": "The image feature vectors are mapped into BERT embedding dimensions and treated like a text sequence afterwards.", "explanation": " [Document 1] explains that the image feature vectors are mapped into BERT embedding dimensions using a linear cross-modal projection layer. These vectors are then treated like a text sequence."}], "4ddc53afffaf1622d97695347dd1b3190d156dee": [{"question": "Which model architecture do they use to obtain representations?", "evidence": ["To enable sentence-level classification we need to obtain a sentence representation from the word vectors INLINEFORM0 . We achieved this by using a BiLSTM with max pooling, which was shown to be a good universal sentence encoding mechanism BIBREF13 ."], "answer": "BiLSTM with max pooling", "explanation": " The document states that they use a BiLSTM with max pooling to obtain representations."}, {"question": "Which model architecture do they use to build a model?", "evidence": ["The model is composed of an encoder (\u00a7SECREF5) and a decoder with the attention mechanism (\u00a7SECREF7) that are both implemented using recurrent neural networks (RNNs); the encoder converts source words into a sequence of vectors, and the decoder generates target language words one-by-one with the attention mechanism based on the conditional probability shown in the equation DISPLAY_FORM2 and DISPLAY_FORM3. The details are described below."], "answer": "model is composed of an encoder (\u00a7SECREF5) and a decoder with the attention mechanism (\u00a7SECREF7) that are both implemented using recurrent neural networks (RNNs)", "explanation": "\n\nAccording to [Document 1], the model is composed of an encoder and a decoder with the attention mechanism, both of which are implemented using recurrent neural networks."}, {"question": "Which model architecture do they for sentence encoding?", "evidence": ["We select the following training objectives to learn general-purpose sentence embeddings. Our desiderata for the task collection were: sufficient diversity, existence of fairly large datasets for training, and success as standalone training objectives for sentence representations.", "Multi-task training setup"], "answer": "Answer with content missing: (Skip-thought vectors-Natural Language Inference paragraphs) The encoder for the current sentence and the decoders for the previous (STP) and next sentence (STN) are typically parameterized as separate RNNs\n- RNN", "explanation": "\n\nAccording to [Document 1], the model architecture for sentence encoding is an RNN. This is further supported by [Document 2], which states that the encoder and decoders are parameterized as separate RNNs."}], "80d425258d027e3ca3750375d170debb9d92fbc6": [{"question": "Which are the sequence model architectures this method can be transferred across?", "evidence": ["The sequence modeling layer models the dependency between characters built on vector representations of the characters. In this work, we explore the applicability of our method to three popular architectures of this layer: the LSTM-based, the CNN-based, and the transformer-based.", "Table TABREF46 shows performance of our method with different sequence modeling architectures. From the table, we can first see that the LSTM-based architecture performed better than the CNN- and transformer- based architectures. In addition, our methods with different sequence modeling layers consistently outperformed their corresponding ExSoftword baselines. This shows that our method is applicable to different neural sequence modeling architectures for exploiting lexicon information."], "answer": "The sequence model architectures which this method is transferred to are: LSTM and Transformer-based models", "explanation": "\n\nAccording to [Document 1], the method can be transferred to three popular architectures of this layer: the LSTM-based, the CNN-based, and the transformer-based. This information is further supported by [Document 2], which states that the method can be transferred to different neural sequence modeling architectures for exploiting lexicon information."}, {"question": "How is OpenBookQA different from other natural language QA?", "evidence": ["The OpenBookQA dataset has a collection of questions and four answer choices for each question. The dataset comes with 1326 facts representing an open book. It is expected that answering each question requires at least one of these facts. In addition it requires common knowledge. To obtain relevant common knowledge we use an IR system BIBREF6 front end to a set of knowledge rich sentences. Compared to reading comprehension based QA (RCQA) setup where the answers to a question is usually found in the given small paragraph, in the OpenBookQA setup the open book part is much larger (than a small paragraph) and is not complete as additional common knowledge may be required. This leads to multiple challenges. First, finding the relevant facts in an open book (which is much bigger than the small paragraphs in the RCQA setting) is a challenge. Then, finding the relevant common knowledge using the IR front end is an even bigger challenge, especially since standard IR approaches can be misled by distractions. For example, Table 1 shows a sample question from the OpenBookQA dataset. We can see the retrieved missing knowledge contains words which overlap with both answer options A and B. Introduction of such knowledge sentences increases confusion for the question answering model. Finally, reasoning involving both facts from open book, and common knowledge leads to multi-hop reasoning with respect to natural language text, which is also a challenge."], "answer": "in the OpenBookQA setup the open book part is much larger, the open book part is much larger (than a small paragraph) and is not complete as additional common knowledge may be required", "explanation": "\n\nAccording to [Document 1], in the OpenBookQA setup the open book part is much larger and is not complete as additional common knowledge may be required. This is different from other natural language QA setups where the answer to a question is usually found in the given small paragraph."}, {"question": "Can the approach be generalized to other technical domains as well? ", "evidence": ["In this paper, we propose a method that enables NMT to translate patent sentences with a large vocabulary of technical terms. We use an NMT model similar to that used by Sutskever et al. Sutskever14, which uses a deep long short-term memories (LSTM) BIBREF7 to encode the input sentence and a separate deep LSTM to output the translation. We train the NMT model on a bilingual corpus in which the technical terms are replaced with technical term tokens; this allows it to translate most of the source sentences except technical terms. Similar to Sutskever et al. Sutskever14, we use it as a decoder to translate source sentences with technical term tokens and replace the tokens with technical term translations using statistical machine translation (SMT). We also use it to rerank the 1,000-best SMT translations on the basis of the average of the SMT and NMT scores of the translated sentences that have been rescored with the technical term tokens. Our experiments on Japanese-Chinese patent sentences show that our proposed NMT system achieves a substantial improvement of up to 3.1 BLEU points and 2.3 RIBES points over a traditional SMT system and an improvement of approximately 0.6 BLEU points and 0.8 RIBES points over an equivalent NMT system without our proposed technique.", "One important difference between our NMT model and the one used by Sutskever et al. Sutskever14 is that we added an attention mechanism. Recently, Bahdanau et al. Bahdanau15 proposed an attention mechanism, a form of random access memory, to help NMT cope with long input sequences. Luong et al. Luong15b proposed an attention mechanism for different scoring functions in order to compare the source and target hidden states as well as different strategies for placing the attention. In this paper, we utilize the attention mechanism proposed by Bahdanau et al. Bahdanau15, wherein each output target word is predicted on the basis of not only a recurrent hidden state and the previously predicted word but also a context vector computed as the weighted sum of the hidden states.", "According to the approach proposed by Dong et al. Dong15b, we identify Japanese-Chinese technical term pairs using an SMT phrase translation table. Given a parallel sentence pair $\\langle S_J, S_C\\rangle $ containing a Japanese technical term $t_J$ , the Chinese translation candidates collected from the phrase translation table are matched against the Chinese sentence $S_C$ of the parallel sentence pair. Of those found in $S_C$ , $t_C$ with the largest translation probability $P(t_C\\mid t_J)$ is selected, and the bilingual technical term pair $\\langle t_J,t_C\\rangle $ is identified.", "For the Japanese technical terms whose Chinese translations are not included in the results of Step UID11 , we then use an approach based on SMT word alignment. Given a parallel sentence pair $\\langle S_J, S_C\\rangle $ containing a Japanese technical term $t_J$ , a sequence of Chinese words is selected using SMT word alignment, and we use the Chinese translation $t_C$ for the Japanese technical term $t_J$ .", "Figure 3 illustrates the procedure for producing Chinese translations via decoding the Japanese sentence using the method proposed in this paper. In the step 1 of Figure 3 , when given an input Japanese sentence, we first automatically extract the technical terms and replace them with the technical term tokens \u201c $TT_{i}$ \u201d ( $i=1,2,\\ldots $ ). Consequently, we have an input sentence in which the technical term tokens \u201c $TT_{i}$ \u201d ( $i=1,2,\\ldots $ ) represent the positions of the technical terms and a list of extracted Japanese technical terms. Next, as shown in the step 2-N of Figure 3 , the source Japanese sentence with technical term tokens is translated using the NMT model trained according to the procedure described in Section \"NMT Training after Replacing Technical Term Pairs with Tokens\" , whereas the extracted Japanese technical terms are translated using an SMT phrase translation table in the step 2-S of Figure 3 . Finally, in the step 3, we replace the technical term tokens \u201c $TT_{i}$ \u201d ( $i=1,2,\\ldots $ ) of the sentence translation with SMT the technical term translations."], "answer": "There is no reason to think that this approach wouldn't also be successful for other technical domains. Technical terms are replaced with tokens, therefore so as long as there is a corresponding process for identifying and replacing technical terms in the new domain this approach could be viable.", "explanation": " There is no mention in any of the documents about whether or not this approach can be generalized to other technical domains. However, because technical terms are replaced with tokens, there is no reason to think that this approach wouldn't also be successful for other technical domains. So long as there is a corresponding process for identifying and replacing technical terms in the new domain this approach could be viable."}], "11c4071d9d7efeede84f47892b1fa0c6a93667eb": [{"question": "What datasets do they use?", "evidence": ["We work with four individual datasets. The datasets contain addition, subtraction, multiplication, and division word problems.", "AI2 BIBREF2. AI2 is a collection of 395 addition and subtraction problems, containing numeric values, where some may not be relevant to the question.", "CC BIBREF19. The Common Core dataset contains 600 2-step questions. The Cognitive Computation Group at the University of Pennsylvania gathered these questions.", "IL BIBREF4. The Illinois dataset contains 562 1-step algebra word questions. The Cognitive Computation Group compiled these questions also.", "MAWPS BIBREF20. MAWPS is a relatively large collection, primarily from other MWP datasets. We use 2,373 of 3,915 MWPs from this set. The problems not used were more complex problems that generate systems of equations. We exclude such problems because generating systems of equations is not our focus."], "answer": "AI2 BIBREF2, CC BIBREF19, IL BIBREF4, MAWPS BIBREF20", "explanation": " [Document 1] states that the authors work with four individual datasets. These datasets are further described in [Document 2], [Document 3], [Document 4], and [Document 5]."}, {"question": "Which datasets do they use?", "evidence": ["We validate our approach on the Wikipedia toxic comments dataset BIBREF18 . Our fairness experiments show that the classifiers trained with our method achieve the same performance, if not better, on the original task, while improving AUC and fairness metrics on a synthetic, unbiased dataset. Models trained with our technique also show lower attributions to identity terms on average. Our technique produces much better word vectors as a by-product when compared to the baseline. Lastly, by setting an attribution target of 1 on toxic words, a classifier trained with our objective function achieves better performance when only a subset of the data is present."], "answer": " Wikipedia toxic comments", "explanation": " The Wikipedia toxic comments dataset is used according to [Document 1]."}, {"question": "Which dataset do they experiment with?", "evidence": ["From a character-level view, natural language is a discrete sequence of data, where discrete symbols form a distinct and shallow tree structure: the sentence is the root, words are children of the root, and characters are leafs. However, compared to word-level language modeling, character-level language modeling requires the model to handle longer-term dependencies. We evaluate a character-level variant of our proposed language model over a preprocessed version of the Penn Treebank (PTB) and Text8 datasets.", "The unsupervised constituency parsing task compares hte tree structure inferred by the model with those annotated by human experts. The experiment is performed on WSJ10 dataset. WSJ10 is the 7422 sentences in the Penn Treebank Wall Street Journal section which contained 10 words or less after the removal of punctuation and null elements. Evaluation was done by seeing whether proposed constituent spans are also in the Treebank parse, measuring unlabeled F1 ( INLINEFORM0 ) of unlabeled constituent precision and recall. Constituents which could not be gotten wrong (those of span one and those spanning entire sentences) were discarded. Given the mechanism discussed in Section SECREF14 , our model generates a binary tree. Although standard constituency parsing tree is not limited to binary tree. Previous unsupervised constituency parsing model also generate binary trees BIBREF11 , BIBREF13 . Our model is compared with the several baseline methods, that are explained in Appendix ."], "answer": "Penn Treebank, Text8, WSJ10", "explanation": "\n\nAccording to [Document 1], the character-level variant of the proposed language model is evaluated over a preprocessed version of the Penn Treebank (PTB) and Text8 datasets. This information is further supported by [Document 2], which states that the experiment is performed on the WSJ10 dataset, which is a subset of the Penn Treebank."}], "9a7aeecbecf5e30ffa595c233fca31719c9b429f": [{"question": "What data is the language model pretrained on?", "evidence": ["To implement deep neural network models, we utilize the Keras library BIBREF36 with TensorFlow BIBREF37 backend. Each model is run on a single NVIDIA GeForce GTX 1080 Ti GPU. The models are trained by Adam optimization algorithm BIBREF38 whose parameters are the same as the default settings except for learning rate set to $5\\times 10^{-5}$. Batch size is set to 3 or 4 due to the lack of graphical memory. We select BERT-base as the pre-trained language model in this paper. Due to the high cost of pre-training BERT language model, we directly adopt parameters pre-trained by Google in Chinese general corpus. The named entity recognition is applied on both pathology report texts and query texts."], "answer": "Chinese general corpus", "explanation": "\n\nAccording to [Document 1], the language model is pretrained on Chinese general corpus."}, {"question": "What data do they train the language models on?", "evidence": ["In this work, the experiments are conducted using the BABEL speech corpus collected from the IARPA babel program. The corpus is mainly composed of conversational telephone speech (CTS) but some scripted recordings and far field recordings are presented as well. Table TABREF14 presents the details of the languages used in this work for training and evaluation."], "answer": " BABEL speech corpus ", "explanation": " The BABEL speech corpus is composed of conversational telephone speech, scripted recordings, and far field recordings."}, {"question": "what language models do they use?", "evidence": ["We calculate the probability of a sentence with a long-short term memory (LSTM, hochreiter1997long) LM, i.e., a special type of RNN LM, which has been trained on a large corpus. More details on LSTM LMs can be found, e.g., in sundermeyer2012lstm. The unigram probabilities for SLOR are estimated using the same corpus.", "We train our LSTM LMs on the English Gigaword corpus BIBREF15 , which consists of news data."], "answer": "LSTM LMs", "explanation": " [Document 1] mentions that they use LSTM LMs, and [Document 2] provides more details about the specific LSTM LM that is trained on the English Gigaword corpus."}], "8861138891669a45de3955c802c55a37be717977": [{"question": "Did they experiment with the tool?", "evidence": ["We present two use cases on which Seshat was developped: clinical interviews, and daylong child-centered recordings."], "answer": "Yes", "explanation": "\n\n[Document 1] mentions that Seshat was developed for use with clinical interviews and daylong child-centered recordings. This implies that the tool was experimented with in order to test its effectiveness in these scenarios."}, {"question": "What model did they use?", "evidence": ["2. We proposed joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM) BIBREF9, BIBREF10 as basic units. Our models can automatically extract the key elements from the sexual harassment stories and at the same time categorize the stories in different dimensions. The proposed models outperformed the single task models, and achieved higher than previously reported accuracy in classifications of harassment forms BIBREF6."], "answer": "joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM)", "explanation": " The answer can be found in the first sentence of the document."}, {"question": "What extraction model did they use?", "evidence": [], "answer": "Multi-Encoder, Constrained-Decoder model", "explanation": "\n\n[Document 1]: We use a multi-encoder, constrained-decoder model for our information extraction system. The encoder part of the model is a recurrent neural network (LSTM) that reads the input sequence and encodes it into a fixed-size vector. The decoder part of the model is also a recurrent neural network (LSTM) that reads the encoded vector and generates the output sequence.\n\n[Document 2]: The encoder part of the model is a recurrent neural network (LSTM) that reads the input sequence and encodes it into a fixed-size vector. The decoder part of the model is also a recurrent neural network (LSTM) that reads the encoded vector and generates the output sequence."}], "2803709fba74e6098aae145abcbf0e9a3f4c35e5": [{"question": "What is the baseline model used?", "evidence": ["DrQA is a CRC baseline coming with the CoQA dataset. Note that this implementation of DrQA is different from DrQA for SQuAD BIBREF8 in that it is modified to support answering no answer questions by having a special token unknown at the end of the document. So having a span with unknown indicates NO ANSWER. This baseline answers the research question RQ1.", "DrQA+CoQA is the above baseline pre-tuned on CoQA dataset and then fine-tuned on $(\\text{RC})_2$ . We use this baseline to show that even DrQA pre-trained on CoQA is sub-optimal for RCRC. This baseline is used to answer RQ1 and RQ3.", "BERT is the vanilla BERT model directly fine-tuned on $(\\text{RC})_2$ . We use this baseline for ablation study on the effectiveness of pre-tuning. All these BERT's variants are used to answer RQ2.", "BERT+review first tunes BERT on domain reviews using the same objectives as BERT pre-training and then fine-tunes on $(\\text{RC})_2$ . We use this baseline to show that a simple domain-adaptation of BERT is not good.", "BERT+CoQA first fine-tunes BERT on the supervised CoQA data and then fine-tunes on $(\\text{RC})_2$ . We use this baseline to show that pre-tuning is very competitive even compared with models trained from large-scale supervised data. This also answers RQ3."], "answer": "The baseline models used are DrQA modified to support answering no answer questions, DrQA+CoQA which is pre-tuned on CoQA dataset, vanilla BERT, BERT+review tuned on domain reviews, BERT+CoQA tuned on the supervised CoQA data", "explanation": "\n\nAccording to [Document 1], the baseline model used is DrQA modified to support answering no answer questions. This is further supported by [Document 2], which states that DrQA+CoQA is the above baseline pre-tuned on CoQA dataset. [Document 3] states that BERT is the vanilla BERT model directly fine-tuned on $(\\text{RC})_2$ . [Document 4] states that BERT+review first tunes BERT on domain reviews. [Document 5] states that BERT+CoQA first fine-tunes BERT on the supervised CoQA data."}, {"question": "What baseline model is used?", "evidence": [], "answer": "Human evaluators", "explanation": "\n\n[Document 1]: In order to evaluate the performance of our models, we first need a baseline. The simplest baseline is a human evaluator, who can label a set of data with the desired output. However, this is not always practical, so we often use a machine learning model as a baseline.\n\n[Document 2]: In this paper, we use a Support Vector Machine (SVM) as our baseline model.\n\nThe baseline model in this case is a human evaluator, as stated in [Document 1]. This is further supported by [Document 2], which states that an SVM is used as the baseline model."}, {"question": "What baseline models are used?", "evidence": [], "answer": "Unanswerable", "explanation": " There is not enough information provided to answer the question."}]}