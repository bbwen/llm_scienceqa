{"question_id": "f6346828c2f44529dc307abf04dd246bfeb4a9b2", "question": "Do they evaluate which compression method yields the most gains?", "predicted_answer": " no</s>", "predicted_evidence": ["In this paper, we demonstrate that a combination of matrix factorization and weight pruning is better than single solutions for BERT-oriented model compression. Similar phenomena has been reported in the computer vision scenarios BIBREF28, which shows that low-rank and sparsity are complementary to each other. Here we provide another explanation to support this observation."], "explanation": " no</s>"}
{"question_id": "c5221bb28e58a4f13cf2eccce0e1b1bec7dd3c13", "question": "Is the data specific to a domain?", "predicted_answer": " yes</s>", "predicted_evidence": ["The Semantic Scholar corpus BIBREF21 (SemScholar) is a large dataset of scientific publications made available by AI2. From its 39M articles, we downloaded 3,3M PDFs (the rest were behind paywalls, did not have a link or it was broken) and extracted 12.5M figures and captions through PDFFigures2 BIBREF22. We randomly selected 500K papers to train the FCC task on their figures and captions and another 500K to train Vecsigrafo on the text of their titles and abstracts.", "Springer Nature's SciGraph contains 7M scientific publications organized in 22 scientific fields or categories. Since SciGraph does not provide a link to the PDF of the publication, we selected the intersection with SemScholar, producing a smaller corpus of 80K papers (in addition to the 1M papers from SemScholar mentioned above) and 82K figures that we used for training certain FCC configurations and supervised baselines (section SECREF14).", "The Textbook Question Answering corpus BIBREF23 includes 1,076 lessons and 26,260 multi-modal test questions from middle school science curricula. Its complexity and scope make it a challenging textual and visual question answering dataset.", "Wikipedia. We used the January 2018 English Wikipedia dataset as one of the corpora on which to train Vecsigrafo. As opposed to SciGraph or SemScholar, specific of the scientific domain, Wikipedia is a source of general-purpose information."], "explanation": " yes</s>"}
{"question_id": "10edfb9428b8a4652274c13962917662fdf84f8a", "question": "How long is the dataset?", "predicted_answer": " 808, 265, and 264</s>", "predicted_evidence": ["For training our classification models, we use a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, provided by BIBREF23. These repositories are split into training, validation, and test splits containing 808, 265, and 264 commits, respectively. In order to minimize the occurrence of duplicate commits in two of these splits (such as in both training and test), commits from no repository belong to more than one split. However, 808 commits may not be sufficient to train deep learning models. Hence, in order to answer RQ4, we augment the training split with commits mined using regular expression matching on the commit messages from the same set of open-source Java projects. This almost doubles the number of commits in the training split to 1493. We then repeat our experiments for the first three research questions on the augmented dataset, and evaluate our trained models on the same validation and test splits."], "explanation": " 808, 265, and 264</s>"}
{"question_id": "389cc454ac97609e9d0f2b2fe70bf43218dd8ba7", "question": "How do they use Wikipedia to automatically collect a query-focused summarization dataset?", "predicted_answer": " the statements' citations in wikipedia articles as pivots to align the queries and", "predicted_evidence": ["In order to advance query-focused summarization with limited data, we improve the summarization model with data augmentation. Specifically, we transform Wikipedia into a large-scale query-focused summarization dataset (named as WikiRef). To automatically construct query-focused summarization examples using Wikipedia, the statements' citations in Wikipedia articles as pivots to align the queries and documents. Figure FIGREF1 shows an example that is constructed by the proposed method. We first take the highlighted statement as the summary. Its supporting citation is expected to provide an adequate context to derive the statement, thus can serve as the source document. On the other hand, the section titles give a hint about which aspect of the document is the summary's focus. Therefore, we use the article title and the section titles of the statement to form the query. Given that Wikipedia is the largest online encyclopedia, we can automatically construct massive query-focused summarization examples."], "explanation": " the statements' citations in wikipedia articles as pivots to align the queries and"}
{"question_id": "dfaeb8faf04505a4178945c933ba217e472979d8", "question": "what is the source of their dataset?", "predicted_answer": " google news.</s>", "predicted_evidence": ["The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 ."], "explanation": " google news.</s>"}
{"question_id": "0dc9050c832a6091bc9db3f7fa7be72139f51177", "question": "do language share categories? ", "predicted_answer": " yes</s>", "predicted_evidence": ["We present BCF, a cognitively motivated Bayesian model for learning Categories and structured Features from large sets of concept mentions and their linguistic contexts (see Figure 1 ). Our model induces categories (as groups of concepts), feature types which are shared across categories (as groups of features or context words), and category-feature type associations. Figure 2 shows example output of BCF as learnt from the English Wikipedia, and Figure 21 shows example categories and features learnt for five additional languages."], "explanation": " yes</s>"}
{"question_id": "e9a0a69eacd554141f56b60ab2d1912cc33f526a", "question": "what was the baseline?", "predicted_answer": " randomly assigning any of the sentiment values to each of the test instances</s>", "predicted_evidence": ["The baseline systems are developed by randomly assigning any of the sentiment values to each of the test instances. Then, similar evaluation techniques are applied to the baseline system and the results are presented in Table TABREF29 ."], "explanation": " randomly assigning any of the sentiment values to each of the test instances</s>"}
{"question_id": "b584739622d0c53830e60430b13fd3ae6ff43669", "question": "What are the evaluation metrics and criteria used to evaluate the model performance?", "predicted_answer": " automatic evaluation metrics [document 2]: perplexity since the", "predicted_evidence": ["Experiments ::: Automatic Evaluation Metrics", "Perplexity Since the canonical measure of a good language model is perplexity, which indicates the error rate of the expected word. We choose perplexity to evaluate the model performance.", "Response-Intent Prediction (RIP) $\\&$ Response-Slot Prediction (RSP) Different from open-domain dialog systems, we care about the intents of the system response in non-collaborative tasks as we hope to know if the system response satisfies user intents. For example, in the anti-scam task, if the attacker elicits information from the system, we need to know if the system refuses or agrees to provide the information. Therefore we care about intent prediction for the generated system response. Since our baselines are more suited for social chat as they cannot produce system intents, we use the system intent and slot classifiers trained in our model to predict their responses' intents and slots. The intent predictor achieves a $84\\%$ accuracy and the semantic slot predictor achieves $77\\%$ on the AntiScam dataset. Then we compare the predicted values with human-annotated ground truth in the dataset to compute the response-intent prediction (RIP) and response-slot prediction (RSP).", "Extended Response-Intent Prediction (ERIP) $\\&$ Extended Response-Slot Prediction (ERSP) With Response-Intent Prediction, we verify the predicted intents to evaluate the coherence of the dialog. However, the real mapping between human-intent and system-intent is much more complicated as there might be multiple acceptable system-intents for the same human-intent. Therefore, we also design a metric to evaluate if the predicted system-intent is in the set of acceptable intents. Specifically, we estimate the transition probability $p(I_i|I_j)$ by counting the frequency of all the bi-gram human-intent and system-intent pairs in the training data. During the test stage, if the predicted intent matches the ground truth, we set the score as 1, otherwise we set the score as $p(I_{predict}|I_i)$ where $I_i$ is the intent of the input human utterance. We then report the average value of those scores over turns as the final extended response-intent prediction result.", "Automatic metrics only validate the system\u2019s performance on a single dimension at a time. The ultimate holistic evaluation should be conducted by having the trained system interact with human users. Therefore we also conduct human evaluations for the dialog system built on AntiScam. We test our models and baselines with 15 college-student volunteers. Each of them is asked to pretend to be an attacker and interact with all the models for at least three times to avoid randomness. We in total collect 225 number of dialogs. Each time, volunteers are required to use similar sentences and strategies to interact with all five models and score each model based on the metrics listed below at the end of the current round. Each model receives a total of 45 human ratings, and the average score is reported as the final human-evaluation score. In total, we design five different metrics to assess the models' conversational ability whilst interacting with humans. The results are shown in Table TABREF19.", "Fluency Fluency is used to explore different models' language generation quality.", "Coherence Different from single sentence's fluency, coherence focuses more on the logical consistency between sentences in each turn.", "Engagement In the anti-scam scenario, one of our missions is to keep engaging with the attackers to waste their time. So we directly ask volunteers (attackers) to what extend they would like to continue chatting with the system.", "Dialog length (Length) Engagement is a subjective metric. Anti-scam system's goal is to engage user in the conversation longer in order to limit their harm to other potential victims. So we count the dialog length as another metric to evaluate system performance.", "Task Success Score (TaskSuc) The other goal of the anti-scam system is to elicit attacker's personal information. We count the average type of information (name, address and phone number) that the system obtained from attackers as the task success score."], "explanation": " automatic evaluation metrics [document 2]: perplexity since the"}
{"question_id": "f13a5b6a67a9b10fde68e8b33792879b8146102c", "question": "what lexical features are extracted?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["Usually, INLINEFORM0 is a INLINEFORM1 weight matrix and INLINEFORM2 is the transposition. Since the answer in our task is a simple \u201cyes\u201d or \u201cno\u201d, we use a INLINEFORM3 matrix for INLINEFORM4 . As the distance between a clause and an emotion words is a very important feature according to BIBREF31 , we simply add this distance into the softmax function as an additional feature in our work."], "explanation": " unanswerable</s>"}
{"question_id": "6657ece018b1455035421b822ea2d7961557c645", "question": "What NMT techniques did they explore?", "predicted_answer": " convs2s and transformer</s>", "predicted_evidence": ["We trained translation models for two established NMT architectures for each language, namely, ConvS2S and Transformer. As the purpose of this work is to provide a baseline benchmark, we have not performed significant hyperparameter optimization, and have left that as future work."], "explanation": " convs2s and transformer</s>"}
{"question_id": "5e29f16d7302f24ab93b7707d115f4265a0d14b0", "question": "How much additional data do they manage to generate from translations?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["FLOAT SELECTED: Figure 1: The pipeline of proposed method. \u201cSMT\u201d and \u201cDRP\u201d denote statistical machine translation and discourse relation parser respectively."], "explanation": " unanswerable</s>"}
{"question_id": "6cb3007a09ab0f1602cdad20cc0437fbdd4d7f3e", "question": "What models do they experiment on?", "predicted_answer": " [document 1]: every classical model was considered on the condition it could", "predicted_evidence": ["Every classical model was considered on the condition it could take matrices as input for fitting and was trained with the default settings because of the size of the dataset. Five models were trained: Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), both with a value of 1 in the penalty parameter C of the error term. The gamma value of the RBF SVM which indicates how much influence a single training example has, was set to 2. The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning. The gradient of the loss is estimated each sample at a time and the SGDC is updated along the way with a decreasing learning rate. The parameters for maximum epochs and the stopping criterion were defined using the default values in scikit-learn. The final classifier was two models based on the Bayes theorem: Multinomial Na\u00efve Bayes, which works with occurrence counts, and Bernoulli Na\u00efve Bayes, which is designed for binary features.", "Six different deep learning models were considered. All of these models have been used in an aggression detection task. The models are Pooled GRU BIBREF25, Stacked LSTM with Attention BIBREF25, LSTM and GRU with Attention BIBREF25, 2D Convolution with Pooling BIBREF26, GRU with Capsule BIBREF27, LSTM with Capsule and Attention BIBREF26 and BERT BIBREF24. These models has been used in HASOC 2019 and achieved a third place finish in English task and a eighth place finish in German and Hindi subtasks BIBREF26. Parameters described in BIBREF26 were used as the default parameters in order to ease the training process. The code for the deep learning has been made available on Github ."], "explanation": " [document 1]: every classical model was considered on the condition it could"}
{"question_id": "fb06ed5cf9f04ff2039298af33384ca71ddbb461", "question": "How much data do they manage to gather online?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["Subsequently, we extracted parallel sentences using the trained model, and parallel articles collected from Wikipedia. There were INLINEFORM0 bilingual English-Tamil and INLINEFORM1 English-Hindi titles on the Wikimedia dumps collected in December 2017."], "explanation": " unanswerable</s>"}
{"question_id": "a064d01d45a33814947161ff208abb88d4353b26", "question": "what are the existing annotation tools?", "predicted_answer": " bibref2, bibref3, bibre", "predicted_evidence": ["Existing annotation tools BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 mainly focus on providing a visual interface for user annotation process but rarely consider the post-annotation quality analysis, which is necessary due to the inter-annotator disagreement. In addition to the annotation quality, efficiency is also critical in large-scale annotation task, while being relatively less addressed in existing annotation tools BIBREF6 , BIBREF7 . Besides, many tools BIBREF6 , BIBREF4 require a complex system configuration on either local device or server, which is not friendly to new users."], "explanation": " bibref2, bibref3, bibre"}
{"question_id": "e56c1f0e9eabda41f929d0dfd5cfa50edd69fa89", "question": "Were any datasets other than WMT used to test the model?", "predicted_answer": " yes</s>", "predicted_evidence": ["The evaluation is on two widely used public machine translation datasets: IWSLT14 German-to-English (De-En) BIBREF9, BIBREF1 and WMT14 English-to-German (En-De) dataset BIBREF4, BIBREF10. To compare with previous works, we also reverse WMT14 English-to-German dataset and obtain WMT14 German-to-English dataset."], "explanation": " yes</s>"}
{"question_id": "e14e3e0944ec3290d1985e9a3da82a7df17575cd", "question": "Which dataset do they evaluate on?", "predicted_answer": " a large scale chinese conversation corpus comprised of 20 million context-res", "predicted_evidence": ["Our experiments are conducted on a large scale Chinese conversation corpus comprised of 20 million context-response pairs. We compare our model with generative models and retrieval models in terms of fluency, relevance, diversity and originality. The experiments show that our method outperforms traditional generative models on relevance, diversity and originality. We further find that the revised response achieves better relevance compared to its prototype and other retrieval results, demonstrating that the editing process does not only promote response originality but also improve the relevance of retrieval results."], "explanation": " a large scale chinese conversation corpus comprised of 20 million context-res"}
{"question_id": "18fbfb1f88c5487f739aceffd23210a7d4057145", "question": "what models did they compare with?", "predicted_answer": " bimpm, densenet, highwaylstm</s>", "predicted_evidence": ["In the sequence labeling task,we explore sub-task named entity recognition using CoNLL03 dataset BIBREF5 , which is a public available used in many studies to test the accuracy of their proposed methods BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF1 . For strategy finetune-only and strategy stack-and-finetune, we implemented two models: one with BERT and the other with BERT adding a Bi-LSTM on top. Eval measure is accuracy and F1 score.", "In the task of text categorization, we used Yahoo Answer Classification Dataset. The Dataset is consists of 10 classes, but due to the huge amount of the dataset, we just select two class of them. As for the upper model,we choose DenseNet BIBREF33 and HighwayLSTM BIBREF34 .", "Apart from the BERT fine-tuning only model and BERT+ BIMPM model, we also devise two new network structures by modifying the BIMPM model. In the first model is to remove the first bi-LSTM of BIMPM, which is the input layer for the matching layer in BIMPM. In the second model, we combine the matching layer of BIMPM and with a transformer BIBREF16 , a model we call Sim-Transformer by replacing the output layer of the matching layer, originally a bi-LSTM model, with a transformer model. From the experimental results shown in Table 4, we can see that due to the strong expressive ability of the BERT, there is almost no difference in the experimental results of removing the first bi-LSTM and BIMPM. In addition, we also find that Sim-Transformer's performance without fine-tuning is nearly four percentage points lower than BIMPM, but it out-performs BIMPM after fine-tuning. In general, the results show that BERT + Sim-Transformer out-performs BERT-only model by 4.7%, thus confirming our hypotheses again."], "explanation": " bimpm, densenet, highwaylstm</s>"}
{"question_id": "c4c06f36454fbfdc5d218fb84ce74eaf7f78fc98", "question": "What is the performance for the three languages tested?", "predicted_answer": " german: <unk> <unk> <unk> <unk> <unk> - <unk>", "predicted_evidence": ["Table TABREF12 shows the performance of our cross-lingual models in German, Italian and Dutch. We summarize the results as follows:", "FLOAT SELECTED: Table 1: Results of zero-shot cross-lingual semantic parsing for models trained in English and tested in German, Italian and Dutch.2"], "explanation": " german: <unk> <unk> <unk> <unk> <unk> - <unk> "}
{"question_id": "6d8a51e2790043497ed2637a1abc36bdffb39b71", "question": "What datasets are used for experiments?", "predicted_answer": " alime and quora dataset</s>", "predicted_evidence": ["We conduct experiments on the AliMe and Quora dataset for the query-bag matching based information-seeking conversation. Compared with baselines, we verify the effectiveness of our model. Our model obtains 0.05 and 0.03 $\\text{R}_{10}@1$ gains comparing to the strongest baseline in the two datasets. The ablation study shows the usefulness of the components. The contributions in this paper are summarized as follows: 1) To the best of our knowledge, we are the first to adopt query-bag matching in the information-seeking conversation. 2) We propose the mutual coverage model to measure the information coverage in the query-bag matching. 3) We release the composite Quora dataset to facilitate the research in this area."], "explanation": " alime and quora dataset</s>"}
{"question_id": "4ae0b50c88a174cfc283b90cd3c9407de13fd370", "question": "Do they use pretrained word embeddings?", "predicted_answer": " no</s>", "predicted_evidence": ["In this effort, we develop our supervised WSD model that leverages a Bidirectional Long Short-Term Memory (BLSTM) network. This network works with neural sense vectors (i.e. sense embeddings), which are learned during model training, and employs neural word vectors (i.e. word embeddings), which are learned through an unsupervised deep learning approach called GloVe (Global Vectors for word representation) BIBREF2 for the context words. By evaluating our one-model-fits-all WSD network over the public gold standard dataset of SensEval-3 BIBREF3 , we demonstrate that the accuracy of our model in terms of F-measure is comparable with the state-of-the-art WSD algorithms'."], "explanation": " no</s>"}
{"question_id": "61a2599acfbd3d75de58e97ecdba2d9cf0978324", "question": "What is the strong rivaling system?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["The previous work that is most relevant to our work here is that of BIBREF9, who developed the ClaimBuster system, which assigns each sentence in a document a score, i.e., a number between 0 and 1 showing how worthy it is of fact-checking. The system is trained on their own dataset of about eight thousand debate sentences (1,673 of them check-worthy), annotated by students, university professors, and journalists. Unfortunately, this dataset is not publicly available and it contains sentences without context as about 60% of the original sentences had to be thrown away due to lack of agreement."], "explanation": " unanswerable</s>"}
{"question_id": "8330242b56b63708a23c6a92db4d4bcf927a4576", "question": "What types of hate speech are considered?", "predicted_answer": " language which explicitly or implicitly threatens or demeans a person or", "predicted_evidence": [], "explanation": " language which explicitly or implicitly threatens or demeans a person or "}
{"question_id": "e96b0d64c8d9fdd90235c499bf1ec562d2cbb8b2", "question": "What are their baseline models?", "predicted_answer": " turbo tagger and turbo parser</s>", "predicted_evidence": ["Our first experiment measures tagging and parsing accuracy on the TLE and approximates the global impact of grammatical errors on automatic annotation via performance comparison between the original and error corrected sentence versions. In this, and subsequent experiments, we utilize version 2.2 of the Turbo tagger and Turbo parser BIBREF18 , state of the art tools for statistical POS tagging and dependency parsing."], "explanation": " turbo tagger and turbo parser</s>"}
{"question_id": "ec5e84a1d1b12f7185183d165cbb5eae66d9833e", "question": "Do they report results only on English data?", "predicted_answer": " no</s>", "predicted_evidence": ["To account for spelling mistakes, rather than attempt to correct to a vocabulary of correctly spelled words, we constructed an embedding with a vocabulary that contains both correct and incorrectly spelled words. We do this by using standard algorithms BIBREF25 on a large corpus of student responses (approximately 160 million responses). The embedding we created reflects the imperfect manner in which students use words BIBREF26 . For example, while the words 'happems' and 'ocures' are both incorrectly spelled versions of 'happens' and 'occurs' respectively, our embedding exhibits a high cosine similarity between the word vectors of the correct and incorrect versions. The embedding we created was an embedding into 200 dimensional space with a vocabulary consisting of 1.12 million words. Using spelling dictionaries we approximate that the percentage of correctly spelled words in the vocabulary of this embedding is approximately 7%, or roughly 80,000 words, while the remaining 93% are either misspellings, made up words or words from other languages. Lastly, due to the prevalence of words that are concatenated (due to a missing space), we split up any word with a Levenstein distance that is greater than two from our vocabulary into smaller words that are in the vocabulary. This ensures that any sentence is tokenized into a list of elements, almost all of which have valid embeddings."], "explanation": " no</s>"}
{"question_id": "8795bb1f874e5f3337710d8c3d5be49e672ab43a", "question": "What crowdsourcing platform did they obtain the data from?", "predicted_answer": " mechanical turk</s>", "predicted_evidence": ["We crowdsourced questions about these paragraphs on Mechanical Turk. We asked workers to find two or more co-referring spans in the paragraph, and to write questions such that answering them would require the knowledge that those spans are coreferential. We did not ask them to explicitly mark the co-referring spans. Workers were asked to write questions for a random sample of paragraphs from our pool, and we showed them examples of good and bad questions in the instructions (see Appendix ). For each question, the workers were also required to select one or more spans in the corresponding paragraph as the answer, and these spans are not required to be same as the coreferential spans that triggered the questions. We used an uncased base BERT QA model BIBREF9 trained on SQuAD 1.1 BIBREF0 as an adversary running in the background that attempted to answer the questions written by workers in real time, and the workers were able to submit their questions only if their answer did not match the adversary's prediction. Appendix further details the logistics of the crowdsourcing tasks. Some basic statistics of the resulting dataset can be seen in Table ."], "explanation": " mechanical turk</s>"}
{"question_id": "818c85ee26f10622c42ae7bcd0dfbdf84df3a5e0", "question": "What are the eight predefined categories?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["FLOAT SELECTED: Table 3: Categories of Movie Reviews"], "explanation": " unanswerable</s>"}
{"question_id": "141f23e87c10c2d54d559881e641c983e3ec8ef3", "question": "what was the baseline model?", "predicted_answer": " bert-large bibref3</s>", "predicted_evidence": ["Models. As the baseline model, we used BERT-large BIBREF3. We fine-tuned it on the original training set of each dataset and evaluated it on a modified development set. For $\\sigma _4$ vocabulary anonymization, we train the model after the anonymization. For ARC, MCTest, and MultiRC, we fine-tuned a model that had already been trained on RACE to see the performance gained by transfer learning BIBREF27. We report the hyperparameters of our models in Appendix C. Although we trained the baseline model on the original training set, it is assumed that the upper-bound performance can be achieved by a model trained on the modified training set. Therefore, in Section SECREF16, we also see the extent to which the performance improves when the model is trained on the modified training set."], "explanation": " bert-large bibref3</s>"}
{"question_id": "71fd0efea1b441d86d9a75255815ba3efe09779b", "question": "How do the authors measure the extent to which LGI has learned the task?", "predicted_answer": "'move left\u2019 and'move right' random pixels, with", "predicted_evidence": ["The first syntaxes that LGI has learned are the \u2018move left\u2019 and \u2018move right\u2019 random pixels, with the corresponding results shown in Figure 3. After 50000 steps training, LGI could not only reconstruct the input image with high precision but also predict the 'mentally' moved object with specified morphology, correct manipulated direction and position just after the command sentence completed. The predicted text can complete the word \u2018move\u2019 given the first letter \u2018m\u2019 (till now, LGI has only learned syntaxes of \u2018move left or right\u2019). LGI tried to predict the second word \u2018right\u2019 with initial letter \u2018r\u2019, however, after knowing the command text is \u2018l\u2019, it turned to complete the following symbols with \u2018eft\u2019. It doesn\u2019t care if the sentence length is 12 or 11, the predicted image and text just came at proper time and position. Even if the command asked to move out of screen, LGI still could reconstruct the partially occluded image with high fidelity.", "Based on the same network, LGI continued to learn syntax \u2018this is \u2026\u2019. Just like a parent teaching child numbers by pointing to number instances, Figure 4 demonstrates that, after training of 50000 steps, LGI could classify figures in various morphology with correct identity (accuracy = 72.7%). Note that, the classification process is not performed by softmax operation, but by directly textizing operation (i.e. rounding followed by a symbol mapping operation), which is more biologically plausible than the softmax operation."], "explanation": "'move left\u2019 and'move right' random pixels, with"}
{"question_id": "347dc2fd6427b39cf2358d43864750044437dff8", "question": "How many Universal Dependency features are considered?", "predicted_answer": " unanswerable</s>", "predicted_evidence": [], "explanation": " unanswerable</s>"}
{"question_id": "539f5c27e1a2d240e52b711d0a50a3a6ddfa5cb2", "question": "How do they deal with imbalanced datasets?", "predicted_answer": " data expansion using oversampling. attempts include duplicating the text", "predicted_evidence": ["Imbalanced dataset is a challenge for the task because the top patterns are taking too much attention so that most weights might be determined by the easier ones. We have tried different methods to deal with this problem. The first method is data expansion using oversampling. Attempts include duplicating the text with low pattern proportion, replacing first few characters with paddings in the window, randomly changing digits, and shifting the context window. The other method is to add loss control in the model as mentioned in SECREF2. The loss function helps the model to focus on harder cases in different classes and therefore reduce the impact of the imbalanced data. The experimental results are in SECREF11."], "explanation": " data expansion using oversampling. attempts include duplicating the text"}
{"question_id": "b79c85fa84712d3028cb5be2af873c634e51140e", "question": "Which six speech acts are included in the taxonomy?", "predicted_answer": " assertion, recommendation expression, question, request, and", "predicted_evidence": ["Using Searle's speech act taxonomy BIBREF3 , we established a list of six speech act categories that are commonly seen on Twitter: Assertion, Recommendation Expression, Question, Request, and Miscellaneous. Table TABREF1 shows an example tweet for each of these categories."], "explanation": " assertion, recommendation expression, question, request, and"}
{"question_id": "452e2d7d7d9e1bb4914903479cd7caff9f6fae42", "question": "Are pretrained embeddings used?", "predicted_answer": " yes</s>", "predicted_evidence": ["We experimented with a dataset of 16K annotated tweets made available by the authors of BIBREF0 . Of the 16K tweets, 3383 are labeled as sexist, 1972 as racist, and the remaining are marked as neither sexist nor racist. For the embedding based methods, we used the GloVe BIBREF5 pre-trained word embeddings. GloVe embeddings have been trained on a large tweet corpus (2B tweets, 27B tokens, 1.2M vocab, uncased). We experimented with multiple word embedding sizes for our task. We observed similar results with different sizes, and hence due to lack of space we report results using embedding size=200. We performed 10-Fold Cross Validation and calculated weighted macro precision, recall and F1-scores."], "explanation": " yes</s>"}
{"question_id": "f741d32b92630328df30f674af16fbbefcad3f93", "question": "Which baselines were they used for evaluation?", "predicted_answer": " multilingual word embeddings and multilingual sentence representations</s>", "predicted_evidence": ["In this paper, we propose initial strong baselines which represent two complementary directions of research: one based on the aggregation of multilingual word embeddings, and another one, which directly learns multilingual sentence representations. Details on each approach are given in section \"Multilingual word representations\" and \"Multilingual sentence representations\" respectively. In contrast to previous works on cross-lingual document classification with RVC2, we explore training the classifier on all languages and transfer it to all others, ie. we do not limit our study to the transfer between English and a foreign language."], "explanation": " multilingual word embeddings and multilingual sentence representations</s>"}
{"question_id": "c87966e7f497975b76a60f6be50c33d296a4a4e7", "question": "How is cyberbullying defined?", "predicted_answer": " aggressive language: (aggr) regardless of the author's", "predicted_evidence": ["We asked our annotators to consider the full message thread for each tweet as displayed on Twitter's web interface. We also gave them a list of up to 15 recent mentions by the author of the tweet, directed towards any of the other accounts mentioned in the original thread. Then we asked annotators to interpret each tweet in light of this social context, and had them provide us with labels for five key cyberbullying criteria. We defined these criteria in terms of the author account (\u201cwho posted the given tweet?\u201d) and the target (\u201cwho was the tweet about?\u201d \u2013 not necessarily the first mention). We also stated that \u201cif the target is not on Twitter or their handle cannot be identified\u201d the annotator should \u201cplease write OTHER.\u201d With this framework established, we gave the definitions for our five cyberbullying criteria as follows.", "Aggressive language: (aggr) Regardless of the author's intent, the language of the tweet could be seen as aggressive. The user either addresses a group or individual, and the message contains at least one phrase that could be described as confrontational, derogatory, insulting, threatening, hostile, violent, hateful, or sexually abusive.", "Repetition: (rep) The target user has received at least two aggressive messages in total (either from the author or from another user in the visible thread).", "Harmful intent: (harm) The tweet was designed to tear down or disadvantage the target user by causing them distress or by harming their public image. The target does not respond agreeably as to a joke or an otherwise lighthearted comment.", "Visibility among peers: (peer) At least one other user besides the target has liked, retweeted, or responded to at least one of the author's messages.", "Power imbalance: (power) Power is derived from authority and perceived social advantage. Celebrities and public figures are more powerful than common users. Minorities and disadvantaged groups have less power. Bullies can also derive power from peer support."], "explanation": " aggressive language: (aggr) regardless of the author's"}
{"question_id": "16b816925567deb734049416c149747118e13963", "question": "How long is the dataset?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["Datasets. In order for the results to be consistent with previous works, we experimented with the benchmark datasets from SemEval 2014 task 4 BIBREF30 and SemEval 2016 task 5 BIBREF34 competitions. The laptop dataset is taken from SemEval 2014 and is used for both AE and ASC tasks. However, the restaurant dataset for AE is a SemEval 2014 dataset while for ASC is a SemEval 2016 dataset. The reason for the difference is to be consistent with the previous works. A summary of these datasets can be seen in Tables TABREF8 and TABREF8.", "FLOAT SELECTED: Table 1. Laptop and restaurant datasets for AE. S: Sentences; A: Aspects; Rest16: Restaurant dataset from SemEval 2016.", "FLOAT SELECTED: Table 2. Laptop and restaurant datasets for ASC. Pos, Neg, Neu: Number of positive, negative, and neutral sentiments, respectively; Rest14: Restaurant dataset from SemEval 2014"], "explanation": " unanswerable</s>"}
{"question_id": "cfc73e0c82cf1630b923681c450a541a964688b9", "question": "How do they operationalize socioeconomic status from twitter user data?", "predicted_answer": " by combining our twitter data with the socioeconomic maps of insee by assign", "predicted_evidence": ["To obtain a unique representative location of each user, we extracted the sequence of all declared locations from their geolocated tweets. Using this set of locations we selected the most frequent to be the representative one, and we took it as a proxy for the user's home location. Further we limited our users to ones located throughout the French territory thus not considering others tweeting from places outside the country. This selection method provided us with $110,369$ geolocated users who are either detected as French speakers or assigned to be such by Twitter and all associated to specific 'home' GPS coordinates in France. To verify the spatial distribution of the selected population, we further assessed the correlations between the true population distributions (obtained from census data BIBREF28 ) at different administrative level and the geolocated user distribution aggregated correspondingly. More precisely, we computed the $R^2$ coefficient of variation between the inferred and official population distributions (a) at the level of 22 regions. Correlations at this level induced a high coefficient of $R^2\\simeq 0.89$ ( $p<10^{-2}$ ); (b) At the arrondissement level with 322 administrative units and coefficient $R^2\\simeq 0.87$ ( $p<10^{-2}$ ); and (c) at the canton level with 4055 units with a coefficient $R\\simeq 0.16$ ( $p<10^{-2}$ ). Note that the relatively small coefficient at this level is due to the interplay of the sparsity of the inferred data and the fine grained spatial resolution of cantons. All in all, we can conclude that our sample is highly representative in terms of spatial population distribution, which at the same time validate our selection method despite the potential inherent biases induced by the method taking the most frequented GPS coordinates as the user's home location.", "The second dataset we used was released in December 2016 by the National Institute of Statistics and Economic Studies (INSEE) of France. This data corpus BIBREF29 contains a set of sociodemographic aggregated indicators, estimated from the 2010 tax return in France, for each 4 hectare ( $200m \\times 200m$ ) square patch across the whole French territory. Using these indicators, one can estimate the distribution of the average socioeconomic status (SES) of people with high spatial resolution. In this study, we concentrated on three indicators for each patch $i$ , which we took to be good proxies of the socioeconomic status of the people living within them. These were the $S^i_\\mathrm {inc}$ average yearly income per capita (in euros), the $S^i_{\\mathrm {own}}$ fraction of owners (not renters) of real estate, and the $S^i_\\mathrm {den}$ density of population defined respectively as", "To overcome this limitation we combined our Twitter data with the socioeconomic maps of INSEE by assigning each geolocated Twitter user to a patch closest to their estimated home location (within 1 km). This way we obtained for all $110,369$ geolocated users their dynamical linguistic data, their egocentric social network as well as a set of SES indicators."], "explanation": " by combining our twitter data with the socioeconomic maps of insee by assign"}
{"question_id": "67104a5111bf8ea626532581f20b33b851b5abc1", "question": "How many parameters does their model have?", "predicted_answer": " the documents do not mention the number of parameters in the model.", "predicted_evidence": [], "explanation": " the documents do not mention the number of parameters in the model. "}
{"question_id": "753a187c1dd8d96353187fbb193b5f86293a796c", "question": "did they crowdsource annotations?", "predicted_answer": " yes</s>", "predicted_evidence": ["In order to address the drawbacks of existing datasets, we introduce a new corpus based on the Snopes fact-checking website. Our corpus consists of 6,422 validated claims with comprehensive annotations based on the data collected by Snopes fact-checkers and our crowd-workers. The corpus covers multiple domains, including discussion blogs, news, and social media, which are often found responsible for the creation and distribution of unreliable information. In addition to validated claims, the corpus comprises over 14k documents annotated with evidence on two granularity levels and with the stance of the evidence with respect to the claims. Our data allows training machine learning models for the four steps of the automated fact-checking process described above: document retrieval, evidence extraction, stance detection, and claim validation."], "explanation": " yes</s>"}
{"question_id": "a222dc5d804a7b453a0f7fbc1d6c1b165a3ccdd6", "question": "What is the Semantic Web?", "predicted_answer": " semantic web is a network of interlinked, semantically-enable", "predicted_evidence": [], "explanation": " semantic web is a network of interlinked, semantically-enable"}
{"question_id": "e462efb58c71f186cd6b315a2d861cbb7171f65b", "question": "What platform was used for crowdsourcing?", "predicted_answer": " amazon mechanical turk (amt)</s>", "predicted_evidence": ["Our goal is to identify which of the actions extracted from the transcripts are visually depicted in the videos. We create an annotation task on Amazon Mechanical Turk (AMT) to identify actions that are visible."], "explanation": " amazon mechanical turk (amt)</s>"}
{"question_id": "13fb28e8b7f34fe600b29fb842deef75608c1478", "question": "By how much did their model outperform baselines?", "predicted_answer": " by a large margin.</s>", "predicted_evidence": ["Table TABREF28 shows results on the event expression tasks. Our initial submits RUN 4 and 5 outperformed the memorization baseline on every metric on every task. The precision of event span identification is close to the max report. However, our system got lower recall. One of the main reason is that our training objective function is accuracy-oriented. Table TABREF29 shows results on the phase 2 subtask."], "explanation": " by a large margin.</s>"}
{"question_id": "a4a9971799c8860b50f219c93f050ebf6a627b3d", "question": "What languages feature in the dataset?", "predicted_answer": " english</s>", "predicted_evidence": ["Building on the findings of previous research efforts, this paper aims to study the effects of using new textual and psycholinguistic signals to detect extremist content online. These signals are developed based on insights gathered from analyzing propaganda material published by known extremist groups. In this study, we focus mainly on the ISIS group as they are one of the leading terrorist groups that utilise social media to share their propaganda and recruit individuals. We analyze the propaganda material they publish in their online English magazine called Dabiq, and use data-mining techniques to computationally uncover contextual text and psychological properties associated with these groups. From our analysis of these texts, we are able to extract a set of signals that provide some insight into the mindset of the radical group. This allows us to create a general radical profile that we apply as a signal to detect pro-ISIS supporters on Twitter. Our results show that these identified signals are indeed critical to help improve existing efforts to detect online radicalization.", "Another angle for analyzing written text is by looking at the psychological properties that can be inferred regarding their authors. This is typically called psycholinguistics, where one examines how the use of the language can be indicative of different psychological states. Examples of such psychological properties include introversion, extroversion, sensitivity, and emotions. One of the tools that automates the process of extracting psychological meaning from text is the Linguistic Inquiry and Word Count (LIWC) BIBREF8 tool. This approach has been used in the literature to study the behaviour of different groups and to predict their psychological states, such as predicting depression BIBREF9 . More recently, it has also been applied to uncover different psychological properties of extremist groups and understand their intentions behind the recruitment campaigns BIBREF10 .", "We acquired a publicly available dataset of tweets posted by known pro-ISIS Twitter accounts that was published during the 2015 Paris attacks by Kaggle data science community. The dataset consists of around INLINEFORM0 tweets posted by more than 100 users. These tweets were labelled as being pro-ISIS by looking at specific indicators, such as a set of keywords used (in the user's name, description, tweet text), their network of follower/following of other known radical accounts, and sharing of images of the ISIS flag or some radical leaders. To validate that these accounts are indeed malicious, we checked the current status of the users' accounts in the dataset and found that most of them had been suspended by Twitter. This suggests that they did, in fact, possess a malicious behaviour that opposes the Twitter platform terms of use which caused them to be suspended. We filter out any tweets posted by existing active users and label this dataset as known-bad."], "explanation": " english</s>"}
{"question_id": "0ba8f04c3fd64ee543b9b4c022310310bc5d3c23", "question": "what are the previous state of the art for sentiment categorization?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["As with the last task, we first extract the vector representation of all the tweets in the dataset using Tweet2Vec and use that to train a logistic regression classifier using the vector representations. Even though there are three classes, the SemEval task is a binary task. The performance is measured as the average F1-score of the positive and the negative class. Table 3 shows the performance of our model compared to the top four models in the SemEval 2015 competition (note that only the F1-score is reported by SemEval for this task) and ParagraphVec. Our model outperforms all these models, again without resorting to any feature engineering."], "explanation": " unanswerable</s>"}
{"question_id": "a8f189fad8b72f8b2b4d2da4ed8475d31642d9e7", "question": "How do they show that binary paragraph vectors capture semantics?", "predicted_answer": " by demonstrating that the semantics it captured can be useful for different text collections.", "predicted_evidence": ["To assess the performance of binary paragraph vectors, we carried out experiments on three datasets: 20 Newsgroups, a cleansed version (also called v2) of Reuters Corpus Volume 1 (RCV1) and English Wikipedia. As paragraph vectors can be trained with relatively large vocabularies, we did not perform any stemming of the source text. However, we removed stop words as well as words shorter than two characters and longer than 15 characters. Results reported by BIBREF15 indicate that performance of PV-DBOW can be improved by including n-grams in the model. We therefore evaluated two variants of Binary PV-DBOW: one predicting words in documents and one predicting words and bigrams. Since 20 Newsgroups is a relatively small dataset, we used all words and bigrams from its documents. This amounts to a vocabulary with slightly over one million elements. For the RCV1 dataset we used words and bigrams with at least 10 occurrences in the text, which gives a vocabulary with approximately 800 thousands elements. In case of English Wikipedia we used words and bigrams with at least 100 occurrences, which gives a vocabulary with approximately 1.5 million elements.", "In the experiments presented thus far we had at our disposal training sets with documents similar to the documents for which we inferred binary codes. One could ask a question, if it is possible to use binary paragraph vectors without collecting a domain-specific training set? For example, what if we needed to hash documents that are not associated with any available domain-specific corpus? One solution could be to train the model with a big generic text corpus, that covers a wide variety of domains. BIBREF21 evaluated this approach for real-valued paragraph vectors, with promising results. It is not obvious, however, whether short binary codes would also perform well in similar settings. To shed light on this question we trained Binary PV-DBOW with bigrams on the English Wikipedia, and then inferred binary codes for the test parts of the 20 Newsgroups and RCV1 datasets. The results are presented in Table TABREF14 and in Figure FIGREF11 . The model trained on an unrelated text corpus gives lower retrieval precision than models with domain-specific training sets, which is not surprising. However, it still performs remarkably well, indicating that the semantics it captured can be useful for different text collections. Importantly, these results were obtained without domain-specific finetuning."], "explanation": " by demonstrating that the semantics it captured can be useful for different text collections."}
{"question_id": "5181aefb8a7272b4c83a1f7cb61f864ead6a1f1f", "question": "Do they compare to other methods?", "predicted_answer": " yes</s>", "predicted_evidence": [], "explanation": " yes</s>"}
{"question_id": "811b67460e65232b8f363dc3f329ffecdfcc4ab2", "question": "Which repositories did they collect from?", "predicted_answer": " gh archive</s>", "predicted_evidence": ["Although GitHub provides a set of APIs (application programming interfaces) that allow end-users to access its data in a programmatic manner, it doesn't allow flexible querying on the repository meta data necessary for our data collection purposes. Therefore, we turn to GH Archive, which collects all the GitHub event data and make them accessible through flexible APIs. Specifically, we collected every repository from GH Archive that:", "Has at least one pull request or pull request review comment event between November 2017 and September 2019,", "Has 50 or more starts,", "Has a size between 1MB and 1GB, and", "Has a permissive license.", "Note the \u201cand\u201d in the list above\u2014a repository needs to meet all the conditions mentioned above to be eligible. The first two criteria (pull request events and the number of starts) are a sign of a quality repository. As for the license, we allowed apache-2.0 (Apache License 2.0), mit (MIT License), bsd-3-clause (BSD 3-Clause License), bsd-2-clause (BSD 2-Clause License), cc0-1.0 (Creative Commons Zero v1.0 Universal), unlicense (Unlicense), cc-by-4.0 (Creative Commons Attribution 4.0), and bsl-1.0 (Boost Software License 1.0 (BSL-1.0). A repository's number of stars, size, and license are determined as of the event in the first condition."], "explanation": " gh archive</s>"}
{"question_id": "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b", "question": "How long is their dataset?", "predicted_answer": " unanswerable</s>", "predicted_evidence": [], "explanation": " unanswerable</s>"}
{"question_id": "3ccc4ccebc3b0de5546b1208e8094a839fd4a4ab", "question": "What is case syncretism?", "predicted_answer": " noun case\u2014and thus syntactic function\u2014is ambiguous</s>", "predicted_evidence": ["Our summary finding is that character-level models lag the oracle in nearly all languages (\u00a7 \"Experiments\" ). The difference is small, but suggests that there is value in modeling morphology. When we tease apart the results by part of speech and dependency type, we trace the difference back to the character-level model's inability to disambiguate words even when encoded with arbitrary context (\u00a7 \"Analysis\" ). Specifically, it struggles with case syncretism, in which noun case\u2014and thus syntactic function\u2014is ambiguous. We show that the oracle relies on morphological case, and that a character-level model provided only with morphological case rivals the oracle, even when case is provided by another predictive model (\u00a7 \"Characters and case syncretism\" ). Finally, we show that the crucial morphological features vary by language (\u00a7 \"Understanding head selection\" )."], "explanation": " noun case\u2014and thus syntactic function\u2014is ambiguous</s>"}
{"question_id": "0ba1514fb193c52a15c31ffdcd5c3ddbb2bb2c40", "question": "How much better is performance of Nigerian Pitdgin English sentiment classification of models that use additional Nigerian English data compared to orginal English-only models?", "predicted_answer": " unanswerable</s>", "predicted_evidence": [], "explanation": " unanswerable</s>"}
{"question_id": "b0e894536857cb249bd75188c3ca5a04e49ff0b6", "question": "How do attention, recurrent and convolutional networks differ on the language classes they accept?", "predicted_answer": " unanswerable</s>", "predicted_evidence": [], "explanation": " unanswerable</s>"}
{"question_id": "e459760879f662b2205cbdc0f5396dbfe41323ae", "question": "Which other tasks are evaluated?", "predicted_answer": " product category classification and review headline generation</s>", "predicted_evidence": ["The results of our experiments can be summarized in three statements. First, our preliminary experiments show that doing bias-based attribute representation and attention-based injection is not an effective method to incorporate user and product information in sentiment classification models. Second, despite using only a simple BiLSTM with attention classifier, we significantly outperform previous state-of-the-art models that use more complicated architectures (e.g., models that use hierarchical models, external memory networks, etc.). Finally, we show that these attribute representations transfer well to other tasks such as product category classification and review headline generation."], "explanation": " product category classification and review headline generation</s>"}
{"question_id": "f8264609a44f059b74168995ffee150182a0c14f", "question": "What models are explored in this paper?", "predicted_answer": " svm, logistic regression, ann, lstm, and random", "predicted_evidence": ["These previous approaches lack a clear contextual analysis used in NLP. We considered the semantic meaning of each word and we feel that the presence of particular words influence the meaning. We reckoned this important since we felt the contextual meaning of the text needs to be preserved and analyzed for better classification. Other studies emphasize the user and features related to them. In [4], \u201c45 features\u2026[were used] for predicting accuracy...across four types: structural, user, content, and temporal,\u201d so features included characteristics beyond the text. Article [6] \"learn[s] the representations of news articles, creators and subjects simultaneously.\" In our project, we emphasize the content by working with articles whose labels only relate to the text, nothing outside that scope, and have used SVM, Logistic Regression, ANN, LSTM, and Random Forest."], "explanation": " svm, logistic regression, ann, lstm, and random"}
{"question_id": "2d924e888a92dc0b14cdb5584e73e87254c3d1ee", "question": "Do they reduce language variation of text by enhancing frequencies?", "predicted_answer": " yes</s>", "predicted_evidence": [], "explanation": " yes</s>"}
{"question_id": "f3b851c9063192c86a3cc33b2328c02efa41b668", "question": "Does this study perform experiments to prove their claim that indeed personalized profiles will have inclination towards particular cuisines?", "predicted_answer": " yes</s>", "predicted_evidence": ["A more sophisticated approach to classify based on the ingredients was adopted by using the K Nearest Neighbors Model. The Yummly dataset from Kaggle is used to train the model. The ingredients extracted from the images are used as a test set. The model was run successfully for k-values ranging from 1-25. The radar charts for some of the k values are shown in Fig 7, 8 and 9.", "Thus from these charts, we see that the user likes to eat Italian and Mexican food on most occasions. This is also in sync with the rudimentary method that we had used earlier."], "explanation": " yes</s>"}
{"question_id": "627ce8a1db08a732d5a8f7e1f8a72e3de89847e6", "question": "What characterizes the 303 domains? e.g. is this different subject tags?", "predicted_answer": " 303 sub areas from stack exchange data dumps. the full list of area", "predicted_evidence": ["We use 303 sub areas from Stack Exchange data dumps. The full list of area names is in the appendix. We do not include Stack Overflow because it is too specific to programming related questions. We also exclude all questions under the following language sub areas: Chinese, German, Spanish, Russian, Japanese, Korean, Latin, Ukrainian. This ensures that the questions in MQR are mostly English sentences. Having questions from 303 Stack Exchange sites makes the MQR dataset cover a broad range of domains."], "explanation": " 303 sub areas from stack exchange data dumps. the full list of area"}
{"question_id": "04796aaa59eeb2176339c0651838670fd916074d", "question": "what boosting techniques were used?", "predicted_answer": " light gradient boosting machine (lgbm) algorithm</s>", "predicted_evidence": ["We investigate an important family of ensemble methods known as boosting, and more specifically a Light Gradient Boosting Machine (LGBM) algorithm, which consists of an implementation of fast gradient boosting on decision trees. In this study, we use a library implemented by Microsoft BIBREF18 . In our model, we learn a linear combination of the prediction given by the base classifiers and the input text features to predict the labels. As features, we consider the average term frequency-inverse document frequency (TF-IDF) score for each instance and the frequency of occurrence of quantitative information elements (QIEF) (e.g. percentage, population, dose of medicine). Finally, the output of the binary cross entropy with logits layer (predicted probabilities for the three classes) and the feature information are fed to the LGBM."], "explanation": " light gradient boosting machine (lgbm) algorithm</s>"}
{"question_id": "7d503b3d4d415cf3e91ab08bd5a1a2474dd1047b", "question": "How long is the vocabulary of subwords?", "predicted_answer": " 500</s>", "predicted_evidence": ["In this section, we examine the pre-trained language models of ESuLMo in terms of PPL. All the models' training and evaluation are done on One Billion Word dataset BIBREF19 . During training, we strictly follow the same hyper-parameter published by ELMo, including the hidden size, embedding size, and the number of LSTM layers. Meanwhile, we train each model on 4 Nvidia P40 GPUs, which takes about three days for each epoch. Table TABREF5 shows that our pre-trained language models can improve the performance of RNN-based language models by a large margin and our subword-aware language models outperform all previous RNN-based language models, including ELMo, in terms of PPL. During the experiment, we find that 500 is the best vocabulary size for both segmentation algorithms, and BPE is better than ULM in our setting."], "explanation": " 500</s>"}
{"question_id": "73a5783cad4ed468a8dbb31b5de2c618ce351ad1", "question": "by how much did performance improve?", "predicted_answer": " 31%</s>", "predicted_evidence": ["FLOAT SELECTED: Table 6: The accuracy of various models and baselines on the original PERSONACHAT test set.", "Our main result, reported in Table TABREF16 , is that utilizing the deployment examples improves accuracy on the Dialogue task regardless of the number of available supervised (HH) Dialogue examples. The boost in quality is naturally most pronounced when the HH Dialogue training set is small (i.e., where the learning curve is steepest), yielding an increase of up to 9.4 accuracy points, a 31% improvement. However, even when the entire PersonaChat dataset of 131k examples is used\u2014a much larger dataset than what is available for most dialogue tasks\u2014adding deployment examples is still able to provide an additional 1.6 points of accuracy on what is otherwise a very flat region of the learning curve. It is interesting to note that the two types of deployment examples appear to provide complementary signal, with models performing best when they use both example types, despite them coming from the same conversations. We also calculated hit rates with 10,000 candidates (instead of 20), a setup more similar to the interactive setting where there may be many candidates that could be valid responses. In that setting, models trained with the deployment examples continue to outperform their HH-only counterparts by significant margins (see Appendix SECREF8 )."], "explanation": " 31%</s>"}
{"question_id": "9a9774eacb8f75bcfa07a4e60ed5eb02646467e3", "question": "what is the size of their corpus?", "predicted_answer": " 6,422 validated claims with comprehensive annotations based on the data collected by", "predicted_evidence": ["In order to address the drawbacks of existing datasets, we introduce a new corpus based on the Snopes fact-checking website. Our corpus consists of 6,422 validated claims with comprehensive annotations based on the data collected by Snopes fact-checkers and our crowd-workers. The corpus covers multiple domains, including discussion blogs, news, and social media, which are often found responsible for the creation and distribution of unreliable information. In addition to validated claims, the corpus comprises over 14k documents annotated with evidence on two granularity levels and with the stance of the evidence with respect to the claims. Our data allows training machine learning models for the four steps of the automated fact-checking process described above: document retrieval, evidence extraction, stance detection, and claim validation."], "explanation": " 6,422 validated claims with comprehensive annotations based on the data collected by"}
{"question_id": "3d547a7dda18a2dd5dc89f12d25d7fe782d66450", "question": "What computational linguistic methods were used for the analysis?", "predicted_answer": " unanswerable</s>", "predicted_evidence": [], "explanation": " unanswerable</s>"}
{"question_id": "b3432f52af0b95929e6723dd1f01ce029d90a268", "question": "Does Elmo learn all possible logic rules?", "predicted_answer": " no</s>", "predicted_evidence": ["Switching to ELMo word embeddings improves performance by 2.9 percentage points on an average, corresponding to about 53 test sentences. Of these, about 32 sentences (60% of the improvement) correspond to A-but-B and negation style sentences, which is substantial when considering that only 24.5% of test sentences include these discourse relations ([tab:sst2]Table tab:sst2). As further evidence that ELMo helps on these specific constructions, the non-ELMo baseline model (no-project, no-distill) gets 255 sentences wrong in the test corpus on average, only 89 (34.8%) of which are A-but-B style or negations."], "explanation": " no</s>"}
{"question_id": "181027f398a6b79b1ba44d8d41cc1aba0d6f5212", "question": "what other representations do they compare with?", "predicted_answer": " paragraph vector</s>", "predicted_evidence": ["Table TABREF15 presents P@10, MAP and MRR results of our KeyVec model and competing embedding methods in academic paper retrieval. word2vec averaging generates an embedding for a document by averaging the word2vec vectors of its constituent words. In the experiment, we used two different versions of word2vec: one from public release, and the other one trained specifically on our own academic corpus (113 GB). From Table TABREF15 , we observe that as a document-embedding model, Paragraph Vector gave better retrieval results than word2vec averagings did. In contrast, our KeyVec outperforms all the competitors given its unique capability of capturing and embedding the key information of documents."], "explanation": " paragraph vector</s>"}
{"question_id": "6bce04570d4745dcfaca5cba64075242308b65cf", "question": "what was the baseline?", "predicted_answer": " bigru+crf model</s>", "predicted_evidence": ["The BiGRU+CRF model was used as the baseline model. Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model."], "explanation": " bigru+crf model</s>"}
{"question_id": "cb58605a7c230043bd0d6e8d5b068f8b533f45fe", "question": "did they collect the human labeled data?", "predicted_answer": " yes</s>", "predicted_evidence": ["The data set obtained from Pypestream is permitted to be used for the research purpose only, and for a security reason, we are not allowed to share the data set. It was once originaly used for creating a conversational intelligence system(chatbot) to support customer inqueries about a particular service. The data set is a two-column comma separated value format data with one column of \"sentence\" and the other column of \"class\". It contains 2,212 unique sentences of user expressions asking questions and aswering to the questions the chatbot asked to the users(see Table.TABREF9). The sentences are all in English without having any missspelled words, and labeled with 19 distinct classes that are identified and designed by humans. Additional data set that only contains the sentences was made for the purpose of this study by taking out the \"class\" column from the original data set."], "explanation": " yes</s>"}
{"question_id": "d5bce5da746a075421c80abe10c97ad11a96c6cd", "question": "Which baselines did they compare against?", "predicted_answer": " memorization baseline</s>", "predicted_evidence": ["Table TABREF28 shows results on the event expression tasks. Our initial submits RUN 4 and 5 outperformed the memorization baseline on every metric on every task. The precision of event span identification is close to the max report. However, our system got lower recall. One of the main reason is that our training objective function is accuracy-oriented. Table TABREF29 shows results on the phase 2 subtask.", "FLOAT SELECTED: Table 3: System performance comparison. Note that Run4 means the window size is 4, Run5 means the window size is 5", "FLOAT SELECTED: Table 4: Phase 2: DocTimeRel"], "explanation": " memorization baseline</s>"}
{"question_id": "134a66580c363287ec079f353ead8f770ac6d17b", "question": "What fine-grained semantic types are considered?", "predicted_answer": " unanswerable</s>", "predicted_evidence": [], "explanation": " unanswerable</s>"}
{"question_id": "909ecf675f874421eecc926a9f7486475aa1423c", "question": "How do they use extracted intent to rescore?", "predicted_answer": " guides the best lattice path selection process by increasing the probability of intent recognition", "predicted_evidence": ["A commonly known limitation of the current ASR systems is their inability to recognize long sequences of words precisely. In this paper, we propose a new method of incorporating domain knowledge into automatic speech recognition which alleviates this weakness. Our approach allows performing fast ASR domain adaptation by providing a library of intent examples used for lattice rescoring. The method guides the best lattice path selection process by increasing the probability of intent recognition. At the same time, the method does not rescore paths of unessential turns which do not contain intent examples. As a result, our approach improves the understanding of spontaneous conversations by recognizing semantically important transcription segments while adding minimal computational overhead. Our method is domain agnostic and can be easily adapted to a new one by providing the library of intent examples expected to appear in the new domain. The increased intent annotation coverage allows us to train more sophisticated models for downstream tasks, opening the prospects of true spoken language understanding."], "explanation": " guides the best lattice path selection process by increasing the probability of intent recognition"}
{"question_id": "22488c8628b6db5fd708b6471c31a8eac31f66df", "question": "What is the size of Multi-news dataset?", "predicted_answer": " 44,972</s>", "predicted_evidence": ["We split our dataset into training (80%, 44,972), validation (10%, 5,622), and test (10%, 5,622) sets. Table TABREF5 compares Multi-News to other news datasets used in experiments below. We choose to compare Multi-News with DUC data from 2003 and 2004 and TAC 2011 data, which are typically used in multi-document settings. Additionally, we compare to the single-document CNNDM dataset, as this has been recently used in work which adapts SDS to MDS BIBREF11 . The number of examples in our Multi-News dataset is two orders of magnitude larger than previous MDS news data. The total number of words in the concatenated inputs is shorter than other MDS datasets, as those consist of 10 input documents, but larger than SDS datasets, as expected. Our summaries are notably longer than in other works, about 260 words on average. While compressing information into a shorter text is the goal of summarization, our dataset tests the ability of abstractive models to generate fluent text concise in meaning while also coherent in the entirety of its generally longer output, which we consider an interesting challenge."], "explanation": " 44,972</s>"}
{"question_id": "77bbe1698e001c5889217be3164982ea36e85752", "question": "What are the baseline models?", "predicted_answer": " cross-bilstm-cnn and att-bilstm", "predicted_evidence": ["This paper explores two types of cross-structures to help cope with the problem: Cross-BiLSTM-CNN and Att-BiLSTM-CNN. Previous studies have tried to stack multiple LSTMs for sequence-labeling NER BIBREF1. As they follow the trend of stacking forward and backward LSTMs independently, the Baseline-BiLSTM-CNN is only able to learn higher-level representations of past or future per se. Instead, Cross-BiLSTM-CNN, which interleaves every layer of the two directions, models cross-context in an additive manner by learning higher-level representations of the whole context of each token. On the other hand, Att-BiLSTM-CNN models cross-context in a multiplicative manner by capturing the interaction between past and future with a dot-product self-attentive mechanism BIBREF5, BIBREF6."], "explanation": " cross-bilstm-cnn and att-bilstm"}
{"question_id": "d0dc6729b689561370b6700b892c9de8871bb44d", "question": "How did they constrain training using the parameters?", "predicted_answer": " by fixing certain parameters of the parent model, letting the rest be fine-t", "predicted_evidence": ["In this paper, we give a method for substantially improving NMT results on these languages. Neural models learn representations of their input that are often useful across tasks. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model, letting the rest be fine-tuned by the child model. We report NMT improvements from transfer learning of 5.6 Bleu on average, and we provide an analysis of why the method works. The final NMT system approaches strong SBMT baselines in in all four language pairs, and exceeds SBMT performance in one of them. Furthermore, we show that NMT is an exceptional re-scorer of `traditional' MT output; even NMT that on its own is worse than SBMT is consistently able to improve upon SBMT system output when incorporated as a re-scoring model.", "A justification for this approach is that in scenarios where we have limited training data, we need a strong prior distribution over models. The parent model trained on a large amount of bilingual data can be considered an anchor point, the peak of our prior distribution in model space. When we train the child model initialized with the parent model, we fix parameters likely to be useful across tasks so that they will not be changed during child-model training. In the French-English to Uzbek-English example, as a result of the initialization, the English word embeddings from the parent model are copied, but the Uzbek words are initially mapped to random French embeddings. The English embeddings should be kept but the Uzbek embeddings should be modified during training of the child model. Freezing certain portions of the parent model and fine tuning others can be considered a hard approximation to a tight prior or strong regularization applied to some of the parameters. We also experiment with ordinary L2 regularization, but find it does not significantly improve over the parameter freezing described above."], "explanation": " by fixing certain parameters of the parent model, letting the rest be fine-t"}
{"question_id": "06d5de706348dbe8c29bfacb68ce65a2c55d0391", "question": "What is the computational complexity of old method", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["Text: \u201cI like kitties and doggies\u201d", "Window: 2", "Bigrams: {(I like), (like kitties), (kitties and), (and doggies)} and this one:", "Window: 4", "Bigrams: {(I like), (I kitties), (I and), (like kitties), (like and), (like doggies), (kitties and), (kitties doggies), (and doggies)}."], "explanation": " unanswerable</s>"}
{"question_id": "f637bba86cfb94ca8ac4b058faf839c257d5eaa0", "question": "What model architecture do they use for the decoder?", "predicted_answer": " gru language model</s>", "predicted_evidence": ["Prior work BIBREF11 has figured out how to edit prototype in an unconditional setting, but it cannot be applied to the response generation directly. In this paper, we propose a prototype editing method in a conditional setting. Our idea is that differences between responses strongly correlates with differences in their contexts (i.e. if a word in prototype context is changed, its related words in the response are probably modified in the editing.). We realize this idea by designing a context-aware editing model that is built upon a encoder-decoder model augmented with an editing vector. The edit vector is computed by the weighted average of insertion word embeddings and deletion word embeddings. Larger weights mean that the editing model should pay more attention on corresponding words in revision. For instance, in Table TABREF1 , we wish words like \u201cdessert\", \u201cTofu\" and \u201cvegetables\" get larger weights than words like \u201cand\" and \u201c at\". The encoder learns the prototype representation with a gated recurrent unit (GRU), and feeds the representation to a decoder together with the edit vector. The decoder is a GRU language model, that regards the concatenation of last step word embedding and the edit vector as inputs, and predicts the next word with an attention mechanism."], "explanation": " gru language model</s>"}
{"question_id": "8de0e1fdcca81b49615a6839076f8d42226bf1fe", "question": "Which dataset do they use?", "predicted_answer": " 500 rescored intent annotations found in the lattices in", "predicted_evidence": ["To evaluate the effectiveness of the proposed algorithm, we have sampled a dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain. The correctness of the rescoring was judged by two annotators, who labeled 250 examples each. The annotators read the whole conversation transcript and listened to the recording to establish whether the rescoring is meaningful. In cases when a rescored word was technically incorrect (e.g., mistaken tense of a verb), but the rescoring led to the recognition of the correct intent, we labeled the intent annotation as correct. The results are shown in Table TABREF22. Please note that every result above 50% indicates an improvement over the ASR best path recognition, since we correct more ASR errors than we introduce new mistakes."], "explanation": " 500 rescored intent annotations found in the lattices in"}
{"question_id": "f5bc07df5c61dcb589a848bd36f4ce9c22abd46a", "question": "Other than privacy, what are the other major ethical challenges in clinical data?", "predicted_answer": " discrimination, biases, data quality, reporting bias, observational bias,", "predicted_evidence": ["Unlocking knowledge from free text in the health domain has a tremendous societal value. However, discrimination can occur when individuals or groups receive unfair treatment as a result of automated processing, which might be a result of biases in the data that were used to train models. The question is therefore what the most important biases are and how to overcome them, not only out of ethical but also legal responsibility. Related to the question of bias is so-called algorithm transparency BIBREF44 , BIBREF45 , as this right to explanation requires that influences of bias in training data are charted. In addition to sampling bias, which we introduced in section 2, we discuss in this section further sources of bias. Unlike sampling bias, which is a corpus-level bias, these biases here are already present in documents, and therefore hard to account for by introducing larger corpora.", "paragraph4 0.9ex plus1ex minus.2ex-1em Data quality Texts produced in the clinical settings do not always tell a complete or accurate patient story (e.g. due to time constraints or due to patient treatment in different hospitals), yet important decisions can be based on them. As language is situated, a lot of information may be implicit, such as the circumstances in which treatment decisions are made BIBREF47 . If we fail to detect a medical concept during automated processing, this can not necessarily be a sign of negative evidence. Work on identifying and imputing missing values holds promise for reducing incompleteness, see Lipton et al. LiptonEtAl2016 for an example in sequential modeling applied to diagnosis classification.", "paragraph4 0.9ex plus1ex minus.2ex-1em Reporting bias Clinical texts may include bias coming from both patient's and clinician's reporting. Clinicians apply their subjective judgments to what is important during the encounter with patients. In other words, there is separation between, on the one side, what is observed by the clinician and communicated by the patient, and on the other, what is noted down. Cases of more serious illness may be more accurately documented as a result of clinician's bias (increased attention) and patient's recall bias. On the other hand, the cases of stigmatized diseases may include suppressed information. In the case of traffic injuries, documentation may even be distorted to avoid legal consequences BIBREF48 .", "We need to be aware that clinical notes may reflect health disparities. These can originate from prejudices held by healthcare practitioners which may impact patients' perceptions; they can also originate from communication difficulties in the case of ethnic differences BIBREF49 . Finally, societal norms can play a role. Brady et al. BradyEtAl2016 find that obesity is often not documented equally well for both sexes in weight-addressing clinics. Young males are less likely to be recognized as obese, possibly due to societal norms seeing them as \u201cstocky\" as opposed to obese. Unless we are aware of such bias, we may draw premature conclusions about the impact of our results.", "paragraph4 0.9ex plus1ex minus.2ex-1em Observational bias Although variance in health outcome is affected by social, environmental and behavioral factors, these are rarely noted in clinical reports BIBREF13 . The bias of missing explanatory factors because they can not be identified within the given experimental setting is also known as the streetlight effect. In certain cases, we could obtain important prior knowledge (e.g. demographic characteristics) from data other than clinical notes.", "paragraph4 0.9ex plus1ex minus.2ex-1em Dual use We have already mentioned linking personal health information from online texts to clinical records as a motivation for exploring surrogate data sources. However, this and many other applications also have potential to be applied in both beneficial and harmful ways. It is easy to imagine how sensitive information from clinical notes can be revealed about an individual who is present in social media with a known identity. More general examples of dual use are when the NLP tools are used to analyze clinical notes with a goal of determining individuals' insurability and employability."], "explanation": " discrimination, biases, data quality, reporting bias, observational bias,"}
{"question_id": "6c50871294562e4886ede804574e6acfa8d1a5f9", "question": "what was their result?", "predicted_answer": " <unk>no answer></s>", "predicted_evidence": ["To evaluate our model, we wrapped the provided scripts for the shared task and calculated the Pearson correlation coefficient and the Spearman rank coefficient with the gold standard in the validation set, as well as the same values over a subset of the same data formed by taking every instance with a gold emotion intensity score greater than or equal to 0.5.", "FLOAT SELECTED: Table 2: Summary of the best results.", "In this section we report the results of the experiments we performed to test our proposed model. In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any additional resources, except pre-trained word embeddings. These results are, however, reversed for the test dataset, where our model performs worse than the baseline. This shows that the model is not able to generalize well, which we think is related to the missing semantic information due to the vocabulary gap we observed between the datasets and the GloVe embeddings."], "explanation": " <unk>no answer></s>"}
{"question_id": "63279ecb2ba4e51c1225e63b81cb021abc10d0d1", "question": "How does byte-pair encoding work?", "predicted_answer": " byte pair encoding (bpe) bibref5</s>", "predicted_evidence": [], "explanation": " byte pair encoding (bpe) bibref5</s>"}
{"question_id": "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e", "question": "Which dataset do they use?", "predicted_answer": " annual retail trade survey of u.s. retail and food services firms for the", "predicted_evidence": ["We present the results of our experiments with both the Evo-Devo and the Machine Learning mechanisms described earlier using the U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12 ."], "explanation": " annual retail trade survey of u.s. retail and food services firms for the"}
{"question_id": "180c7bea8caf05ca97d9962b90eb454be4176425", "question": "What contextual language model is used?", "predicted_answer": " bert</s>", "predicted_evidence": ["To study the semantic nuances between fake news and satire, we use BERT BIBREF8, which stands for Bidirectional Encoder Representations from Transformers, and represents a state-of-the-art contextual language model. BERT is a method for pre-training language representations, meaning that it is pre-trained on a large text corpus and then used for downstream NLP tasks. Word2Vec BIBREF9 showed that we can use vectors to properly represent words in a way that captures semantic or meaning-related relationships. While Word2Vec is a context-free model that generates a single word-embedding for each word in the vocabulary, BERT generates a representation of each word that is based on the other words in the sentence. It was built upon recent work in pre-training contextual representations, such as ELMo BIBREF10 and ULMFit BIBREF11, and is deeply bidirectional, representing each word using both its left and right context. We use the pre-trained models of BERT and fine-tune it on the dataset of fake news and satire articles using Adam optimizer with 3 types of decay and 0.01 decay rate. Our BERT-based binary classifier is created by adding a single new layer in BERT's neural network architecture that will be trained to fine-tune BERT to our task of classifying fake news and satire articles."], "explanation": " bert</s>"}
{"question_id": "39d20b396f12f0432770c15b80dc0d740202f98d", "question": "How many of the attribute-value pairs are found in images?", "predicted_answer": " unanswerable</s>", "predicted_evidence": [], "explanation": " unanswerable</s>"}
{"question_id": "576a3ed6e4faa4c3893db632e97a52ac6e864aac", "question": "How long is the dataset?", "predicted_answer": " 5,124 sentences (97,681 tokens)</s>", "predicted_evidence": ["The TLE currently contains 5,124 sentences (97,681 tokens) with POS tag and dependency annotations in the English Universal Dependencies (UD) formalism BIBREF2 , BIBREF3 . The sentences were obtained from the FCE corpus BIBREF1 , a collection of upper intermediate English learner essays, containing error annotations with 75 error categories BIBREF7 . Sentence level segmentation was performed using an adaptation of the NLTK sentence tokenizer. Under-segmented sentences were split further manually. Word level tokenization was generated using the Stanford PTB word tokenizer."], "explanation": " 5,124 sentences (97,681 tokens)</s>"}
{"question_id": "63cdac43a643fc1e06da44910458e89b2c7cd921", "question": "How was the dataset collected?", "predicted_answer": " using crowdsourcing</s>", "predicted_evidence": ["The data was collected using crowdsourcing. Each speaker was recorded saying each wording for each intent twice. The phrases to record were presented in a random order. Participants consented to data being released and provided demographic information about themselves. The demographic information about these anonymized speakers (age range, gender, speaking ability, etc.) is included along with the dataset."], "explanation": " using crowdsourcing</s>"}
{"question_id": "0f1f81b6d4aa0da38b4cc8b060926e7df61bb646", "question": "What is the machine learning method used to make the predictions?", "predicted_answer": " sgnn</s>", "predicted_evidence": ["Following BIBREF21 (BIBREF21), we evaluate on the standard multiple choice narrative cloze (MCNC) dataset BIBREF2. As SGNN proposed by BIBREF21 (BIBREF21) achieved state-of-the-art performances for this task, we use the framework of SGNN, and only replace their input event embeddings with our intent and sentiment-enhanced event embeddings."], "explanation": " sgnn</s>"}
{"question_id": "2f142cd11731d29d0c3fa426e26ef80d997862e0", "question": "Do they report results only on English data?", "predicted_answer": " no</s>", "predicted_evidence": ["FLOAT SELECTED: Figure 4: Typical tokens obtained by different layers of the sifted multi-task learning method. In our proposed method, typical tokens are captured by shared layer (SL), selected sharing layer for fake news detection (SSLFND), selected sharing layer for stance detection (SSL-SD), private layer for fake news detection (PL-FND), and private layer for stance detection (PL-SD) respectively. A column of the same color represents the distribution of one token in different layers, while the last two columns denote unique tokens captured by different layers."], "explanation": " no</s>"}
{"question_id": "9d963d385bd495a7e193f8a498d64c1612e6c20c", "question": "Which dataset do they use?", "predicted_answer": " english penn treebank bibref40, transformed into stanford dependencies", "predicted_evidence": ["For the evaluation on English language we use the English Penn Treebank BIBREF40 , transformed into Stanford dependencies BIBREF41 with the predicted PoS tags as in BIBREF32 .", "We also use the spmrl datasets, a collection of parallel dependency and constituency treebanks for morphologically rich languages BIBREF42 . In this case, we use the predicted PoS tags provided by the organizers. We observed some differences between the constituency and dependency predicted input features provided with the corpora. For experiments where dependency parsing is the main task, we use the input from the dependency file, and the converse for constituency, for comparability with other work. d-mtl models were trained twice (one for each input), and dependency and constituent scores are reported on the model trained on the corresponding input."], "explanation": " english penn treebank bibref40, transformed into stanford dependencies "}
{"question_id": "a1a0365bf6968cbdfd1072cf3923c26250bc955c", "question": "What type of neural models are used?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["Sequence to Sequence approaches for dialogue modelling", "The problem with rule-based models was that they were often domain dependent and could not be easily ported to a new domain. They also depended on hand crafted rules which was both expensive and required domain expertise. Two factors which when combined spell doom for scalbility. All of this changed in 2015 when Vinyals et al proposed an approach BIBREF2 inspired from the recent progress in machine translation BIBREF1 . Vinyals et al used the sequence to sequence learning architecture for conversation agents. Their model was the first model which could be trained end-to-end, and could generate a new output utterance based on just the input sentence and no other hand crafted features.", "Language Model based approaches for dialogue modelling", "Though sequence-to-sequence based models have achieved a lot of success, another push in the field has been to instead train a language model over the entire dialogue as one single sequence BIBREF18 . These works argue that a language model is better suited to dialogue modeling, as it learns how the conversation evolves as information progresses."], "explanation": " unanswerable</s>"}
{"question_id": "cb4086ad022197da79f28dc609d0de90108c4543", "question": "Do they outperform current NER state-of-the-art models?", "predicted_answer": " yes</s>", "predicted_evidence": ["Experiment ::: Data", "We evaluate our model in two English NER datasets and four Chinese NER datasets.", "(1) CoNLL2003 is one of the most evaluated English NER datasets, which contains four different named entities: PERSON, LOCATION, ORGANIZATION, and MISC BIBREF34.", "(2) OntoNotes 5.0 is an English NER dataset whose corpus comes from different domains, such as telephone conversation, newswire. We exclude the New Testaments portion since there is no named entity in it BIBREF8, BIBREF7. This dataset has eleven entity names and seven value types, like CARDINAL, MONEY, LOC.", "(3) BIBREF35 released OntoNotes 4.0. In this paper, we use the Chinese part. We adopted the same pre-process as BIBREF36.", "(4) The corpus of the Chinese NER dataset MSRA came from news domain BIBREF37.", "(5) Weibo NER was built based on text in Chinese social media Sina Weibo BIBREF38, and it contained 4 kinds of entities.", "(6) Resume NER was annotated by BIBREF33.", "Experiment ::: Results on Chinese NER Datasets", "We first present our results in the four Chinese NER datasets. Since Chinese NER is directly based on the characters, it is more straightforward to show the abilities of different models without considering the influence of word representation.", "As shown in Table TABREF29, the vanilla Transformer does not perform well and is worse than the BiLSTM and CNN based models. However, when relative positional encoding combined, the performance was enhanced greatly, resulting in better results than the BiLSTM and CNN in all datasets. The number of training examples of the Weibo dataset is tiny, therefore the performance of the Transformer is abysmal, which is as expected since the Transformer is data-hungry. Nevertheless, when enhanced with the relative positional encoding and unscaled attention, it can achieve even better performance than the BiLSTM-based model. The superior performance of the adapted Transformer in four datasets ranging from small datasets to big datasets depicts that the adapted Transformer is more robust to the number of training examples than the vanilla Transformer. As the last line of Table TABREF29 depicts, the scaled attention will deteriorate the performance.", "FLOAT SELECTED: Table 2: The F1 scores on Chinese NER datasets. \u2663,\u2660 are results reported in (Zhang and Yang, 2018) and (Gui et al., 2019a), respectively. \u201cw/ scale\u201d means TENER using the scaled attention in Eq.(19). \u2217 their results are not directly comparable with ours, since they used 100d pre-trained character and bigram embeddings. Other models use the same embeddings.", "FLOAT SELECTED: Table 4: The F1 scores on English NER datasets. We only list results based on non-contextualized embeddings, and methods utilized pre-trained language models, pre-trained features, or higher dimension word vectors are excluded. TENER (Ours) uses the Transformer encoder both in the character-level and wordlevel. \u201cw/ scale\u201d means TENER using the scaled attention in Eq.(19). \u201cw/ CNN-char\u201d means TENER using CNN as character encoder instead of AdaTrans.", "In summary, to improve the performance of the Transformer-based model in the NER task, we explicitly utilize the directional relative positional encoding, reduce the number of parameters and sharp the attention distribution. After the adaptation, the performance raises a lot, making our model even performs better than BiLSTM based models. Furthermore, in the six NER datasets, we achieve state-of-the-art performance among models without considering the pre-trained language models or designed features."], "explanation": " yes</s>"}
{"question_id": "3e3d123960e40bcb1618e11999bd2031ccc1d155", "question": "What are the languages of the datasets?", "predicted_answer": " unanswerable</s>", "predicted_evidence": [], "explanation": " unanswerable</s>"}
{"question_id": "bb7c80ab28c2aebfdd0bd90b22a55dbdf3a8ed5b", "question": "What type of attention is used in the recognition system?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["To emit a character the speller uses the attention mechanism to find a set of relevant activations of the listener INLINEFORM0 and summarize them into a context INLINEFORM1 . The history of previously emitted characters is encapsulated in a recurrent state INLINEFORM2 : DISPLAYFORM0"], "explanation": " unanswerable</s>"}
{"question_id": "8276671a4d4d1fbc097cd4a4b7f5e7fadd7b9833", "question": "What deep neural network models are used in evaluation?", "predicted_answer": " neural image captioning (nic) model bibref14, image", "predicted_evidence": ["Our main goal in this section is to see if Image Captioning models could learn well with Vietnamese language. To accomplish this task, we train and evaluate our dataset with two published Image Captioning models applying encoder-decoder architecture. The models we propose are Neural Image Captioning (NIC) model BIBREF14, Image Captioning model from the Pytorch-tutorial BIBREF15 by Yunjey.", "Overall, CNN is first used for extracting image features for encoder part. The image features which are presented in vectors will be used as layers for decoding. For decoder part, RNN - LSTM are used to embed the vectors to target sentences using words/tokens provided in vocabulary."], "explanation": " neural image captioning (nic) model bibref14, image"}
{"question_id": "d211a37830c59aeab4970fdb2e03d9b7368b421c", "question": "what surface-form features were used?", "predicted_answer": " inlineform0 -grams: word inlineform1 -gram", "predicted_evidence": ["The following surface-form features were used:", "INLINEFORM0 -grams: word INLINEFORM1 -grams (contiguous sequences of INLINEFORM2 tokens), non-contiguous word INLINEFORM3 -grams ( INLINEFORM4 -grams with one token replaced by *), character INLINEFORM5 -grams (contiguous sequences of INLINEFORM6 characters), unigram stems obtained with the Porter stemming algorithm;", "General-domain word embeddings:", "dense word representations generated with word2vec on ten million English-language tweets, summed over all tokens in the tweet,", "word embeddings distributed as part of ConceptNet 5.5 BIBREF15 , summed over all tokens in the tweet;", "General-domain word clusters: presence of tokens from the word clusters generated with the Brown clustering algorithm on 56 million English-language tweets; BIBREF11", "Negation: presence of simple negators (e.g., not, never); negation also affects the INLINEFORM0 -gram features\u2014a term INLINEFORM1 becomes INLINEFORM2 if it occurs after a negator and before a punctuation mark;", "Twitter-specific features: the number of tokens with all characters in upper case, the number of hashtags, presence of positive and negative emoticons, whether the last token is a positive or negative emoticon, the number of elongated words (e.g., soooo);", "Punctuation: presence of exclamation and question marks, whether the last token contains an exclamation or question mark."], "explanation": " inlineform0 -grams: word inlineform1 -gram"}
{"question_id": "37ac705166fa87dc74fe86575bf04bea56cc4930", "question": "what evaluation metrics were used?", "predicted_answer": " mae: mean absolute error between the predicted and ground-truth", "predicted_evidence": ["Evaluation Metrics: We study the variants of the same model by training with different proportions of the negotiation seen, namely, $f \\in \\lbrace 0.0, 0.2, 0.4, 0.6, 0.8, 1.0\\rbrace $. We compare the models on two evaluation metrics: MAE: Mean Absolute Error between the predicted and ground-truth agreed prices along with Accuracy$\\pm k$: the percentage of cases where the predicted price lies within $k$ percent of the ground-truth. We use $k=5$ and $k=10$ in our experiments."], "explanation": " mae: mean absolute error between the predicted and ground-truth"}
{"question_id": "3cc0d773085dc175b85955e95911a2cfaab2cdc4", "question": "Which pairs of languages do they consider similar enough to capture phonetic structure?", "predicted_answer": " english, german and french</s>", "predicted_evidence": ["In Fig. FIGREF14 , the cosine similarities of the segment pairs get smaller as the edit distances increase, and the trend is observed in all languages. The gap between each edit distance groups, i.e. (0,1), (1,2), (2,3), (3,4), is obvious. This means that INLINEFORM0 learned from English can successfully encode the sequential phonetic structures into fixed-length vector for the target languages to some good extend even though it has never seen any audio data of the target languages. Another interesting fact is the corresponding variance between languages. In the source language, English, the variances of the five edit distance groups are fixed at 0.030, which means that the cosine similarity in each edit distance group is centralized. However, the variances of the groups in the target languages vary. In French and German, the variance grows from 0.030 to 0.060 as the edit distance increases from 0 to 4. For Czech/Spanish, the variance starts at a larger value of 0.040/0.050 and increases to 0.050/0.073. We suspect that the fluctuating variance is related to the similarity between languages. English, German and French are more similar compared with Czech and Spanish. Among the four target languages, German has the highest lexical similarity with English (0.60) and the second highest is French (0.27), while for Czech and Spanish, the lexical similarity scores is 0 BIBREF48 ."], "explanation": " english, german and french</s>"}
{"question_id": "e752dc4d721a2cf081108b6bd71e3d10b4644354", "question": "What statistical test is performed?", "predicted_answer": " t-test</s>", "predicted_evidence": ["Four sets of experiments are conducted. The first experiment compares DUPMN with other sentiment analysis methods. The second experiment evaluates the effectiveness of different hop size INLINEFORM0 of memory network. The third experiment evaluates the effectiveness of UMN and PMN in different datasets. The fourth set of experiment examines the effect of memory size INLINEFORM1 on the performance of DUPMN. Performance measures include Accuracy (ACC), Root-Mean-Square-Error (RMSE), and Mean Absolute Error (MAE) for our model. For other baseline methods in Group 2 and Group 3, their reported results are used. We also show the p-value by comparing the result of 10 random tests for both our model and the state-of-the-art model in the t-test ."], "explanation": " t-test</s>"}
{"question_id": "108f99fcaf620fab53077812e8901870896acf36", "question": "What kind of evaluations do use to evaluate dialogue?", "predicted_answer": " automatic and human evaluation.</s>", "predicted_evidence": ["Evaluating open-domain chit-chat models is challenging, especially in multiple languages and at the dialogue-level. Hence, we evaluate our models using both automatic and human evaluation. In both cases, human-annotated dialogues are used, which show the importance of the provided dataset.", "Experiments ::: Evaluation Metrics ::: Automatic", "For each language, we evaluate responses generated by the models using perplexity (ppl.) and BLEU BIBREF67 with reference to the human-annotated responses. Although these automatic measures are not perfect BIBREF68, they help to roughly estimate the performance of different models under the same test set. More recently, BIBREF69 has shown the correlation between perplexity and human judgment in open-domain chit-chat models.", "Following ACUTE-EVAL, the annotator is provided with two full dialogues made by self-chat or human-dialogue. The annotator is asked to choose which of the two dialogues is better in terms of engagingness, interestingness, and humanness. For each comparison, we sample 60\u2013100 conversations from both models. In Appendix C, we report the exact questions and instructions given to the annotators, and the user interface used in the evaluation. We hired native speakers annotators for all six considered languages. The annotators were different from the dataset collection annotators to avoid any possible bias."], "explanation": " automatic and human evaluation.</s>"}
{"question_id": "22c125c461f565f5437dac74bf19c2ef317bad86", "question": "How many users does their dataset have?", "predicted_answer": " 33,458</s>", "predicted_evidence": ["We begin with a hate speech lexicon containing words and phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%. We use the majority decision for each tweet to assign a label. Some tweets were not assigned labels as there was no majority class. This results in a sample of 24,802 labeled tweets."], "explanation": " 33,458</s>"}
{"question_id": "283d358606341c399e369f2ba7952cd955326f73", "question": "Is manual annotation performed?", "predicted_answer": " yes</s>", "predicted_evidence": ["Let INLINEFORM0 be a set of Chinese movie reviews with no categorical information. The ultimate task of movie review classification is to label them into different predefined categories as INLINEFORM1 . Starting from scratch, we need to collect such review set INLINEFORM2 from an online review website and then manually label them into generic categories INLINEFORM3 . Based on the collected dataset, we can apply natural language processing techniques to get raw text features and further learn the classifiers. In the following subsections, we will go through and elaborate all the subtasks shown in Figure FIGREF5 ."], "explanation": " yes</s>"}
{"question_id": "a926d71e6e58066d279d9f7dc3210cd43f410164", "question": "What languages were included in this multilingual population?", "predicted_answer": " unanswerable</s>", "predicted_evidence": [], "explanation": " unanswerable</s>"}
{"question_id": "d4b84f48460517bc0a6d4e0c38f6853c58081166", "question": "How do they combine the socioeconomic maps with Twitter data? ", "predicted_answer": " assigning each geolocated twitter user to a patch closest to their estimated home", "predicted_evidence": ["To overcome this limitation we combined our Twitter data with the socioeconomic maps of INSEE by assigning each geolocated Twitter user to a patch closest to their estimated home location (within 1 km). This way we obtained for all $110,369$ geolocated users their dynamical linguistic data, their egocentric social network as well as a set of SES indicators."], "explanation": " assigning each geolocated twitter user to a patch closest to their estimated home"}
{"question_id": "028d0d9b7a71133e51a14a32cd09dea1e2f39f05", "question": "How did they define standard language?", "predicted_answer": " standard language is a language that is used in a standard way.</s>", "predicted_evidence": ["The basic form of negation in French includes two negative particles: ne (no) before the verb and another particle after the verb that conveys more accurate meaning: pas (not), jamais (never), personne (no one), rien (nothing), etc. Due to this double construction, the first part of the negation (ne) is optional in spoken French, but it is obligatory in standard writing. Sociolinguistic studies have previously observed the realization of ne in corpora of recorded everyday spoken interactions. Although all the studies do not converge, a general trend is that ne realization is more frequent in speakers with higher socioeconomic status than in speakers with lower status BIBREF30 , BIBREF31 . We built upon this research to set out to detect both negation variants in the tweets using regular expressions. We are namely interested in the rate of usage of the standard negation (featuring both negative particles) across users:", "In written French, adjectives and nouns are marked as being plural by generally adding the letters s or x at the end of the word. Because these endings are mute (without counterpart in spoken French), their omission is the most frequent spelling error in adults BIBREF32 . Moreover, studies showed correlations between standard spelling and social status of the writers, in preteens, teens and adults BIBREF33 , BIBREF32 , BIBREF34 . We then set to estimate the use of standard plural across users:"], "explanation": " standard language is a language that is used in a standard way.</s>"}
{"question_id": "1d7b99646a1bc05beec633d7a3beb083ad1e8734", "question": "How is the training time compared to the original position encoding? ", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["For our machine translation experiments, the result was a modest 7% decrease in steps per second, but we were able to maintain the same model and batch sizes on P100 GPUs as Vaswani et al. vaswani2017."], "explanation": " unanswerable</s>"}
{"question_id": "179bc57b7b5231ea6ad3e93993a6935dda679fa2", "question": "Do they compare against Reinforment-Learning approaches?", "predicted_answer": " no</s>", "predicted_evidence": ["FLOAT SELECTED: Table 1: Results (F1) on CoNLL 2012 test set. CoNLL is the average of MUC, B3, and CEAFe."], "explanation": " no</s>"}
{"question_id": "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763", "question": "What dataset do they use?", "predicted_answer": " a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source", "predicted_evidence": ["For training our classification models, we use a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, provided by BIBREF23. These repositories are split into training, validation, and test splits containing 808, 265, and 264 commits, respectively. In order to minimize the occurrence of duplicate commits in two of these splits (such as in both training and test), commits from no repository belong to more than one split. However, 808 commits may not be sufficient to train deep learning models. Hence, in order to answer RQ4, we augment the training split with commits mined using regular expression matching on the commit messages from the same set of open-source Java projects. This almost doubles the number of commits in the training split to 1493. We then repeat our experiments for the first three research questions on the augmented dataset, and evaluate our trained models on the same validation and test splits."], "explanation": " a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source"}
{"question_id": "940a16e9db8be5b5f4e67d9c7622b3df99ac10a5", "question": "what domains are explored in this paper?", "predicted_answer": " unanswerable</s>", "predicted_evidence": [], "explanation": " unanswerable</s>"}
{"question_id": "d922eaa5aa135c1ae211827c6a599b4d69214563", "question": "Do they treat differerent turns of conversation differently when modeling features?", "predicted_answer": " no</s>", "predicted_evidence": ["Sentiment and objective Information (SOI)- relativity of subjectivity and sentiment with emotion are well studied in the literature. To craft these features we use SentiwordNet BIBREF5 , we create sentiment and subjective score per word in each sentences. SentiwordNet is the result of the automatic annotation of all the synsets of WORDNET according to the notions of positivity, negativity, and neutrality. Each synset s in WORDNET is associated to three numerical scores Pos(s), Neg(s), and Obj(s) which indicate how positive, negative, and objective (i.e., neutral) the terms contained in the synset are. Different senses of the same term may thus have different opinion-related properties. These scores are presented per sentence and their lengths are equal to the length of each sentence. In case that the score is not available, we used a fixed score 0.001.", "Emotion Lexicon feature (emo)- presence of emotion words is the first flag for a sentence to be emotional. We use NRC Emotion Lexicon BIBREF6 with 8 emotion tags (e.i. joy, trust, anticipation, surprise, anger, fear, sadness, disgust). We demonstrate the presence of emotion words as an 8 dimension feature, presenting all 8 emotion categories of the NRC lexicon. Each feature represent one emotion category, where 0.001 indicates of absent of the emotion and 1 indicates the presence of the emotion. The advantage of this feature is their portability in transferring emotion learning across genres."], "explanation": " no</s>"}
{"question_id": "342ada55bd4d7408e1fcabf1810b92d84c1dbc41", "question": "by how much did the performance improve?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["FLOAT SELECTED: Table 1: Summarization performance."], "explanation": " unanswerable</s>"}
{"question_id": "84765903b8c7234ca2919d0a40e3c6a5bcedf45d", "question": "Do any of the models use attention?", "predicted_answer": " yes</s>", "predicted_evidence": ["After a small grid search we decided to inherit most of the hyperparameters of the model from the best results achieved in BIBREF3 where -to-Mizar translation is learned. We used relatively small LSTM cells consisting of 2 layers with 128 units. The \u201cscaled Luong\u201d version of the attention mechanism was used, as well as dropout with rate equal $0.2$. The number of training steps was 10000. (This setting was used for all our experiments described below.)"], "explanation": " yes</s>"}
{"question_id": "df01e98095ba8765d9ab0d40c9e8ef34b64d3700", "question": "What NLI dataset was used?", "predicted_answer": " stanford natural language inference bibref7</s>", "predicted_evidence": ["The Stanford Natural Language Inference BIBREF7 dataset contains 570k human annotated hypothesis/premise pairs. This is the most widely used entailment dataset for natural language inference."], "explanation": " stanford natural language inference bibref7</s>"}
{"question_id": "f77d7cddef3e021d70e16b9e16cecfd4b8ee80d3", "question": "Which dataset do they train on?", "predicted_answer": " english-german, thai-english</s>", "predicted_evidence": ["To test out these approaches, we conduct two sets of NMT experiments: high resource (English INLINEFORM0 German) and low resource (Thai INLINEFORM1 English).", "The English-German data comes from WMT 2014. The training set has 4m sentences and we take newstest2012/newstest2013 as the dev set and newstest2014 as the test set. We keep the top 50k most frequent words, and replace the rest with UNK. The teacher model is a INLINEFORM0 LSTM (as in Luong2015) and we train two student models: INLINEFORM1 and INLINEFORM2 . The Thai-English data comes from IWSLT 2015. There are 90k sentences in the training set and we take 2010/2011/2012 data as the dev set and 2012/2013 as the test set, with a vocabulary size is 25k. Size of the teacher model is INLINEFORM3 (which performed better than INLINEFORM4 , INLINEFORM5 models), and the student model is INLINEFORM6 . Other training details mirror Luong2015."], "explanation": " english-german, thai-english</s>"}
{"question_id": "7ba6330d105f49c7f71dba148bb73245a8ef2966", "question": "what are the performance metrics?", "predicted_answer": " rouge scores</s>", "predicted_evidence": ["BIBREF6 published SumRepo, a repository of summaries for the DUC2004 dataset generated by several baseline and state-of-the-art methods . We evaluate summaries generated by a selection of these methods on the same data that we use for testing. We calculate Rouge scores with the Rouge toolkit BIBREF9 . In order to compare our results to BIBREF6 we use the same Rouge settings as they do and report results for Rouge-1, Rouge-2 and Rouge-4 recall. The baselines include a basic centroid-based model without an anti-redundancy filter and feature reduction."], "explanation": " rouge scores</s>"}
{"question_id": "1c2d4dc1e842b962c6407d6436f3dc73dd44ce55", "question": "what four learning strategies are investigated?", "predicted_answer": " model@!start@$<unk>iii<unk>_<unk>tag<unk>$@!", "predicted_evidence": ["Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{I}_{Base}$@!END@", "This is a simple but effective model for multi-domain learning, in which all domains are aggregated together and will be further used for training a set of shared parameters. Notably, domains in this model are not explicitly informed of their differences.", "We achieve this by pre-training our basic model $Model^{I}_{Base}$ with BERT BIBREF28, which is one of the most successful learning frameworks. Then we investigate if BERT can provide domain information and bring the model good domain adaptability. To avoid introducing new structures, we use the feature-based BERT with its parameters fixed.", "Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{III}_{Tag}$@!END@", "The domain type can also be introduced directly as a feature vector, which can augment learned representations with domain-aware ability.", "Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{IV}_{Meta}$@!END@", "In order to overcome the above limitations, we try to bridge the communication gap between different domains when updating shared parameters via meta-learning BIBREF33, BIBREF34, BIBREF35."], "explanation": " model@!start@$<unk>iii<unk>_<unk>tag<unk>$@!"}
{"question_id": "4a91432abe3f54fcbdd00bb85dc0df95b16edf42", "question": "How long is their dataset?", "predicted_answer": " 85.4 million tweets</s>", "predicted_evidence": ["We begin with a hate speech lexicon containing words and phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%. We use the majority decision for each tweet to assign a label. Some tweets were not assigned labels as there was no majority class. This results in a sample of 24,802 labeled tweets."], "explanation": " 85.4 million tweets</s>"}
{"question_id": "0a70af6ba334dfd3574991b1dd06f54fc6a700f2", "question": "What nuances between fake news and satire were discovered?", "predicted_answer": " satire articles are more sophisticated, or less easy to read, than fake news", "predicted_evidence": ["We addressed the challenge of identifying nuances between fake news and satire. Inspired by the humor and social message aspects of satire articles, we tested two classification approaches based on a state-of-the-art contextual language model, and linguistic features of textual coherence. Evaluation of our methods pointed to the existence of semantic and linguistic differences between fake news and satire. In particular, both methods achieved a significantly better performance than the baseline language-based method. Lastly, we studied the feature importance of our linguistic-based method to help shed light on the nuances between fake news and satire. For instance, we observed that satire articles are more sophisticated, or less easy to read, than fake news articles."], "explanation": " satire articles are more sophisticated, or less easy to read, than fake news"}
{"question_id": "8d1b6c88f06ee195d75af32ede85dbd6477c8497", "question": "What countries and languages are represented in the datasets?", "predicted_answer": " unanswerable</s>", "predicted_evidence": [], "explanation": " unanswerable</s>"}
{"question_id": "1f2952cd1dc0c891232fa678b6c219f6b4d31958", "question": "Which vocabulary size was the better performer?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["FLOAT SELECTED: Table 2: BLEU scores from German-English (De-En) and English-German (En-De) experiments on NewsTest2013 (NT13) and NewsTest2014 (NT14) along with their vocabulary sizes, class imbalance D, and mean sequence length \u00b5."], "explanation": " unanswerable</s>"}
{"question_id": "b6e97d1b1565732b1b3f1d74e6d2800dd21be37a", "question": "what was their newly established state of the art results?", "predicted_answer": " exp(4.188) [fixed,zerofill,precision=1]", "predicted_evidence": ["We compare models on three datasets. The smallest of them is the Penn Treebank corpus by BIBREF13 with preprocessing from BIBREF14 . We also include another word level corpus: Wikitext-2 by BIBREF15 . It is about twice the size of Penn Treebank with a larger vocabulary and much lighter preprocessing. The third corpus is Enwik8 from the Hutter Prize dataset BIBREF16 . Following common practice, we use the first 90 million characters for training, and the remaining 10 million evenly split between validation and test.", "We tested LSTMs of various depths and an RHN of depth 5 with parameter budgets of 10 and 24 million matching the sizes of the Medium and Large LSTMs by BIBREF18 . The results are summarised in Table TABREF9 .", "Notably, in our experiments even the RHN with only 10M parameters has better perplexity than the 24M one in the original publication. Our 24M version improves on that further. However, a shallow LSTM-based model with only 10M parameters enjoys a very comfortable margin over that, with deeper models following near the estimated noise range. At 24M, all depths obtain very similar results, reaching exp(4.065) [fixed,zerofill,precision=1] at depth 4. Unsurprisingly, NAS whose architecture was chosen based on its performance on this dataset does almost equally well, even better than in BIBREF1 .", "Wikitext-2 is not much larger than Penn Treebank, so it is not surprising that even models tuned for Penn Treebank perform reasonably on this dataset, and this is in fact how results in previous works were produced. For a fairer comparison, we also tune hyperparameters on the same dataset. In Table TABREF14 , we report numbers for both approaches. All our results are well below the previous state of the are for models without dynamic evaluation or caching. That said, our best result, exp(4.188) [fixed,zerofill,precision=1] compares favourably even to the Neural Cache BIBREF6 whose innovations are fairly orthogonal to the base model."], "explanation": " exp(4.188) [fixed,zerofill,precision=1]"}
{"question_id": "4d887ce7dc43528098e7a3d9cd13c6c36f158c53", "question": "Does the new relative position encoder require more parameters?", "predicted_answer": " no</s>", "predicted_evidence": ["The edge between input elements $x_i$ and $x_j$ is represented by vectors $a^V_{ij}, a^K_{ij} \\in \\mathbb {R}^{d_a}$ . The motivation for learning two distinct edge representations is that $a^V_{ij}$ and $a^K_{ij}$ are suitable for use in eq. ( 6 ) and eq. ( 7 ), respectively, without requiring additional linear transformations. These representations can be shared across attention heads. We use $d_a = d_z$ ."], "explanation": " no</s>"}
{"question_id": "98aa86ee948096d6fe16c02c1e49920da00e32d4", "question": "What fusion methods are applied?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["FLOAT SELECTED: FIGURE 1 | Representation of our processing pipeline. Existing methods refers to our previous work described in Papegnies et al. (2017b) (content-based method) and Papegnies et al. (2019) (graph-based method), whereas the contribution presented in this article appears on the right side (fusion strategies). Figure available at 10.6084/m9.figshare.7442273 under CC-BY license."], "explanation": " unanswerable</s>"}
{"question_id": "16cc37e4f8e2db99eaf89337a3d9ada431170d5b", "question": "how were the data instances chosen?", "predicted_answer": " twitter search api</s>", "predicted_evidence": ["Therefore, in order to target only those tweets that are rich in human activity content, we formulate a set of queries that allows us to use the Twitter Search API to find instances of users tweeting about common human activities. Each query contains a first-person, past-tense verb within a phrase that describes a common activity that people do. Using this approach, we are able to retrieve a set of tweets that contains a high concentration of human activity content, and we also find that users who wrote these tweets are much more likely to have written other tweets that describe human activities (Table TABREF1 ). We build our set of human activity queries from two sources: the Event2Mind dataset BIBREF15 and a set of short activity surveys, which we collect ourselves, to obtain nearly 30K queries (Table TABREF2 ) ."], "explanation": " twitter search api</s>"}
{"question_id": "7c2d6bc913523d77e8fdc82c60598ee95b445d84", "question": "How is subjective text automatically neutralized?", "predicted_answer": " by suggesting edits that would make it more neutral.</s>", "predicted_evidence": ["This work presents data and algorithms for automatically reducing bias in text. We focus on a particular kind of bias: inappropriate subjectivity (\u201csubjective bias\u201d). Subjective bias occurs when language that should be neutral and fair is skewed by feeling, opinion, or taste (whether consciously or unconsciously). In practice, we identify subjective bias via the method of BIBREF2: using Wikipedia's neutral point of view (NPOV) policy. This policy is a set of principles which includes \u201cavoiding stating opinions as facts\u201d and \u201cpreferring nonjudgemental language\u201d.", "We aim to debias text by suggesting edits that would make it more neutral. This contrasts with prior research which has debiased representations of text by removing dimensions of prejudice from word embeddings BIBREF3, BIBREF4 and the hidden states of predictive models BIBREF5, BIBREF6. To avoid overloading the definition of \u201cdebias,\u201d we refer to our kind of text debiasing as neutralizing that text. Figure FIGREF1 gives an example."], "explanation": " by suggesting edits that would make it more neutral.</s>"}
{"question_id": "48ff9645a506aa2c17810d2654d1f0f0d9e609ee", "question": "What downstream tasks are analyzed?", "predicted_answer": " a classification task (imdb sentiment classification - bibref13", "predicted_evidence": ["Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - BIBREF13) and a question answering task (SQuAD v1.1 - BIBREF14)."], "explanation": " a classification task (imdb sentiment classification - bibref13"}
{"question_id": "7c2c15ea3f1b1375b8aaef1103a001069d9915bb", "question": "What are the other obstacles to automatic translations which are not mentioned in the abstract?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["(1) Excessive focus on English and European languages as one of the involved languages in MT approaches and poor research on low-resource language pairs such as African and/or South American languages. (2) The limitations of SMT approaches for translating across domains. Most MT systems exhibit good performance on law and the legislative domains due to the large amount of data provided by the European Union. In contrast, translations performed on sports and life-hacks commonly fail, because of the lack of training data. (3) How to translate the huge amount of data from social networks that uniquely deal with no-standard speech texts from users (e.g., tweets). (4) The difficult translations among morphologically rich languages. This challenge shares the same problem with the first one, namely that most research work focuses on English as one of the involved languages. Therefore, MT systems which translate content between, for instance, Arabic and Spanish are rare. (5) For the speech translation task, the parallel data for training differs widely from real user speech."], "explanation": " unanswerable</s>"}
{"question_id": "230ff86b7b90b87c33c53014bb1e9c582dfc107f", "question": "What morphological typologies are considered?", "predicted_answer": " agglutinative and fusional</s>", "predicted_evidence": ["The biggest improvement over the word baseline is achieved by the models that have access to morphology for all languages (except for English) as expected. Character trigrams consistently outperformed characters by a small margin. Same pattern is observed on the results of the development set. IOW has the values between 0% to 38% while IOC values range between 2%-10% dependending on the properties of the language and the dataset. We analyze the results separately for agglutinative and fusional languages and reveal the links between certain linguistic phenomena and the IOC, IOW values."], "explanation": " agglutinative and fusional</s>"}
{"question_id": "65b39676db60f914f29f74b7c1264422ee42ad5c", "question": "what are the other methods they compare to?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["Overall prediction accuracy can be calculated by subtracting one with the average result of error rate division on each party by number of its remaining candidates. We achieve 0.548 prediction accuracy, which is not good enough BIBREF1 . The model accuracy is mainly affected by the large error rate on Democratic candidates (1.33 from 2 candidates)."], "explanation": " unanswerable</s>"}
{"question_id": "da077b385d619305033785af5b204696d6145bd8", "question": "Does the query-bag matching model use a neural network?", "predicted_answer": " no</s>", "predicted_evidence": ["In the following subsections, we will introduce our proposed Query-bag Matching (QBM) model which output is the matching probability indicating whether the query and bag are asking the same questions. The basic Q-Q (query-question) matching model hybrid CNN (hCNN) BIBREF5 is presented as the background. Then we will show the base model and its two components designed to promote the performance: Mutual Coverage and Bag Representation. For better understanding, the whole model is shown in Figure FIGREF2.", "After getting the Q-Q matching representation $r_i$, we combine the $\\lbrace r_1, \\dots , r_n\\rbrace $ by element-wise max and mean pooling in order to get $r_p$ to represent the query-bag matching representation: rp = [ max_pooling { r1, ..., rn }; mean_pooling { r1, ..., rn } ] where [;] denotes concatenation. After that, an MLP with softmax is applied to predict whether the query and the bag is asking the same question. Finally, the loss function minimizes the cross entropy of the training data. Due to the out-of-order of the bag, we do not model the bag representation by CNN or LSTM, and experiments show the pooling-based method works well."], "explanation": " no</s>"}
{"question_id": "9aabcba3d44ee7d0bbf6a2c019ab9e0f02fab244", "question": "Are there any other standard linguistic features used, other than ngrams?", "predicted_answer": " yes, bag of words and more advanced syntactic features.</s>", "predicted_evidence": ["This paper is structured as follows. In the next sections we will review related work on sentiment classification, followed by a detailed explanation of our approach and our data collection, annotation and processing efforts. After that, we describe our baseline n-gram sentiment classifier model, followed by the explanation of how the baseline model is extended to incorporate contextual information. Next, we describe our analysis of the variation of sentiment within each of the contextual categories. We then evaluate our models and finally summarize our findings and contributions and discuss possible paths for future work.", "There have been several works that do sentiment classification on Twitter using standard sentiment classification techniques, with variations of n-gram and bag of words being the most common. There have been attempts at using more advanced syntactic features as is done in sentiment classification for other domains BIBREF1 , BIBREF2 , however the 140 character limit imposed on tweets makes this hard to do as each article in the Twitter training set consists of sentences of no more than several words, many of them with irregular form BIBREF3 ."], "explanation": " yes, bag of words and more advanced syntactic features.</s>"}
{"question_id": "1128a600a813116cba9a2cf99d8568ae340f327a", "question": "What datasets do they use in the experiment?", "predicted_answer": " product reviews that collected based on the dataset, constructed by bibref27", "predicted_evidence": ["For classification task, we test our model on 16 classification datasets, the first 14 datasets are product reviews that collected based on the dataset, constructed by BIBREF27 , contains Amazon product reviews from different domains: Books, DVDs, Electronics and Kitchen and so on. The goal in each domain is to classify a product review as either positive or negative. The datasets in each domain are partitioned randomly into training data, development data and testing data with the proportion of 70%, 10% and 20% respectively. The detailed statistics are listed in Table 1 .", "The remaining two datasets are two sub-datasets about movie reviews.", "IMDB The movie reviews with labels of subjective or objective BIBREF28 .", "MR The movie reviews with two classes BIBREF29 .", "For sequence tagging task, we use the Wall Street Journal(WSJ) portion of Penn Treebank (PTB) BIBREF33 , CoNLL 2000 chunking, and CoNLL 2003 English NER datasets. The statistics of these datasets are described in Table 4 ."], "explanation": " product reviews that collected based on the dataset, constructed by bibref27"}
{"question_id": "6e3e9818551fc2f8450bbf09b0fe82ac2506bc7a", "question": "How do they prove that RNNs with arbitrary precision are as powerful as a pushdown automata?", "predicted_answer": " by proving that the output of the new rnn is equal to the summe", "predicted_evidence": ["Theorem 1.11. For any context-free language INLINEFORM0 , suppose we relabel and write INLINEFORM1 for some regular language INLINEFORM2 , whose corresponding minimum-size DFA has INLINEFORM3 states. Then there exists a simple RNN with a hidden layer of size INLINEFORM4 whose INLINEFORM5 -language is INLINEFORM6 .", "Proof. Consider the simple RNN with INLINEFORM0 as its INLINEFORM1 -language described in the proof of Theorem 1.1 and the simple RNN with INLINEFORM2 as its INLINEFORM3 -language constructed to prove Theorem 1.10. Merge the INLINEFORM4 nodes in the input layer corresponding to the input and merge the single output nodes of both RNNs. Stack the two hidden layers, and add no new edges. There were INLINEFORM5 hidden nodes in the first RNN and INLINEFORM6 in the second, so altogether the new RNN has INLINEFORM7 hidden nodes.", "The output of the new RNN is equal to the summed output of the two original RNNs, and from the proofs of Theorems 1.1 and 1.10 these outputs are always nonnegative. Thus the output of the new RNN is INLINEFORM0 if and only if the outputs of both old RNNs were INLINEFORM1 , immediately proving the theorem. INLINEFORM2", "Discussion 1.12. This result shows that simple RNNs with arbitrary precision are at least as computationally powerful as PDAs."], "explanation": " by proving that the output of the new rnn is equal to the summe"}
{"question_id": "014a3aa07686ee18a86c977bf0701db082e8480b", "question": "What does post-processing do to the output?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["Many classifiers can predict a score or confidence about the prediction. Turning this score into the prediction is usually performed by setting a threshold, such as 0 and 0.5, so labels which have a score assigned greater than that are assigned to the sample. This might be not the optimal threshold in the multi-label classification setup and there are many approaches to set it (BIBREF9). Although these methods concentrate in the sample or label, we have had good results with a much more general approach.", "As described in BIBREF1, Read and Pfahringer BIBREF10 introduce a method (referred hereinafter to as LCA) to estimate the threshold globally. Their method chooses the threshold that minimizes the difference between the label cardinality of the training set and the predicted set.", "where $LCard(D_T)$ denotes the label cardinality of training set and $LCard(H_t(D_S))$ the label cardinality of the predictions on test set if $t$ was applied as the threshold. For that the predictions need to be normalized to unity. We also tested this method not for the label cardinality over all samples and labels but only labelwise. In our implementation, the scores of the SVM were not normalized, which produced slightly different results from a normalized approach.", "For the HMC subtask B, we used a simple threshold based on the results obtained for LCA. Especially, using multiple models per node could cause a different scaling."], "explanation": " unanswerable</s>"}
{"question_id": "f3b4e52ba962a0004064132d123fd9b78d9e12e2", "question": "Which datasets do they evaluate on?", "predicted_answer": " nist06, nist02, nist03, nist04, n", "predicted_evidence": ["We use a subset of the data available for NIST OpenMT08 task . The parallel training corpus contains approximate 2 million sentence pairs. We choose NIST 2006 (NIST06) dataset as our development set, and the NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets as our test sets. We will use this dataset to evaluate the performance of our partial decoding and context-aware decoding strategy from the perspective of translation quality and latency."], "explanation": " nist06, nist02, nist03, nist04, n"}
{"question_id": "1e7e3f0f760cd628f698b73d82c0f946707855ca", "question": "do they focus on english verbs?", "predicted_answer": " yes</s>", "predicted_evidence": ["Verb Phrase Data The pattern assignment uses the phrase distribution INLINEFORM0 . To do this, we use the \u201cEnglish All\u201d dataset in Google Syntactic N-Grams. The dataset contains counted syntactic ngrams extracted from the English portion of the Google Books corpus. It contains 22,230 different verbs (without stemming), and 147,056 verb phrases. For a fixed verb, we compute the probability of phrase INLINEFORM1 by: DISPLAYFORM0"], "explanation": " yes</s>"}
{"question_id": "c7d3bccee59ab683e6bf047579bc6eab9de9d973", "question": "What deep learning models do they plan to use?", "predicted_answer": " cnns in combination with lstms</s>", "predicted_evidence": ["Our current language model approach is effective but does not account for out of vocabulary words nor long distance dependencies. CNNs in combination with LSTMs seem to be a particularly promising way to overcome these limitations (e.g., BIBREF12 ) which we will explore and compare to our existing results.", "After evaluating CNNs and LSTMs we will explore how to include domain knowledge in these models. One possibility is to create word embeddings from domain specific materials and provide those to the CNNs along with more general text. Another is to investigate the use of Tree\u2013Structured LSTMs BIBREF13 . These have the potential advantage of preserving non-linear structure in text, which may be helpful in recognizing some of the unusual variations of words and concepts that are characteristic of humor."], "explanation": " cnns in combination with lstms</s>"}
{"question_id": "25f699c7a33e77bd552782fb3886b9df9d02abb2", "question": "What sources did they get the data from?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["FLOAT SELECTED: Table 2: Data sources, prevalence and category of text"], "explanation": " unanswerable</s>"}
{"question_id": "62edffd051d056cf60e17deafcc55a8c9af398cb", "question": "What is the source of the crosslingual word embeddings?", "predicted_answer": " muse bibref17</s>", "predicted_evidence": ["Multilingual word embeddings. We use the MUSE BIBREF17 pre-trained multilingual word embeddings and keep them fixed during training."], "explanation": " muse bibref17</s>"}
{"question_id": "81e101b2c803257492d67a00e8a1d9a07cbab136", "question": "How does Overton handles contradictory or incomplete supervision data?", "predicted_answer": " overton models the sources of these labels, which may come human annotators", "predicted_evidence": ["The supervision is described under each task, e.g., there are three (conflicting) sources for the Intent task. A task requires labels at the appropriate granularity (singleton, sequence, or set) and type (multiclass or bitvector). The labels are tagged by the source that produced them: these labels may be incomplete and even contradictory. Overton models the sources of these labels, which may come human annotators, or from engineer-defined heuristics such as data augmentation or heuristic labelers. Overton learns the accuracy of these sources using ideas from the Snorkel project BIBREF1. In particular, it estimates the accuracy of these sources and then uses these accuracies to compute a probability that each training point is correct BIBREF9. Overton incorporates this information into the loss function for a task; this also allows Overton to automatically handle common issues like rebalancing classes."], "explanation": " overton models the sources of these labels, which may come human annotators"}
{"question_id": "a7313c29b154e84b571322532f5cab08e9d49e51", "question": "What is the architecture of the decoder?", "predicted_answer": " the decoder consists of $m$ layer decoder blocks. the", "predicted_evidence": ["The decoder consists of $M$ layer decoder blocks. The inputs of the decoder are the output of the encoder $H_e^M$ and the output of the previous step of the decoder $\\lbrace y_1,...,y_{t-1} \\rbrace $. The output through the $M$ layer Transformer decoder blocks is defined as", "In each step $t$, the $h_{dt}^M$ is projected to blackthe vocabulary space and the decoder outputs the highest probability token as the next token. The Transformer decoder block consists of a self-attention module, a context-attention module, and a two-layer feed-forward network."], "explanation": " the decoder consists of $m$ layer decoder blocks. the"}
{"question_id": "d976c22e9d068e4e31fb46e929023459f8290a63", "question": "Are the two paragraphs encoded independently?", "predicted_answer": " yes</s>", "predicted_evidence": ["In this method, paragraphs are encoded separately, and the concatenation of the resulted encoding is going through the classifier. First, each paragraph is encoded with LSTM. The hidden state at the end of each sentence is extracted, and the resulting matrix is going through gated CNN BIBREF1 for extraction of single encoding for each paragraph. The accuracy is barely above $50\\%$ , which depicts that this method is not very promising."], "explanation": " yes</s>"}
{"question_id": "e6bc11bd6cfd4b2138c29602b9b56fc5378a4293", "question": "what domain do the opinions fall under?", "predicted_answer": " computers, wireless routers, and speakers</s>", "predicted_evidence": ["We used Bing Liu's dataset BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 . Aspects in these review datasets were annotated manually."], "explanation": " computers, wireless routers, and speakers</s>"}
{"question_id": "98b97d24f31e9c535997e9b6cb126eb99fc72a90", "question": "What empirical evaluation was used?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["With regard to research question RQ2 on the understanding of semantic and linguistic nuances between fake news and satire - a key advantage of studying the coherence metrics is explainability. While the pre-trained model of BERT gives the best result, it is not easily interpretable. The coherence metrics allow us to study the differences between fake news and satire in a straightforward manner."], "explanation": " unanswerable</s>"}
{"question_id": "3d662fb442d5fc332194770aac835f401c2148d9", "question": "Do they report results only on English data?", "predicted_answer": " yes</s>", "predicted_evidence": ["We use 303 sub areas from Stack Exchange data dumps. The full list of area names is in the appendix. We do not include Stack Overflow because it is too specific to programming related questions. We also exclude all questions under the following language sub areas: Chinese, German, Spanish, Russian, Japanese, Korean, Latin, Ukrainian. This ensures that the questions in MQR are mostly English sentences. Having questions from 303 Stack Exchange sites makes the MQR dataset cover a broad range of domains."], "explanation": " yes</s>"}
{"question_id": "d9949dd4865e79c53284932d868ca8fd10d55e70", "question": "Did they use crowdsourcing for the annotations?", "predicted_answer": " yes</s>", "predicted_evidence": ["Based on explicit annotation guidelines written in Greek and our proposal of the definition of offensive language, a team of three volunteers were asked to classify each tweet found in the dataset with one of the following tags: Offensive, Not Offensive and Spam, which was introduced to filter out spam from the dataset. Inter-annotator agreement was subsequently calculated and labels with 100% agreement were deemed acceptable annotations. In cases of disagreement, labels with majority agreement above 66% were selected as the actual annotations of the tweets in question. For labels with complete disagreement between annotators, one of the authors of this paper reviewed the tweets with two extra human judges, to get the desired majority agreement above 66%. Figure FIGREF6 is a confusion matrix that shows the inter-annotator agreement or reliability, statistically measured by Cohen's kappa coefficient. The benchmark annotated dataset produced contained 4,779 tweets, containing over 29% offensive content. The final distribution of labels in the new Offensive Greek Tweet Dataset (OGTD), along with the breakdown of the data into training and testing, is showing in Table TABREF5."], "explanation": " yes</s>"}
{"question_id": "cf82251a6a5a77e29627560eb7c05c3eddc20825", "question": "How does lattice rescoring improve inference?", "predicted_answer": " the rescoring procedure improves winomt accuracy by up to 30% with", "predicted_evidence": ["We further show that regularised adaptation with EWC can reduce bias while limiting degradation in general translation quality. We also present a lattice rescoring procedure in which initial hypotheses produced by the biased baseline system are transduced to create gender-inflected search spaces which can be rescored by the adapted model. We believe this approach, rescoring with models targeted to remove bias, is novel in NMT. The rescoring procedure improves WinoMT accuracy by up to 30% with no decrease in BLEU on the general test set."], "explanation": " the rescoring procedure improves winomt accuracy by up to 30% with"}
{"question_id": "b54525a0057aa82b73773fa4dacfd115d8f86f1c", "question": "What are the two applications of neuro-symbolism?", "predicted_answer": " learning a knowledge graph embedding space for context understanding in automotive", "predicted_evidence": ["Applications of Neuro-symbolism ::: Application I: Learning a Knowledge Graph Embedding Space for Context Understanding in Automotive Driving Scenes ::: Introduction", "Applications of Neuro-symbolism ::: Application II: Neural Question-Answering using Commonsense Knowledge Bases ::: Introduction"], "explanation": " learning a knowledge graph embedding space for context understanding in automotive"}
{"question_id": "935873b97872820b7b6100d6a785fba286b94900", "question": "On which datasets does LadaBERT achieve state-of-the-art?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["FLOAT SELECTED: Table 3: Performance comparison on various model sizes"], "explanation": " unanswerable</s>"}
{"question_id": "d1d59bca40b8b308c0a35fed1b4b7826c85bc9f8", "question": "How many languages do they at most attempt to use to generate discourse relation labelled data?", "predicted_answer": " three</s>", "predicted_evidence": ["FLOAT SELECTED: Figure 2: Numbers of implicit discourse relation instances from different agreements of explicit instances in three back-translations. En-Fr denotes instances that are implicit in English but explicit in back-translation of French, same for En-De and En-Cz. The overlap means they share the same relational arguments. The numbers under \u201cTwo-Votes\u201d and \u201cThree-Votes\u201d are the numbers of discourse relation agreement / disagreement between explicits in back-translations of two or three languages."], "explanation": " three</s>"}
{"question_id": "595fe416a100bc7247444f25b11baca6e08d9291", "question": "What profile features are used?", "predicted_answer": " native language identification (nli) stylistic</s>", "predicted_evidence": ["Recent years have seen a large increase in the amount of disinformation and fake news spread on social media. False information was used to spread fear and anger among people, which in turn, provoked crimes in some countries. The US in the recent years experienced many similar cases during the presidential elections, such as the one commonly known as \u201cPizzagate\" . Later on, Twitter declared that they had detected a suspicious campaign originated in Russia by an organization named Internet Research Agency (IRA), and targeted the US to affect the results of the 2016 presidential elections. The desired goals behind these accounts are to spread fake and hateful news to further polarize the public opinion. Such attempts are not limited to Twitter, since Facebook announced in mid-2019 that they detected a similar attempt originating from UAE, Egypt and Saudi Arabia and targeting other countries such as Qatar, Palestine, Lebanon and Jordan. This attempt used Facebook pages, groups, and user accounts with fake identities to spread fake news supporting their ideological agendas. The automatic detection of such attempts is very challenging, since the true identity of these suspicious accounts is hidden by imitating the profiles of real persons from the targeted audience; in addition, sometimes they publish their suspicious idea in a vague way through their tweets' messages.", "As Twitter declared, although the IRA campaign was originated in Russia, it has been found that IRA trolls concealed their identity by tweeting in English. Furthermore, for any possibility of unmasking their identity, the majority of IRA trolls changed their location to other countries and the language of the Twitter interface they use. Thus, we propose the following features to identify these users using only their tweets text:", "Native Language Identification (NLI): This feature was inspired by earlier works on identifying native language of essays writers BIBREF22. We aim to detect IRA trolls by identifying their way of writing English tweets. As shown in BIBREF19, English tweets generated by non-English speakers have a different syntactic pattern . Thus, we use state-of-the-art NLI features to detect this unique pattern BIBREF23, BIBREF24, BIBREF25; the feature set consists of bag of stopwords, Part-of-speech tags (POS), and syntactic dependency relations (DEPREL). We extract the POS and the DEPREL information using spaCy, an off-the-shelf POS tagger. We clean the tweets from the special characters and maintained dots, commas, and first-letter capitalization of words. We use regular expressions to convert a sequence of dots to a single dot, and similarly for sequence of characters.", "Stylistic: We extract a set of stylistic features following previous works in the authorship attribution domain BIBREF27, BIBREF28, BIBREF29, such as: the count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions. In addition, we extract the uppercase ratio and the tweet length."], "explanation": " native language identification (nli) stylistic</s>"}
{"question_id": "998fa38634000f2d7b52d16518b9e18e898ce933", "question": "Does the SESAME dataset include discontiguous entities?", "predicted_answer": " yes</s>", "predicted_evidence": ["The next step consists of detecting mentions to entities in the raw text. To do this, we tag character segments that exactly match one of the known names of an entity. For instance, we can tag two different entities in the following text:"], "explanation": " yes</s>"}
{"question_id": "756a8a9125e6984e0ca768b653c6c760efa3db66", "question": "What was their accuracy score?", "predicted_answer": " 95.6%</s>", "predicted_evidence": ["This section provides a summary of the evaluation of KALM and KALM-QA, where KALM is evaluated for knowledge authoring and KALM-QA is evaluated for question answering. We have created a total of 50 logical frames, mostly derived from FrameNet but also some that FrameNet is missing (like Restaurant, Human_Gender) for representing the meaning of English sentences. Based on the 50 frames, we have manually constructed 250 sentences that are adapted from FrameNet exemplar sentences and evaluate these sentences on KALM, SEMAFOR, SLING, and Stanford KBP system. KALM achieves an accuracy of 95.6%\u2014much higher than the other systems.", "For KALM-QA, we evaluate it on two datasets. The first dataset is manually constructed general questions based on the 50 logical frames. KALM-QA achieves an accuracy of 95% for parsing the queries. The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy\u2014much higher than the state-of-the-art machine learning approach BIBREF14 . Details of the evaluations can be found in BIBREF5 and BIBREF6 ."], "explanation": " 95.6%</s>"}
{"question_id": "55612e92791296baf18013d2c8dd0474f35af770", "question": "What questions were asked in the annotation process?", "predicted_answer": " first identify the entities occurring in the dataset and, then, annotate them with", "predicted_evidence": ["In the framework of the challenge, we were required to first identify the entities occurring in the dataset and, then, annotate them with of the 13 possible types. Table TABREF12 provides a description for each type of entity that we made available both to the annotators and to the participants of the challenge."], "explanation": " first identify the entities occurring in the dataset and, then, annotate them with"}
{"question_id": "d5e716c1386b6485e63075e980f80d44564d0aa2", "question": "What patterns were extracted which were correlated with factual arguments?", "predicted_answer": " topic-specific terminology, explanatory language, and argument phrases</s>", "predicted_evidence": ["Table TABREF13 provides examples of patterns learned for each class that are characteristic of that class. We observe that patterns associated with factual arguments often include topic-specific terminology, explanatory language, and argument phrases. In contrast, the patterns associated with feeling based arguments are often based on the speaker's own beliefs or claims, perhaps assuming that they themselves are credible BIBREF20 , BIBREF24 , or they involve assessment or evaluations of the arguments of the other speaker BIBREF29 . They are typically also very creative and diverse, which may be why it is hard to get higher accuracies for feeling classification, as shown by Table TABREF11 ."], "explanation": " topic-specific terminology, explanatory language, and argument phrases</s>"}
{"question_id": "b9a3836cff16af7454c7a8b0e5ff90206d0db1f5", "question": "Which of their proposed domain adaptation methods proves best overall?", "predicted_answer": " evo-devo mechanism</s>", "predicted_evidence": ["We ran our Evo-Devo mechanism with the 486 ASR sentences (see Table TABREF14 ) and measured the accuracy after each repair. On an average we have achieved about 5 to 10% improvements in the accuracy of the sentences. Fine-tuning the repair and fitness functions, namely Equation (), would probably yield much better performance accuracies. However, experimental results confirm that the proposed Evo-Devo mechanism is an approach that is able to adapt INLINEFORM0 to get closer to INLINEFORM1 . We present a snapshot of the experiments with Google ASR (Ga) and calculate accuracy with respect to the user spoken question as shown in Table TABREF16 .", "In the machine learning technique of adaptation, we considers INLINEFORM0 pairs as the predominant entity and tests the accuracy of classification of errors.", "The combination of features INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , INLINEFORM3 , INLINEFORM4 namely, (bag of consonants, bag of vowels, left context, number of words, right context) gave the best results with INLINEFORM5 % improvement in accuracy in classification over 10-fold validation."], "explanation": " evo-devo mechanism</s>"}
{"question_id": "622efbecd9350a0f4487bdff2b8b362ef2541f3c", "question": "did the top teams experiment with lexicons?", "predicted_answer": " yes</s>", "predicted_evidence": [], "explanation": " yes</s>"}
{"question_id": "c8b2fb9e0d5fb9014a25b88d559d93b6dceffbc0", "question": "How many instances are explored in the few-shot experiments?", "predicted_answer": " unanswerable</s>", "predicted_evidence": [], "explanation": " unanswerable</s>"}
{"question_id": "90eeb1b27f84c83ffcc8a88bc914a947c01a0c8b", "question": "What is the previous state-of-the-art?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["The state-of-the-art methods for this task can be roughly categorized into two streams. The first is based on semantic parsing BIBREF3 , BIBREF4 , which typically learns a grammar that can parse natural language to a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem BIBREF4 , BIBREF5 , BIBREF6 .", "On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 or distributed representations BIBREF11 , BIBREF12 . Designing large training datasets for these methods is relatively easy BIBREF7 , BIBREF13 , BIBREF14 . These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North America because of the lack of sophisticated representation for the mathematical function highest. To select the correct answer, one has to retrieve all the heights of the mountains, and sort them in descending order, and then pick the first entry. We propose a method based on textual evidence which can answer such questions without solving the mathematic functions implicitly.", "We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answering BIBREF16 , BIBREF12 , and the success of syntactic dependencies for relation extraction BIBREF17 , BIBREF18 , we propose a Multi-Channel Convolutional Neural Network (MCCNN) which could exploit both syntactic and sentential information for relation extraction.", "FLOAT SELECTED: Table 1: Results on the test set."], "explanation": " unanswerable</s>"}
{"question_id": "8cf52ba480d372fc15024b3db704952f10fdca27", "question": "what are the baselines?", "predicted_answer": " esim</s>", "predicted_evidence": ["In this paper, we generate word embedding vectors on the training corpus based on word2vec BIBREF9 . Then we propose an algorithm to combine the generated one with the pre-trained word embedding vectors on a large general text corpus based on vector concatenation. The new word representation maintains information learned from both general text corpus and task-domain. The nice property of the algorithm is simplicity and little extra computational cost will be added. It can address word out-of-vocabulary issue effectively. This method can be applied to most NLP deep neural network models and is language-independent. We integrated our methods with ESIM(baseline model) BIBREF10 . The experimental results have shown that the proposed method has significantly improved the performance of original ESIM model and obtained state-of-the-art results on both Ubuntu Dialogue Corpus and Douban Conversation Corpus BIBREF11 . On Ubuntu Dialogue Corpus (V2), the improvement to the previous best baseline model (single) on INLINEFORM0 is 3.8% and our ensemble model on INLINEFORM1 is 75.9%. On Douban Conversation Corpus, the improvement to the previous best model (single) on INLINEFORM2 is 3.6%."], "explanation": " esim</s>"}
{"question_id": "fa800a21469a70fa6490bfc67cabdcc8bf086fb5", "question": "Is the dataset used in other work?", "predicted_answer": " yes</s>", "predicted_evidence": ["We experiment with the dataset of Waseem and Hovy c53cecce142c48628b3883d13155261c, containing tweets manually annotated for hate speech. The authors retrieved around $136k$ tweets over a period of two months. They bootstrapped their collection process with a search for commonly used slurs and expletives related to religious, sexual, gender and ethnic minorities. From the results, they identified terms and references to entities that frequently showed up in hateful tweets. Based on this sample, they used a public Twitter api to collect the entire corpus of ca. $136k$ tweets. After having manually annotated a randomly sampled subset of $16,914$ tweets under the categories racism, sexism or none themselves, they asked an expert to review their annotations in order to mitigate against any biases. The inter-annotator agreement was reported at $\\kappa =0.84$ , with a further insight that $85\\%$ of all the disagreements occurred in the sexism class.", "The dataset was released as a list of $16,907$ tweet IDs along with their corresponding annotations. Using python's Tweepy library, we could only retrieve $16,202$ of the tweets since some of them have now been deleted or their visibility limited. Of the ones retrieved, 1,939 (12%) are labelled as racism, 3,148 (19.4%) as sexism, and the remaining 11,115 (68.6%) as none; this distribution follows the original dataset very closely (11.7%, 20.0%, 68.3%)."], "explanation": " yes</s>"}
{"question_id": "88d1bd21b53b8be4f9d3cb26ecc3cbcacffcd63e", "question": "Do they authors offer any hypothesis for why the parameters of Zipf's law and Heaps' law differ on Twitter?", "predicted_answer": " no</s>", "predicted_evidence": [], "explanation": " no</s>"}
{"question_id": "3f717e6eceab0a066af65ddf782c1ebc502c28c0", "question": "Which datasets do they evaluate on?", "predicted_answer": " figer (gold) bibref0 and bbn bibre", "predicted_evidence": ["We use two datasets: FIGER (GOLD) BIBREF0 and BBN BIBREF5. The sizes of their tag sets are 113 and 47, respectively. FIGER (GOLD) allows mentions to have multiple type paths, but BBN does not. Another commonly used dataset, OntoNotes BIBREF1, is not used since it contains many pronoun and common noun phrase mentions such as \u201cit,\u201d \u201che,\u201d \u201ca thrift institution,\u201d which are not suitable to directly apply entity linking on."], "explanation": " figer (gold) bibref0 and bbn bibre"}
{"question_id": "ebb4db9c24aa36db9954dd65ea079a798df80558", "question": "What of the two strategies works best?", "predicted_answer": " pkd-skip</s>", "predicted_evidence": ["We further investigate the performance gain from two different patient teacher designs: PKD-Last vs. PKD-Skip. Results of both PKD variants on the GLUE benchmark (with BERT$_6$ as the student) are summarized in Table TABREF23. Although both strategies achieved improvement over the vanilla KD baseline (see Table TABREF16), PKD-Skip performs slightly better than PKD-Last. Presumably, this might be due to the fact that distilling information across every $k$ layers captures more diverse representations of richer semantics from low-level to high-level, while focusing on the last $k$ layers tends to capture relatively homogeneous semantic information."], "explanation": " pkd-skip</s>"}
{"question_id": "8c288120139615532838f21094bba62a77f92617", "question": "How long are the datasets?", "predicted_answer": " 5130</s>", "predicted_evidence": ["We used the Mboshi5K corpus BIBREF13 as a test set for all the experiments reported here. Mboshi (Bantu C25) is a typical Bantu language spoken in Congo-Brazzaville. It is one of the languages documented by the BULB (Breaking the Unwritten Language Barrier) project BIBREF14 . This speech dataset was collected following a real language documentation scenario, using Lig_Aikuma, a mobile app specifically dedicated to fieldwork language documentation, which works both on android powered smartphones and tablets BIBREF15 . The corpus is multilingual (5130 Mboshi speech utterances aligned to French text) and contains linguists' transcriptions in Mboshi (in the form of a non-standard graphemic form close to the language phonology). It is also enriched with automatic forced-alignment between speech and transcriptions. The dataset is made available to the research community. More details on this corpus can be found in BIBREF13 ."], "explanation": " 5130</s>"}
{"question_id": "d5d48b812576470edbf978fc18c00bd24930a7b7", "question": "How many different sentence constructions are translated in gender neutral languages?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["FLOAT SELECTED: Table 3: Templates used to infer gender biases in the translation of job occupations and adjectives to the English language."], "explanation": " unanswerable</s>"}
{"question_id": "5206b6f40a91fc16179829041c1139a6c6d91ce7", "question": "what manual evaluation is presented?", "predicted_answer": " grammatical correctness and the sentence transitions</s>", "predicted_evidence": ["In this section we present the results from our experiments. Table TABREF15 shows the results from the manual evaluation. Note that each criterion scores between 0 and 3 which leads to a maximum total score of 12. The achieved total score range from 5 to 10 with an average of 8.1. In particular, the grammatical correctness and the sentence transitions were very good. Each of them scored on average 2.3 out of 3. The speech content yielded the lowest scores. This indicates that the topic model may need some improvement.", "FLOAT SELECTED: Table 6: Results from manual evaluation"], "explanation": " grammatical correctness and the sentence transitions</s>"}
{"question_id": "9baca9bdb8e7d5a750f8cbe3282beb371347c164", "question": "How do they preprocess Tweets?", "predicted_answer": " removed re-tweets by default. removed any expression", "predicted_evidence": ["To obtain meaningful linguistic data we pre-processed the incoming tweet streams in several ways. As our central question here deals with language semantics of individuals, re-tweets do not bring any additional information to our study, thus we removed them by default. We also removed any expressions considered to be semantically meaningless like URLs, emoticons, mentions of other users (denoted by the @ symbol) and hashtags (denoted by the # symbol) to simplify later post-processing. In addition, as a last step of textual pre-processing, we downcased and stripped the punctuation from the text of every tweet."], "explanation": " removed re-tweets by default. removed any expression"}
{"question_id": "f8bba20d1781ce2b14fad28d6eff024e5a6c2c02", "question": "How do they measure topic quality?", "predicted_answer": " normalised pointwise mutual information (npmi) bibref31,", "predicted_evidence": ["We further evaluate the semantic coherence of the words in a topic learnt by LDA, PTM, DMR, LF-LDA, WF-LDA, GPU-DMM and MetaLDA. Here we use the Normalised Pointwise Mutual Information (NPMI) BIBREF31 , BIBREF32 to calculate topic coherence score for topic INLINEFORM0 with top INLINEFORM1 words: INLINEFORM2 , where INLINEFORM3 is the probability of word INLINEFORM4 , and INLINEFORM5 is the joint probability of words INLINEFORM6 and INLINEFORM7 that co-occur together within a sliding window. Those probabilities were computed on an external large corpus, i.e., a 5.48GB Wikipedia dump in our experiments. The NPMI score of each topic in the experiments is calculated with top 10 words ( INLINEFORM8 ) by the Palmetto package. Again, we report the average scores and the standard deviations over 5 random runs."], "explanation": " normalised pointwise mutual information (npmi) bibref31,"}
{"question_id": "8e9561541f2e928eb239860c2455a254b5aceaeb", "question": "What language pairs are affected?", "predicted_answer": " english and japanese</s>", "predicted_evidence": ["M-Bert's ability to transfer between languages that are written in different scripts, and thus have effectively zero lexical overlap, is surprising given that it was trained on separate monolingual corpora and not with a multilingual objective. To probe deeper into how the model is able to perform this generalization, Table TABREF14 shows a sample of pos results for transfer across scripts.", "Among the most surprising results, an M-Bert model that has been fine-tuned using only pos-labeled Urdu (written in Arabic script), achieves 91% accuracy on Hindi (written in Devanagari script), even though it has never seen a single pos-tagged Devanagari word. This provides clear evidence of M-Bert's multilingual representation ability, mapping structures onto new vocabularies based on a shared representation induced solely from monolingual language model training data.", "However, cross-script transfer is less accurate for other pairs, such as English and Japanese, indicating that M-Bert's multilingual representation is not able to generalize equally well in all cases. A possible explanation for this, as we will see in section SECREF18 , is typological similarity. English and Japanese have a different order of subject, verb and object, while English and Bulgarian have the same, and M-Bert may be having trouble generalizing across different orderings.", "Generalizing across typological features ", "Table TABREF20 shows macro-averaged pos accuracies for transfer between languages grouped according to two typological features: subject/object/verb order, and adjective/noun order BIBREF11 . The results reported include only zero-shot transfer, i.e. they do not include cases training and testing on the same language. We can see that performance is best when transferring between languages that share word order features, suggesting that while M-Bert's multilingual representation is able to map learned structures onto new vocabularies, it does not seem to learn systematic transformations of those structures to accommodate a target language with different word order.", "Our results show that M-Bert is able to perform cross-lingual generalization surprisingly well. More importantly, we present the results of a number of probing experiments designed to test various hypotheses about how the model is able to perform this transfer. Our experiments show that while high lexical overlap between languages improves transfer, M-Bert is also able to transfer between languages written in different scripts\u2014thus having zero lexical overlap\u2014indicating that it captures multilingual representations. We further show that transfer works best for typologically similar languages, suggesting that while M-Bert's multilingual representation is able to map learned structures onto new vocabularies, it does not seem to learn systematic transformations of those structures to accommodate a target language with different word order."], "explanation": " english and japanese</s>"}
{"question_id": "eafea4a24d103fdecf8f347c7d84daff6ef828a3", "question": "Which training dataset do they use?", "predicted_answer": " 20 newsgroups, a cleansed version (also called v2) of", "predicted_evidence": ["To assess the performance of binary paragraph vectors, we carried out experiments on three datasets: 20 Newsgroups, a cleansed version (also called v2) of Reuters Corpus Volume 1 (RCV1) and English Wikipedia. As paragraph vectors can be trained with relatively large vocabularies, we did not perform any stemming of the source text. However, we removed stop words as well as words shorter than two characters and longer than 15 characters. Results reported by BIBREF15 indicate that performance of PV-DBOW can be improved by including n-grams in the model. We therefore evaluated two variants of Binary PV-DBOW: one predicting words in documents and one predicting words and bigrams. Since 20 Newsgroups is a relatively small dataset, we used all words and bigrams from its documents. This amounts to a vocabulary with slightly over one million elements. For the RCV1 dataset we used words and bigrams with at least 10 occurrences in the text, which gives a vocabulary with approximately 800 thousands elements. In case of English Wikipedia we used words and bigrams with at least 100 occurrences, which gives a vocabulary with approximately 1.5 million elements.", "In this work we present Binary Paragraph Vector models, an extensions to PV-DBOW and PV-DM that learn short binary codes for text documents. One inspiration for binary paragraph vectors comes from a recent work by BIBREF11 on learning binary codes for images. Specifically, we introduce a sigmoid layer to the paragraph vector models, and train it in a way that encourages binary activations. We demonstrate that the resultant binary paragraph vectors significantly outperform semantic hashing codes. We also evaluate binary paragraph vectors in transfer learning settings, where training and inference are carried out on unrelated text corpora. Finally, we study models that simultaneously learn short binary codes for document filtering and longer, real-valued representations for ranking. While BIBREF11 employed a supervised criterion to learn image codes, binary paragraph vectors remain unsupervised models: they learn to predict words in documents."], "explanation": " 20 newsgroups, a cleansed version (also called v2) of"}
{"question_id": "3e4e415e346a313f5a7c3764fe0f51c11f51b071", "question": "What language is the model tested on?", "predicted_answer": " unanswerable</s>", "predicted_evidence": [], "explanation": " unanswerable</s>"}
{"question_id": "3a25f82512d56d9e1ffba72f977f515ae3ba3cca", "question": "Does programme plans gathering and open sourcing some large dataset for Icelandic language?", "predicted_answer": " yes</s>", "predicted_evidence": ["As mentioned above, a number of language resources have been made available at the repository m\u00e1lf\u00f6ng. Most of these are now also available at the CLARIN-IS website and will be integrated into the CLARIN Virtual Language Observatory. Below we give a brief and non-exhaustive overview of language resources for Icelandic which will be developed in the programme.", "We will update the IGC with new data from more sources and continue collecting data from rights holders who have given their permission for using their material. A new version will be released each year during the five-year programme.", "Treebanks. The largest of the syntactically parsed treebanks that exist is the Icelandic Parsed Historical Corpus (IcePaHC; Wallenberg et al., 2011; R\u00f6gnvaldsson et al., 2011, 2012), which contains one million words from the 12th to the 21st century. The scheme used for the syntactic annotation is based on the Penn Parsed Corpora of Historical English BIBREF24, BIBREF25. On the other hand, no Universal Dependencies (UD)-treebanks are available for Icelandic. Within the programme, a UD-treebank will by built, based on IcePaHC, and extended with new material.", "Morphological database. The Database of Icelandic Morphology (DIM; Bjarnad\u00f3ttir et al., 2019) contains inflectional paradigms of about 287,000 lemmas. A part of the database, DMII-Core, only includes data in a prescriptive context and is suited for language learners, creating teaching material and other prescriptive uses. It consists of the inflection of approx. 50,000 words. We will extend it by reviewing ambiguous inflection forms. We will define format for data publication as the core will be available for use by a third party. For the sake of simplifying the process of adding material to the database and its maintenance, we will take advantage of the lexicon acquisition tool described in Section SECREF16 and adapt it for DIM."], "explanation": " yes</s>"}
{"question_id": "57ee20f494d8ce3fae46028c3f3551d180dba3e0", "question": "What languages do they evaluate their methods on?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["trusted resources: We randomly collected 200 news articles about broad topics commonly reported by news wires over our target time period. These range from news about celebrities and disasters to financial and political affairs as seen in table 1 . Since we operate on Chinese social media, we gathered news articles from Xinhua News Agency, the leading news-wire in China. To ensure a fair evaluation, we collected the news articles before judging rumours, not knowing which rumours we would find later on. We also only consider news articles published before the timestamps of the social media messages.", "For our social media stream, we chose Sina Weibo, a Chinese social media service with more than 200 million active users. Micro-blogs from Sina Weibo are denoted as 'weibos'."], "explanation": " unanswerable</s>"}
{"question_id": "2ddfb40a9e73f382a2eb641c8e22bbb80cef017b", "question": "What datasets did they use?", "predicted_answer": " conll-2002 and -2003 sets, containing dutch, spanish,", "predicted_evidence": ["We perform ner experiments on two datasets: the publicly available CoNLL-2002 and -2003 sets, containing Dutch, Spanish, English, and German BIBREF5 , BIBREF6 ; and an in-house dataset with 16 languages, using the same CoNLL categories. Table TABREF4 shows M-Bert zero-shot performance on all language pairs in the CoNLL data.", "We perform pos experiments using Universal Dependencies (UD) BIBREF7 data for 41 languages. We use the evaluation sets from BIBREF8 . Table TABREF7 shows M-Bert zero-shot results for four European languages. We see that M-Bert generalizes well across languages, achieving over INLINEFORM0 accuracy for all pairs.", "We sample 5000 pairs of sentences from WMT16 BIBREF14 and feed each sentence (separately) to M-Bert with no fine-tuning. We then extract the hidden feature activations at each layer for each of the sentences, and average the representations for the input tokens except [cls] and [sep], to get a vector for each sentence, at each layer INLINEFORM0 , INLINEFORM1 . For each pair of sentences, e.g. INLINEFORM2 , we compute the vector pointing from one to the other and average it over all pairs: INLINEFORM3 , where INLINEFORM4 is the number of pairs. Finally, we translate each sentence, INLINEFORM5 , by INLINEFORM6 , find the closest German sentence vector, and measure the fraction of times the nearest neighbour is the correct pair, which we call the \u201cnearest neighbor accuracy\u201d."], "explanation": " conll-2002 and -2003 sets, containing dutch, spanish,"}
{"question_id": "641fe5dc93611411582e6a4a0ea2d5773eaf0310", "question": "What does it mean for sentences to be \"lexically overlapping\"?", "predicted_answer": " share words</s>", "predicted_evidence": ["Explanations for a given question here take the form of a list of sentences, where each sentence is a reference to a specific table row in the table store. To increase their utility for knowledge and inference analyses, we require that each sentence in an explanation be explicitly lexically connected (i.e. share words) with either the question, answer, or other sentences in the explanation. We call this lexically-connected set of sentences an explanation graph."], "explanation": " share words</s>"}
{"question_id": "c25014b7e57bb2949138d64d49f356d69838bc25", "question": "What is the baseline?", "predicted_answer": " rule-based unigram matching</s>", "predicted_evidence": ["We also implement a rule-based unigram matching baseline, which takes the entry with highest unigram overlap with the query string to be the top match. This model only returns the top match, so only top-1 recall is evaluated, and top-3 recall is not applicable. Both neural models outperform the baseline, but by far the best performing model is BERT with fine-tuning, which retrieves the correct match for nearly 90% of queries (Table TABREF21)."], "explanation": " rule-based unigram matching</s>"}
{"question_id": "9595fdf7b51251679cd39bc4f6befc81f09c853c", "question": "How is the remotely sensed data annotated?", "predicted_answer": " we randomly selected a sample of inlineform1 locations and involved architects to assign", "predicted_evidence": ["Next we aimed to estimate SES from architectural/urban features associated to the home locations. Thus, for each home location we collected two additional satellite views at different resolutions as well as six Street View images, each with a horizontal view of approximately INLINEFORM0 . We randomly selected a sample of INLINEFORM1 locations and involved architects to assign a SES score (from 1 to 9) to a sample set of selected locations based on the satellite and Street View around it (both samples had 333 overlapping locations). For validation, we took users from each annotated SES class and computed the distribution of their incomes inferred from the IRIS census data (see Section SECREF6 ). Violin plots in Fig. FIGREF12 d show that in expert annotated data, as expected, the inferred income values were positively correlated with the annotated SES classes. Labels were then categorized into two socioeconomic classes for comparison purposes. All in all, both annotators assigned the same label to the overlapping locations in INLINEFORM2 of samples."], "explanation": " we randomly selected a sample of inlineform1 locations and involved architects to assign"}
{"question_id": "6df57a21ca875e63fb39adece6a9ace5bb2b2cfa", "question": "How does labeling scheme look like?", "predicted_answer": " abb: represents abbreviations such as trl representing technology readiness level", "predicted_evidence": ["SE concepts are less ambiguous as compared to generic natural language text. A word usually means one concept. For example, the word `system' usually means the same when referring to a `complex system', `system structure', or `management system' in the SE domain. In generic text, the meaning of terms like `evaluation', `requirement', or `analysis' may contextually differ. We would like domain specific phrases such as `system evaluation', `performance requirement', or `system analysis' to be single entities. Based on the operational and system concepts described in BIBREF0, we carefully construct a set of concept-labels for the SE handbook which is shown in the next section.", "CONCEPT RECOGNITION ::: BIO Labelling Scheme", "abb: represents abbreviations such as TRL representing Technology Readiness Level.", "grp: represents a group of people or an individual such as Electrical Engineers, Systems Engineers or a Project Manager.", "syscon: represents any system concepts such as engineering unit, product, hardware, software, etc. They mostly represent physical concepts.", "opcon: represents operational concepts such as decision analysis process, technology maturity assessment, system requirements review, etc.", "seterm: represents generic terms that are frequently used in SE text and those that do not fall under syscon or opcon such as project, mission, key performance parameter, audit etc.", "event: represents event-like information in SE text such as Pre-Phase A, Phase A, Phase B, etc.", "org: represents an organization such as `NASA', `aerospace industry', etc.", "art: represents names of artifacts or instruments such as `AS1300'", "cardinal: represents numerical values such as `1', `100', 'one' etc.", "loc: represents location-like entities such as component facilities or centralized facility.", "mea: represents measures, features, or behaviors such as cost, risk, or feasibility."], "explanation": " abb: represents abbreviations such as trl representing technology readiness level"}
{"question_id": "9240ee584d4354349601aeca333f1bc92de2165e", "question": "What is the agreement of the dataset?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["The intra-annotator reliability of both datasets $D_\\textrm {\\textit {LR}}$ and $D_\\textrm {\\textit {HR}}$ is shown in Table TABREF21. The reliability was calculated using the Krippendorff's alpha coefficient. Krippendorff's alpha can handle missing values, which in this case was necessary since many of the texts were annotated by only a few annotators."], "explanation": " unanswerable</s>"}
{"question_id": "e8fa4303b36a47a5c87f862458442941bbdff7d9", "question": "What machine learning techniques are used in the model architecture?", "predicted_answer": " recurrent networks bibref23, bibref24, con", "predicted_evidence": ["Classical IMT approaches relay on the statistical formalization of the MT problem. Given a source sentence $\\mathbf {x}$, SMT aims at finding its most likely translation $\\hat{\\mathbf {y}}$ BIBREF18:", "For years, the prevailing approach to compute this expression have been phrase-based models BIBREF19. These models rely on a log-linear combination of different models BIBREF20: namely, phrase-based alignment models, reordering models and language models; among others BIBREF21, BIBREF22. However, more recently, this approach has shifted into neural models (see se:NMT).", "Prefix-based IMT proposed a user\u2013computer collaboration that starts with the system proposing an initial translation $\\mathbf {y}$ of length $I$. Then, the user corrects the leftmost wrong word $y_i$, inherently validating all preceding words. These words form a validated prefix $\\tilde{\\mathbf {y}}_p$, that includes the corrected word $\\tilde{y}_i$. The system reacts to this user feedback, generating a suffix $\\hat{\\mathbf {y}}_s$ that completes $\\tilde{\\mathbf {y}}_p$ to obtain a new translation of $\\mathbf {x}:\\hat{\\mathbf {y}}~=~\\tilde{\\mathbf {y}}_p\\,\\hat{\\mathbf {y}}_s$. This process is repeated until the user accepts the complete system suggestion. fi:IMT illustrates this protocol.", "Interactive Machine Translation ::: Neural Machine Translation", "In NMT, eq:SMT is modeled by a neural network with parameters $\\mathbf {\\Theta }$:", "This neural network usually follows an encoder-decoder architecture, featuring recurrent networks BIBREF23, BIBREF24, convolutional networks BIBREF25 or attention mechanisms BIBREF26. Model parameters are jointly estimated on large parallel corpora, using stochastic gradient descent BIBREF27, BIBREF28. At decoding time, the system obtains the most likely translation using a beam search method.", "Interactive Machine Translation ::: Prefix-based Interactive Neural Machine Translation", "The prefix-based IMT protocol (see se:PBIMT) can be naturally included into NMT systems since sentences are generated from left to right. In order to take into account the user's feedback and generate compatible hypothesis, the search space must be constraint. Given a prefix $\\tilde{\\mathbf {y}}_p$, only a single path accounts for it. The branching of the search process starts once this path has been covered. Introducing the validated prefix $\\tilde{\\mathbf {y}}_p$, eq:NMT becomes:", "which implies a search over the space of translations, but constrained by the validated prefix $\\tilde{\\mathbf {y}}_p$ BIBREF15."], "explanation": " recurrent networks bibref23, bibref24, con"}
{"question_id": "95083d486769b9b5e8c57fe2ef1b452fc3ea5012", "question": "what state of the art models do they compare to?", "predicted_answer": " ptb <unk>23 (d=discriminative, g=generative,", "predicted_evidence": ["FLOAT SELECTED: Table 2: Parsing results on PTB \u00a723 (D=discriminative, G=generative, S=semisupervised). ? indicates the (Vinyals et al., 2015) result with trained only on the WSJ corpus without ensembling.", "FLOAT SELECTED: Table 3: Parsing results on CTB 5.1.", "FLOAT SELECTED: Table 4: Language model perplexity results."], "explanation": " ptb <unk>23 (d=discriminative, g=generative,"}
{"question_id": "ced63053eb631c78a4ddd8c85ec0f3323a631a54", "question": "what emotion cause dataset was used?", "predicted_answer": " a simplified chinese emotion cause corpus bibref31</s>", "predicted_evidence": ["We conduct experiments on a simplified Chinese emotion cause corpus BIBREF31 , the only publicly available dataset on this task to the best of our knowledge. The corpus contains 2,105 documents from SINA city news. Each document has only one emotion word and one or more emotion causes. The documents are segmented into clauses manually. The main task is to identify which clause contains the emotion cause."], "explanation": " a simplified chinese emotion cause corpus bibref31</s>"}
{"question_id": "c53b036eff430a9d0449fb50b8d2dc9d2679d9fe", "question": "How much improvement do the introduced model achieve compared to the previous models?", "predicted_answer": " grurntn reduced the bpc from 1.39 to 1.33 (0.06", "predicted_evidence": ["In this section, we report our experiment results on PTB character-level language modeling using our baseline models GRURNN and LSTMRNN as well as our proposed models GRURNTN and LSTMRNTN. Fig. 8 shows performance comparisons from every model based on the validation set's BPC per epoch. In this experiment, GRURNN made faster progress than LSTMRNN, but eventually LSTMRNN converged into a better BPC based on the development set. Our proposed model GRURNTN made faster and quicker progress than LSTMRNTN and converged into a similar BPC in the last epoch. Both proposed models produced lower BPC than our baseline models from the first epoch to the last epoch.", "Table 1 shows PTB test set BPC among our baseline models, our proposed models and several published results. Our proposed model GRURNTN and LSTMRNTN outperformed both baseline models. GRURNTN reduced the BPC from 1.39 to 1.33 (0.06 absolute / 4.32% relative BPC) from the baseline GRURNN, and LSTMRNTN reduced the BPC from 1.37 to 1.34 (0.03 absolute / 2.22% relative BPC) from the baseline LSTMRNN. Overall, GRURNTN slightly outperformed LSTMRNTN, and both proposed models outperformed all of the baseline models on the character-level language modeling task.", "In this section, we report our experiment results on PTB word-level language modeling using our baseline models GRURNN and LSTMRNN and our proposed models GRURNTN and LSTMRNTN. Fig. 9 compares the performance from every models based on the validation set's PPL per epoch. In this experiment, GRURNN made faster progress than LSTMRNN. Our proposed GRURNTN's progress was also better than LSTMRNTN. The best model in this task was GRURNTN, which had a consistently lower PPL than the other models.", "Table 1 shows the PTB test set PPL among our baseline models, proposed models, and several published results. Both our proposed models outperformed their baseline models. GRURNTN reduced the perplexity from 97.78 to 87.38 (10.4 absolute / 10.63% relative PPL) over the baseline GRURNN and LSTMRNTN reduced the perplexity from 108.26 to 96.97 (11.29 absolute / 10.42% relative PPL) over the baseline LSTMRNN. Overall, LSTMRNTN improved the LSTMRNN model and its performance closely resembles the baseline GRURNN. However, GRURNTN outperformed all the baseline models as well as the other models by a large margin.", "We presented a new RNN architecture by combining the gating mechanism and tensor product concepts. Our proposed architecture can learn long-term dependencies from temporal and sequential data using gating units as well as more powerful interaction between the current input and previous hidden layers by introducing tensor product operations. From our experiment on the PennTreeBank corpus, our proposed models outperformed the baseline models with a similar number of parameters in character-level language modeling and word-level language modeling tasks. In a character-level language modeling task, GRURNTN obtained 0.06 absolute (4.32% relative) BPC reduction over GRURNN, and LSTMRNTN obtained 0.03 absolute (2.22% relative) BPC reduction over LSTMRNN. In a word-level language modeling task, GRURNTN obtained 10.4 absolute (10.63% relative) PPL reduction over GRURNN, and LSTMRNTN obtained 11.29 absolute (10.42% relative PPL) reduction over LSTMRNN. In the future, we will investigate the possibility of combining our model with other stacked RNNs architecture, such as Gated Feedback RNN (GFRNN). We would also like to explore other possible tensor operations and integrate them with our RNN architecture. By applying these ideas together, we expect to gain further performance improvement. Last, for further investigation we will apply our proposed models to other temporal and sequential tasks, such as speech recognition and video recognition."], "explanation": " grurntn reduced the bpc from 1.39 to 1.33 (0.06"}
{"question_id": "8cbe3fa4ec0f66071e3d6b829b09b6395b631c44", "question": "Are their methods fully supervised?", "predicted_answer": " no</s>", "predicted_evidence": ["Rumour detection on social media is challenging due to the short texts, creative lexical variations and high volume of the streams. The task becomes even harder if we attempt to perform rumour detection on-the-fly, without looking into the future. We provide an effective and highly scalable approach to detect rumours instantly after they were posted with zero delay. We introduce a new features category called novelty based features. Novelty based features compensate the absence of repeated information by consulting additional data sources - news wire articles. We hypothesize that information not confirmed by official news is an indication of rumours. Additionally we introduce pseudo feedback for classification. In a nutshell, documents that are similar to previously detected rumours are considered to be more likely to also be a rumour. The proposed features can be computed in constant time and space allowing us to process high-volume streams in real-time (Muthukrishnan, 2005). Our experiments reveal that novelty based features and pseudo feedback significantly increases detection performance for early rumour detection."], "explanation": " no</s>"}
{"question_id": "4e59808a7f73ac499b9838d3c0ce814196a02473", "question": "How do they measure conditional information strength?", "predicted_answer": " by masking the previous tokens, and comparing autoregressive nmt", "predicted_evidence": ["We further analyze why the decoder is more sensitive by masking the previous tokens, and comparing autoregressive NMT with the non-autoregressive counterpart. We find that the preceding tokens in the decoder provide strong conditional information, which partially explain the previous two observations on the decoder."], "explanation": " by masking the previous tokens, and comparing autoregressive nmt"}
{"question_id": "272defe245d1c5c091d3bc51399181da2da5e5f0", "question": "Which knowledge base do they use to retrieve facts?", "predicted_answer": " babi movie dialog dataset</s>", "predicted_evidence": ["The model performance is evaluated on the QA and Recs tasks of the bAbI Movie Dialog dataset using HITS@k evaluation metric, which is equal to the number of correct answers in the top- INLINEFORM0 results. In particular, the performance for the QA task is evaluated according to HITS@1, while the performance for the Recs task is evaluated according to HITS@100.", "Differently from BIBREF2 , the relevant knowledge base facts, taken from the knowledge base in triple form distributed with the dataset, are retrieved by INLINEFORM0 implemented by exploiting the Elasticsearch engine and not according to an hash lookup operator which applies a strict filtering procedure based on word frequency. In our work, INLINEFORM1 returns at most the top 30 relevant facts for INLINEFORM2 . Each entity in questions and documents is recognized using the list of entities provided with the dataset and considered as a single word of the dictionary INLINEFORM3 ."], "explanation": " babi movie dialog dataset</s>"}
{"question_id": "b3b9d7c8722e8ec41cbbae40e68458485a5ba25c", "question": "How is the speech alignment quality evaluated?", "predicted_answer": " according to the following 3-point scale: wrong alignment partial alignment, some", "predicted_evidence": ["The evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:", "Wrong alignment", "Partial alignment, some words or sentences may be missing", "Correct alignment, allowing non-spoken syllables at start or end."], "explanation": " according to the following 3-point scale: wrong alignment partial alignment, some"}
{"question_id": "654306d26ca1d9e77f4cdbeb92b3802aa9961da1", "question": "By how much did the new model outperform multilingual BERT?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["FLOAT SELECTED: Table 6: Results for POS tagging (standard deviation in parentheses)", "FLOAT SELECTED: Table 8: NER results for in-domain test set (standard deviation in parentheses)", "FLOAT SELECTED: Table 9: NER results for out of domain test set (standard deviation in parentheses)", "FLOAT SELECTED: Table 10: Labeled attachment score (LAS) parsing results for for predicted (p.seg) and gold (g.seg) segmentation. *Best performing combination in the TDT treebank (ELMo + transition-based parser)."], "explanation": " unanswerable</s>"}
{"question_id": "cb029240d4dedde74fcafad6a46c1cfc2621b934", "question": "In what was does an LSTM mimic the prefrontal cortex?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["In this paper, we first introduced a PFC layer to involve representations from both language and vision subsystems to form a human-like thinking system (the LGI system). The LGI contains three subsystems: the vision, language, and PFC subsystem, which are trained separately. The development, recognition and learning mechanism is discussed in the cocurrent paper [10]. In the language subsystem, we use an LSTM layer to mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output, instead of traditional softmax classifier. We propose to train the LGI with the NFP loss function, which endows the capacity to describe the image content in form of symbol text and manipulated images according to language commands. LGI shows its ability to learn eight different syntaxes or tasks in a cumulative learning way, and form the first machine thinking loop with the interaction between imagined pictures and language text."], "explanation": " unanswerable</s>"}
{"question_id": "8d14dd9c67d71494b4468000ff9683afdd11af7e", "question": "Do they report the annotation agreement?", "predicted_answer": " yes</s>", "predicted_evidence": ["Our study focuses on the Twitter ecosystem and a small part of its network. The initial sampling of tweets was based on a machine learning classifier of aggressive English language. This classifier has an F1 score of 0.90 BIBREF35. Even with this filter, only 0.7% of tweets were deemed by a majority of MTurk workers as cyberbullying (Table TABREF17). This extreme class imbalance can disadvantage a wide range of machine learning models. Moreover, the MTurk workers exhibited only moderate inter-annotator agreement (Table TABREF17). We also acknowledge that notions of harmful intent and power imbalance can be subjective, since they may depend on the particular conventions or social structure of a given community. For these reasons, we recognize that cyberbullying still has not been unambiguously defined. Moreover, their underlying constructs are difficult to identify. In this study, we did not train workers to recognize subtle cues for interpersonal popularity, nor the role of anonymity in creating a power imbalance."], "explanation": " yes</s>"}
{"question_id": "b8bbdc3987bb456739544426c6037c78ede01b77", "question": "Is the proposed system compared to existing systems?", "predicted_answer": " yes</s>", "predicted_evidence": ["We also evaluated the coreference resolution model on the test-set of CALOR (Table TABREF11), obtaining an average precision, recall and F-1 of 65.59%, 48.86% and 55.77% respectively. The same model reached a average F-1 of 68.8% for English BIBREF6. Comparable measurements are not available for French. F-1 scores for French are believed to be lower because of the lower amount of annotated data."], "explanation": " yes</s>"}
{"question_id": "7efbd9adbc403de4be6b1fb1999dd5bed9d6262c", "question": "what are the proposed semantic features?", "predicted_answer": " opinion words, vulgar words, emoticons, speech", "predicted_evidence": ["We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic. Some of these features were motivated by various works on speech act classification, while others are novel features. Overall we selected 3313 binary features, composed of 1647 semantic and 1666 syntactic features.", "Opinion Words: We used the \"Harvard General Inquirer\" lexicon BIBREF8 , which is a dataset used commonly in sentiment classification tasks, to identify 2442 strong, negative and positive opinion words (such as robust, terrible, untrustworthy, etc). The intuition here is that these opinion words tend to signal certain speech acts such as expressions and recommendations. One binary feature indicates whether any of these words appear in a tweet.", "Vulgar Words: Similar to opinion words, vulgar words can either signal great emotions or an informality mostly seen in expressions than any other kind of speech act (least seen in assertions). We used an online collection of vulgar words to collect a total of 349 vulgar words. A binary feature indicates the appearance or lack thereof of any of these words.", "Emoticons: Emoticons have become ubiquitous in online communication and so cannot be ignored. Like vulgar words, emoticons can also signal emotions or informality. We used an online collection of text-based emoticons to collect a total of 362 emoticons. A binary feature indicates the appearance or lack thereof of any of these emoticons.", "Speech Act Verbs: There are certain verbs (such as ask, demand, promise, report, etc) that typically signal certain speech acts. Wierzbicka BIBREF9 has compiled a total of 229 English speech act verbs divided into 37 groups. Since this is a collection of verbs, it is crucially important to only consider the verbs in a tweet and not any other word class (since some of these words can appear in multiple part-of-speech categories). In order to do this, we used Owoputi et al.'s BIBREF10 Twitter part-of-speech tagger to identify all the verbs in a tweet, which were then stemmed using Porter Stemming BIBREF11 . The stemmed verbs were then compared to the 229 speech act verbs (which were also stemmed using Porter Stemming). Thus, we have 229 binary features coding the appearance or lack thereof of each of these verbs.", "N-grams: In addition to the verbs mentioned, there are certain phrases and non-verb words that can signal certain speech acts. For example, the phrase \"I think\" signals an expression, the phrase \"could you please\" signals a request and the phrase \"is it true\" signals a question. Similarly, the non-verb word \"should\" can signal a recommendation and \"why\" can signal a question."], "explanation": " opinion words, vulgar words, emoticons, speech"}
{"question_id": "2d5d0b0c54105717bf48559b914fefd0c94964a6", "question": "What tokenization methods are used?", "predicted_answer": " word-level and the subword-level tokenization of bibref3", "predicted_evidence": ["This work is a first attempt towards using contemporary neural machine translation (NMT) techniques to perform machine translation for Nigerian Pidgin, establishing solid baselines that will ease and spur future work. We evaluate the performance of supervised and unsupervised neural machine translation models using word-level and the subword-level tokenization of BIBREF3."], "explanation": " word-level and the subword-level tokenization of bibref3"}
{"question_id": "2974237446d04da33b78ce6d22a477cdf80877b7", "question": "How do they define rumors?", "predicted_answer": " the presence of information unconfirmed by the official media</s>", "predicted_evidence": ["To increase instantaneous detection performance, we compensate for the absence of future information by consulting additional data sources. In particular, we make use of news wire articles, which are considered to be of high credibility. This is reasonable as according to Petrovic et. al (2013), in the majority of cases, news wires lead social media for reporting news. When a message arrives from a social media stream, we build features based on its novelty with respect to the confirmed information in the trusted sources. In a nutshell, the presence of information unconfirmed by the official media is construed as an indication of being a rumour. Note that this closely resembles the definition of what a rumour is."], "explanation": " the presence of information unconfirmed by the official media</s>"}
{"question_id": "256dfa501a71d7784520a527f43aec0549b1afea", "question": "What is the performance of the best model in the sign language recognition task?", "predicted_answer": " spatial ai-lstm</s>", "predicted_evidence": ["FLOAT SELECTED: TABLE I AVERAGE CROSS SUBJECT (CS) ACCURACY ACROSS ALL TEST SUBJECTS FOR DIFFERENT PROPOSED ARCHITECTURES AND BASELINES. STANDARD DEVIATION ACROSS TEST SUBJECTS\u2019 ACCURACY IS ALSO SHOWN.", "Table TABREF28 shows the comparative results among our proposed architectures and baselines. Overall, we use data from 12 subjects for our experiments which sum up to 13107 sign gesture samples in total. To evaluate model performance on a specific subject (test subject), we adopt cross subject evaluation criteria. Suppose, X is the test subject. We train our networks with all sign samples except those are from subject X. We use subject X's data as test split to evaluate the performance of the networks. Table TABREF28 shows the average test accuracy for all 12 subjects. We can see that 3D CNN network alone performs worse than simpler baselines. But when coupled with AI-LSTM as Max CNN-LSTM, it shows an increase in recognition accuracy by 2% from AI-LSTM alone. This is because some of the signs are confused by the AI-LSTM network because of similar skeletal motion pattern. Incorporating spatial relationship among joints leads to a significant accuracy gain. The Spatial AI-LSTM is trained only on skeletal data but outperforms the combined network by 6%."], "explanation": " spatial ai-lstm</s>"}
{"question_id": "f61268905626c0b2a715282478a5e373adda516c", "question": "Which method best performs on the offensive language identification task?", "predicted_answer": " lstm and gru with attention</s>", "predicted_evidence": ["The performance of the deep learning models is presented in table TABREF18. As we can see LSTM and GRU with Attention outperformed all the other models in-terms of macro-f1. Notably it outperformed all other classifical models and deep learning models in precision, recall and f1 for Offensive class as well as the Not Offensive class. However, fine tuning BERT-Base Multilingual Cased model did not achieve good results. For this task monolingual Greek word embeddings perform significantly better than the multilingual bert embeddings. LSTM and GRU with Attention can be considered as the best model trained for OGTD."], "explanation": " lstm and gru with attention</s>"}
{"question_id": "301a453abaa3bc15976817fefce7a41f3b779907", "question": "what were the evaluation metrics?", "predicted_answer": " micro-averaged f-score of the class 1 (intake) and", "predicted_evidence": ["The official evaluation metric for this task was micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1"], "explanation": " micro-averaged f-score of the class 1 (intake) and"}
{"question_id": "079ca5810060e1cdc12b5935d8c248492f0478b9", "question": "Can the model be extended to other languages?", "predicted_answer": " yes</s>", "predicted_evidence": [], "explanation": " yes</s>"}
{"question_id": "fe6181ab0aecf5bc8c3def843f82e530347d918b", "question": "What are the baseline models?", "predicted_answer": " baseline, baseline$+(t)$</s>", "predicted_evidence": ["We first train an MLE model as our baseline, trained on the Conceptual Captions training split alone. We referred to this model as Baseline. For a baseline approach that utilizes (some of) the Caption-Quality data, we merge positively-rated captions from the Caption-Quality training split with the Conceptual Captions examples and finetune the baseline model. We call this model Baseline$+(t)$, where $t \\in [0,1]$ is the rating threshold for the included positive captions. We train models for two variants, $t\\in \\lbrace 0.5, 0.7\\rbrace $, which results in $\\sim $72K and $\\sim $51K additional (pseudo-)ground-truth captions, respectively. Note that the Baseline$+(t)$ approaches attempt to make use of the same additional dataset as our two reinforced models, OnPG and OffPG, but they need to exclude below-threshold captions due to the constraints in MLE."], "explanation": " baseline, baseline$+(t)$</s>"}
{"question_id": "516b691ef192f136bb037c12c3c9365ef5a6604c", "question": "How does the introduced model combine the both factors?", "predicted_answer": " by using tensor product, we increase our model expressiveness by using second", "predicted_evidence": ["Previously in Sections \"Experiment Settings\" and \"Recursive Neural Tensor Network\" , we discussed that the gating mechanism concept can helps RNNs learn long-term dependencies from sequential input data and that adding more powerful interaction between the input and hidden layers simultaneously with the tensor product operation in a bilinear form improves neural network performance and expressiveness. By using tensor product, we increase our model expressiveness by using second-degree polynomial interactions, compared to first-degree polynomial interactions on standard dot product followed by addition in common RNNs architecture. Therefore, in this paper we proposed a Gated Recurrent Neural Tensor Network (GRURNTN) to combine these two advantages into an RNN architecture. In this architecture, the tensor product operation is applied between the current input and previous hidden layer multiplied by the reset gates for calculating the current candidate hidden layer values. The calculation is parameterized by tensor weight. To construct a GRURNTN, we defined the formulation as:", "As with GRURNTN, we also applied the tensor product operation for the LSTM unit to improve its performance. In this architecture, the tensor product operation is applied between the current input and the previous hidden layers to calculate the current memory cell. The calculation is parameterized by the tensor weight. We call this architecture a Long Short Term Memory Recurrent Neural Tensor Network (LSTMRNTN). To construct an LSTMRNTN, we defined its formulation:"], "explanation": " by using tensor product, we increase our model expressiveness by using second"}
{"question_id": "00c443f8d32d6baf7c7cea8f4ca9fa749532ccfd", "question": "What language is this dataset in?", "predicted_answer": " english</s>", "predicted_evidence": ["The Phoenix dataset is an attempt to take both the new advances in event data described above, along with decades of knowledge regarding best practices, in order to create a new iteration of event data. The dataset makes use of 450 English-language news sites, which are each scraped every hour for new content. New data is generated on a daily basis, coded according to the CAMEO event ontology, with an average of 2,200 events generated per day. The full dataset examined here contains 254,060 total events spread across 102 days of generated data. Based on publicly available information, the project also makes use of the most up-to-date actor dictionaries of any available machine-coded event dataset."], "explanation": " english</s>"}
{"question_id": "87b65b538d79e1218fa19aaac71e32e9b49208df", "question": "What are all the domains the corpus came from?", "predicted_answer": " unanswerable</s>", "predicted_evidence": [], "explanation": " unanswerable</s>"}
{"question_id": "97055ab0227ed6ac7a8eba558b94f01867bb9562", "question": "Were human evaluations conducted?", "predicted_answer": " yes</s>", "predicted_evidence": ["Human evaluation, albeit time- and labor-consuming, conforms to the ultimate goal of open-domain conversation systems. We asked three educated volunteers to annotate the results using a common protocol known as pointwise annotation nbciteacl,ijcai,seq2BF. In other words, annotators were asked to label either \u201c0\u201d (bad), \u201c1\u201d (borderline), or \u201c2\u201d (good) to a query-reply pair. The subjective evaluation was performed in a strict random and blind fashion to rule out human bias."], "explanation": " yes</s>"}
{"question_id": "5370a0062aae7fa4e700ae47aa143be5c5fc6b22", "question": "With how many languages do they experiment in the multilingual setup?", "predicted_answer": " between one and ten languages from the globalphone collection and evaluate on six", "predicted_evidence": ["Next, we explore how multilingual annotated data can be used to improve feature extraction for a zero-resource target language. We train multilingual bnfs on between one and ten languages from the GlobalPhone collection and evaluate on six other languages (simulating different zero-resource targets). We show that training on more languages consistently improves performance on word discrimination, and that the improvement is not simply due to more training data: an equivalent amount of data from one language fails to give the same benefit. In fact, we observe the largest gain in performance when adding the second training language, which is already better than adding three times as much data from the same language. Moreover, when compared to our best results from training unsupervised on target language data only, we find that bnfs trained on just a single other language already outperform the target-language-only training, with multilingual bnfs doing better by a wide margin."], "explanation": " between one and ten languages from the globalphone collection and evaluate on six"}
{"question_id": "35d2eae3a7c9bed54196334a09344591f9cbb5c8", "question": "what combination of features helped improve the classification?", "predicted_answer": " a combination of all features for the document level, but the latter made more accurate", "predicted_evidence": ["Most notably, we have found that taking into consideration multiple linguistic dimensions when assessing linguistic complexity is especially useful for sentence-level analysis. In our experiments, using only word-frequency features was almost as predictive as a combination of all features for the document level, but the latter made more accurate predictions for sentences, resulting in a 7% difference in accuracy. Besides L2 course book materials, we tested both our document- and sentence-level models also on unseen data with promising results.", "FLOAT SELECTED: Table 2. The complete feature set."], "explanation": " a combination of all features for the document level, but the latter made more accurate"}
{"question_id": "f5913e37039b9517a323ec700b712e898316161b", "question": "what dataset was used in their experiment?", "predicted_answer": " conll 2003 english ner</s>", "predicted_evidence": ["Here we compare the efficiency of our system with four widely used annotation tools. We extract 100 sentences from CoNLL 2003 English NER BIBREF8 training data, with each sentence containing at least 4 entities. Two undergraduate students without any experience on those tools are invited to annotate those sentences. Their average annotation time is shown in Figure FIGREF25 , where \u201cYedda+R\u201d suggests annotation using Yedda with the help of system recommendation. The inter-annotator agreements for those tools are closed, which around 96.1% F1-score. As we can see from the figure, our Yedda system can greatly reduce the annotation time. With the help of system recommendation, the annotation time can be further reduced. We notice that \u201cYedda+R\u201d has larger advantage with the increasing numbers of annotated sentences, this is because the system recommendation gives better suggestions when it learns larger annotated sentences. The \u201cYedda+R\u201d gives 16.47% time reduction in annotating 100 sentences."], "explanation": " conll 2003 english ner</s>"}
{"question_id": "4738158f92b5b520ceba6207e8029ae082786dbe", "question": "What are previoius similar models authors are referring to?", "predicted_answer": " hierarchical and pointer-generator models</s>", "predicted_evidence": ["FLOAT SELECTED: Table 1: ROUGE F1 scores on the test set. Our hierarchical (Hier-NSE) model outperform previous hierarchical and pointer-generator models. Hier-NSE-factor is the factored model and Hier-NSE-SC is the self-critic model."], "explanation": " hierarchical and pointer-generator models</s>"}
{"question_id": "726c5c1b6951287f4bae22978f9a91d22d9bef61", "question": "what are the baselines?", "predicted_answer": " reschke crf, reschke noisy-or, resch", "predicted_evidence": ["We evaluate on four categories of architecture:", "reschke2014 proposed several methods for event extraction in this scenario. We compare against three notable examples drawn from this work:", "Reschke CRF: a conditional random field model.", "Reschke Noisy-OR: a sequence tagger with a \"Noisy-OR\" form of aggregation that discourages the model from predicting the same value for multiple slots.", "Reschke Best: a sequence tagger using a cost-sensitive classifier, optimized with SEARN BIBREF17 , a learning-to-search framework."], "explanation": " reschke crf, reschke noisy-or, resch"}
{"question_id": "b8b79a6123716cb9fabf751b31dff424235a2ee2", "question": "How were missing hypotheses discovered?", "predicted_answer": " the experts were instructed to re-consider each problem and be especially wary of", "predicted_evidence": ["We have randomly selected 150 problems out of the RTE corpus which were marked as \u201cYES\u201d (i.e. entailment holds). The problems were not further selected nor doctored by us. The problems were then re-rated by experts in logic and/or linguistics. For each problem, three experts were consulted, and each expert rated 30 problems. More precisely, the experts were instructed to re-consider each problem and be especially wary of missing hypotheses. If they considered the entailment to hold, we still gave the instruction to optionally mention any additional implicit hypothesis that they would be using. Similarly, if they considered that there was no entailment in the problem, they were given prompted to (optionally) give an argument for their judgement."], "explanation": " the experts were instructed to re-consider each problem and be especially wary of"}
{"question_id": "6edef748370e63357a57610b5784204c9715c0b4", "question": "How is validity identified and what metric is used to quantify it?", "predicted_answer": " the metric we use to evaluate the efficacy of our model is an app", "predicted_evidence": ["Our training sample has vastly over-sampled alerts compared with a typical responses in order to make it easier to train an engine. This also means that a typical test train split would not necessarily be useful in determining the efficacy of our models. The metric we use to evaluate the efficacy of our model is an approximation of the probability that a held-out alert is flagged if a fixed percentage of a typical population were to be flagged as potential alerts."], "explanation": " the metric we use to evaluate the efficacy of our model is an app"}
{"question_id": "1d9d7c96c5e826ac06741eb40e89fca6b4b022bd", "question": "What additional evidence they use?", "predicted_answer": " using textual evidence not only mitigates representational issues in relation extraction, but", "predicted_evidence": ["Knowledge bases like Freebase capture real world facts, and Web resources like Wikipedia provide a large repository of sentences that validate or support these facts. For example, a sentence in Wikipedia says, Denali (also known as Mount McKinley, its former official name) is the highest mountain peak in North America, with a summit elevation of 20,310 feet (6,190 m) above sea level. To answer our example question against a KB using a relation extractor, we can use this sentence as external evidence, filter out wrong answers and pick the correct one.", "Using textual evidence not only mitigates representational issues in relation extraction, but also alleviates the data scarcity problem to some extent. Consider the question, who was queen isabella's mother. Answering this question involves predicting two constraints hidden in the word mother. One constraint is that the answer should be the parent of Isabella, and the other is that the answer's gender is female. Such words with multiple latent constraints have been a pain-in-the-neck for both semantic parsing and relation extraction, and requires larger training data (this phenomenon is coined as sub-lexical compositionality by wang2015). Most systems are good at triggering the parent constraint, but fail on the other, i.e., the answer entity should be female. Whereas the textual evidence from Wikipedia, ...her mother was Isabella of Barcelos ..., can act as a further constraint to answer the question correctly."], "explanation": " using textual evidence not only mitigates representational issues in relation extraction, but"}
{"question_id": "c17ece1dad42d92c78fca2e3d8afa9a20ff19598", "question": "How are the individuals with bipolar disorder identified?", "predicted_answer": " recurring episodes of depressed and elated (hypomanic or", "predicted_evidence": ["Concurrently, it has been argued for mental health research that it would constitute a `valuable critical step' BIBREF10 to analyse first-hand accounts by individuals with lived experience of severe mental health issues in blog posts, tweets, and discussion forums. Several severe mental health difficulties, e.g., bipolar disorder (BD) and schizophrenia are considered as chronic and clinical recovery, defined as being relapse and symptom free for a sustained period of time BIBREF11 , is considered difficult to achieve BIBREF12 , BIBREF13 , BIBREF14 . Moreover, clinically recovered individuals often do not regain full social and educational/vocational functioning BIBREF15 , BIBREF16 . Therefore, research originating from initiatives by people with lived experience of mental health issues has been advocating emphasis on the individual's goals in recovery BIBREF17 , BIBREF18 . This movement gave rise to the concept of personal recovery BIBREF19 , BIBREF20 , loosely defined as a `way of living a satisfying, hopeful, and contributing life even with limitations caused by illness' BIBREF18 . The aspects of personal recovery have been conceptualised in various ways BIBREF21 , BIBREF22 , BIBREF23 . According to the frequently used CHIME model BIBREF24 , its main components are Connectedness, Hope and optimism, Identity, Meaning and purpose, and Empowerment. Here, we focus on BD, which is characterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12 . Bipolar spectrum disorders were estimated to affect approximately 2% of the UK population BIBREF13 with rates ranging from 0.1%-4.4% across 11 other European, American and Asian countries BIBREF26 . Moreover, BD is associated with a high risk of suicide BIBREF27 , making its prevention and treatment important tasks for society. BD-specific personal recovery research is motivated by mainly two facts: First, the pole of positive/elevated mood and ongoing mood instability constitute core features of BD and pose special challenges compared to other mental health issues, such as unipolar depression BIBREF25 . Second, unlike for some other severe mental health difficulties, return to normal functioning is achievable given appropriate treatment BIBREF28 , BIBREF16 , BIBREF29 ."], "explanation": " recurring episodes of depressed and elated (hypomanic or"}
{"question_id": "77bc886478925c8e9fb369b1ba5d05c42b3cd79a", "question": "what is the architecture of their model?", "predicted_answer": " bidirectional encoding model bibref2, which has achieved state-", "predicted_evidence": ["We train a Bidirectional Encoding model BIBREF2 , which has achieved state-of-the-art results for Twitter stance detection on our dataset. The model encodes the entity using a bidirectional LSTM (BiLSTM), which is then used to initialize a BiLSTM that encodes the article and produces a prediction. To reduce the sequence length, we use the same context window that was presented to annotators for training the LSTM. We use pretrained GloVe embeddings BIBREF13 and tune hyperparameters on a validation set. The best model achieves a test accuracy of INLINEFORM0 and a macro-averaged test F1 score of INLINEFORM1 . It significantly outperforms baselines such as a bag-of-n-grams (accuracy: INLINEFORM2 ; F1: INLINEFORM3 )."], "explanation": " bidirectional encoding model bibref2, which has achieved state-"}
{"question_id": "74ebfba06f37cc95dfe59c3790ebe6165e6be19c", "question": "How are the positive instances annotated? e.g. by annotators, or by laughter from the audience?", "predicted_answer": " laughter sentences were selected by annotators.</s>", "predicted_evidence": ["We collected INLINEFORM0 TED Talk transcripts. An example transcription is given in Figure FIGREF4 . The collected transcripts were split into sentences using the Stanford CoreNLP tool BIBREF11 . In this study, sentences containing or immediately followed by `(Laughter)' were used as `Laughter' sentences, as shown in Figure FIGREF4 ; all other sentences were defined as `No-Laughter' sentences. Following BIBREF1 and BIBREF3 , we selected the same numbers ( INLINEFORM1 ) of `Laughter' and `No-Laughter' sentences. To minimize possible topic shifts between positive and negative instances, for each positive instance, we picked one negative instance nearby (the context window was 7 sentences in this study). For example, in Figure FIGREF4 , a negative instance (corresponding to `sent-2') was selected from the nearby sentences ranging from `sent-7' to `sent+7'."], "explanation": " laughter sentences were selected by annotators.</s>"}
{"question_id": "2b021e1486343d503bab26c2282f56cfdab67248", "question": "What is the best performing model?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["FLOAT SELECTED: Table 3: EI-reg task results.", "FLOAT SELECTED: Table 4: V-reg task results.", "FLOAT SELECTED: Table 5: EI-oc task results.", "FLOAT SELECTED: Table 6: V-oc task results."], "explanation": " unanswerable</s>"}
{"question_id": "9d6b2672b11d49c37a6bfb06172d39742d48aef4", "question": "What type of language models are used? e.g. trigrams, bigrams?", "predicted_answer": " ngram language models</s>", "predicted_evidence": ["Our system estimated tweet probabilities using Ngram language models. We created models from two different corpora - a collection of funny tweets from the @midnight program, and a corpus of news data that is freely available for research. We scored tweets by assigning them a probability based on each model. Tweets that have a higher probability according to the funny tweet model are considered funnier since they are more like the humorous training data. However, tweets that have a lower probability according to the news language model are viewed as funnier since they are least like the (unfunny) news corpus. We took a standard approach to language modeling and used bigrams and trigrams as features in our models. We used KenLM BIBREF8 with modified Kneser-Ney smoothing and a back-off technique as our language modeling tool."], "explanation": " ngram language models</s>"}
{"question_id": "20632fc4d2b693b5aabfbbc99ee5c1e9fc485dea", "question": "What are the parts of the \"multimodal\" resources?", "predicted_answer": " unanswerable</s>", "predicted_evidence": ["From the perspective of computational processing, diagrammatic representations present a formidable challenge, as they involve tasks from both computer vision and natural language processing. On the one hand, diagrams have a spatial organisation \u2013 layout \u2013 which needs to be segmented to identify meaningful units and their position. Making sense of how diagrams exploit the 2D layout space falls arguably within the domain of computer vision. On the other hand, diagrams also have a discourse structure, which uses the layout space to set up discourse relations between instances of natural language, various types of images, arrows and lines, thus forming a unified discourse organisation. The need to parse this discourse structure shifts the focus towards the field of natural language processing."], "explanation": " unanswerable</s>"}
{"question_id": "4ddc53afffaf1622d97695347dd1b3190d156dee", "question": "What model architectures are used?", "predicted_answer": " convolutional neural network (cnn) bibref7,", "predicted_evidence": ["We first measure gender biases in st and abt datasets. We explore three neural models used in previous works on abusive language classification: Convolutional Neural Network (CNN) BIBREF7 , Gated Recurrent Unit (GRU) BIBREF14 , and Bidirectional GRU with self-attention ( INLINEFORM0 -GRU) BIBREF8 , but with a simpler mechanism used in BIBREF15 . Hyperparameters are found using the validation set by finding the best performing ones in terms of original AUC scores. These are the used hyperparameters:"], "explanation": " convolutional neural network (cnn) bibref7,"}
{"question_id": "80d425258d027e3ca3750375d170debb9d92fbc6", "question": "Can their method be transferred to other Q&A platforms (in other languages)?", "predicted_answer": " yes. the method can be transferred to other q&a platforms (in other languages", "predicted_evidence": [], "explanation": " yes. the method can be transferred to other q&a platforms (in other languages"}
{"question_id": "11c4071d9d7efeede84f47892b1fa0c6a93667eb", "question": "What datasets do they look at?", "predicted_answer": " europarl v9 parallel data set for training german (de) and english", "predicted_evidence": ["We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages. We use 1.8M sentences of this corpus and build models in English to German and vice versa. To segment initial words (i.e. before any subword processing) we use the Moses word tokenizer and detokenizer. We evaluate with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track."], "explanation": " europarl v9 parallel data set for training german (de) and english"}
{"question_id": "9a7aeecbecf5e30ffa595c233fca31719c9b429f", "question": "What package was used to build the ngram language models?", "predicted_answer": " kenlm toolkit</s>", "predicted_evidence": ["Once we had the corpora ready, we used the KenLM Toolkit to train the N-gram language models on each corpus. We trained using both bigrams and trigrams on the tweet and news data. Our language models accounted for unknown words and were built both with and without considering sentence or tweet boundaries."], "explanation": " kenlm toolkit</s>"}
{"question_id": "8861138891669a45de3955c802c55a37be717977", "question": "what tools did they use?", "predicted_answer": " nearest neighbors (nn) estimator naive bayes (nb) estimat", "predicted_evidence": ["Nearest neighbors (NN) Estimator: We used the pre-trained GloVe embeddings BIBREF11 of concepts to implement a nearest-neighbor estimator as follows. Given a concept $c$, we extract all labeled concepts within a given radius $r$ (cosine similarity $0.3$). In one variant, $c$'s controversiality score is taken to be the fraction of controversial concepts among them. In another variant, labeled concepts are weighted by their cosine similarity to $c$.", "Naive Bayes (NB) Estimator: A Naive Bayes model was learned, with a bag-of-words feature set, using the word counts in the sentences of our training data \u2013 the contexts of the controversial and non-controversial concepts. The controversiality score of a concept $c$ for its occurrence in a sentence $s$, is taken as the posterior probability (according to the NB model) of $s$ to contain a controversial concept, given the words of $s$ excluding $c$, and taking a prior of $0.5$ for controversiality (as is the case in the datasets). The controversiality score of $c$ is then defined as the average score over all sentences referencing $c$.", "Recurrent neural network (RNN): A bidirectional RNN using the architecture suggested in BIBREF10 was similarly trained. The network receives as input a concept and a referring sentence, and outputs a score. The controversiality score of a concept is defined, as above, to be the average of these scores."], "explanation": " nearest neighbors (nn) estimator naive bayes (nb) estimat"}
{"question_id": "2803709fba74e6098aae145abcbf0e9a3f4c35e5", "question": "What is used as a baseline model?", "predicted_answer": " neural crf model with and without elmo embeddings</s>", "predicted_evidence": ["The experimental results for NER are summarized in Table TABREF20. The top part of the table shows recently published results. BIBREF14's work is using gazetteers with HSCRF and BIBREF4's work is using the Flair language model which is much larger than ELMo. BIBREF27 is the current state-of-the-art language model that uses cloze-driven pretraining. The bottom part of the table is shows our baseline models and results with included gazetteers. We experiment with the Neural CRF model with and without ELMo embeddings. Including ELMo embeddings the CoNLL-03 and Ontonotes 5, F$_1$ score improves from 92.34 to 92.86 and 89.11 to 89.32 respectively. Without ELMo embeddings the F$_1$ score improves from 90.42 to 91.12 and 86.63 to 87 respectively. We observe that GazSelfAttn relative improvements are similar with and without ELMo embeddings. We obtain slightly better CoNLL-03 F$_1$ score compared to BIBREF14 work that uses the HSCRF model, and we match the Ononotes 5 F$_1$ scores of BIBREF4 that uses a much bigger model. BIBREF14 Ononotes 5 results use subset of the dataset labels and are not comparable. Note that because of computation constrains, we did not perform extensive hyperparameter tuning except for the gazetteer dropout rate."], "explanation": " neural crf model with and without elmo embeddings</s>"}
